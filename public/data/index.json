{
  "meta": {
    "lastUpdated": "2026-01-01T03:44:54.045Z",
    "totalPapers": 507,
    "categories": [
      "agents",
      "code-generation",
      "evaluation",
      "multi-agent",
      "planning",
      "prompting",
      "rag",
      "reasoning",
      "robotics",
      "tool-use"
    ],
    "years": [
      2025,
      2024
    ]
  },
  "papers": [
    {
      "id": "2512.25070",
      "title": "Scaling Open-Ended Reasoning to Predict the Future",
      "authors": [
        "Nikhil Chandak",
        "Shashwat Goel",
        "Ameya Prabhu",
        "Moritz Hardt",
        "Jonas Geiping"
      ],
      "abstract": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.",
      "publishedDate": "2025-12-31T18:59:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.25070",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.25065",
      "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search",
      "authors": [
        "Rohit Dwivedula",
        "Divyanshu Saxena",
        "Sujay Yadalam",
        "Daehyeok Kim",
        "Aditya Akella"
      ],
      "abstract": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments. We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code. We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.",
      "publishedDate": "2025-12-31T18:58:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.25065",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.25055",
      "title": "Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings",
      "authors": [
        "Tianzhi He",
        "Farrokh Jazizadeh"
      ],
      "abstract": "This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype's performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.",
      "publishedDate": "2025-12-31T18:51:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.25055",
      "categories": [
        "evaluation",
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.25052",
      "title": "AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG",
      "authors": [
        "Chao Peng",
        "Bin Wang",
        "Zhilei Long",
        "Jinfang Sheng"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter to eliminate manual tuning and adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits epsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.",
      "publishedDate": "2025-12-31T18:48:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.25052",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.25015",
      "title": "MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes",
      "authors": [
        "Siddhant Agarwal",
        "Adya Dhuler",
        "Polly Ruhnke",
        "Melvin Speisman",
        "Md Shad Akhtar",
        "Shweta Yadav"
      ],
      "abstract": "Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a vital resource for detecting depressive symptoms in memes on social media through the Large Language Model (LLM) generated and human-annotated explanations. We introduce MAMAMemeia, a collaborative multi-agent multi-aspect discussion framework grounded in the clinical psychology method of Cognitive Analytic Therapy (CAT) Competencies. MAMAMemeia improves upon the current state-of-the-art by 7.55% in macro-F1 and is established as the new benchmark compared to over 30 methods.",
      "publishedDate": "2025-12-31T18:06:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.25015",
      "categories": [
        "multi-agent",
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.25014",
      "title": "Diffusion Language Models are Provably Optimal Parallel Samplers",
      "authors": [
        "Haozhe Jiang",
        "Nika Haghtalab",
        "Lijie Chen"
      ],
      "abstract": "Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. We prove that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. We further justify the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Our results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.",
      "publishedDate": "2025-12-31T18:03:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.25014",
      "categories": [
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24991",
      "title": "Efficiently Estimating Data Efficiency for Language Model Fine-tuning",
      "authors": [
        "Gyung Hyun Je",
        "Colin Raffel"
      ],
      "abstract": "While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task's data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task's data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.",
      "publishedDate": "2025-12-31T17:37:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24991",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24986",
      "title": "PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes",
      "authors": [
        "Luca Collorone",
        "Mert Kiray",
        "Indro Spinelli",
        "Fabio Galasso",
        "Benjamin Busam"
      ],
      "abstract": "Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a \"render and wait\" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.",
      "publishedDate": "2025-12-31T17:32:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24986",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24985",
      "title": "DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments",
      "authors": [
        "Yohan Park",
        "Hyunwoo Ha",
        "Wonjun Jo",
        "Tae-Hyun Oh"
      ],
      "abstract": "Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.",
      "publishedDate": "2025-12-31T17:31:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24985",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "code-generation",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.24969",
      "title": "Large language models and the entropy of English",
      "authors": [
        "Colin Scheibner",
        "Lindsay M. Smith",
        "William Bialek"
      ],
      "abstract": "We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.",
      "publishedDate": "2025-12-31T16:54:44Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24969",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24957",
      "title": "AMAP Agentic Planning Technical Report",
      "authors": [
        "Yulan Hu",
        "Xiangwen Zhang",
        "Sheng Ouyang",
        "Hao Yi",
        "Lu Xu",
        "Qinglin Lang",
        "Lide Tan",
        "Xiang Cheng",
        "Tianchen Ye",
        "Zhicong Li",
        "Ge Chen",
        "Wenjin Yang",
        "Zheng Pan",
        "Shaopan Xiong",
        "Siran Yang",
        "Ju Huang",
        "Yan Zhang",
        "Jiamang Wang",
        "Yong Liu",
        "Yinfeng Huang",
        "Tucheng Lin",
        "Xin Li",
        "Ning Guo"
      ],
      "abstract": "We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.",
      "publishedDate": "2025-12-31T16:39:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24957",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24947",
      "title": "CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement",
      "authors": [
        "Wentao Zhang",
        "Tao Fang",
        "Lina Lu",
        "Lifei Wang",
        "Weihe Zhong"
      ],
      "abstract": "Accurate and interpretable crop disease diagnosis is essential for agricultural decision-making, yet existing methods often rely on costly supervised fine-tuning and perform poorly under domain shifts. We propose Caption--Prompt--Judge (CPJ), a training-free few-shot framework that enhances Agri-Pest VQA through structured, interpretable image captions. CPJ employs large vision-language models to generate multi-angle captions, refined iteratively via an LLM-as-Judge module, which then inform a dual-answer VQA process for both recognition and management responses. Evaluated on CDDMBench, CPJ significantly improves performance: using GPT-5-mini captions, GPT-5-Nano achieves \\textbf{+22.7} pp in disease classification and \\textbf{+19.5} points in QA score over no-caption baselines. The framework provides transparent, evidence-based reasoning, advancing robust and explainable agricultural diagnosis without fine-tuning. Our code and data are publicly available at: https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis.",
      "publishedDate": "2025-12-31T16:21:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24947",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24940",
      "title": "Iterative Deployment Improves Planning Skills in LLMs",
      "authors": [
        "Augusto B. Corrêa",
        "Yoav Gelberg",
        "Luckeciano C. Melo",
        "Ilia Shumailov",
        "André G. Pereira",
        "Yarin Gal"
      ],
      "abstract": "We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.",
      "publishedDate": "2025-12-31T16:03:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24940",
      "categories": [
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24939",
      "title": "Vibe Coding, Interface Flattening",
      "authors": [
        "Hongrui Jin"
      ],
      "abstract": "Large language models are reshaping programming by enabling 'vibe coding': the development of softwares through natural-language interaction with model-driven toolchains. This article argues that vibe coding is best understood as interface flattening, a reconfiguration in which previously distinct modalities (GUI, CLI, and API) appear to converge into a single conversational surface, even as the underlying chain of translation from intention to machinic effect lengthens and thickens. Drawing on Friedrich Kittler's materialist media theory and Alexander Galloway's account of interfaces as sites of protocol control, the paper situates programming as a historically localised interface arrangement rather than an essential relation to computation. Through a materialist reconstruction of the contemporary vibe-coding stack, it shows how remote compute infrastructures, latency and connectivity, structured outputs, function/tool calling, and interoperability standards such as the Model Context Protocol relocate control and meaning-making power to model and protocol providers. The apparent democratisation of technical capability therefore depends on new dependencies and new literacies. By foregrounding the tension between experiential flattening and infrastructural thickening, I demonstrate how LLM-mediated development redistributes symbolic labour/power, obscures responsibility, and privatises competencies previously dispersed across programming communities, contributing a critical lens on the political economy of AI-mediated human-computer interaction.",
      "publishedDate": "2025-12-31T16:00:59Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24939",
      "categories": [
        "code-generation",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.24903",
      "title": "FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation",
      "authors": [
        "Zichen Tang",
        "Haihong E",
        "Rongjin Li",
        "Jiacheng Liu",
        "Linwei Jia",
        "Zhuodi Hao",
        "Zhongjun Yang",
        "Yuanze Li",
        "Haolin Tian",
        "Xinyi Hu",
        "Peizhi Zhao",
        "Yuan Liu",
        "Zhengyu Wang",
        "Xianghe Wang",
        "Yiling Huang",
        "Xueyuan Lin",
        "Ruofei Bai",
        "Zijian Xie",
        "Qian Huang",
        "Ruining Cao",
        "Haocheng Gao"
      ],
      "abstract": "We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.",
      "publishedDate": "2025-12-31T15:00:03Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24903",
      "categories": [
        "rag",
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24867",
      "title": "Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements",
      "authors": [
        "Yiming Liang",
        "Yizhi Li",
        "Yantao Du",
        "Ge Zhang",
        "Jiayi Zhou",
        "Yuchen Wu",
        "Yinzhu Piao",
        "Denghui Cao",
        "Tong Sun",
        "Ziniu Li",
        "Li Du",
        "Bo Lei",
        "Jiaheng Liu",
        "Chenghua Lin",
        "Zhaoxiang Zhang",
        "Wenhao Huang",
        "Jiajun Zhang"
      ],
      "abstract": "Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries. However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annotation. We propose Encyclo-K, a statement-based benchmark that rethinks benchmark construction from the ground up. Our key insight is that knowledge statements, not questions, can serve as the unit of curation, and questions can then be constructed from them. We extract standalone knowledge statements from authoritative textbooks and dynamically compose them into evaluation questions through random sampling at test time. This design directly addresses all three limitations: the combinatorial space is too vast to memorize, and model rankings remain stable across dynamically generated question sets, enabling reliable periodic dataset refresh; each question aggregates 8-10 statements for comprehensive multi-knowledge assessment; annotators only verify formatting compliance without requiring domain expertise, substantially reducing annotation costs. Experiments on over 50 LLMs demonstrate that Encyclo-K poses substantial challenges with strong discriminative power. Even the top-performing OpenAI-GPT-5.1 achieves only 62.07% accuracy, and model performance displays a clear gradient distribution--reasoning models span from 16.04% to 62.07%, while chat models range from 9.71% to 50.40%. These results validate the challenges introduced by dynamic evaluation and multi-statement comprehensive understanding. These findings establish Encyclo-K as a scalable framework for dynamic evaluation of LLMs' comprehensive understanding over multiple fine-grained disciplinary knowledge statements.",
      "publishedDate": "2025-12-31T13:55:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24867",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24851",
      "title": "VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents",
      "authors": [
        "Xunyi Zhao",
        "Gengze Zhou",
        "Qi Wu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their 3D spatial reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents.",
      "publishedDate": "2025-12-31T13:21:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24851",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation",
        "agents",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.24848",
      "title": "PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI",
      "authors": [
        "Srija Mukhopadhyay",
        "Sathwik Reddy",
        "Shruthi Muthukumar",
        "Jisun An",
        "Ponnurangam Kumaraguru"
      ],
      "abstract": "Personalized AI agents rely on access to a user's digital footprint, which often includes sensitive data from private emails, chats and purchase histories. Yet this access creates a fundamental societal and privacy risk: systems lacking social-context awareness can unintentionally expose user secrets, threatening digital well-being. We introduce PrivacyBench, a benchmark with socially grounded datasets containing embedded secrets and a multi-turn conversational evaluation to measure secret preservation. Testing Retrieval-Augmented Generation (RAG) assistants reveals that they leak secrets in up to 26.56% of interactions. A privacy-aware prompt lowers leakage to 5.12%, yet this measure offers only partial mitigation. The retrieval mechanism continues to access sensitive data indiscriminately, which shifts the entire burden of privacy preservation onto the generator. This creates a single point of failure, rendering current architectures unsafe for wide-scale deployment. Our findings underscore the urgent need for structural, privacy-by-design safeguards to ensure an ethical and inclusive web for everyone.",
      "publishedDate": "2025-12-31T13:16:45Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24848",
      "categories": [
        "rag",
        "evaluation",
        "agents",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.24818",
      "title": "Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback",
      "authors": [
        "Shulun Chen",
        "Runlong Zhou",
        "Zihan Zhang",
        "Maryam Fazel",
        "Simon S. Du"
      ],
      "abstract": "Aligning large language models (LLMs) with human preferences has proven effective for enhancing model capabilities, yet standard preference modeling using the Bradley-Terry model assumes transitivity, overlooking the inherent complexity of human population preferences. Nash learning from human feedback (NLHF) addresses this by framing non-transitive preferences as a two-player zero-sum game, where alignment reduces to finding the Nash equilibrium (NE). However, existing algorithms typically rely on regularization, incurring unavoidable bias when computing the duality gap in the original game. In this work, we provide the first convergence guarantee for Optimistic Multiplicative Weights Update ($\\mathtt{OMWU}$) in NLHF, showing that it achieves last-iterate linear convergence after a burn-in phase whenever an NE with full support exists, with an instance-dependent linear convergence rate to the original NE, measured by duality gaps. Compared to prior results in Wei et al. (2020), we do not require the assumption of NE uniqueness. Our analysis identifies a novel marginal convergence behavior, where the probability of rarely played actions grows exponentially from exponentially small values, enabling exponentially better dependence on instance-dependent constants than prior results. Experiments corroborate the theoretical strengths of $\\mathtt{OMWU}$ in both tabular and neural policy classes, demonstrating its potential for LLM applications.",
      "publishedDate": "2025-12-31T12:08:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24818",
      "categories": [],
      "year": 2025
    },
    {
      "id": "2512.24796",
      "title": "LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)",
      "authors": [
        "Rongge Xu",
        "Hui Dai",
        "Yiming Fu",
        "Jiedong Jiang",
        "Tianjiao Nie",
        "Hongwei Wang",
        "Junkai Wang",
        "Holiverse Yang",
        "Jiatong Yang",
        "Zhi-Hao Zhang"
      ],
      "abstract": "Large language models (LLMs) have made rapid progress in formal theorem proving, yet current benchmarks under-measure the kind of abstraction and library-mediated reasoning that organizes modern mathematics. In parallel with FATE's emphasis on frontier algebra, we introduce LeanCat, a Lean benchmark for category-theoretic formalization -- a unifying language for mathematical structure and a core layer of modern proof engineering -- serving as a stress test of structural, interface-level reasoning. Part I: 1-Categories contains 100 fully formalized statement-level tasks, curated into topic families and three difficulty tiers via an LLM-assisted + human grading process. The best model solves 8.25% of tasks at pass@1 (32.50%/4.17%/0.00% by Easy/Medium/High) and 12.00% at pass@4 (50.00%/4.76%/0.00%). We also evaluate LeanBridge which use LeanExplore to search Mathlib, and observe consistent gains over single-model baselines. LeanCat is intended as a compact, reusable checkpoint for tracking both AI and human progress toward reliable, research-level formalization in Lean.",
      "publishedDate": "2025-12-31T11:33:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24796",
      "categories": [
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24776",
      "title": "Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models",
      "authors": [
        "Ákos Prucs",
        "Márton Csutora",
        "Mátyás Antal",
        "Márk Marosi"
      ],
      "abstract": "Large Language Models (LLMs) are demonstrating rapid improvements on complex reasoning benchmarks, particularly when allowed to utilize intermediate reasoning steps before converging on a final solution. However, current literature often overlooks the significant computational burden associated with generating long reasoning sequences. For industrial applications, model selection depends not only on raw accuracy but also on resource constraints and inference costs. In this work, we conduct a test-time-compute aware evaluation of both contemporary and older open-source LLMs, mapping their Pareto frontiers across math- and reasoning-intensive benchmarks. Our findings identify the Mixture of Experts (MoE) architecture as a strong candidate to balance performance and efficiency in our evaluation setting. Furthermore, we trace the trajectory of Pareto efficiency over time to derive an emergent trend regarding accuracy gain per unit of compute. Finally, we demonstrate that there is a saturation point for inference-time compute. Beyond a certain threshold, accuracy gains diminish, indicating that while extended reasoning capabilities are beneficial, they cannot overcome intrinsic model limitations regarding specific complexities.",
      "publishedDate": "2025-12-31T10:51:32Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24776",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24739",
      "title": "SLM-TTA: A Framework for Test-Time Adaptation of Generative Spoken Language Models",
      "authors": [
        "Yuan-Kuei Wu",
        "Yang Liu",
        "Yiteng Huang",
        "Zhaojun Yang",
        "Haibin Wu",
        "Ruizhe Huang",
        "Yi-Te",
        "Hsu",
        "Shuyu Kong",
        "Ming Sun",
        "Florian Metze",
        "Li Wan"
      ],
      "abstract": "Spoken Language Models (SLMs) are increasingly central to modern speech-driven applications, but performance degrades under acoustic shift - real-world noise, reverberation, and microphone variation. Prior solutions rely on offline domain adaptation, which is post-hoc, data-intensive, and slow. We introduce the first test-time adaptation (TTA) framework for generative SLMs that process interleaved audio-text prompts. Our method updates a small, targeted subset of parameters during inference using only the incoming utterance, requiring no source data or labels. This stabilizes token distributions and improves robustness to acoustic variability without degrading core task accuracy. Evaluated on automatic speech recognition, speech translation, and 19 audio understanding tasks from AIR-Bench, our approach yields consistent gains under diverse corruptions. Because adaptation touches only a small fraction of weights, it is both compute- and memory-efficient, supporting deployment on resource-constrained platforms. This work enhances the robustness and adaptability of generative SLMs for real-world speech-driven applications.",
      "publishedDate": "2025-12-31T09:13:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24739",
      "categories": [
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.24695",
      "title": "Nested Learning: The Illusion of Deep Learning Architectures",
      "authors": [
        "Ali Behrouz",
        "Meisam Razaviyayn",
        "Peilin Zhong",
        "Vahab Mirrokni"
      ],
      "abstract": "Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.",
      "publishedDate": "2025-12-31T07:59:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24695",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24687",
      "title": "Quantum Visual Word Sense Disambiguation: Unraveling Ambiguities Through Quantum Inference Model",
      "authors": [
        "Wenbo Qiao",
        "Peng Zhang",
        "Qinghua Hu"
      ],
      "abstract": "Visual word sense disambiguation focuses on polysemous words, where candidate images can be easily confused. Traditional methods use classical probability to calculate the likelihood of an image matching each gloss of the target word, summing these to form a posterior probability. However, due to the challenge of semantic uncertainty, glosses from different sources inevitably carry semantic biases, which can lead to biased disambiguation results. Inspired by quantum superposition in modeling uncertainty, this paper proposes a Quantum Inference Model for Unsupervised Visual Word Sense Disambiguation (Q-VWSD). It encodes multiple glosses of the target word into a superposition state to mitigate semantic biases. Then, the quantum circuit is executed, and the results are observed. By formalizing our method, we find that Q-VWSD is a quantum generalization of the method based on classical probability. Building on this, we further designed a heuristic version of Q-VWSD that can run more efficiently on classical computing. The experiments demonstrate that our method outperforms state-of-the-art classical methods, particularly by effectively leveraging non-specialized glosses from large language models, which further enhances performance. Our approach showcases the potential of quantum machine learning in practical applications and provides a case for leveraging quantum modeling advantages on classical computers while quantum hardware remains immature.",
      "publishedDate": "2025-12-31T07:47:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24687",
      "categories": [
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24686",
      "title": "BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis",
      "authors": [
        "Songqi Zhou",
        "Ruixue Liu",
        "Boman Su",
        "Jiazhou Wang",
        "Yixing Wang",
        "Benben Jiang"
      ],
      "abstract": "Fault diagnosis of lithium-ion batteries is critical for system safety. While existing deep learning methods exhibit superior detection accuracy, their \"black-box\" nature hinders interpretability. Furthermore, restricted by binary classification paradigms, they struggle to provide root cause analysis and maintenance recommendations. To address these limitations, this paper proposes BatteryAgent, a hierarchical framework that integrates physical knowledge features with the reasoning capabilities of Large Language Models (LLMs). The framework comprises three core modules: (1) A Physical Perception Layer that utilizes 10 mechanism-based features derived from electrochemical principles, balancing dimensionality reduction with physical fidelity; (2) A Detection and Attribution Layer that employs Gradient Boosting Decision Trees and SHAP to quantify feature contributions; and (3) A Reasoning and Diagnosis Layer that leverages an LLM as the agent core. This layer constructs a \"numerical-semantic\" bridge, combining SHAP attributions with a mechanism knowledge base to generate comprehensive reports containing fault types, root cause analysis, and maintenance suggestions. Experimental results demonstrate that BatteryAgent effectively corrects misclassifications on hard boundary samples, achieving an AUROC of 0.986, which significantly outperforms current state-of-the-art methods. Moreover, the framework extends traditional binary detection to multi-type interpretable diagnosis, offering a new paradigm shift from \"passive detection\" to \"intelligent diagnosis\" for battery safety management.",
      "publishedDate": "2025-12-31T07:38:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24686",
      "categories": [
        "rag",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24661",
      "title": "Do Large Language Models Know What They Are Capable Of?",
      "authors": [
        "Casey O. Barkan",
        "Sid Black",
        "Oliver Sourbut"
      ],
      "abstract": "We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.",
      "publishedDate": "2025-12-31T06:14:46Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24661",
      "categories": [
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24636",
      "title": "How Do Agentic AI Systems Deal With Software Energy Concerns? A Pull Request-Based Study",
      "authors": [
        "Tanjum Motin Mitul",
        "Md. Masud Mazumder",
        "Md Nahidul Islam Opu",
        "Shaiful Chowdhury"
      ],
      "abstract": "As Software Engineering enters its new era (SE 3.0), AI coding agents increasingly automate software development workflows. However, it remains unclear how exactly these agents recognize and address software energy concerns-an issue growing in importance due to large-scale data centers, energy-hungry language models, and battery-constrained devices. In this paper, we examined the energy awareness of agent-authored pull requests (PRs) using a publicly available dataset. We identified 216 energy-explicit PRs and conducted a thematic analysis, deriving a taxonomy of energy-aware work. Our further analysis of the applied optimization techniques shows that most align with established research recommendations. Although building and running these agents is highly energy intensive, encouragingly, the results indicate that they exhibit energy awareness when generating software artifacts. However, optimization-related PRs are accepted less frequently than others, largely due to their negative impact on maintainability.",
      "publishedDate": "2025-12-31T05:13:56Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24636",
      "categories": [
        "code-generation",
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.24635",
      "title": "DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information",
      "authors": [
        "Zhili Huang",
        "Ling Xu",
        "Chao Liu",
        "Weifeng Sun",
        "Xu Zhang",
        "Yan Lei",
        "Meng Yan",
        "Hongyu Zhang"
      ],
      "abstract": "Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair. To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.",
      "publishedDate": "2025-12-31T05:13:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24635",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24618",
      "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
      "authors": [
        "Junru Lu",
        "Jiarui Qin",
        "Lingfeng Qiao",
        "Yinghui Li",
        "Xinyi Dai",
        "Bo Ke",
        "Jianfeng He",
        "Ruizhi Qiao",
        "Di Yin",
        "Xing Sun",
        "Yunsheng Wu",
        "Yinsong Liu",
        "Shuangyin Liu",
        "Mingkong Tang",
        "Haodong Lin",
        "Jiayi Kuang",
        "Fanxu Meng",
        "Xiaojuan Tang",
        "Yunjia Xi",
        "Junjie Huang",
        "Haotong Yang",
        "Zhenyi Shen",
        "Yangning Li",
        "Qianwen Zhang",
        "Yifei Yu",
        "Siyu An",
        "Junnan Dong",
        "Qiufeng Wang",
        "Jie Wang",
        "Keyu Chen",
        "Wei Wen",
        "Taian Guo",
        "Zhifeng Shen",
        "Daohai Yu",
        "Jiahao Li",
        "Ke Li",
        "Zongyi Li",
        "Xiaoyu Tan"
      ],
      "abstract": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
      "publishedDate": "2025-12-31T04:25:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24618",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "planning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24617",
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "authors": [
        "Xingwei Qu",
        "Shaowen Wang",
        "Zihao Huang",
        "Kai Hua",
        "Fan Yin",
        "Rui-Jie Zhu",
        "Jundong Zhou",
        "Qiyang Min",
        "Zihao Wang",
        "Yizhi Li",
        "Tianyu Zhang",
        "He Xing",
        "Zheng Zhang",
        "Yuxuan Song",
        "Tianyu Zheng",
        "Zhiyuan Zeng",
        "Chenghua Lin",
        "Ge Zhang",
        "Wenhao Huang"
      ],
      "abstract": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\\textbf{+2.69$\\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.",
      "publishedDate": "2025-12-31T04:19:33Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24617",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24615",
      "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
      "authors": [
        "Yuchen Shi",
        "Yuzheng Cai",
        "Siqi Cai",
        "Zihan Xu",
        "Lichao Chen",
        "Yulei Qin",
        "Zhijian Zhou",
        "Xiang Fei",
        "Chaofan Qiu",
        "Xiaoyu Tan",
        "Gang Li",
        "Zongyi Li",
        "Haojia Lin",
        "Guocan Cai",
        "Yong Mao",
        "Yunsheng Wu",
        "Ke Li",
        "Xing Sun"
      ],
      "abstract": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \\textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \\textbf{Workflow} mode for standard tasks and a \\textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \\textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \\textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
      "publishedDate": "2025-12-31T04:17:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24615",
      "categories": [
        "agents",
        "prompting",
        "code-generation",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24613",
      "title": "Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning",
      "authors": [
        "Zheyu Shi",
        "Dong Qiu",
        "Shanlong Yu"
      ],
      "abstract": "This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.",
      "publishedDate": "2025-12-31T04:10:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24613",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.24609",
      "title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization",
      "authors": [
        "Dong Qiu",
        "Duo Xu",
        "Limengxi Yue"
      ],
      "abstract": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.",
      "publishedDate": "2025-12-31T03:59:18Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24609",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24594",
      "title": "A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs",
      "authors": [
        "Zhongyi Wang",
        "Tengjie Lin",
        "Mingshuai Chen",
        "Haokun Li",
        "Mingqi Yang",
        "Xiao Yi",
        "Shengchao Qin",
        "Yixing Luo",
        "Xiaofeng Li",
        "Bin Gu",
        "Liqiang Lu",
        "Jianwei Yin"
      ],
      "abstract": "Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.",
      "publishedDate": "2025-12-31T03:31:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24594",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24591",
      "title": "Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning",
      "authors": [
        "Fuyu Dong",
        "Ke Li",
        "Di Wang",
        "Nan Luo",
        "Yiming Zhang",
        "Kaiyu Li",
        "Jianfei Yang",
        "Quan Wang"
      ],
      "abstract": "Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.",
      "publishedDate": "2025-12-31T03:28:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24591",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24587",
      "title": "MultiRisk: Multiple Risk Control via Iterative Score Thresholding",
      "authors": [
        "Sunay Joshi",
        "Yan Sun",
        "Hamed Hassani",
        "Edgar Dobriban"
      ],
      "abstract": "As generative AI systems are increasingly deployed in real-world applications, regulating multiple dimensions of model behavior has become essential. We focus on test-time filtering: a lightweight mechanism for behavior control that compares performance scores to estimated thresholds, and modifies outputs when these bounds are violated. We formalize the problem of enforcing multiple risk constraints with user-defined priorities, and introduce two efficient dynamic programming algorithms that leverage this sequential structure. The first, MULTIRISK-BASE, provides a direct finite-sample procedure for selecting thresholds, while the second, MULTIRISK, leverages data exchangeability to guarantee simultaneous control of the risks. Under mild assumptions, we show that MULTIRISK achieves nearly tight control of all constraint risks. The analysis requires an intricate iterative argument, upper bounding the risks by introducing several forms of intermediate symmetrized risk functions, and carefully lower bounding the risks by recursively counting jumps in symmetrized risk functions between appropriate risk levels. We evaluate our framework on a three-constraint Large Language Model alignment task using the PKU-SafeRLHF dataset, where the goal is to maximize helpfulness subject to multiple safety constraints, and where scores are generated by a Large Language Model judge and a perplexity filter. Our experimental results show that our algorithm can control each individual risk at close to the target level.",
      "publishedDate": "2025-12-31T03:25:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24587",
      "categories": [
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24574",
      "title": "Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time",
      "authors": [
        "Zhenyu Zhang",
        "Xiaoxia Wu",
        "Zhongzhu Zhou",
        "Qingyang Wu",
        "Yineng Zhang",
        "Pragaash Ponnusamy",
        "Harikaran Subbaraj",
        "Jue Wang",
        "Shuaiwen Leon Song",
        "Ben Athiwaratkun"
      ],
      "abstract": "Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.",
      "publishedDate": "2025-12-31T02:46:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24574",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24572",
      "title": "Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs' Legal Reasoning Capabilities",
      "authors": [
        "Hongseok Oh",
        "Wonseok Hwang",
        "Kyoung-Woon On"
      ],
      "abstract": "We introduce the Korean Canonical Legal Benchmark (KCL), a benchmark designed to assess language models' legal reasoning capabilities independently of domain-specific knowledge. KCL provides question-level supporting precedents, enabling a more faithful disentanglement of reasoning ability from parameterized knowledge. KCL consists of two components: (1) KCL-MCQA, multiple-choice problems of 283 questions with 1,103 aligned precedents, and (2) KCL-Essay, open-ended generation problems of 169 questions with 550 aligned precedents and 2,739 instance-level rubrics for automated evaluation. Our systematic evaluation of 30+ models shows large remaining gaps, particularly in KCL-Essay, and that reasoning-specialized models consistently outperform their general-purpose counterparts. We release all resources, including the benchmark dataset and evaluation code, at https://github.com/lbox-kr/kcl.",
      "publishedDate": "2025-12-31T02:35:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24572",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24570",
      "title": "On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study",
      "authors": [
        "Shiqi Kuang",
        "Zhao Tian",
        "Tao Xiao",
        "Dong Wang",
        "Junjie Chen"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.",
      "publishedDate": "2025-12-31T02:30:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24570",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24565",
      "title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use",
      "authors": [
        "Wenrui Liu",
        "Zixiang Liu",
        "Elsie Dai",
        "Wenhan Yu",
        "Lei Yu",
        "Tong Yang"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.",
      "publishedDate": "2025-12-31T02:09:48Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24565",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24560",
      "title": "Localized Calibrated Uncertainty in Code Language Models",
      "authors": [
        "David Gros",
        "Prem Devanbu"
      ],
      "abstract": "Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of \"Minimal Intent Aligning Patches\" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.",
      "publishedDate": "2025-12-31T02:00:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24560",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24556",
      "title": "Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs",
      "authors": [
        "Muhammad Abdullahi Said",
        "Muhammad Sammani Sani"
      ],
      "abstract": "As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.",
      "publishedDate": "2025-12-31T01:40:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24556",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24551",
      "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
      "authors": [
        "Yuanhao Cai",
        "Kunpeng Li",
        "Menglin Jia",
        "Jialiang Wang",
        "Junzhe Sun",
        "Feng Liang",
        "Weifeng Chen",
        "Felix Juefei-Xu",
        "Chu Wang",
        "Ali Thabet",
        "Xiaoliang Dai",
        "Xuan Ju",
        "Alan Yuille",
        "Ji Hou"
      ],
      "abstract": "Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO",
      "publishedDate": "2025-12-31T01:19:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24551",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24532",
      "title": "From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning",
      "authors": [
        "Amir Tahmasbi",
        "Sadegh Majidi",
        "Kazem Taram",
        "Aniket Bera"
      ],
      "abstract": "Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.",
      "publishedDate": "2025-12-31T00:36:03Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24532",
      "categories": [
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24526",
      "title": "Generative AI-enhanced Sector-based Investment Portfolio Construction",
      "authors": [
        "Alina Voronina",
        "Oleksandr Romanko",
        "Ruiwen Cao",
        "Roy H. Kwon",
        "Rafael Mendoza-Arriaga"
      ],
      "abstract": "This paper investigates how Large Language Models (LLMs) from leading providers (OpenAI, Google, Anthropic, DeepSeek, and xAI) can be applied to quantitative sector-based portfolio construction. We use LLMs to identify investable universes of stocks within S&P 500 sector indices and evaluate how their selections perform when combined with classical portfolio optimization methods. Each model was prompted to select and weight 20 stocks per sector, and the resulting portfolios were compared with their respective sector indices across two distinct out-of-sample periods: a stable market phase (January-March 2025) and a volatile phase (April-June 2025). Our results reveal a strong temporal dependence in LLM portfolio performance. During stable market conditions, LLM-weighted portfolios frequently outperformed sector indices on both cumulative return and risk-adjusted (Sharpe ratio) measures. However, during the volatile period, many LLM portfolios underperformed, suggesting that current models may struggle to adapt to regime shifts or high-volatility environments underrepresented in their training data. Importantly, when LLM-based stock selection is combined with traditional optimization techniques, portfolio outcomes improve in both performance and consistency. This study contributes one of the first multi-model, cross-provider evaluations of generative AI algorithms in investment management. It highlights that while LLMs can effectively complement quantitative finance by enhancing stock selection and interpretability, their reliability remains market-dependent. The findings underscore the potential of hybrid AI-quantitative frameworks, integrating LLM reasoning with established optimization techniques, to produce more robust and adaptive investment strategies.",
      "publishedDate": "2025-12-31T00:19:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24526",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24505",
      "title": "Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems",
      "authors": [
        "Samuel Golladay",
        "Majid Bani-Yaghoub"
      ],
      "abstract": "Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.",
      "publishedDate": "2025-12-30T23:05:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24505",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.24478",
      "title": "HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors",
      "authors": [
        "Hyunjun Kim"
      ],
      "abstract": "Causal discovery from observational data remains fundamentally limited by identifiability constraints. Recent work has explored leveraging Large Language Models (LLMs) as sources of prior causal knowledge, but existing approaches rely on heuristic integration that lacks theoretical grounding. We introduce HOLOGRAPH, a framework that formalizes LLM-guided causal discovery through sheaf theory--representing local causal beliefs as sections of a presheaf over variable subsets. Our key insight is that coherent global causal structure corresponds to the existence of a global section, while topological obstructions manifest as non-vanishing sheaf cohomology. We propose the Algebraic Latent Projection to handle hidden confounders and Natural Gradient Descent on the belief manifold for principled optimization. Experiments on synthetic and real-world benchmarks demonstrate that HOLOGRAPH provides rigorous mathematical foundations while achieving competitive performance on causal discovery tasks with 50-100 variables. Our sheaf-theoretic analysis reveals that while Identity, Transitivity, and Gluing axioms are satisfied to numerical precision (<10^{-6}), the Locality axiom fails for larger graphs, suggesting fundamental non-local coupling in latent variable projections. Code is available at [https://github.com/hyunjun1121/holograph](https://github.com/hyunjun1121/holograph).",
      "publishedDate": "2025-12-30T21:47:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24478",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24470",
      "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models",
      "authors": [
        "Kim Alexander Christensen",
        "Andreas Gudahl Tufte",
        "Alexey Gusev",
        "Rohan Sinha",
        "Milan Ganai",
        "Ole Andreas Alsos",
        "Marco Pavoned",
        "Martin Steinert"
      ],
      "abstract": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert->fallback maneuver->operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird's-eye-view perception and short-horizon replanning.",
      "publishedDate": "2025-12-30T21:20:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24470",
      "categories": [
        "agents",
        "planning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24461",
      "title": "Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents",
      "authors": [
        "Seohui Bae",
        "Jeonghye Kim",
        "Youngchul Sung",
        "Woohyung Lim"
      ],
      "abstract": "In this paper, we propose a test-time adaptive agent that performs exploratory inference through posterior-guided belief refinement without relying on gradient-based updates or additional training for LLM agent operating under partial observability. Our agent maintains an external structured belief over the environment state, iteratively updates it via action-conditioned observations, and selects actions by maximizing predicted information gain over the belief space. We estimate information gain using a lightweight LLM-based surrogate and assess world alignment through a novel reward that quantifies the consistency between posterior belief and ground-truth environment configuration. Experiments show that our method outperforms inference-time scaling baselines such as prompt-augmented or retrieval-enhanced LLMs, in aligning with latent world states with significantly lower integration overhead.",
      "publishedDate": "2025-12-30T20:51:28Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24461",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.24449",
      "title": "PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression",
      "authors": [
        "Bo Jiang",
        "Taolue Yang",
        "Youyuan Liu",
        "Xubin He",
        "Sheng Di",
        "Sian Jin"
      ],
      "abstract": "Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \\textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \\textbf{153.2}\\% higher memory reduction rate for the K cache and \\textbf{179.6}\\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \\textbf{75.7}\\% for K and \\textbf{171.7}\\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV",
      "publishedDate": "2025-12-30T20:05:32Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24449",
      "categories": [
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24440",
      "title": "Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features",
      "authors": [
        "Theodore MacMillan",
        "Nicholas T. Ouellette"
      ],
      "abstract": "Large data-driven physics models like DeepMind's weather model GraphCast have empirically succeeded in parameterizing time operators for complex dynamical systems with an accuracy reaching or in some cases exceeding that of traditional physics-based solvers. Unfortunately, how these data-driven models perform computations is largely unknown and whether their internal representations are interpretable or physically consistent is an open question. Here, we adapt tools from interpretability research in Large Language Models to analyze intermediate computational layers in GraphCast, leveraging sparse autoencoders to discover interpretable features in the neuron space of the model. We uncover distinct features on a wide range of length and time scales that correspond to tropical cyclones, atmospheric rivers, diurnal and seasonal behavior, large-scale precipitation patterns, specific geographical coding, and sea-ice extent, among others. We further demonstrate how the precise abstraction of these features can be probed via interventions on the prediction steps of the model. As a case study, we sparsely modify a feature corresponding to tropical cyclones in GraphCast and observe interpretable and physically consistent modifications to evolving hurricanes. Such methods offer a window into the black-box behavior of data-driven physics models and are a step towards realizing their potential as trustworthy predictors and scientifically valuable tools for discovery.",
      "publishedDate": "2025-12-30T19:50:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24440",
      "categories": [
        "code-generation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.24415",
      "title": "Language Model Agents Under Attack: A Cross Model-Benchmark of Profit-Seeking Behaviors in Customer Service",
      "authors": [
        "Jingyu Zhang"
      ],
      "abstract": "Customer-service LLM agents increasingly make policy-bound decisions (refunds, rebooking, billing disputes), but the same ``helpful'' interaction style can be exploited: a small fraction of users can induce unauthorized concessions, shifting costs to others and eroding trust in agentic workflows. We present a cross-domain benchmark of profit-seeking direct prompt injection in customer-service interactions, spanning 10 service domains and 100 realistic attack scripts grouped into five technique families. Across five widely used models under a unified rubric with uncertainty reporting, attacks are highly domain-dependent (airline support is most exploitable) and technique-dependent (payload splitting is most consistently effective). We release data and evaluation code to support reproducible auditing and to inform the design of oversight and recovery workflows for trustworthy, human centered agent interfaces.",
      "publishedDate": "2025-12-30T18:57:52Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24415",
      "categories": [
        "agents",
        "evaluation",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24340",
      "title": "DermaVQA-DAS: Dermatology Assessment Schema (DAS) & Datasets for Closed-Ended Question Answering & Segmentation in Patient-Generated Dermatology Images",
      "authors": [
        "Wen-wai Yim",
        "Yujuan Fu",
        "Asma Ben Abacha",
        "Meliha Yetisgen",
        "Noel Codella",
        "Roberto Andres Novoa",
        "Josep Malvehy"
      ],
      "abstract": "Recent advances in dermatological image analysis have been driven by large-scale annotated datasets; however, most existing benchmarks focus on dermatoscopic images and lack patient-authored queries and clinical context, limiting their applicability to patient-centered care. To address this gap, we introduce DermaVQA-DAS, an extension of the DermaVQA dataset that supports two complementary tasks: closed-ended question answering (QA) and dermatological lesion segmentation. Central to this work is the Dermatology Assessment Schema (DAS), a novel expert-developed framework that systematically captures clinically meaningful dermatological features in a structured and standardized form. DAS comprises 36 high-level and 27 fine-grained assessment questions, with multiple-choice options in English and Chinese. Leveraging DAS, we provide expert-annotated datasets for both closed QA and segmentation and benchmark state-of-the-art multimodal models. For segmentation, we evaluate multiple prompting strategies and show that prompt design impacts performance: the default prompt achieves the best results under Mean-of-Max and Mean-of-Mean evaluation aggregation schemes, while an augmented prompt incorporating both patient query title and content yields the highest performance under majority-vote-based microscore evaluation, achieving a Jaccard index of 0.395 and a Dice score of 0.566 with BiomedParse. For closed-ended QA, overall performance is strong across models, with average accuracies ranging from 0.729 to 0.798; o3 achieves the best overall accuracy (0.798), closely followed by GPT-4.1 (0.796), while Gemini-1.5-Pro shows competitive performance within the Gemini family (0.783). We publicly release DermaVQA-DAS, the DAS schema, and evaluation protocols to support and accelerate future research in patient-centered dermatological vision-language modeling (https://osf.io/72rp3).",
      "publishedDate": "2025-12-30T16:48:20Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24340",
      "categories": [
        "prompting",
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.24331",
      "title": "Spatial-aware Vision Language Model for Autonomous Driving",
      "authors": [
        "Weijie Wei",
        "Zhipeng Luo",
        "Ling Feng",
        "Venice Erin Liong"
      ],
      "abstract": "While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM's existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.",
      "publishedDate": "2025-12-30T16:35:00Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24331",
      "categories": [
        "rag",
        "evaluation",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24330",
      "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
      "authors": [
        "Yong Xien Chng",
        "Tao Hu",
        "Wenwen Tong",
        "Xueheng Li",
        "Jiandong Chen",
        "Haojia Yu",
        "Jiefan Lu",
        "Hewei Guo",
        "Hanming Deng",
        "Chengjun Xie",
        "Gao Huang",
        "Dahua Lin",
        "Lewei Lu"
      ],
      "abstract": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.",
      "publishedDate": "2025-12-30T16:31:45Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24330",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24329",
      "title": "World model inspired sarcasm reasoning with large language model agents",
      "authors": [
        "Keito Inoshita",
        "Shinnosuke Mizuno"
      ],
      "abstract": "Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.",
      "publishedDate": "2025-12-30T16:31:08Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24329",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.24314",
      "title": "QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs",
      "authors": [
        "Shupeng Li",
        "Weipeng Lu",
        "Linyun Liu",
        "Chen Lin",
        "Shaofei Li",
        "Zhendong Tan",
        "Hanjun Zhong",
        "Yucheng Zeng",
        "Chenghao Zhu",
        "Mengyue Liu",
        "Daxiang Dong",
        "Jianmin Wu",
        "Yunting Xiao",
        "Annan Li",
        "Danyu Liu",
        "Jingnan Zhang",
        "Licen Liu",
        "Dawei Yin",
        "Dou Shen"
      ],
      "abstract": "Domain-specific enhancement of Large Language Models (LLMs) within the financial context has long been a focal point of industrial application. While previous models such as BloombergGPT and Baichuan-Finance primarily focused on knowledge enhancement, the deepening complexity of financial services has driven a growing demand for models that possess not only domain knowledge but also robust financial reasoning and agentic capabilities. In this paper, we present QianfanHuijin, a financial domain LLM, and propose a generalizable multi-stage training paradigm for industrial model enhancement. Our approach begins with Continual Pre-training (CPT) on financial corpora to consolidate the knowledge base. This is followed by a fine-grained Post-training pipeline designed with increasing specificity: starting with Financial SFT, progressing to Finance Reasoning RL and Finance Agentic RL, and culminating in General RL aligned with real-world business scenarios. Empirical results demonstrate that QianfanHuijin achieves superior performance across various authoritative financial benchmarks. Furthermore, ablation studies confirm that the targeted Reasoning RL and Agentic RL stages yield significant gains in their respective capabilities. These findings validate our motivation and suggest that this fine-grained, progressive post-training methodology is poised to become a mainstream paradigm for various industrial-enhanced LLMs.",
      "publishedDate": "2025-12-30T16:10:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24314",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24289",
      "title": "Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs",
      "authors": [
        "Jonathan Schmoll",
        "Adam Jatowt"
      ],
      "abstract": "The manual, resource-intensive process of complying with the EU Taxonomy presents a significant challenge for companies. While Large Language Models (LLMs) offer a path to automation, research is hindered by a lack of public benchmark datasets. To address this gap, we introduce a novel, structured dataset from 190 corporate reports, containing ground-truth economic activities and quantitative Key Performance Indicators (KPIs). We use this dataset to conduct the first systematic evaluation of LLMs on the core compliance workflow. Our results reveal a clear performance gap between qualitative and quantitative tasks. LLMs show moderate success in the qualitative task of identifying economic activities, with a multi-step agentic framework modestly enhancing precision. Conversely, the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting. We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports, and find that model confidence scores are poorly calibrated. We conclude that while LLMs are not ready for full automation, they can serve as powerful assistive tools for human experts. Our dataset provides a public benchmark for future research.",
      "publishedDate": "2025-12-30T15:28:03Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24289",
      "categories": [
        "evaluation",
        "agents",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.24271",
      "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
      "authors": [
        "Zhe Huang",
        "Hao Wen",
        "Aiming Hao",
        "Bingze Song",
        "Meiqi Wu",
        "Jiahong Wu",
        "Xiangxiang Chu",
        "Sheng Lu",
        "Haoqian Wang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.",
      "publishedDate": "2025-12-30T14:53:33Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24271",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24268",
      "title": "RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation",
      "authors": [
        "Pankayaraj Pathmanathan",
        "Michael-Andrei Panaitescu-Liess",
        "Cho-Yu Jason Chiang",
        "Furong Huang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.",
      "publishedDate": "2025-12-30T14:43:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24268",
      "categories": [
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24212",
      "title": "RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation",
      "authors": [
        "Ming-Ming Yu",
        "Yi Chen",
        "Börje F. Karlsson",
        "Wenjun Wu"
      ],
      "abstract": "Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.",
      "publishedDate": "2025-12-30T13:25:22Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24212",
      "categories": [
        "prompting",
        "robotics",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24181",
      "title": "MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring",
      "authors": [
        "Qipeng Wang",
        "Rui Sheng",
        "Yafei Li",
        "Huamin Qu",
        "Yushi Sun",
        "Min Zhu"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated significant promise in clinical diagnosis. However, current models struggle to emulate the iterative, diagnostic hypothesis-driven reasoning of real clinical scenarios. Specifically, current LLMs suffer from three critical limitations: (1) generating hallucinated medical content due to weak grounding in verified knowledge, (2) asking redundant or inefficient questions rather than discriminative ones that hinder diagnostic progress, and (3) losing coherence over multi-turn dialogues, leading to contradictory or inconsistent conclusions. To address these challenges, we propose MedKGI, a diagnostic framework grounded in clinical practices. MedKGI integrates a medical knowledge graph (KG) to constrain reasoning to validated medical ontologies, selects questions based on information gain to maximize diagnostic efficiency, and adopts an OSCE-format structured state to maintain consistent evidence tracking across turns. Experiments on clinical benchmarks show that MedKGI outperforms strong LLM baselines in both diagnostic accuracy and inquiry efficiency, improving dialogue efficiency by 30% on average while maintaining state-of-the-art accuracy.",
      "publishedDate": "2025-12-30T12:31:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24181",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24165",
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "authors": [
        "Zefeng He",
        "Xiaoye Qu",
        "Yafu Li",
        "Tong Zhu",
        "Siyuan Huang",
        "Yu Cheng"
      ],
      "abstract": "While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.",
      "publishedDate": "2025-12-30T11:51:18Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24165",
      "categories": [
        "reasoning",
        "planning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.24149",
      "title": "Large Emotional World Model",
      "authors": [
        "Changhao Song",
        "Yazhou Zhang",
        "Hui Gao",
        "Chang Yang",
        "Peng Zhang"
      ],
      "abstract": "World Models serve as tools for understanding the current state of the world and predicting its future dynamics, with broad application potential across numerous fields. As a key component of world knowledge, emotion significantly influences human decision-making. While existing Large Language Models (LLMs) have shown preliminary capability in capturing world knowledge, they primarily focus on modeling physical-world regularities and lack systematic exploration of emotional factors. In this paper, we first demonstrate the importance of emotion in understanding the world by showing that removing emotionally relevant information degrades reasoning performance. Inspired by theory of mind, we further propose a Large Emotional World Model (LEWM). Specifically, we construct the Emotion-Why-How (EWH) dataset, which integrates emotion into causal relationships and enables reasoning about why actions occur and how emotions drive future world states. Based on this dataset, LEWM explicitly models emotional states alongside visual observations and actions, allowing the world model to predict both future states and emotional transitions. Experimental results show that LEWM more accurately predicts emotion-driven social behaviors while maintaining comparable performance to general world models on basic tasks.",
      "publishedDate": "2025-12-30T11:26:01Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24149",
      "categories": [
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24133",
      "title": "Bridging Visual Intuition and Chemical Expertise: An Autonomous Analysis Framework for Nonadiabatic Dynamics Simulations via Mentor-Engineer-Student Collaboration",
      "authors": [
        "Yifei Zhu",
        "Jiahui Zhang",
        "Binni Huang",
        "Zhenggang Lan"
      ],
      "abstract": "Analyzing nonadiabatic molecular dynamics trajectories traditionally heavily relies on expert intuition and visual pattern recognition, a process that is difficult to formalize. We present VisU, a vision-driven framework that leverages the complementary strengths of two state-of-the-art large language models to establish a \"virtual research collective.\" This collective operates through a \"Mentor-Engineer-Student\" paradigm that mimics the collaborative intelligence of a professional chemistry laboratory. Within this ecosystem, the Mentor provides physical intuition through visual reasoning, while the Engineer adaptively constructs analysis scripts, and the Student executes the pipeline and manages the data and results. VisU autonomously orchestrates a four-stage workflow comprising Preprocessing, Recursive Channel Discovery, Important-Motion Identification, and Validation/Summary. This systematic approach identifies reaction channels and key nuclear motions while generating professional academic reports. By bridging visual insight with chemical expertise, VisU establishes a new paradigm for human-AI collaboration in the analysis of excited-state dynamics simulation results, significantly reducing dependence on manual interpretation and enabling more intuitive, scalable mechanistic discovery.",
      "publishedDate": "2025-12-30T10:36:33Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24133",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.24125",
      "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training",
      "authors": [
        "Yi Liu",
        "Sukai Wang",
        "Dafeng Wei",
        "Xiaowei Cai",
        "Linqing Zhong",
        "Jiange Yang",
        "Guanghui Ren",
        "Jinyu Zhang",
        "Maoqing Yao",
        "Chuankang Li",
        "Xindong He",
        "Liliang Chen",
        "Jianlan Luo"
      ],
      "abstract": "General-purpose robotic systems operating in open-world environments must achieve both broad generalization and high-precision action execution, a combination that remains challenging for existing Vision-Language-Action (VLA) models. While large Vision-Language Models (VLMs) improve semantic generalization, insufficient embodied reasoning leads to brittle behavior, and conversely, strong reasoning alone is inadequate without precise control. To provide a decoupled and quantitative assessment of this bottleneck, we introduce Embodied Reasoning Intelligence Quotient (ERIQ), a large-scale embodied reasoning benchmark in robotic manipulation, comprising 6K+ question-answer pairs across four reasoning dimensions. By decoupling reasoning from execution, ERIQ enables systematic evaluation and reveals a strong positive correlation between embodied reasoning capability and end-to-end VLA generalization. To bridge the gap from reasoning to precise execution, we propose FACT, a flow-matching-based action tokenizer that converts continuous control into discrete sequences while preserving high-fidelity trajectory reconstruction. The resulting GenieReasoner jointly optimizes reasoning and action in a unified space, outperforming both continuous-action and prior discrete-action baselines in real-world tasks. Together, ERIQ and FACT provide a principled framework for diagnosing and overcoming the reasoning-precision trade-off, advancing robust, general-purpose robotic manipulation.",
      "publishedDate": "2025-12-30T10:18:42Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24125",
      "categories": [
        "evaluation",
        "robotics",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24120",
      "title": "Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design",
      "authors": [
        "Chandini Vysyaraju",
        "Raghuvir Duvvuri",
        "Avi Goyal",
        "Dmitry Ignatov",
        "Radu Timofte"
      ],
      "abstract": "Automated neural network architecture design remains a significant challenge in computer vision. Task diversity and computational constraints require both effective architectures and efficient search methods. Large Language Models (LLMs) present a promising alternative to computationally intensive Neural Architecture Search (NAS), but their application to architecture generation in computer vision has not been systematically studied, particularly regarding prompt engineering and validation strategies. Building on the task-agnostic NNGPT/LEMUR framework, this work introduces and validates two key contributions for computer vision. First, we present Few-Shot Architecture Prompting (FSAP), the first systematic study of the number of supporting examples (n = 1, 2, 3, 4, 5, 6) for LLM-based architecture generation. We find that using n = 3 examples best balances architectural diversity and context focus for vision tasks. Second, we introduce Whitespace-Normalized Hash Validation, a lightweight deduplication method (less than 1 ms) that provides a 100x speedup over AST parsing and prevents redundant training of duplicate computer vision architectures. In large-scale experiments across seven computer vision benchmarks (MNIST, CIFAR-10, CIFAR-100, CelebA, ImageNette, SVHN, Places365), we generated 1,900 unique architectures. We also introduce a dataset-balanced evaluation methodology to address the challenge of comparing architectures across heterogeneous vision tasks. These contributions provide actionable guidelines for LLM-based architecture search in computer vision and establish rigorous evaluation practices, making automated design more accessible to researchers with limited computational resources.",
      "publishedDate": "2025-12-30T10:01:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24120",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24119",
      "title": "GeoBench: Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation",
      "authors": [
        "Yuan Feng",
        "Yue Yang",
        "Xiaohan He",
        "Jiatong Zhao",
        "Jianlong Chen",
        "Zijun Chen",
        "Daocheng Fu",
        "Qi Liu",
        "Renqiu Xia",
        "Bo Zhang",
        "Junchi Yan"
      ],
      "abstract": "Geometric problem solving constitutes a critical branch of mathematical reasoning, requiring precise analysis of shapes and spatial relationships. Current evaluations of geometric reasoning in vision-language models (VLMs) face limitations, including the risk of test data contamination from textbook-based benchmarks, overemphasis on final answers over reasoning processes, and insufficient diagnostic granularity. To address these issues, we present GeoBench, a hierarchical benchmark featuring four reasoning levels in geometric problem-solving: Visual Perception, Goal-Oriented Planning, Rigorous Theorem Application, and Self-Reflective Backtracking. Through six formally verified tasks generated via TrustGeoGen, we systematically assess capabilities ranging from attribute extraction to logical error correction. Experiments reveal that while reasoning models like OpenAI-o3 outperform general MLLMs, performance declines significantly with increasing task complexity. Key findings demonstrate that sub-goal decomposition and irrelevant premise filtering critically influence final problem-solving accuracy, whereas Chain-of-Thought prompting unexpectedly degrades performance in some tasks. These findings establish GeoBench as a comprehensive benchmark while offering actionable guidelines for developing geometric problem-solving systems.",
      "publishedDate": "2025-12-30T09:56:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24119",
      "categories": [
        "evaluation",
        "prompting",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.24113",
      "title": "CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation",
      "authors": [
        "Jiaxin Hu",
        "Tao Wang",
        "Bingsan Yang",
        "Hongrun Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated a remarkable capacity in understanding user preferences for recommendation systems. However, they are constrained by several critical challenges, including their inherent \"Black-Box\" characteristics, susceptibility to knowledge hallucination, and limited online learning capacity. These factors compromise their trustworthiness and adaptability. Conversely, cognitive architectures such as Soar offer structured and interpretable reasoning processes, yet their knowledge acquisition is notoriously laborious. To address these complementary challenges, we propose a novel cognitive recommender agent called CogRec which synergizes the strengths of LLMs with the Soar cognitive architecture. CogRec leverages Soar as its core symbolic reasoning engine and leverages an LLM for knowledge initialization to populate its working memory with production rules. The agent operates on a Perception-Cognition-Action(PCA) cycle. Upon encountering an impasse, it dynamically queries the LLM to obtain a reasoned solution. This solution is subsequently transformed into a new symbolic production rule via Soar's chunking mechanism, thereby enabling robust online learning. This learning paradigm allows the agent to continuously evolve its knowledge base and furnish highly interpretable rationales for its recommendations. Extensive evaluations conducted on three public datasets demonstrate that CogRec demonstrates significant advantages in recommendation accuracy, explainability, and its efficacy in addressing the long-tail problem.",
      "publishedDate": "2025-12-30T09:50:50Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24113",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24097",
      "title": "Factorized Learning for Temporally Grounded Video-Language Models",
      "authors": [
        "Wenzheng Zeng",
        "Difei Gao",
        "Mike Zheng Shou",
        "Hwee Tou Ng"
      ],
      "abstract": "Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a \"grounding then answering with evidence referencing\" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.",
      "publishedDate": "2025-12-30T09:13:20Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24097",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24077",
      "title": "LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm",
      "authors": [
        "Chunhui Wan",
        "Xunan Dai",
        "Zhuo Wang",
        "Minglei Li",
        "Yanpeng Wang",
        "Yinan Mao",
        "Yu Lan",
        "Zhiwen Xiao"
      ],
      "abstract": "The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike \"blind\" mutation operators, LoongFlow integrates LLMs into a cognitive \"Plan-Execute-Summarize\" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.",
      "publishedDate": "2025-12-30T08:39:28Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24077",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24063",
      "title": "How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns",
      "authors": [
        "Haoyue Bai",
        "Yiyou Sun",
        "Wenjie Hu",
        "Shi Qiu",
        "Maggie Ziyu Huan",
        "Peiyang Song",
        "Robert Nowak",
        "Dawn Song"
      ],
      "abstract": "Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.",
      "publishedDate": "2025-12-30T08:16:20Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24063",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.24052",
      "title": "AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives",
      "authors": [
        "Yanxi Chen",
        "Wenhui Zhu",
        "Xiwen Chen",
        "Zhipeng Wang",
        "Xin Li",
        "Peijie Qiu",
        "Hao Wang",
        "Xuanzhao Dong",
        "Yujian Xiong",
        "Anderson Schneider",
        "Yuriy Nevmyvaka",
        "Yalin Wang"
      ],
      "abstract": "Although Large Audio-Language Models (LALMs) deliver state-of-the-art (SOTA) performance, they frequently suffer from hallucinations, e.g. generating text not grounded in the audio input. We analyze these grounding failures and identify a distinct taxonomy: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. To address this, we introduce the AHA (Audio Hallucination Alignment) framework. By leveraging counterfactual hard negative mining, our pipeline constructs a high-quality preference dataset that forces models to distinguish strict acoustic evidence from linguistically plausible fabrications. Additionally, we establish AHA-Eval, a diagnostic benchmark designed to rigorously test these fine-grained temporal reasoning capabilities. We apply this data to align Qwen2.5-Omni. The resulting model, Qwen-Audio-AHA, achieves a 13.7% improvement on AHA-Eval. Crucially, this benefit generalizes beyond our diagnostic set. Our model shows substantial gains on public benchmarks, including 1.3% on MMAU-Test and 1.6% on MMAR, outperforming latest SOTA methods.",
      "publishedDate": "2025-12-30T07:52:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24052",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24040",
      "title": "ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment",
      "authors": [
        "Natchaya Temyingyong",
        "Daman Jain",
        "Neeraj Kumarsahu",
        "Prabhat Kumar",
        "Rachata Phondi",
        "Wachiravit Modecrua",
        "Krittanon Kaewtawee",
        "Krittin Pachtrachai",
        "Touchapon Kraisingkorn"
      ],
      "abstract": "Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL) approaches. In real-world software engineering, however, such curated datasets are rarely available during the initial cold start of agent development, where engineers instead face messy production logs and evolving failure modes. We present ROAD (Reflective Optimization via Automated Debugging), a novel framework that bypasses the need for refined datasets by treating optimization as a dynamic debugging investigation rather than a stochastic search. Unlike traditional mutation strategies, ROAD utilizes a specialized multi-agent architecture, comprising an Analyzer for root-cause analysis, an Optimizer for pattern aggregation, and a Coach for strategy integration, to convert unstructured failure logs into robust, structured Decision Tree Protocols. We evaluated ROAD across both a standardized academic benchmark and a live production Knowledge Management engine. Experimental results demonstrate that ROAD is highly sample-efficient, achieving a 5.6 percent increase in success rate (73.6 percent to 79.2 percent) and a 3.8 percent increase in search accuracy within just three automated iterations. Furthermore, on complex reasoning tasks in the retail domain, ROAD improved agent performance by approximately 19 percent relative to the baseline. These findings suggest that mimicking the human engineering loop of failure analysis and patching offers a viable, data-efficient alternative to resource-intensive RL training for deploying reliable LLM agents.",
      "publishedDate": "2025-12-30T07:31:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24040",
      "categories": [
        "agents",
        "prompting",
        "reasoning",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24023",
      "title": "RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations",
      "authors": [
        "Xingqi He",
        "Yujie Zhang",
        "Shuyong Gao",
        "Wenjie Li",
        "Lingyi Hong",
        "Mingxi Chen",
        "Kaixun Jiang",
        "Jiyuan Fu",
        "Wenqiang Zhang"
      ],
      "abstract": "Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.",
      "publishedDate": "2025-12-30T06:50:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24023",
      "categories": [
        "prompting",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24022",
      "title": "FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing",
      "authors": [
        "Yunkai Dang",
        "Donghao Wang",
        "Jiacheng Yang",
        "Yifan Jiang",
        "Meiyi Zhu",
        "Yuekun Yang",
        "Cong Wang",
        "Qi Fan",
        "Wenbin Li",
        "Yang Gao"
      ],
      "abstract": "Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.",
      "publishedDate": "2025-12-30T06:48:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24022",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.24014",
      "title": "iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning",
      "authors": [
        "Sijia Chen",
        "Di Niu"
      ],
      "abstract": "Large language models (LLMs), when guided by explicit textual plans, can perform reliable step-by-step reasoning during problem-solving. However, generating accurate and effective textual plans remains challenging due to LLM hallucinations and the high diversity of task-specific questions. To address this, we draw inspiration from human Implicit Cognition (IC), the subconscious process by which decisions are guided by compact, generalized patterns learned from past experiences without requiring explicit verbalization. We propose iCLP, a novel framework that enables LLMs to adaptively generate latent plans (LPs), which are compact encodings of effective reasoning instructions. iCLP first distills explicit plans from existing step-by-step reasoning trajectories. It then learns discrete representations of these plans via a vector-quantized autoencoder coupled with a codebook. Finally, by fine-tuning LLMs on paired latent plans and corresponding reasoning steps, the models learn to perform implicit planning during reasoning. Experimental results on mathematical reasoning and code generation tasks demonstrate that, with iCLP, LLMs can plan in latent space while reasoning in language space. This approach yields significant improvements in both accuracy and efficiency and, crucially, demonstrates strong cross-domain generalization while preserving the interpretability of chain-of-thought reasoning.",
      "publishedDate": "2025-12-30T06:19:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24014",
      "categories": [
        "code-generation",
        "reasoning",
        "planning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.24008",
      "title": "SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing",
      "authors": [
        "Gaurab Chhetri",
        "Subasish Das",
        "Tausif Islam Chowdhury"
      ],
      "abstract": "Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in which coordinated persona-based large language model (LLM) agents deliver task-specific retrieval and emergent personalization. SPARK formalizes a persona space defined by role, expertise, task context, and domain, and introduces a Persona Coordinator that dynamically interprets incoming queries to activate the most relevant specialized agents. Each agent executes an independent retrieval-augmented generation process, supported by dedicated long- and short-term memory stores and context-aware reasoning modules. Inter-agent collaboration is facilitated through structured communication protocols, including shared memory repositories, iterative debate, and relay-style knowledge transfer. Drawing on principles from cognitive architectures, multi-agent coordination theory, and information retrieval, SPARK models how emergent personalization properties arise from distributed agent behaviors governed by minimal coordination rules. The framework yields testable predictions regarding coordination efficiency, personalization quality, and cognitive load distribution, while incorporating adaptive learning mechanisms for continuous persona refinement. By integrating fine-grained agent specialization with cooperative retrieval, SPARK provides insights for next-generation search systems capable of capturing the complexity, fluidity, and context sensitivity of human information-seeking behavior.",
      "publishedDate": "2025-12-30T06:09:12Z",
      "arxivUrl": "https://arxiv.org/abs/2512.24008",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23988",
      "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
      "authors": [
        "Zhenyu Zhang",
        "Shujian Zhang",
        "John Lambert",
        "Wenxuan Zhou",
        "Zhangyang Wang",
        "Mingqing Chen",
        "Andrew Hard",
        "Rajiv Mathews",
        "Lun Wang"
      ],
      "abstract": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.",
      "publishedDate": "2025-12-30T05:09:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23988",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23982",
      "title": "Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education",
      "authors": [
        "Hung-Fu Chang",
        "MohammadShokrolah Shirazi",
        "Lizhou Cao",
        "Supannika Koolmanojwong Mobasser"
      ],
      "abstract": "Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.",
      "publishedDate": "2025-12-30T04:39:59Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23982",
      "categories": [
        "code-generation",
        "agents",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23966",
      "title": "Efficient Context Scaling with LongCat ZigZag Attention",
      "authors": [
        "Chen Zhang",
        "Yang Bai",
        "Jiahuan Li",
        "Anchun Gui",
        "Keheng Wang",
        "Feifan Liu",
        "Guanyu Wu",
        "Yuwei Jiang",
        "Defei Bu",
        "Li Wei",
        "Haihang Jing",
        "Hongyin Tang",
        "Xin Chen",
        "Xiangzhou Huang",
        "Fengcun Li",
        "Rongxiang Weng",
        "Yulei Qian",
        "Yifan Lu",
        "Yerui Sun",
        "Jingang Wang",
        "Yuchen Xie",
        "Xunliang Cai"
      ],
      "abstract": "We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.",
      "publishedDate": "2025-12-30T03:39:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23966",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23959",
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "authors": [
        "Chulun Zhou",
        "Chunkang Zhang",
        "Guoxin Yu",
        "Fandong Meng",
        "Jie Zhou",
        "Wai Lam",
        "Mo Yu"
      ],
      "abstract": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.",
      "publishedDate": "2025-12-30T03:13:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23959",
      "categories": [
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23932",
      "title": "A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming",
      "authors": [
        "Ioanna Gemou",
        "Evangelos Lamprou"
      ],
      "abstract": "Accurate disease prediction is vital for timely intervention, effective treatment, and reducing medical complications. While symbolic AI has been applied in healthcare, its adoption remains limited due to the effort required for constructing high-quality knowledge bases. This work introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to overcome this barrier. McCoy orchestrates an LLM to translate medical literature into ASP code, combines it with patient data, and processes it using an ASP solver to arrive at the final diagnosis. This integration yields a robust, interpretable prediction framework that leverages the strengths of both paradigms. Preliminary results show McCoy has strong performance on small-scale disease diagnosis tasks.",
      "publishedDate": "2025-12-30T01:32:08Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23932",
      "categories": [
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23881",
      "title": "Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack",
      "authors": [
        "Roee Ziv",
        "Raz Lapid",
        "Moshe Sipper"
      ],
      "abstract": "Audio-language models combine audio encoders with large language models to enable multimodal reasoning, but they also introduce new security vulnerabilities. We propose a universal targeted latent space attack, an encoder-level adversarial attack that manipulates audio latent representations to induce attacker-specified outputs in downstream language generation. Unlike prior waveform-level or input-specific attacks, our approach learns a universal perturbation that generalizes across inputs and speakers and does not require access to the language model. Experiments on Qwen2-Audio-7B-Instruct demonstrate consistently high attack success rates with minimal perceptual distortion, revealing a critical and previously underexplored attack surface at the encoder level of multimodal systems.",
      "publishedDate": "2025-12-29T21:56:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23881",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23880",
      "title": "CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution",
      "authors": [
        "Xu Huang",
        "Junwu Chen",
        "Yuxing Fei",
        "Zhuohan Li",
        "Philippe Schwaller",
        "Gerbrand Ceder"
      ],
      "abstract": "Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from \"LLM + tool use\" to \"LLM + skill acquisition\". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search and code extraction, and self-reflection via introspection and knowledge graph exploration, among others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.",
      "publishedDate": "2025-12-29T21:50:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23880",
      "categories": [
        "agents",
        "tool-use",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23852",
      "title": "Trellis: Learning to Compress Key-Value Memory in Attention Models",
      "authors": [
        "Mahdi Karami",
        "Ali Behrouz",
        "Praneeth Kacham",
        "Vahab Mirrokni"
      ],
      "abstract": "Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications.",
      "publishedDate": "2025-12-29T20:32:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23852",
      "categories": [
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23850",
      "title": "The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models",
      "authors": [
        "Rahul Baxi"
      ],
      "abstract": "Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.",
      "publishedDate": "2025-12-29T20:29:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23850",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23848",
      "title": "Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs",
      "authors": [
        "Yukun Zhang",
        "Stefan Elbl Droguett",
        "Samyak Jain"
      ],
      "abstract": "This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper's top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (>7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.",
      "publishedDate": "2025-12-29T20:24:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23848",
      "categories": [
        "rag",
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23844",
      "title": "From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering",
      "authors": [
        "Tao Dong",
        "Harini Sampath",
        "Ja Young Lee",
        "Sherry Y. Shi",
        "Andrew Macvean"
      ],
      "abstract": "As Large Language Models (LLMs) evolve from code generators into collaborative partners for software engineers, our methods for evaluation are lagging. Current benchmarks, focused on code correctness, fail to capture the nuanced, interactive behaviors essential for successful human-AI partnership. To bridge this evaluation gap, this paper makes two core contributions. First, we present a foundational taxonomy of desirable agent behaviors for enterprise software engineering, derived from an analysis of 91 sets of user-defined agent rules. This taxonomy defines four key expectations of agent behavior: Adhere to Standards and Processes, Ensure Code Quality and Reliability, Solving Problems Effectively, and Collaborating with the User. Second, recognizing that these expectations are not static, we introduce the Context-Adaptive Behavior (CAB) Framework. This emerging framework reveals how behavioral expectations shift along two empirically-derived axes: the Time Horizon (from immediate needs to future ideals), established through interviews with 15 expert engineers, and the Type of Work (from enterprise production to rapid prototyping, for example), identified through a prompt analysis of a prototyping agent. Together, these contributions offer a human-centered foundation for designing and evaluating the next generation of AI agents, moving the field's focus from the correctness of generated code toward the dynamics of true collaborative intelligence.",
      "publishedDate": "2025-12-29T20:18:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23844",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "tool-use",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23836",
      "title": "Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?",
      "authors": [
        "Dingmin Wang",
        "Ji Ma",
        "Shankar Kumar"
      ],
      "abstract": "The success of expanded context windows in Large Language Models (LLMs) has driven increased use of broader context in retrieval-augmented generation. We investigate the use of LLMs for retrieval augmented question answering. While longer contexts make it easier to incorporate targeted knowledge, they introduce more irrelevant information that hinders the model's generation process and degrades its performance. To address the issue, we design an adaptive prompting strategy which involves splitting the retrieved information into smaller chunks and sequentially prompting a LLM to answer the question using each chunk. Adjusting the chunk size allows a trade-off between incorporating relevant information and reducing irrelevant information. Experimental results on three open-domain question answering datasets demonstrate that the adaptive strategy matches the performance of standard prompting while using fewer tokens. Our analysis reveals that when encountering insufficient information, the LLM often generates incorrect answers instead of declining to respond, which constitutes a major source of error. This finding highlights the need for further research into enhancing LLMs' ability to effectively decline requests when faced with inadequate information.",
      "publishedDate": "2025-12-29T19:59:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23836",
      "categories": [
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23808",
      "title": "MiMo-Audio: Audio Language Models are Few-Shot Learners",
      "authors": [
        "Xiaomi LLM-Core Team",
        ":",
        "Dong Zhang",
        "Gang Wang",
        "Jinlong Xue",
        "Kai Fang",
        "Liang Zhao",
        "Rui Ma",
        "Shuhuai Ren",
        "Shuo Liu",
        "Tao Guo",
        "Weiji Zhuang",
        "Xin Zhang",
        "Xingchen Song",
        "Yihan Yan",
        "Yongzhe He",
        "Cici",
        "Bowen Shen",
        "Chengxuan Zhu",
        "Chong Ma",
        "Chun Chen",
        "Heyu Chen",
        "Jiawei Li",
        "Lei Li",
        "Menghang Zhu",
        "Peidian Li",
        "Qiying Wang",
        "Sirui Deng",
        "Weimin Xiong",
        "Wenshan Huang",
        "Wenyu Yang",
        "Yilin Jiang",
        "Yixin Yang",
        "Yuanyuan Tian",
        "Yue Ma",
        "Yue Yu",
        "Zihan Zhang",
        "Zihao Yue",
        "Bangjun Xiao",
        "Bingquan Xia",
        "Bofei Gao",
        "Bowen Ye",
        "Can Cai",
        "Chang Liu",
        "Chenhong He",
        "Chunan Li",
        "Dawei Zhu",
        "Duo Zhang",
        "Fengyuan Shi",
        "Guoan Wang",
        "Hailin Zhang",
        "Hanglong Lv",
        "Hanyu Li",
        "Hao Tian",
        "Heng Qu",
        "Hongshen Xu",
        "Houbin Zhang",
        "Huaqiu Liu",
        "Jiangshan Duo",
        "Jianguang Zuo",
        "Jianyu Wei",
        "Jiebao Xiao",
        "Jinhao Dong",
        "Jun Shi",
        "Junhao Hu",
        "Kainan Bao",
        "Kang Zhou",
        "Linghao Zhang",
        "Meng Chen",
        "Nuo Chen",
        "Peng Zhang",
        "Qianli Chen",
        "Qiantong Wang",
        "Rang Li",
        "Shaohui Liu",
        "Shengfan Wang",
        "Shicheng Li",
        "Shihua Yu",
        "Shijie Cao",
        "Shimao Chen",
        "Shuhao Gu",
        "Weikun Wang",
        "Wenhan Ma",
        "Xiangwei Deng",
        "Xing Yong",
        "Xing Zhang",
        "Xu Wang",
        "Yifan Song",
        "Yihao Zhao",
        "Yingbo Zhao",
        "Yizhao Gao",
        "Yu Cheng",
        "Yu Tu",
        "Yudong Wang",
        "Zhaojun Huang",
        "Zhengju Tang",
        "Zhenru Lin",
        "Zhichao Song",
        "Zhipeng Xu",
        "Zhixian Zheng",
        "Zihan Jiang"
      ],
      "abstract": "Existing audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks (MMSU, MMAU, MMAR, MMAU-Pro), spoken dialogue benchmarks (Big Bench Audio, MultiChallenge Audio) and instruct-TTS evaluations, approaching or surpassing closed-source models. Model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-Audio.",
      "publishedDate": "2025-12-29T19:06:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23808",
      "categories": [
        "evaluation",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23701",
      "title": "Eliciting Behaviors in Multi-Turn Conversations",
      "authors": [
        "Jing Huang",
        "Shujian Zhang",
        "Lun Wang",
        "Andrew Hard",
        "Rajiv Mathews",
        "John Lambert"
      ],
      "abstract": "Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.",
      "publishedDate": "2025-12-29T18:57:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23701",
      "categories": [
        "evaluation",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23686",
      "title": "PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech",
      "authors": [
        "Deepak Babu Piskala"
      ],
      "abstract": "Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families. Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench Code: https://github.com/prdeepakbabu/ProfASR-Bench",
      "publishedDate": "2025-12-29T18:43:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23686",
      "categories": [
        "evaluation",
        "rag",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23676",
      "title": "Web World Models",
      "authors": [
        "Jichen Feng",
        "Yifan Zhang",
        "Chenggong Zhang",
        "Yifu Lu",
        "Shilong Liu",
        "Mengdi Wang"
      ],
      "abstract": "Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.",
      "publishedDate": "2025-12-29T18:31:45Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23676",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23675",
      "title": "End-to-End Test-Time Training for Long Context",
      "authors": [
        "Arnuv Tandon",
        "Karan Dalal",
        "Xinhao Li",
        "Daniel Koceja",
        "Marcel Rød",
        "Sam Buchanan",
        "Xiaolong Wang",
        "Jure Leskovec",
        "Sanmi Koyejo",
        "Tatsunori Hashimoto",
        "Carlos Guestrin",
        "Jed McCaleb",
        "Yejin Choi",
        "Yu Sun"
      ],
      "abstract": "We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.",
      "publishedDate": "2025-12-29T18:30:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23675",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23646",
      "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
      "authors": [
        "Keda Tao",
        "Wenjie Du",
        "Bohan Yu",
        "Weiqiang Wang",
        "Jian Liu",
        "Huan Wang"
      ],
      "abstract": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.",
      "publishedDate": "2025-12-29T17:59:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23646",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23631",
      "title": "BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization",
      "authors": [
        "Iris Xu",
        "Guangtao Zeng",
        "Zexue He",
        "Charles Jin",
        "Aldo Pareja",
        "Dan Gutfreund",
        "Chuang Gan",
        "Zhang-Wei Hong"
      ],
      "abstract": "Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.",
      "publishedDate": "2025-12-29T17:41:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23631",
      "categories": [
        "code-generation",
        "agents",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23611",
      "title": "Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing",
      "authors": [
        "Yuwen Li",
        "Wei Zhang",
        "Zelong Huang",
        "Mason Yang",
        "Jiajun Wu",
        "Shawn Guo",
        "Huahao Hu",
        "Lingyi Sun",
        "Jian Yang",
        "Mingjie Tang",
        "Byran Dai"
      ],
      "abstract": "Enabling Large Language Models (LLMs) to reliably invoke external tools remains a critical bottleneck for autonomous agents. Existing approaches suffer from three fundamental challenges: expensive human annotation for high-quality trajectories, poor generalization to unseen tools, and quality ceilings inherent in single-model synthesis that perpetuate biases and coverage gaps. We introduce InfTool, a fully autonomous framework that breaks these barriers through self-evolving multi-agent synthesis. Given only raw API specifications, InfTool orchestrates three collaborative agents (User Simulator, Tool-Calling Assistant, and MCP Server) to generate diverse, verified trajectories spanning single-turn calls to complex multi-step workflows. The framework establishes a closed loop: synthesized data trains the model via Group Relative Policy Optimization (GRPO) with gated rewards, the improved model generates higher-quality data targeting capability gaps, and this cycle iterates without human intervention. Experiments on the Berkeley Function-Calling Leaderboard (BFCL) demonstrate that InfTool transforms a base 32B model from 19.8% to 70.9% accuracy (+258%), surpassing models 10x larger and rivaling Claude-Opus, and entirely from synthetic data without human annotation.",
      "publishedDate": "2025-12-29T17:12:39Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23611",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.23609",
      "title": "The Big Three in Marriage Talk: LLM-Assisted Analysis of Moral Ethics and Sentiment on Weibo and Xiaohongshu",
      "authors": [
        "Frank Tian-Fang Ye",
        "Xiaozi Gao"
      ],
      "abstract": "China's marriage registrations have declined dramatically, dropping from 13.47 million couples in 2013 to 6.1 million in 2024. Understanding public attitudes toward marriage requires examining not only emotional sentiment but also the moral reasoning underlying these evaluations. This study analyzed 219,358 marriage-related posts from two major Chinese social media platforms (Sina Weibo and Xiaohongshu) using large language model (LLM)-assisted content analysis. Drawing on Shweder's Big Three moral ethics framework, posts were coded for sentiment (positive, negative, neutral) and moral dimensions (Autonomy, Community, Divinity). Results revealed platform differences: Weibo discourse skewed positive, while Xiaohongshu was predominantly neutral. Most posts across both platforms lacked explicit moral framing. However, when moral ethics were invoked, significant associations with sentiment emerged. Posts invoking Autonomy ethics and Community ethics were predominantly negative, whereas Divinity-framed posts tended toward neutral or positive sentiment. These findings suggest that concerns about both personal autonomy constraints and communal obligations drive negative marriage attitudes in contemporary China. The study demonstrates LLMs' utility for scaling qualitative analysis and offers insights for developing culturally informed policies addressing marriage decline in Chinese contexts.",
      "publishedDate": "2025-12-29T17:05:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23609",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23601",
      "title": "Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation",
      "authors": [
        "Manh Hung Nguyen",
        "Adish Singla"
      ],
      "abstract": "Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.",
      "publishedDate": "2025-12-29T16:53:48Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23601",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23568",
      "title": "ThinkGen: Generalized Thinking for Visual Generation",
      "authors": [
        "Siyu Jiao",
        "Yiheng Lin",
        "Yujie Zhong",
        "Qi She",
        "Wei Zhou",
        "Xiaohan Lan",
        "Zilong Huang",
        "Fei Yu",
        "Yingchen Yu",
        "Yunqing Zhao",
        "Yao Zhao",
        "Yunchao Wei"
      ],
      "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen",
      "publishedDate": "2025-12-29T16:08:50Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23568",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23565",
      "title": "RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature",
      "authors": [
        "Hanzheng Li",
        "Xi Fang",
        "Yixuan Li",
        "Chaozheng Huang",
        "Junjie Wang",
        "Xi Wang",
        "Hongzhe Bai",
        "Bojun Hao",
        "Shenyu Lin",
        "Huiqi Liang",
        "Linfeng Zhang",
        "Guolin Ke"
      ],
      "abstract": "The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.",
      "publishedDate": "2025-12-29T16:05:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23565",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23557",
      "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks",
      "authors": [
        "Toqeer Ali Syed",
        "Mishal Ateeq Almutairi",
        "Mahmoud Abdel Moaty"
      ],
      "abstract": "Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.",
      "publishedDate": "2025-12-29T15:54:33Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23557",
      "categories": [
        "agents",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23545",
      "title": "PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis",
      "authors": [
        "Shengyi Hua",
        "Jianfeng Wu",
        "Tianle Shen",
        "Kangzhe Hu",
        "Zhongzhen Huang",
        "Shujuan Ni",
        "Zhihong Zhang",
        "Yuan Li",
        "Zhe Wang",
        "Xiaofan Zhang"
      ],
      "abstract": "Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.",
      "publishedDate": "2025-12-29T15:34:27Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23545",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23518",
      "title": "Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias",
      "authors": [
        "Hazel Kim",
        "Philip Torr"
      ],
      "abstract": "Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.",
      "publishedDate": "2025-12-29T14:52:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23518",
      "categories": [
        "agents",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23515",
      "title": "Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning",
      "authors": [
        "Zuoyou Jiang",
        "Li Zhao",
        "Rui Sun",
        "Ruohan Sun",
        "Zhongjian Li",
        "Jing Li",
        "Daxin Jiang",
        "Zuo Bai",
        "Cheng Hua"
      ],
      "abstract": "Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. Alpha-R1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1.",
      "publishedDate": "2025-12-29T14:50:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23515",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23511",
      "title": "Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving",
      "authors": [
        "Xinyi Zheng",
        "Ningke Li",
        "Xiaokun Luan",
        "Kailong Wang",
        "Ling Shi",
        "Meng Sun",
        "Haoyu Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning. To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.",
      "publishedDate": "2025-12-29T14:48:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23511",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23489",
      "title": "The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction",
      "authors": [
        "Haoyu Pei",
        "Zhongyang Liu",
        "Xiangyi Xiao",
        "Xiaocong Du",
        "Haipeng Zhang",
        "Kunpeng Zhang",
        "Suting Hong"
      ],
      "abstract": "Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.",
      "publishedDate": "2025-12-29T14:20:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23489",
      "categories": [
        "rag",
        "agents",
        "tool-use",
        "reasoning",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23483",
      "title": "TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding",
      "authors": [
        "Zongsheng Cao",
        "Yangfan He",
        "Anran Liu",
        "Feng Chen",
        "Zepeng Wang",
        "Jun Xie"
      ],
      "abstract": "Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \\emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \\emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.",
      "publishedDate": "2025-12-29T14:10:22Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23483",
      "categories": [
        "rag",
        "tool-use",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23480",
      "title": "Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation",
      "authors": [
        "Toqeer Ali Syed",
        "Mohammad Riyaz Belgaum",
        "Salman Jan",
        "Asadullah Abdullah Khan",
        "Saad Said Alqahtani"
      ],
      "abstract": "The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software production. The current paper includes an example of agentic artificial intelligence (AI) based on autonomous software supply chain security that combines large language model (LLM)-based reasoning, reinforcement learning (RL), and multi-agent coordination. The suggested system utilizes specialized security agents coordinated with the help of LangChain and LangGraph, communicates with actual CI/CD environments with the Model Context Protocol (MCP), and documents all the observations and actions in a blockchain security ledger to ensure integrity and auditing. Reinforcement learning can be used to achieve adaptive mitigation strategies that consider the balance between security effectiveness and the operational overhead, and LLMs can be used to achieve semantic vulnerability analysis, as well as explainable decisions. This framework is tested based on simulated pipelines, as well as, actual world CI/CD integrations on GitHub Actions and Jenkins, including injection attacks, insecure deserialization, access control violations, and configuration errors. Experimental outcomes indicate better detection accuracy, shorter mitigation latency and reasonable build-time overhead than rule-based, provenance only and RL only baselines. These results show that agentic AI can facilitate the transition to self defending, proactive software supply chains rather than reactive verification ones.",
      "publishedDate": "2025-12-29T14:06:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23480",
      "categories": [
        "agents",
        "multi-agent",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23471",
      "title": "Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings",
      "authors": [
        "Thomas Haschka",
        "Joseph Bakarji"
      ],
      "abstract": "Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster -- the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 Newsgroups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.",
      "publishedDate": "2025-12-29T13:55:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23471",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23461",
      "title": "Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance",
      "authors": [
        "Zhuo Li",
        "Pengyu Cheng",
        "Zhechao Yu",
        "Feifei Tong",
        "Anningzhe Gao",
        "Tsung-Hui Chang",
        "Xiang Wan",
        "Erchao Zhao",
        "Xiaoxi Jiang",
        "Guanjun Jiang"
      ],
      "abstract": "Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \\textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \\textbf{D}ebiasing via \\textbf{I}nformation optimization for \\textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \\textit{response length}, \\textit{sycophancy}, and \\textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.",
      "publishedDate": "2025-12-29T13:39:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23461",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23457",
      "title": "Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following",
      "authors": [
        "Kongcheng Zhang",
        "Qi Yao",
        "Shunyu Liu",
        "Wenjian Zhang",
        "Min Cen",
        "Yang Zhou",
        "Wenkai Fang",
        "Yiru Zhao",
        "Baisheng Lai",
        "Mingli Song"
      ],
      "abstract": "Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.",
      "publishedDate": "2025-12-29T13:31:08Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23457",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23453",
      "title": "CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models",
      "authors": [
        "Zongsheng Cao",
        "Yangfan He",
        "Anran Liu",
        "Jun Xie",
        "Feng Chen",
        "Zepeng Wang"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \\textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.",
      "publishedDate": "2025-12-29T13:23:20Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23453",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23440",
      "title": "ClinDEF: A Dynamic Evaluation Framework for Large Language Models in Clinical Reasoning",
      "authors": [
        "Yuqi Tang",
        "Jing Yu",
        "Zichang Su",
        "Kehua Feng",
        "Zhihui Zhu",
        "Libin Wang",
        "Lei Liang",
        "Qiang Zhang",
        "Keyan Ding",
        "Huajun Chen"
      ],
      "abstract": "Clinical diagnosis begins with doctor-patient interaction, during which physicians iteratively gather information, determine examination and refine differential diagnosis through patients' response. This dynamic clinical-reasoning process is poorly represented by existing LLM benchmarks that focus on static question-answering. To mitigate these gaps, recent methods explore dynamic medical frameworks involving interactive clinical dialogues. Although effective, they often rely on limited, contamination-prone datasets and lack granular, multi-level evaluation. In this work, we propose ClinDEF, a dynamic framework for assessing clinical reasoning in LLMs through simulated diagnostic dialogues. Grounded in a disease knowledge graph, our method dynamically generates patient cases and facilitates multi-turn interactions between an LLM-based doctor and an automated patient agent. Our evaluation protocol goes beyond diagnostic accuracy by incorporating fine-grained efficiency analysis and rubric-based assessment of diagnostic quality. Experiments show that ClinDEF effectively exposes critical clinical reasoning gaps in state-of-the-art LLMs, offering a more nuanced and clinically meaningful evaluation paradigm.",
      "publishedDate": "2025-12-29T12:58:58Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23440",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.23430",
      "title": "C2PO: Diagnosing and Disentangling Bias Shortcuts in LLMs",
      "authors": [
        "Xuan Feng",
        "Bo An",
        "Tianlong Gu",
        "Liang Chang",
        "Fengrui Hao",
        "Peipeng Yu",
        "Shuai Zhao"
      ],
      "abstract": "Bias in Large Language Models (LLMs) poses significant risks to trustworthiness, manifesting primarily as stereotypical biases (e.g., gender or racial stereotypes) and structural biases (e.g., lexical overlap or position preferences). However, prior paradigms typically address these in isolation, often mitigating one at the expense of exacerbating the other. To address this, we conduct a systematic exploration of these reasoning failures and identify a primary inducement: the latent spurious feature correlations within the input that drive these erroneous reasoning shortcuts. Driven by these findings, we introduce Causal-Contrastive Preference Optimization (C2PO), a unified alignment framework designed to tackle these specific failures by simultaneously discovering and suppressing these correlations directly within the optimization process. Specifically, C2PO leverages causal counterfactual signals to isolate bias-inducing features from valid reasoning paths, and employs a fairness-sensitive preference update mechanism to dynamically evaluate logit-level contributions and suppress shortcut features. Extensive experiments across multiple benchmarks covering stereotypical bias (BBQ, Unqover), structural bias (MNLI, HANS, Chatbot, MT-Bench), out-of-domain fairness (StereoSet, WinoBias), and general utility (MMLU, GSM8K) demonstrate that C2PO effectively mitigates stereotypical and structural biases while preserving robust general reasoning capabilities.",
      "publishedDate": "2025-12-29T12:49:32Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23430",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23385",
      "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?",
      "authors": [
        "The Anh Nguyen",
        "Triet Huynh Minh Le",
        "M. Ali Babar"
      ],
      "abstract": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.",
      "publishedDate": "2025-12-29T11:22:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23385",
      "categories": [
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23367",
      "title": "Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2",
      "authors": [
        "Yilun Luo",
        "HuaQing Zheng",
        "Haoqian Meng",
        "Wenyuan Liu",
        "Peng Zhang"
      ],
      "abstract": "Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.",
      "publishedDate": "2025-12-29T10:50:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23367",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23365",
      "title": "SpatialMosaic: A Multiview VLM Dataset for Partial Visibility",
      "authors": [
        "Kanghee Lee",
        "Injae Lee",
        "Minseok Kwak",
        "Kwonyoung Ryu",
        "Jungi Hong",
        "Jaesik Park"
      ],
      "abstract": "The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.",
      "publishedDate": "2025-12-29T10:48:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23365",
      "categories": [
        "tool-use",
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23356",
      "title": "A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation",
      "authors": [
        "Xin Zhang",
        "Yang Cao",
        "Baoxing Wu",
        "Xinyi Chen",
        "Kai Song",
        "Siying Li"
      ],
      "abstract": "Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.",
      "publishedDate": "2025-12-29T10:35:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23356",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23328",
      "title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations",
      "authors": [
        "Huan-ang Gao",
        "Zikang Zhang",
        "Tianwei Luo",
        "Kaisen Yang",
        "Xinzhe Juan",
        "Jiahao Qiu",
        "Tianxing Chen",
        "Bingxiang He",
        "Hao Zhao",
        "Hao Zhou",
        "Shilong Liu",
        "Mengdi Wang"
      ],
      "abstract": "Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.",
      "publishedDate": "2025-12-29T09:25:56Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23328",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23327",
      "title": "An Empirical Study of Generative AI Adoption in Software Engineering",
      "authors": [
        "Görkem Giray",
        "Onur Demirörs",
        "Marcos Kalinowski",
        "Daniel Mendez"
      ],
      "abstract": "Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.",
      "publishedDate": "2025-12-29T09:24:52Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23327",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23319",
      "title": "Flexible Keyword-Aware Top-$k$ Route Search",
      "authors": [
        "Ziqiang Yu",
        "Xiaohui Yu",
        "Yueting Chen",
        "Wei Liu",
        "Anbang Song",
        "Bolong Zheng"
      ],
      "abstract": "With the rise of Large Language Models (LLMs), tourists increasingly use it for route planning by entering keywords for attractions, instead of relying on traditional manual map services. LLMs provide generally reasonable suggestions, but often fail to generate optimal plans that account for detailed user requirements, given the vast number of potential POIs and possible routes based on POI combinations within a real-world road network. In this case, a route-planning API could serve as an external tool, accepting a sequence of keywords and returning the top-$k$ best routes tailored to user requests. To address this need, this paper introduces the Keyword-Aware Top-$k$ Routes (KATR) query that provides a more flexible and comprehensive semantic to route planning that caters to various user's preferences including flexible POI visiting order, flexible travel distance budget, and personalized POI ratings. Subsequently, we propose an explore-and-bound paradigm to efficiently process KATR queries by eliminating redundant candidates based on estimated score bounds from global to local levels. Extensive experiments demonstrate our approach's superior performance over existing methods across different scenarios.",
      "publishedDate": "2025-12-29T09:10:50Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23319",
      "categories": [
        "tool-use",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.23307",
      "title": "RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking",
      "authors": [
        "Jiawei Liu",
        "Zhuo Chen",
        "Rui Zhu",
        "Miaokun Chen",
        "Yuyang Gong",
        "Wei Lu",
        "Xiaofeng Wang"
      ],
      "abstract": "Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.",
      "publishedDate": "2025-12-29T08:51:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23307",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23304",
      "title": "MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images",
      "authors": [
        "Md. Sazzadul Islam Prottasha",
        "Nabil Walid Rafi"
      ],
      "abstract": "Multimodal Large Language Models (LLMs) introduce an emerging paradigm for medical imaging by interpreting scans through the lens of extensive clinical knowledge, offering a transformative approach to disease classification. This study presents a critical comparison between two fundamentally different AI architectures: the specialized open-source agent MedGemma and the proprietary large multimodal model GPT-4 for diagnosing six different diseases. The MedGemma-4b-it model, fine-tuned using Low-Rank Adaptation (LoRA), demonstrated superior diagnostic capability by achieving a mean test accuracy of 80.37% compared to 69.58% for the untuned GPT-4. Furthermore, MedGemma exhibited notably higher sensitivity in high-stakes clinical tasks, such as cancer and pneumonia detection. Quantitative analysis via confusion matrices and classification reports provides comprehensive insights into model performance across all categories. These results emphasize that domain-specific fine-tuning is essential for minimizing hallucinations in clinical implementation, positioning MedGemma as a sophisticated tool for complex, evidence-based medical reasoning.",
      "publishedDate": "2025-12-29T08:48:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23304",
      "categories": [
        "agents",
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23300",
      "title": "AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration",
      "authors": [
        "Minjiang Huang",
        "Jipeng Qiang",
        "Yi Zhu",
        "Chaowei Zhang",
        "Xiangyu Zhao",
        "Kui Yu"
      ],
      "abstract": "Audiobook interpretations are attracting increasing attention, as they provide accessible and in-depth analyses of books that offer readers practical insights and intellectual inspiration. However, their manual creation process remains time-consuming and resource-intensive. To address this challenge, we propose AI4Reading, a multi-agent collaboration system leveraging large language models (LLMs) and speech synthesis technology to generate podcast, like audiobook interpretations. The system is designed to meet three key objectives: accurate content preservation, enhanced comprehensibility, and a logical narrative structure. To achieve these goals, we develop a framework composed of 11 specialized agents,including topic analysts, case analysts, editors, a narrator, and proofreaders that work in concert to explore themes, extract real world cases, refine content organization, and synthesize natural spoken language. By comparing expert interpretations with our system's output, the results show that although AI4Reading still has a gap in speech generation quality, the generated interpretative scripts are simpler and more accurate.",
      "publishedDate": "2025-12-29T08:41:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23300",
      "categories": [
        "multi-agent",
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23294",
      "title": "Agentic AI-Enhanced Semantic Communications: Foundations, Architecture, and Applications",
      "authors": [
        "Haixiao Gao",
        "Mengying Sun",
        "Ruichen Zhang",
        "Yanhan Wang",
        "Xiaodong Xu",
        "Nan Ma",
        "Dusit Niyato",
        "Ping Zhang"
      ],
      "abstract": "Semantic communications (SemCom), as one of the key technologies for 6G, is shifting networks from bit transmission to semantic information exchange. On this basis, introducing agentic artificial intelligence (AI) with perception, memory, reasoning, and action capabilities provides a practicable path to intelligent communications. This paper provides a systematic exposition of how agentic AI empowers SemCom from the perspectives of research foundations, system architecture, and application scenarios. We first provide a comprehensive review of existing studies by agent types, covering embedded agents, large language model (LLM)/large vision model (LVM) agents, and reinforcement learning (RL) agents. Additionally, we propose a unified agentic AI-enhanced SemCom framework covering the application layer, the semantic layer, and the cloud-edge collaboration layer, forming a closed loop from intent to encoding to transmission to decoding to action to evaluation. We also present several typical scenarios, including multi-vehicle collaborative perception, multi-robot cooperative rescue, and agentic operations for intellicise (intelligent and concise) networks. Furthermore, we introduce an agentic knowledge base (KB)-based joint source-channel coding case study, AKB-JSCC, where the source KB and channel KB are built by LLM/LVM agents and RL agents, respectively. Experimental results show that AKB-JSCC achieves higher information reconstruction quality under different channel conditions. Finally, we discuss future evolution and research directions, providing a reference for portable, verifiable, and controllable research and deployment of agentic SemCom.",
      "publishedDate": "2025-12-29T08:28:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23294",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "code-generation",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23292",
      "title": "Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control",
      "authors": [
        "Yoonpyo Lee",
        "Kazuma Kobayashi",
        "Sai Puppala",
        "Sajedul Talukder",
        "Seid Koric",
        "Souvik Chakraborty",
        "Syed Bahauddin Alam"
      ],
      "abstract": "The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.",
      "publishedDate": "2025-12-29T08:26:27Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23292",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23244",
      "title": "ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing",
      "authors": [
        "Xingwei Ma",
        "Shiyang Feng",
        "Bo Zhang",
        "Bin Wang"
      ],
      "abstract": "Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.",
      "publishedDate": "2025-12-29T06:58:46Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23244",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23243",
      "title": "Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism",
      "authors": [
        "Siyu Zhang",
        "Ying Chen",
        "Lianlei Shan",
        "Runhe Qiu"
      ],
      "abstract": "Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.",
      "publishedDate": "2025-12-29T06:51:20Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23243",
      "categories": [
        "evaluation",
        "planning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23222",
      "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
      "authors": [
        "Jiaxu Zhang",
        "Tianshu Hu",
        "Yuan Zhang",
        "Zenan Li",
        "Linjie Luo",
        "Guosheng Lin",
        "Xin Chen"
      ],
      "abstract": "Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.",
      "publishedDate": "2025-12-29T05:56:22Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23222",
      "categories": [
        "reasoning",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23219",
      "title": "MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?",
      "authors": [
        "Shiqi Dai",
        "Zizhi Ma",
        "Zhicong Luo",
        "Xuesong Yang",
        "Yibin Huang",
        "Wanyue Zhang",
        "Chi Chen",
        "Zonghao Guo",
        "Wang Xu",
        "Yufei Sun",
        "Maosong Sun"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as localization or navigation, without a unified evaluation of MLLMs'general intelligence. To bridge this gap, we present MM-UAVBench, a comprehensive benchmark that systematically evaluates MLLMs across three core capability dimensions-perception, cognition, and planning-in low-altitude UAV scenarios. MM-UAVBench comprises 19 sub-tasks with over 5.7K manually annotated questions, all derived from real-world UAV data collected from public datasets. Extensive experiments on 16 open-source and proprietary MLLMs reveal that current models struggle to adapt to the complex visual and cognitive demands of low-altitude scenarios. Our analyses further uncover critical bottlenecks such as spatial bias and multi-view understanding that hinder the effective deployment of MLLMs in UAV scenarios. We hope MM-UAVBench will foster future research on robust and reliable MLLMs for real-world UAV intelligence.",
      "publishedDate": "2025-12-29T05:49:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23219",
      "categories": [
        "evaluation",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.23217",
      "title": "TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI",
      "authors": [
        "Jingming Li"
      ],
      "abstract": "A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.",
      "publishedDate": "2025-12-29T05:41:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23217",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "rag",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.23214",
      "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation",
      "authors": [
        "Saif Khalfan Saif Al Mazrouei"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.",
      "publishedDate": "2025-12-29T05:28:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23214",
      "categories": [
        "code-generation",
        "evaluation",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23213",
      "title": "Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process",
      "authors": [
        "Zhijun Chen",
        "Zeyu Ji",
        "Qianren Mao",
        "Junhang Cheng",
        "Bangjie Qin",
        "Hao Wu",
        "Zhuoran Li",
        "Jingzheng Li",
        "Kai Sun",
        "Zizhe Wang",
        "Yikun Ban",
        "Zhu Sun",
        "Xiangyang Ji",
        "Hailong Sun"
      ],
      "abstract": "We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.",
      "publishedDate": "2025-12-29T05:25:49Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23213",
      "categories": [
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23206",
      "title": "Not too long do read: Evaluating LLM-generated extreme scientific summaries",
      "authors": [
        "Zhuoqi Lyu",
        "Qing Ke"
      ],
      "abstract": "High-quality scientific extreme summary (TLDR) facilitates effective science communication. How do large language models (LLMs) perform in generating them? How are LLM-generated summaries different from those written by human experts? However, the lack of a comprehensive, high-quality scientific TLDR dataset hinders both the development and evaluation of LLMs' summarization ability. To address these, we propose a novel dataset, BiomedTLDR, containing a large sample of researcher-authored summaries from scientific papers, which leverages the common practice of including authors' comments alongside bibliography items. We then test popular open-weight LLMs for generating TLDRs based on abstracts. Our analysis reveals that, although some of them successfully produce humanoid summaries, LLMs generally exhibit a greater affinity for the original text's lexical choices and rhetorical structures, hence tend to be more extractive rather than abstractive in general, compared to humans. Our code and datasets are available at https://github.com/netknowledge/LLM_summarization (Lyu and Ke, 2025).",
      "publishedDate": "2025-12-29T05:03:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23206",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23184",
      "title": "From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research",
      "authors": [
        "Hongshen Sun",
        "Juanjuan Zhang"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output (\"model choice\") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes \"model belief,\" a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.",
      "publishedDate": "2025-12-29T03:50:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23184",
      "categories": [],
      "year": 2025
    },
    {
      "id": "2512.23180",
      "title": "GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation",
      "authors": [
        "Tianchen Deng",
        "Xuefeng Chen",
        "Yi Chen",
        "Qu Chen",
        "Yuyao Xu",
        "Lijin Yang",
        "Le Xu",
        "Yu Zhang",
        "Bo Zhang",
        "Wuxiong Huang",
        "Hesheng Wang"
      ],
      "abstract": "Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.",
      "publishedDate": "2025-12-29T03:40:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23180",
      "categories": [
        "tool-use",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23173",
      "title": "EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion",
      "authors": [
        "Zhen Liang",
        "Hai Huang",
        "Zhengkui Chen"
      ],
      "abstract": "Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.",
      "publishedDate": "2025-12-29T03:28:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23173",
      "categories": [
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23169",
      "title": "REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation",
      "authors": [
        "Fulin Shi",
        "Wenyi Xiao",
        "Bin Chen",
        "Liang Din",
        "Leilei Gan"
      ],
      "abstract": "Evaluating the alignment between textual prompts and generated images is critical for ensuring the reliability and usability of text-to-image (T2I) models. However, most existing evaluation methods rely on coarse-grained metrics or static QA pipelines, which lack fine-grained interpretability and struggle to reflect human preferences. To address this, we propose REVEALER, a unified framework for element-level alignment evaluation based on reinforcement-guided visual reasoning. Adopting a structured \"grounding-reasoning-conclusion\" paradigm, our method enables Multimodal Large Language Models (MLLMs) to explicitly localize semantic elements and derive interpretable alignment judgments. We optimize the model via Group Relative Policy Optimization(GRPO) using a composite reward function that incorporates structural format, grounding accuracy, and alignment fidelity. Extensive experiments across four benchmarks-EvalMuse-40K, RichHF, MHaluBench, and GenAI-Bench-demonstrate that REVEALER achieves state-of-the-art performance. Our approach consistently outperforms both strong proprietary models and supervised baselines while demonstrating superior inference efficiency compared to existing iterative visual reasoning methods.",
      "publishedDate": "2025-12-29T03:24:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23169",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23167",
      "title": "SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search",
      "authors": [
        "Yifan Zhang",
        "Giridhar Ganapavarapu",
        "Srideepika Jayaraman",
        "Bhavna Agrawal",
        "Dhaval Patel",
        "Achille Fokoue"
      ],
      "abstract": "Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.",
      "publishedDate": "2025-12-29T03:19:42Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23167",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23165",
      "title": "Evaluating Parameter Efficient Methods for RLVR",
      "authors": [
        "Qingyu Yin",
        "Yulun Wu",
        "Zhennan Shen",
        "Sunbowen Li",
        "Zhilin Wang",
        "Yanshu Li",
        "Chak Tou Leong",
        "Jiale Kang",
        "Jinjin Gu"
      ],
      "abstract": "We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.",
      "publishedDate": "2025-12-29T03:13:08Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23165",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.23136",
      "title": "Understanding EFL Learners' Code-Switching and Teachers' Pedagogical Approaches in LLM-Supported Speaking Practice",
      "authors": [
        "Junyeong Park",
        "Jieun Han",
        "Yeon Su Park",
        "Youngbin Lee",
        "Suin Kim",
        "Juho Kim",
        "Alice Oh",
        "So-Yeon Ahn"
      ],
      "abstract": "For English as a Foreign Language (EFL) learners, code-switching (CSW), or alternating between their native language and the target language (English), can lower anxiety and ease communication barriers. Large language models (LLMs), with their multilingual abilities, offer new opportunities to support CSW in speaking practice. Yet, the pedagogical design of LLM-based tutors remains underexplored. To this end, we conducted a six-week study of LLM-mediated speaking practice with 20 Korean EFL learners, alongside a qualitative study with nine English teachers who designed and refined responses to learner CSW. Findings show that learners used CSW not only to bridge lexical gaps but also to express cultural and emotional nuance, prompting teachers to employ selective interventions and dynamic scaffolding strategies. We conclude with design implications for bilingual LLM-powered tutors that leverage teachers' expertise to transform CSW into meaningful learning opportunities.",
      "publishedDate": "2025-12-29T01:54:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23136",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23132",
      "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems",
      "authors": [
        "Armstrong Foundjem",
        "Lionel Nganyewou Tidjon",
        "Leuson Da Silva",
        "Foutse Khomh"
      ],
      "abstract": "Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.",
      "publishedDate": "2025-12-29T01:27:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23132",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23128",
      "title": "It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents",
      "authors": [
        "Karolina Korgul",
        "Yushi Yang",
        "Arkadiusz Drohomirecki",
        "Piotr Błaszczyk",
        "Will Howard",
        "Lukas Aichberger",
        "Chris Russell",
        "Philip H. S. Torr",
        "Adam Mahdi",
        "Adel Bibi"
      ],
      "abstract": "Web-based agents powered by large language models are increasingly used for tasks such as email management or professional networking. Their reliance on dynamic web content, however, makes them vulnerable to prompt injection attacks: adversarial instructions hidden in interface elements that persuade the agent to divert from its original task. We introduce the Task-Redirecting Agent Persuasion Benchmark (TRAP), an evaluation for studying how persuasion techniques misguide autonomous web agents on realistic tasks. Across six frontier models, agents are susceptible to prompt injection in 25\\% of tasks on average (13\\% for GPT-5 to 43\\% for DeepSeek-R1), with small interface or contextual changes often doubling success rates and revealing systemic, psychologically driven vulnerabilities in web-based agents. We also provide a modular social-engineering injection framework with controlled experiments on high-fidelity website clones, allowing for further benchmark expansion.",
      "publishedDate": "2025-12-29T01:09:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23128",
      "categories": [
        "agents",
        "prompting",
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23765",
      "title": "Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning",
      "authors": [
        "Tiancheng Su",
        "Meicong Zhang",
        "Guoxiu He"
      ],
      "abstract": "Speculative decoding (SD) accelerates large language model (LLM) reasoning by using a small draft model to generate candidate tokens, which the target LLM either accepts directly or regenerates upon rejection. However, excessive alignment between the draft and target models constrains SD to the performance of the target LLM. To address this limitation, we propose Entropy-Aware Speculative Decoding (EASD), a training-free enhancement. Building on standard SD, EASD incorporates a dynamic entropy-based penalty. At each decoding step, we employ the entropy of the sampling distribution to quantify model uncertainty. When both models exhibit high entropy with substantial overlap among their top-N predictions, the corresponding token is rejected and re-sampled by the target LLM. This penalty prevents low-confidence errors from propagating. By incorporating draft-model verification, EASD enables the possibility of surpassing the target model's inherent performance. Experiments across multiple reasoning benchmarks demonstrate that EASD consistently outperforms existing SD methods and, in most cases, surpasses the target LLM itself. We further prove that the efficiency of EASD is comparable to that of SD. The code can be found in the Supplementary Materials.",
      "publishedDate": "2025-12-29T00:45:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23765",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23090",
      "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients",
      "authors": [
        "Armin Berger",
        "Manuela Bergau",
        "Helen Schneider",
        "Saad Ahmad",
        "Tom Anglim Lagones",
        "Gianluca Brugnara",
        "Martha Foltyn-Dumitru",
        "Kai Schlamp",
        "Philipp Vollmuth",
        "Rafet Sifa"
      ],
      "abstract": "Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.",
      "publishedDate": "2025-12-28T21:57:42Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23090",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.23073",
      "title": "Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models",
      "authors": [
        "Mingyuan Zhang",
        "Yue Bai",
        "Yifan Wang",
        "Yiyang Huang",
        "Yun Fu"
      ],
      "abstract": "Explorations in fine-tuning Vision-Language Models (VLMs), such as Low-Rank Adaptation (LoRA) from Parameter Efficient Fine-Tuning (PEFT), have made impressive progress. However, most approaches rely on explicit weight updates, overlooking the extensive representational structures already encoded in pre-trained models that remain underutilized. Recent works have demonstrated that Mask Fine-Tuning (MFT) can be a powerful and efficient post-training paradigm for language models. Instead of updating weights, MFT assigns learnable gating scores to each weight, allowing the model to reorganize its internal subnetworks for downstream task adaptation. In this paper, we rethink fine-tuning for VLMs from a structural reparameterization perspective grounded in MFT. We apply MFT to the language and projector components of VLMs with different language backbones and compare against strong PEFT baselines. Experiments show that MFT consistently surpasses LoRA variants and even full fine-tuning, achieving high performance without altering the frozen backbone. Our findings reveal that effective adaptation can emerge not only from updating weights but also from reestablishing connections among the model's existing knowledge. Code available at: https://github.com/Ming-K9/MFT-VLM",
      "publishedDate": "2025-12-28T20:41:22Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23073",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23760",
      "title": "Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory",
      "authors": [
        "Ken Huang",
        "Jerry Huang"
      ],
      "abstract": "Reinforcement learning is increasingly used to transform large language models into agentic systems that act over long horizons, invoke tools, and manage memory under partial observability. While recent work has demonstrated performance gains through tool learning, verifiable rewards, and continual training, deployed self-improving agents raise unresolved security and governance challenges: optimization pressure can incentivize reward hacking, behavioral drift is difficult to audit or reproduce, and improvements are often entangled in opaque parameter updates rather than reusable, verifiable artifacts. This paper proposes Audited Skill-Graph Self-Improvement (ASG-SI), a framework that treats self-improvement as iterative compilation of an agent into a growing, auditable skill graph. Each candidate improvement is extracted from successful trajectories, normalized into a skill with an explicit interface, and promoted only after passing verifier-backed replay and contract checks. Rewards are decomposed into reconstructible components derived from replayable evidence, enabling independent audit of promotion decisions and learning signals. ASG-SI further integrates experience synthesis for scalable stress testing and continual memory control to preserve long-horizon performance under bounded context. We present a complete system architecture, threat model, and security analysis, and provide a fully runnable reference implementation that demonstrates verifier-backed reward construction, skill compilation, audit logging, and measurable improvement under continual task streams. ASG-SI reframes agentic self-improvement as accumulation of verifiable, reusable capabilities, offering a practical path toward reproducible evaluation and operational governance of self-improving AI agents.",
      "publishedDate": "2025-12-28T19:39:47Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23760",
      "categories": [
        "agents",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23049",
      "title": "Accelerating Language Model Workflows with Prompt Choreography",
      "authors": [
        "TJ Bai",
        "Jason Eisner"
      ],
      "abstract": "Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\\times$) in some workflows dominated by redundant computation.",
      "publishedDate": "2025-12-28T19:21:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23049",
      "categories": [
        "code-generation",
        "agents",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23029",
      "title": "Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware",
      "authors": [
        "Alex Khalil",
        "Guillaume Heilles",
        "Maria Parraga",
        "Simon Heilles"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) has been accompanied by a reliance on cloud-based, proprietary systems, raising significant concerns regarding data privacy, operational sovereignty, and escalating costs. This paper investigates the feasibility of deploying a high-performance, private LLM inference server at a cost accessible to Small and Medium Businesses (SMBs). We present a comprehensive benchmarking analysis of a locally hosted, quantized 30-billion parameter Mixture-of-Experts (MoE) model based on Qwen3, running on a consumer-grade server equipped with a next-generation NVIDIA GPU. Unlike cloud-based offerings, which are expensive and complex to integrate, our approach provides an affordable and private solution for SMBs. We evaluate two dimensions: the model's intrinsic capabilities and the server's performance under load. Model performance is benchmarked against academic and industry standards to quantify reasoning and knowledge relative to cloud services. Concurrently, we measure server efficiency through latency, tokens per second, and time to first token, analyzing scalability under increasing concurrent users. Our findings demonstrate that a carefully configured on-premises setup with emerging consumer hardware and a quantized open-source model can achieve performance comparable to cloud-based services, offering SMBs a viable pathway to deploy powerful LLMs without prohibitive costs or privacy compromises.",
      "publishedDate": "2025-12-28T18:08:01Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23029",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.23025",
      "title": "LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models",
      "authors": [
        "Wenxuan Xu",
        "Arvind Pillai",
        "Subigya Nepal",
        "Amanda C Collins",
        "Daniel M Mackin",
        "Michael V Heinz",
        "Tess Z Griffin",
        "Nicholas C Jacobson",
        "Andrew Campbell"
      ],
      "abstract": "Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by transforming Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM's representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.",
      "publishedDate": "2025-12-28T18:00:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23025",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23024",
      "title": "With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs",
      "authors": [
        "Ciprian Constantinescu",
        "Marius Leordeanu"
      ],
      "abstract": "Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model's reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.",
      "publishedDate": "2025-12-28T17:53:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23024",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23020",
      "title": "OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding",
      "authors": [
        "Wenyuan Huang",
        "Zhao Wang",
        "Zhou Wei",
        "Ting Huang",
        "Fang Zhao",
        "Jian Yang",
        "Zhenyu Zhang"
      ],
      "abstract": "3D visual grounding aims to locate objects based on natural language descriptions in 3D scenes. Existing methods rely on a pre-defined Object Lookup Table (OLT) to query Visual Language Models (VLMs) for reasoning about object locations, which limits the applications in scenarios with undefined or unforeseen targets. To address this problem, we present OpenGround, a novel zero-shot framework for open-world 3D visual grounding. Central to OpenGround is the Active Cognition-based Reasoning (ACR) module, which is designed to overcome the fundamental limitation of pre-defined OLTs by progressively augmenting the cognitive scope of VLMs. The ACR module performs human-like perception of the target via a cognitive task chain and actively reasons about contextually relevant objects, thereby extending VLM cognition through a dynamically updated OLT. This allows OpenGround to function with both pre-defined and open-world categories. We also propose a new dataset named OpenTarget, which contains over 7000 object-description pairs to evaluate our method in open-world scenarios. Extensive experiments demonstrate that OpenGround achieves competitive performance on Nr3D, state-of-the-art on ScanRefer, and delivers a substantial 17.6% improvement on OpenTarget. Project Page at https://why-102.github.io/openground.io/.",
      "publishedDate": "2025-12-28T17:44:20Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23020",
      "categories": [
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.22966",
      "title": "Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks",
      "authors": [
        "Mengdi Chai",
        "Ali R. Zomorrodi"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM's out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.",
      "publishedDate": "2025-12-28T15:15:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22966",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22955",
      "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
      "authors": [
        "Haoyuan Wu",
        "Hai Wang",
        "Jiajia Wu",
        "Jinxiang Ou",
        "Keyao Wang",
        "Weile Chen",
        "Zihao Zheng",
        "Bei Yu"
      ],
      "abstract": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.",
      "publishedDate": "2025-12-28T14:53:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22955",
      "categories": [
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22944",
      "title": "Multiple Token Divergence: Measuring and Steering In-Context Computation Density",
      "authors": [
        "Vincent Herrmann",
        "Eric Alcaide",
        "Michael Wand",
        "Jürgen Schmidhuber"
      ],
      "abstract": "Measuring the in-context computational effort of language models is a key challenge, as metrics like next-token loss fail to capture reasoning complexity. Prior methods based on latent state compressibility can be invasive and unstable. We propose Multiple Token Divergence (MTD), a simple measure of computational effort defined as the KL divergence between a model's full output distribution and that of a shallow, auxiliary prediction head. MTD can be computed directly from pre-trained models with multiple prediction heads, requiring no additional training. Building on this, we introduce Divergence Steering, a novel decoding method to control the computational character of generated text. We empirically show that MTD is more effective than prior methods at distinguishing complex tasks from simple ones. On mathematical reasoning benchmarks, MTD correlates positively with problem difficulty. Lower MTD is associated with more accurate reasoning. MTD provides a practical, lightweight tool for analyzing and steering the computational dynamics of language models.",
      "publishedDate": "2025-12-28T14:13:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22944",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22939",
      "title": "ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving",
      "authors": [
        "Qihang Peng",
        "Xuesong Chen",
        "Chenye Yang",
        "Shaoshuai Shi",
        "Hongsheng Li"
      ],
      "abstract": "Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.",
      "publishedDate": "2025-12-28T14:06:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22939",
      "categories": [
        "code-generation",
        "agents",
        "reasoning",
        "planning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22933",
      "title": "Multimodal Fact-Checking: An Agent-based Approach",
      "authors": [
        "Danni Xu",
        "Shaojing Fan",
        "Harry Cheng",
        "Mohan Kankanhalli"
      ],
      "abstract": "The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.",
      "publishedDate": "2025-12-28T13:58:33Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22933",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.22925",
      "title": "Argus: Token Aware Distributed LLM Inference Optimization",
      "authors": [
        "Panlong Wu",
        "Yifei Zhong",
        "Danyang Chen",
        "Ting Wang",
        "Fangxin Wang"
      ],
      "abstract": "Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.",
      "publishedDate": "2025-12-28T13:38:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22925",
      "categories": [
        "code-generation",
        "tool-use",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22899",
      "title": "HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery",
      "authors": [
        "Yaping Zhang",
        "Qixuan Zhang",
        "Xingquan Zhang",
        "Zhiyuan Chen",
        "Wenwen Zhuang",
        "Yupu Liang",
        "Lu Xiang",
        "Yang Zhao",
        "Jiajun Zhang",
        "Yu Zhou",
        "Chengqing Zong"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \\textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \\textit{Scientific Literacy} (L1), \\textit{Literature Parsing} (L2), \\textit{Literature-based Question Answering} (L3), \\textit{Literature Review Generation} (L4), and \\textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\\% accuracy on basic literacy tasks, performance declines sharply to 25\\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.",
      "publishedDate": "2025-12-28T12:08:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22899",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.22883",
      "title": "Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations",
      "authors": [
        "Tao Li",
        "Quanyan Zhu"
      ],
      "abstract": "Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.",
      "publishedDate": "2025-12-28T11:17:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22883",
      "categories": [
        "agents",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22867",
      "title": "MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments",
      "authors": [
        "Zhuonan Liu",
        "Xinyu Zhang",
        "Zishuo Wang",
        "Tomohito Kawabata",
        "Xuesu Xiao",
        "Ling Xiao"
      ],
      "abstract": "Socially compliant navigation requires structured reasoning over dynamic pedestrians and physical constraints to ensure safe and interpretable decisions. However, existing social navigation datasets often lack explicit reasoning supervision and exhibit highly long-tailed action distributions, limiting models' ability to learn safety-critical behaviors. To address these issues, we introduce MUSON, a multimodal dataset for short-horizon social navigation collected across diverse indoor and outdoor campus scenes. MUSON adopts a structured five-step Chain-of-Thought annotation consisting of perception, prediction, reasoning, action, and explanation, with explicit modeling of static physical constraints and a rationally balanced discrete action space. Compared to SNEI, MUSON provides consistent reasoning, action, and explanation. Benchmarking multiple state-of-the-art Small Vision Language Models on MUSON shows that Qwen2.5-VL-3B achieves the highest decision accuracy of 0.8625, demonstrating that MUSON serves as an effective and reusable benchmark for socially compliant navigation. The dataset is publicly available at https://huggingface.co/datasets/MARSLab/MUSON",
      "publishedDate": "2025-12-28T10:41:39Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22867",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22827",
      "title": "FasterPy: An LLM-based Code Execution Efficiency Optimization Framework",
      "authors": [
        "Yue Wu",
        "Minghao Han",
        "Ruiyin Li",
        "Peng Liang",
        "Amjed Tahir",
        "Zengyang Li",
        "Qiong Feng",
        "Mojtaba Shahin"
      ],
      "abstract": "Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.",
      "publishedDate": "2025-12-28T07:43:08Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22827",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22799",
      "title": "VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM",
      "authors": [
        "Jingchao Wang",
        "Kaiwen Zhou",
        "Zhijian Wu",
        "Kunhua Ji",
        "Dingjiang Huang",
        "Yefeng Zheng"
      ],
      "abstract": "Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.",
      "publishedDate": "2025-12-28T06:12:28Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22799",
      "categories": [
        "prompting",
        "tool-use",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22795",
      "title": "CNSight: Evaluation of Clinical Note Segmentation Tools",
      "authors": [
        "Risha Surana",
        "Adrian Law",
        "Sunwoo Kim",
        "Rishab Sridhar",
        "Angxiao Han",
        "Peiyu Hong"
      ],
      "abstract": "Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.",
      "publishedDate": "2025-12-28T05:40:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22795",
      "categories": [
        "tool-use",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22768",
      "title": "Understanding the Mechanisms of Fast Hyperparameter Transfer",
      "authors": [
        "Nikhil Ghosh",
        "Denny Wu",
        "Alberto Bietti"
      ],
      "abstract": "The growing scale of deep learning models has rendered standard hyperparameter (HP) optimization prohibitively expensive. A promising solution is the use of scale-aware hyperparameters, which can enable direct transfer of optimal HPs from small-scale grid searches to large models with minimal performance loss. To understand the principles governing such transfer strategy, we develop a general conceptual framework for reasoning about HP transfer across scale, characterizing transfer as fast when the suboptimality it induces vanishes asymptotically faster than the finite-scale performance gap. We show formally that fast transfer is equivalent to useful transfer for compute-optimal grid search, meaning that transfer is asymptotically more compute-efficient than direct tuning. While empirical work has found that the Maximal Update Parameterization ($μ$P) exhibits fast transfer when scaling model width, the mechanisms remain poorly understood. We show that this property depends critically on problem structure by presenting synthetic settings where transfer either offers provable computational advantage or fails to outperform direct tuning even under $μ$P. To explain the fast transfer observed in practice, we conjecture that decomposing the optimization trajectory reveals two contributions to loss reduction: (1) a width-stable component that determines the optimal HPs, and (2) a width-sensitive component that improves with width but weakly perturbs the HP optimum. We present empirical evidence for this hypothesis across various settings, including large language model pretraining.",
      "publishedDate": "2025-12-28T04:13:00Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22768",
      "categories": [
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22744",
      "title": "Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL",
      "authors": [
        "Rihong Qiu",
        "Zhibang Yang",
        "Xinke Jiang",
        "Weibin Liao",
        "Xin Gao",
        "Xu Chu",
        "Junfeng Zhao",
        "Yasha Wang"
      ],
      "abstract": "Text-to-SQL translates natural language questions into SQL statements grounded in a target database schema. Ensuring the reliability and executability of such systems requires validating generated SQL, but most existing approaches focus only on syntactic correctness, with few addressing semantic validation (detecting misalignments between questions and SQL). As a consequence, effective semantic validation still faces two key challenges: capturing both global user intent and SQL structural details, and constructing high-quality fine-grained sub-SQL annotations. To tackle these, we introduce HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans, LPs) and local details (via Abstract Syntax Trees, ASTs). To enable better information propagation, we employ a Nested Message Passing Neural Network (NMPNN) to capture inherent relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. Additionally, to generate high-quality negative samples, we propose an AST-driven sub-SQL augmentation strategy, supporting robust optimization of fine-grained semantic inconsistencies. Extensive experiments conducted on Text-to-SQL validation benchmarks (both in-domain and out-of-domain settings) demonstrate that our approach outperforms existing state-of-the-art methods, achieving an average 9.40% improvement of AUPRC and 12.35% of AUROC in identifying semantic inconsistencies. It excels at detecting fine-grained semantic errors, provides large language models with more granular feedback, and ultimately enhances the reliability and interpretability of data querying platforms.",
      "publishedDate": "2025-12-28T02:25:33Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22744",
      "categories": [
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22742",
      "title": "Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning",
      "authors": [
        "Hanze Meng",
        "Jianhao Cao",
        "Rachel Pottinger"
      ],
      "abstract": "Column Type Annotation (CTA) is a fundamental step towards enabling schema alignment and semantic understanding of tabular data. Existing encoder-only language models achieve high accuracy when fine-tuned on labeled columns, but their applicability is limited to in-domain settings, as distribution shifts in tables or label spaces require costly re-training from scratch. Recent work has explored prompting generative large language models (LLMs) by framing CTA as a multiple-choice task, but these approaches face two key challenges: (1) model performance is highly sensitive to subtle changes in prompt wording and structure, and (2) annotation F1 scores remain modest. A natural extension is to fine-tune large language models. However, fully fine-tuning these models incurs prohibitive computational costs due to their scale, and the sensitivity to prompts is not eliminated. In this paper, we present a parameter-efficient framework for CTA that trains models over prompt-augmented data via Low-Rank Adaptation (LoRA). Our approach mitigates sensitivity to prompt variations while drastically reducing the number of necessary trainable parameters, achieving robust performance across datasets and templates. Experimental results on recent benchmarks demonstrate that models fine-tuned with our prompt augmentation strategy maintain stable performance across diverse prompt patterns during inference and yield higher weighted F1 scores than those fine-tuned on a single prompt template. These results highlight the effectiveness of parameter-efficient training and augmentation strategies in developing practical and adaptable CTA systems.",
      "publishedDate": "2025-12-28T02:04:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22742",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22741",
      "title": "Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment for Multi-Modal Sentiment Analysis",
      "authors": [
        "Dongning Rao",
        "Yunbiao Zeng",
        "Zhihua Jiang",
        "Jujian Lv"
      ],
      "abstract": "Human-interaction-involved applications underscore the need for Multi-modal Sentiment Analysis (MSA). Although many approaches have been proposed to address the subtle emotions in different modalities, the power of explanations and temporal alignments is still underexplored. Thus, this paper proposes the Text-routed sparse mixture-of-Experts model with eXplanation and Temporal alignment for MSA (TEXT). TEXT first augments explanations for MSA via Multi-modal Large Language Models (MLLM), and then novelly aligns the epresentations of audio and video through a temporality-oriented neural network block. TEXT aligns different modalities with explanations and facilitates a new text-routed sparse mixture-of-experts with gate fusion. Our temporal alignment block merges the benefits of Mamba and temporal cross-attention. As a result, TEXT achieves the best performance cross four datasets among all tested models, including three recently proposed approaches and three MLLMs. TEXT wins on at least four metrics out of all six metrics. For example, TEXT decreases the mean absolute error to 0.353 on the CH-SIMS dataset, which signifies a 13.5% decrement compared with recently proposed approaches.",
      "publishedDate": "2025-12-28T01:58:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22741",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22738",
      "title": "Harnessing Large Language Models for Biomedical Named Entity Recognition",
      "authors": [
        "Jian Chen",
        "Leilei Su",
        "Cong Sun"
      ],
      "abstract": "Background and Objective: Biomedical Named Entity Recognition (BioNER) is a foundational task in medical informatics, crucial for downstream applications like drug discovery and clinical trial matching. However, adapting general-domain Large Language Models (LLMs) to this task is often hampered by their lack of domain-specific knowledge and the performance degradation caused by low-quality training data. To address these challenges, we introduce BioSelectTune, a highly efficient, data-centric framework for fine-tuning LLMs that prioritizes data quality over quantity. Methods and Results: BioSelectTune reformulates BioNER as a structured JSON generation task and leverages our novel Hybrid Superfiltering strategy, a weak-to-strong data curation method that uses a homologous weak model to distill a compact, high-impact training dataset. Conclusions: Through extensive experiments, we demonstrate that BioSelectTune achieves state-of-the-art (SOTA) performance across multiple BioNER benchmarks. Notably, our model, trained on only 50% of the curated positive data, not only surpasses the fully-trained baseline but also outperforms powerful domain-specialized models like BioMedBERT.",
      "publishedDate": "2025-12-28T01:34:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22738",
      "categories": [
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22737",
      "title": "WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference",
      "authors": [
        "Aiwei Liu",
        "Minghua He",
        "Shaoxun Zeng",
        "Sijun Zhang",
        "Linhao Zhang",
        "Chuhan Wu",
        "Wei Jia",
        "Yuan Liu",
        "Xiao Zhou",
        "Jie Zhou"
      ],
      "abstract": "Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice.",
      "publishedDate": "2025-12-28T01:25:48Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22737",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22725",
      "title": "Mitigating Social Desirability Bias in Random Silicon Sampling",
      "authors": [
        "Sashank Chapala",
        "Maksym Mironov",
        "Songgaojun Deng"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used to simulate population responses, a method known as ``Silicon Sampling''. However, responses to socially sensitive questions frequently exhibit Social Desirability Bias (SDB), diverging from real human data toward socially acceptable answers. Existing studies on social desirability bias in LLM-based sampling remain limited. In this work, we investigate whether minimal, psychologically grounded prompt wording can mitigate this bias and improve alignment between silicon and human samples. We conducted a study using data from the American National Election Study (ANES) on three LLMs from two model families: the open-source Llama-3.1 series and GPT-4.1-mini. We first replicate a baseline silicon sampling study, confirming the persistent Social Desirability Bias. We then test four prompt-based mitigation methods: \\emph{reformulated} (neutral, third-person phrasing), \\emph{reverse-coded} (semantic inversion), and two meta-instructions, \\emph{priming} and \\emph{preamble}, respectively encouraging analytics and sincerity. Alignment with ANES is evaluated using Jensen-Shannon Divergence with bootstrap confidence intervals. Our results demonstrate that reformulated prompts most effectively improve alignment by reducing distribution concentration on socially acceptable answers and achieving distributions closer to ANES. Reverse-coding produced mixed results across eligible items, while the Priming and Preamble encouraged response uniformity and showed no systematic benefit for bias mitigation. Our findings validate the efficacy of prompt-based framing controls in mitigating inherent Social Desirability Bias in LLMs, providing a practical path toward more representative silicon samples.",
      "publishedDate": "2025-12-27T23:21:32Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22725",
      "categories": [
        "prompting",
        "code-generation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.22721",
      "title": "Cyber Resilience in Next-Generation Networks: Threat Landscape, Theoretical Foundations, and Design Paradigms",
      "authors": [
        "Junaid Farooq",
        "Quanyan Zhu"
      ],
      "abstract": "The evolution of networked systems, driven by innovations in software-defined networking (SDN), network function virtualization (NFV), open radio access networks (O-RAN), and cloud-native architectures, is redefining both the operational landscape and the threat surface of critical infrastructures. This book offers an in-depth, interdisciplinary examination of how resilience must be re-conceptualized and re-engineered to address the multifaceted challenges posed by these transformations. Structured across six chapters, this book begins by surveying the contemporary risk landscape, identifying emerging cyber, physical, and AI-driven threats, and analyzing their implications for scalable, heterogeneous network environments. It then establishes rigorous definitions and evaluation frameworks for resilience, going beyond robustness and fault-tolerance to address adaptive, anticipatory, and retrospective mechanisms across diverse application domains. The core of the book delves into advanced paradigms and practical strategies for resilience, including zero trust architectures, game-theoretic threat modeling, and self-healing design principles. A significant portion is devoted to the role of artificial intelligence, especially reinforcement learning and large language models (LLMs), in enabling dynamic threat response, autonomous network control, and multi-agent coordination under uncertainty.",
      "publishedDate": "2025-12-27T23:00:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22721",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22716",
      "title": "Memento-II: Learning by Stateful Reflective Memory",
      "authors": [
        "Jun Wang"
      ],
      "abstract": "We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.",
      "publishedDate": "2025-12-27T22:15:03Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22716",
      "categories": [
        "agents",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22712",
      "title": "Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages",
      "authors": [
        "Anaelia Ovalle",
        "Candace Ross",
        "Sebastian Ruder",
        "Adina Williams",
        "Karen Ullrich",
        "Mark Ibrahim",
        "Levent Sagun"
      ],
      "abstract": "Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.",
      "publishedDate": "2025-12-27T21:55:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22712",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22695",
      "title": "Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference",
      "authors": [
        "Mona Moghadampanah",
        "Adib Rezaei Shahmirzadi",
        "Farhana Amin",
        "Dimitrios S. Nikolopoulos"
      ],
      "abstract": "Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.",
      "publishedDate": "2025-12-27T19:49:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22695",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22673",
      "title": "TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning",
      "authors": [
        "Xiang Cheng",
        "Yulan Hu",
        "Xiangwen Zhang",
        "Lu Xu",
        "Zheng Pan",
        "Xin Li",
        "Yong Liu"
      ],
      "abstract": "Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.",
      "publishedDate": "2025-12-27T18:25:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22673",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "evaluation",
        "planning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.22650",
      "title": "Scaling Unverifiable Rewards: A Case Study on Visual Insights",
      "authors": [
        "Shuyu Gan",
        "James Mooney",
        "Pan Hao",
        "Renxiang Wang",
        "Mingyi Hong",
        "Qianwen Wang",
        "Dongyeop Kang"
      ],
      "abstract": "Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-agent pipeline, instead of repeated refinement over time by prior work. By distributing compute across stages and pruning low-quality branches early using process-specific judges, Selective TTS mitigates the judge drift and stabilizes refinement. Grounded in the data science pipeline, we build an end-to-end multi-agent pipeline for generating visually insightful charts and report of given dataset, and design a reliable LLM-based judge model, aligned with human experts (Kendall's τ=0.55). Our proposed selective TTS then improves insight quality under a fixed compute budget, increasing mean scores from 61.64 to 65.86 while reducing variance. We hope our findings serve as the first step toward to scaling complex, open-ended tasks with unverifiable rewards, such as scientific discovery and story generation.",
      "publishedDate": "2025-12-27T17:01:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22650",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.22631",
      "title": "Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs",
      "authors": [
        "Hadi Mohammadi",
        "Tamas Kozak",
        "Anastasia Giachanou"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning has emerged as a powerful technique for improving the problem-solving capabilities of large language models (LLMs), particularly for tasks requiring multi-step reasoning. However, recent studies show that CoT explanations often fail to reflect the model's actual reasoning process, as models may produce coherent yet misleading justifications or modify answers without acknowledging external cues. Such discrepancies undermine the reliability of CoT-based methods for safety supervision and alignment monitoring, as models can generate plausible but deceptive rationales for incorrect answers. To better understand this limitation, we evaluate two optimization methods, Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO), in their ability to improve CoT faithfulness. Our experiments show that GRPO achieves higher performance than DPO in larger models, with the Qwen2.5-14B-Instruct model attaining the best results across all evaluation metrics. Both approaches exhibit positive correlations between model size and performance, but GRPO shows greater potential for improving faithfulness metrics, albeit with less stable behavior at smaller scales. These results suggest that GRPO offers a promising direction for developing more transparent and trustworthy reasoning in LLMs.",
      "publishedDate": "2025-12-27T16:07:00Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22631",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22629",
      "title": "DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation",
      "authors": [
        "Shiyan Liu",
        "Jian Ma",
        "Rui Qu"
      ],
      "abstract": "As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\\{A, B, Tie\\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \\log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.",
      "publishedDate": "2025-12-27T16:02:00Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22629",
      "categories": [
        "evaluation",
        "rag",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22628",
      "title": "M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation",
      "authors": [
        "Fanglin Xu",
        "Wei Zhang",
        "Jian Yang",
        "Guo Chen",
        "Aishan Liu",
        "Zhoujun Li",
        "Xianglong Liu",
        "Bryan Dai"
      ],
      "abstract": "The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in large language models (LLMs) across four levels: Class, Function, Block, and Line. Spanning 18 programming languages, M2G-Eval includes 17K+ training tasks and 1,286 human-annotated, contamination-controlled test instances. We develop M2G-Eval-Coder models by training Qwen3-8B with supervised fine-tuning and Group Relative Policy Optimization. Evaluating 30 models (28 state-of-the-art LLMs plus our two M2G-Eval-Coder variants) reveals three main findings: (1) an apparent difficulty hierarchy, with Line-level tasks easiest and Class-level most challenging; (2) widening performance gaps between full- and partial-granularity languages as task complexity increases; and (3) strong cross-language correlations, suggesting that models learn transferable programming concepts. M2G-Eval enables fine-grained diagnosis of code generation capabilities and highlights persistent challenges in synthesizing complex, long-form code.",
      "publishedDate": "2025-12-27T16:00:46Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22628",
      "categories": [
        "code-generation",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22627",
      "title": "Chain-of-thought Reviewing and Correction for Time Series Question Answering",
      "authors": [
        "Chen Su",
        "Yuanhe Tian",
        "Yan Song"
      ],
      "abstract": "With the advancement of large language models (LLMs), diverse time series analysis tasks are reformulated as time series question answering (TSQA) through a unified natural language interface. However, existing LLM-based approaches largely adopt general natural language processing techniques and are prone to reasoning errors when handling complex numerical sequences. Different from purely textual tasks, time series data are inherently verifiable, enabling consistency checking between reasoning steps and the original input. Motivated by this property, we propose T3LLM, which performs multi-step reasoning with an explicit correction mechanism for time series question answering. The T3LLM framework consists of three LLMs, namely, a worker, a reviewer, and a student, that are responsible for generation, review, and reasoning learning, respectively. Within this framework, the worker generates step-wise chains of thought (CoT) under structured prompts, while the reviewer inspects the reasoning, identifies erroneous steps, and provides corrective comments. The collaboratively generated corrected CoT are used to fine-tune the student model, internalizing multi-step reasoning and self-correction into its parameters. Experiments on multiple real-world TSQA benchmarks demonstrate that T3LLM achieves state-of-the-art performance over strong LLM-based baselines.",
      "publishedDate": "2025-12-27T15:54:18Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22627",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22615",
      "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
      "authors": [
        "Jiacheng Ye",
        "Shansan Gong",
        "Jiahui Gao",
        "Junming Fan",
        "Shuang Wu",
        "Wei Bi",
        "Haoli Bai",
        "Lifeng Shang",
        "Lingpeng Kong"
      ],
      "abstract": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.",
      "publishedDate": "2025-12-27T14:46:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22615",
      "categories": [
        "robotics",
        "planning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22608",
      "title": "LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation",
      "authors": [
        "Zhongyang Liu",
        "Haoyu Pei",
        "Xiangyi Xiao",
        "Xiaocong Du",
        "Yihui Li",
        "Suting Hong",
        "Kunpeng Zhang",
        "Haipeng Zhang"
      ],
      "abstract": "Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.",
      "publishedDate": "2025-12-27T14:34:44Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22608",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22568",
      "title": "Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI",
      "authors": [
        "Rajesh P. N. Rao",
        "Vishwas Sathish",
        "Linxing Preston Jiang",
        "Matthew Bryan",
        "Prashant Rangarajan"
      ],
      "abstract": "The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.",
      "publishedDate": "2025-12-27T11:54:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22568",
      "categories": [
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22560",
      "title": "RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure",
      "authors": [
        "Wei Gao",
        "Yuheng Zhao",
        "Tianyuan Wu",
        "Shaopan Xiong",
        "Weixun Wang",
        "Dakai An",
        "Lunxi Cao",
        "Dilxat Muhtar",
        "Zichen Liu",
        "Haizhou Zhao",
        "Ju Huang",
        "Siran Yang",
        "Yongbin Li",
        "Wenbo Su",
        "Jiamang Wang",
        "Lin Qu",
        "Bo Zheng",
        "Wei Wang"
      ],
      "abstract": "Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages. We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\\(\\times\\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.",
      "publishedDate": "2025-12-27T11:14:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22560",
      "categories": [
        "agents",
        "code-generation",
        "planning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.22519",
      "title": "Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding",
      "authors": [
        "Khoa Vo",
        "Taisei Hanyu",
        "Yuki Ikebe",
        "Trong Thang Pham",
        "Nhat Chung",
        "Minh Nhat Vu",
        "Duy Nguyen Ho Minh",
        "Anh Nguyen",
        "Anthony Gunderman",
        "Chase Rainwater",
        "Ngan Le"
      ],
      "abstract": "Recent Vision-Language-Action (VLA) models have made impressive progress toward general-purpose robotic manipulation by post-training large Vision-Language Models (VLMs) for action prediction. Yet most VLAs entangle perception and control in a monolithic pipeline optimized purely for action, which can erode language-conditioned grounding. In our real-world tabletop tests, policies over-grasp when the target is absent, are distracted by clutter, and overfit to background appearance. To address these issues, we propose OBEYED-VLA (OBject-centric and gEometrY groundED VLA), a framework that explicitly disentangles perceptual grounding from action reasoning. Instead of operating directly on raw RGB, OBEYED-VLA augments VLAs with a perception module that grounds multi-view inputs into task-conditioned, object-centric, and geometry-aware observations. This module includes a VLM-based object-centric grounding stage that selects task-relevant object regions across camera views, along with a complementary geometric grounding stage that emphasizes the 3D structure of these objects over their appearance. The resulting grounded views are then fed to a pretrained VLA policy, which we fine-tune exclusively on single-object demonstrations collected without environmental clutter or non-target objects. On a real-world UR10e tabletop setup, OBEYED-VLA substantially improves robustness over strong VLA baselines across four challenging regimes and multiple difficulty levels: distractor objects, absent-target rejection, background appearance changes, and cluttered manipulation of unseen objects. Ablation studies confirm that both semantic grounding and geometry-aware grounding are critical to these gains. Overall, the results indicate that making perception an explicit, object-centric component is an effective way to strengthen and generalize VLA-based robotic manipulation.",
      "publishedDate": "2025-12-27T08:31:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22519",
      "categories": [
        "reasoning",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22508",
      "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals",
      "authors": [
        "Lucky Susanto",
        "Anasta Pranawijayana",
        "Cortino Sukotjo",
        "Soni Prasad",
        "Derry Wijaya"
      ],
      "abstract": "Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.",
      "publishedDate": "2025-12-27T07:51:50Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22508",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22496",
      "title": "Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring",
      "authors": [
        "Saisab Sadhu",
        "Ashim Dhor"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as automated tutors to address educator shortages; however, they often fail at pedagogical reasoning, frequently validating incorrect student solutions (sycophancy) or providing overly direct answers that hinder learning. We introduce Hierarchical Pedagogical Oversight (HPO), a framework that adapts structured adversarial synthesis to educational assessment. Unlike cooperative multi-agent systems that often drift toward superficial consensus, HPO enforces a dialectical separation of concerns: specialist agents first distill dialogue context, which then grounds a moderated, five-act debate between opposing pedagogical critics. We evaluate this framework on the MRBench dataset of 1,214 middle-school mathematics dialogues. Our 8B-parameter model achieves a Macro F1 of 0.845, outperforming GPT-4o (0.812) by 3.3% while using 20 times fewer parameters. These results establish adversarial reasoning as a critical mechanism for deploying reliable, low-compute pedagogical oversight in resource-constrained environments.",
      "publishedDate": "2025-12-27T06:42:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22496",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22473",
      "title": "Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds",
      "authors": [
        "Naman Aggarwal",
        "Siddhartha R. Dalal",
        "Vishal Misra"
      ],
      "abstract": "Transformers empirically perform precise probabilistic reasoning in carefully constructed ``Bayesian wind tunnels'' and in large-scale language models, yet the mechanisms by which gradient-based learning creates the required internal geometry remain opaque. We provide a complete first-order analysis of how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. Our core result is an \\emph{advantage-based routing law} for attention scores, \\[ \\frac{\\partial L}{\\partial s_{ij}} = α_{ij}\\bigl(b_{ij}-\\mathbb{E}_{α_i}[b]\\bigr), \\qquad b_{ij} := u_i^\\top v_j, \\] coupled with a \\emph{responsibility-weighted update} for values, \\[ Δv_j = -η\\sum_i α_{ij} u_i, \\] where $u_i$ is the upstream gradient at position $i$ and $α_{ij}$ are attention weights. These equations induce a positive feedback loop in which routing and content specialize together: queries route more strongly to values that are above-average for their error signal, and those values are pulled toward the queries that use them. We show that this coupled specialization behaves like a two-timescale EM procedure: attention weights implement an E-step (soft responsibilities), while values implement an M-step (responsibility-weighted prototype updates), with queries and keys adjusting the hypothesis frame. Through controlled simulations, including a sticky Markov-chain task where we compare a closed-form EM-style update to standard SGD, we demonstrate that the same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional manifolds identified in our companion work as implementing Bayesian inference. This yields a unified picture in which optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (in-context probabilistic reasoning).",
      "publishedDate": "2025-12-27T05:31:44Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22473",
      "categories": [
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23752",
      "title": "Geometric Scaling of Bayesian Inference in LLMs",
      "authors": [
        "Naman Aggarwal",
        "Siddhartha R. Dalal",
        "Vishal Misra"
      ],
      "abstract": "Recent work has shown that small transformers trained in controlled \"wind-tunnel'' settings can implement exact Bayesian inference, and that their training dynamics produce a geometric substrate -- low-dimensional value manifolds and progressively orthogonal keys -- that encodes posterior structure. We investigate whether this geometric signature persists in production-grade language models. Across Pythia, Phi-2, Llama-3, and Mistral families, we find that last-layer value representations organize along a single dominant axis whose position strongly correlates with predictive entropy, and that domain-restricted prompts collapse this structure into the same low-dimensional manifolds observed in synthetic settings. To probe the role of this geometry, we perform targeted interventions on the entropy-aligned axis of Pythia-410M during in-context learning. Removing or perturbing this axis selectively disrupts the local uncertainty geometry, whereas matched random-axis interventions leave it intact. However, these single-layer manipulations do not produce proportionally specific degradation in Bayesian-like behavior, indicating that the geometry is a privileged readout of uncertainty rather than a singular computational bottleneck. Taken together, our results show that modern language models preserve the geometric substrate that enables Bayesian inference in wind tunnels, and organize their approximate Bayesian updates along this substrate.",
      "publishedDate": "2025-12-27T05:29:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23752",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22471",
      "title": "The Bayesian Geometry of Transformer Attention",
      "authors": [
        "Naman Aggarwal",
        "Siddhartha R. Dalal",
        "Vishal Misra"
      ],
      "abstract": "Transformers often appear to perform Bayesian reasoning in context, but verifying this rigorously has been impossible: natural data lack analytic posteriors, and large models conflate reasoning with memorization. We address this by constructing \\emph{Bayesian wind tunnels} -- controlled environments where the true posterior is known in closed form and memorization is provably impossible. In these settings, small transformers reproduce Bayesian posteriors with $10^{-3}$-$10^{-4}$ bit accuracy, while capacity-matched MLPs fail by orders of magnitude, establishing a clear architectural separation. Across two tasks -- bijection elimination and Hidden Markov Model (HMM) state tracking -- we find that transformers implement Bayesian inference through a consistent geometric mechanism: residual streams serve as the belief substrate, feed-forward networks perform the posterior update, and attention provides content-addressable routing. Geometric diagnostics reveal orthogonal key bases, progressive query-key alignment, and a low-dimensional value manifold parameterized by posterior entropy. During training this manifold unfurls while attention patterns remain stable, a \\emph{frame-precision dissociation} predicted by recent gradient analyses. Taken together, these results demonstrate that hierarchical attention realizes Bayesian inference by geometric design, explaining both the necessity of attention and the failure of flat architectures. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models.",
      "publishedDate": "2025-12-27T05:28:58Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22471",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22443",
      "title": "Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models",
      "authors": [
        "Jie Zhou",
        "Xin Chen",
        "Jie Zhang",
        "Zhe Li"
      ],
      "abstract": "Large Language Models (LLMs) are reshaping learning paradigms, cognitive processes, and research methodologies across a wide range of domains. Integrating LLMs with professional fields and redefining the relationship between LLMs and domain-specific applications has become a critical challenge for promoting enterprise digital transformation and broader social development. To effectively integrate LLMs into the accounting domain, it is essential to understand their domain-specific reasoning capabilities. This study introduces the concept of vertical-domain accounting reasoning and establishes evaluation criteria by analyzing the training data characteristics of representative GLM-series models. These criteria provide a foundation for subsequent research on reasoning paradigms and offer benchmarks for improving accounting reasoning performance. Based on this framework, we evaluate several representative models, including GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4, on a set of accounting reasoning tasks. Experimental results show that different prompt engineering strategies lead to varying degrees of performance improvement across models, with GPT-4 achieving the strongest accounting reasoning capability. However, current LLMs still fall short of real-world application requirements. In particular, further optimization is needed for deployment in enterprise-level accounting scenarios to fully realize the potential value of LLMs in this domain.",
      "publishedDate": "2025-12-27T02:39:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22443",
      "categories": [
        "prompting",
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22442",
      "title": "HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG",
      "authors": [
        "Cattalyya Nuengsigkapian"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.",
      "publishedDate": "2025-12-27T02:37:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22442",
      "categories": [
        "rag",
        "evaluation",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22431",
      "title": "Monadic Context Engineering",
      "authors": [
        "Yifan Zhang",
        "Mengdi Wang"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.",
      "publishedDate": "2025-12-27T01:52:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22431",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22418",
      "title": "Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding",
      "authors": [
        "Yi-Hung Chou",
        "Boyuan Jiang",
        "Yi Wen Chen",
        "Mingyue Weng",
        "Victoria Jackson",
        "Thomas Zimmermann",
        "James A. Jones"
      ],
      "abstract": "Large language models (LLMs) are reshaping software engineering by enabling \"vibe coding,\" in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as \"rolling the dice.\" Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.",
      "publishedDate": "2025-12-27T00:38:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22418",
      "categories": [
        "code-generation",
        "prompting",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22414",
      "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models",
      "authors": [
        "Simar Kareer",
        "Karl Pertsch",
        "James Darpinian",
        "Judy Hoffman",
        "Danfei Xu",
        "Sergey Levine",
        "Chelsea Finn",
        "Suraj Nair"
      ],
      "abstract": "Vision-language-action (VLA) models can enable broad open world generalization, but require large and diverse datasets. It is appealing to consider whether some of this data can come from human videos, which cover diverse real-world situations and are easy to obtain. However, it is difficult to train VLAs with human videos alone, and establishing a mapping between humans and robots requires manual engineering and presents a major research challenge. Drawing inspiration from advances in large language models, where the ability to learn from diverse supervision emerges with scale, we ask whether a similar phenomenon holds for VLAs that incorporate human video data. We introduce a simple co-training recipe, and find that human-to-robot transfer emerges once the VLA is pre-trained on sufficient scenes, tasks, and embodiments. Our analysis suggests that this emergent capability arises because diverse pretraining produces embodiment-agnostic representations for human and robot data. We validate these findings through a series of experiments probing human to robot skill transfer and find that with sufficiently diverse robot pre-training our method can nearly double the performance on generalization settings seen only in human data.",
      "publishedDate": "2025-12-27T00:13:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22414",
      "categories": [
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.22387",
      "title": "AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents",
      "authors": [
        "Bhanu Prakash Vangala",
        "Ali Adibifar",
        "Tanu Malik",
        "Ashish Gehani"
      ],
      "abstract": "The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.",
      "publishedDate": "2025-12-26T21:17:22Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22387",
      "categories": [
        "code-generation",
        "agents",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.22364",
      "title": "Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries",
      "authors": [
        "Saurabh Deochake",
        "Debajyoti Mukhopadhyay"
      ],
      "abstract": "Text-to-SQL systems powered by Large Language Models (LLMs) achieve high accuracy on standard benchmarks, yet existing efficiency metrics such as the Valid Efficiency Score (VES) measure execution time rather than the consumption-based costs of cloud data warehouses. This paper presents the first systematic evaluation of cloud compute costs for LLM-generated SQL queries. We evaluate six state-of-the-art LLMs across 180 query executions on Google BigQuery using the StackOverflow dataset (230GB), measuring bytes processed, slot utilization, and estimated cost. Our analysis yields three key findings: (1) reasoning models process 44.5% fewer bytes than standard models while maintaining equivalent correctness (96.7%-100%); (2) execution time correlates weakly with query cost (r=0.16), indicating that speed optimization does not imply cost optimization; and (3) models exhibit up to 3.4x cost variance, with standard models producing outliers exceeding 36GB per query. We identify prevalent inefficiency patterns including missing partition filters and unnecessary full-table scans, and provide deployment guidelines for cost-sensitive enterprise environments.",
      "publishedDate": "2025-12-26T19:51:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22364",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22351",
      "title": "VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement",
      "authors": [
        "Zhengfei Kuang",
        "Rui Lin",
        "Long Zhao",
        "Gordon Wetzstein",
        "Saining Xie",
        "Sanghyun Woo"
      ],
      "abstract": "Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io",
      "publishedDate": "2025-12-26T19:22:39Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22351",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use",
        "planning",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22120",
      "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
      "authors": [
        "Shuoshuo Zhang",
        "Yizhen Zhang",
        "Jingjing Fu",
        "Lei Song",
        "Jiang Bian",
        "Yujiu Yang",
        "Rui Wang"
      ],
      "abstract": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.",
      "publishedDate": "2025-12-26T18:59:47Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22120",
      "categories": [
        "tool-use",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22100",
      "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
      "authors": [
        "Duygu Altinok"
      ],
      "abstract": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.",
      "publishedDate": "2025-12-26T18:02:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22100",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22087",
      "title": "Context as a Tool: Context Management for Long-Horizon SWE-Agents",
      "authors": [
        "Shukai Liu",
        "Jian Yang",
        "Bo Jiang",
        "Yizhi Li",
        "Jinyang Guo",
        "Xianglong Liu",
        "Bryan Dai"
      ],
      "abstract": "Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.",
      "publishedDate": "2025-12-26T17:15:47Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22087",
      "categories": [
        "code-generation",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22082",
      "title": "Agent-based simulation of online social networks and disinformation",
      "authors": [
        "Alejandro Buitrago López",
        "Alberto Ortega Pastor",
        "David Montoro Aguilera",
        "Mario Fernández Tárraga",
        "Jesús Verdú Chacón",
        "Javier Pastor-Galindo",
        "José A. Ruipérez-Valiente"
      ],
      "abstract": "Research on online social networks (OSNs) is often hindered by platform opacity, limited access to data, and ethical constraints. Simulation offer a valuable alternative, but existing frameworks frequently lack realism and explainability. This paper presents a simulation framework that models synthetic social networks with agents endowed with demographic-based personality traits and finite-state behavioral automata, enabling realistic and interpretable actions. A generative module powered by a large language model (LLM) produces context-aware social media posts consistent with each agent's profile and memory. In parallel, a red module implements DISARM-inspired workflows to orchestrate disinformation campaigns executed by malicious agents targeting simulated audiences. A Mastodon-based visualization layer supports real-time inspection and post-hoc validation of agent activity within a familiar interface. We evaluate the resulting synthetic social networks using topological metrics and LLM-based content assessments, demonstrating structural, behavioral, and linguistic realism. Overall, the framework enables the creation of customizable and controllable social network environments for studying information dynamics and the effects of disinformation.",
      "publishedDate": "2025-12-26T16:56:45Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22082",
      "categories": [
        "evaluation",
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.22009",
      "title": "iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception",
      "authors": [
        "Sarthak Mehrotra",
        "Sairam V C Rebbapragada",
        "Mani Hemanth Reddy Bonthu",
        "Vineeth N Balasubramanian"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) show strong potential for interpreting and interacting with complex, pixel-rich Graphical User Interface (GUI) environments. However, building agents that are both efficient for high-level tasks and precise for fine-grained interactions remains challenging. GUI agents must perform routine actions efficiently while also handling tasks that demand exact visual grounding, yet existing approaches struggle when accuracy depends on identifying specific interface elements. These MLLMs also remain large and cannot adapt their reasoning depth to the task at hand. In this work, we introduce iSHIFT: Implicit Slow-fast Hybrid Inference with Flexible Tokens, a lightweight agent that integrates latent thinking (implicit chain-of-thought) with a perception control module. iSHIFT enables an MLLM to switch between a slow mode, which leverages detailed visual grounding for high precision and a fast mode that uses global cues for efficiency. Special perception tokens guide attention to relevant screen regions, allowing the model to decide both how to reason and where to focus. Despite its compact 2.5B size, iSHIFT matches state-of-the-art performance on multiple benchmark datasets.",
      "publishedDate": "2025-12-26T12:09:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22009",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21999",
      "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs",
      "authors": [
        "Jiayu Hu",
        "Beibei Li",
        "Jiangwei Xia",
        "Yanjun Qin",
        "Bing Ji",
        "Zhongshi He"
      ],
      "abstract": "While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding calibration strategies to mitigate them. However, the non-trainable nature of these strategies inherently limits their optimization potential. To this end, we propose an adversarial parametric editing framework for Hallucination mitigation in VLMs, which follows an \\textbf{A}ctivate-\\textbf{L}ocate-\\textbf{E}dit \\textbf{A}dversarially paradigm. Specifically, we first construct an activation dataset that comprises grounded responses (positive samples attentively anchored in visual features) and hallucinatory responses (negative samples reflecting LLM prior bias and internal knowledge artifacts). Next, we identify critical hallucination-prone parameter clusters by analyzing differential hidden states of response pairs. Then, these clusters are fine-tuned using prompts injected with adversarial tuned prefixes that are optimized to maximize visual neglect, thereby forcing the model to prioritize visual evidence over inherent parametric biases. Evaluations on both generative and discriminative VLM tasks demonstrate the significant effectiveness of ALEAHallu in alleviating hallucinations. Our code is available at https://github.com/hujiayu1223/ALEAHallu.",
      "publishedDate": "2025-12-26T11:56:45Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21999",
      "categories": [
        "code-generation",
        "evaluation",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.22315",
      "title": "VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning",
      "authors": [
        "Yang Ding",
        "Yizhen Zhang",
        "Xin Lai",
        "Ruihang Chu",
        "Yujiu Yang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.",
      "publishedDate": "2025-12-26T11:43:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22315",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21964",
      "title": "Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models",
      "authors": [
        "Dunyuan XU",
        "Xikai Yang",
        "Yaoqian Li",
        "Juzheng Miao",
        "Jinpeng Li",
        "Pheng-Ann Heng"
      ],
      "abstract": "Medical Multi-modal Large Language Models (MLLMs) have shown promising clinical performance. However, their sensitivity to real-world input perturbations, such as imaging artifacts and textual errors, critically undermines their clinical applicability. Systematic analysis of such noise impact on medical MLLMs remains largely unexplored. Furthermore, while several works have investigated the MLLMs' robustness in general domains, they primarily focus on text modality and rely on costly fine-tuning. They are inadequate to address the complex noise patterns and fulfill the strict safety standards in medicine. To bridge this gap, this work systematically analyzes the impact of various perturbations on medical MLLMs across both visual and textual modalities. Building on our findings, we introduce a training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' inherent denoising capabilities following the perceive-and-calibrate principle for cross-modal robustness enhancement. For the visual modality, we propose a Perturbation-aware Denoising Calibration (PDC) which leverages MLLMs' own vision encoder to identify noise patterns and perform prototype-guided feature calibration. For text denoising, we design a Self-instantiated Multi-agent System (SMS) that exploits the MLLMs' self-assessment capabilities to refine noisy text through a cooperative hierarchy of agents. We construct a benchmark containing 11 types of noise across both image and text modalities on 2 datasets. Experimental results demonstrate our method achieves the state-of-the-art performance across multiple modalities, showing potential to enhance MLLMs' robustness in real clinical scenarios.",
      "publishedDate": "2025-12-26T10:23:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21964",
      "categories": [
        "evaluation",
        "agents",
        "rag",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21915",
      "title": "Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs",
      "authors": [
        "Yafeng Tang",
        "Xiaoou Ding",
        "Jianzhuo Du",
        "Zishuo Yan",
        "Zhuang Ma",
        "Zheng Liang",
        "Zekai Qian",
        "Hongzhi Wang"
      ],
      "abstract": "Tabular data generation has become increasingly essential for enabling robust machine learning applications, which require large-scale, high-quality data. Existing solutions leverage generative models to learn original data distributions. However, real-world data are naturally heterogeneous with diverse distributions, making it challenging to obtain a universally good model for diverse data generation. To address this limitation, we introduce Diversity-Aware Tabular data gEnerator (DATE), a framework that (i) prepares high-quality and distributionally distinct examples for in-context learning by effectively partitioning the original heterogeneous data into multiple diverse subsets; (ii) harnesses Large Language Models (LLMs) to explore the diversity of the partitioned distribution with decision tree reasoning as feedback, generating high-quality labeled data for each subset. However, the massive generated data inherently involves a trade-off between diversity and quality. To integrate this issue, existing solutions greedily select the validation-best data. However, we prove that the selection in heterogeneous settings does not possess the greedy-choice property, and design a Multi-Arm Bandit-based sampling algorithm that balances the diversity and quality of generated data. Extensive experiments on tabular classification and regression benchmarks demonstrate that DATE consistently outperforms state-of-the-art GAN-based and LLM-based methods. On average, DATE achieves a 23.75% reduction in error rate with just 100 generated data. Empirically, we demonstrate that data generated by DATE can improve the accuracy of Direct Preference Optimization (DPO) and enhance the reasoning capability of LLMs on the target data. Code is available at https://github.com/windblow32/DATE.",
      "publishedDate": "2025-12-26T08:02:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21915",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21913",
      "title": "GQ-VAE: A gated quantized VAE for learning variable length tokens",
      "authors": [
        "Theo Datta",
        "Kayla Huang",
        "Sham Kakade",
        "David Brandfonbrener"
      ],
      "abstract": "While most frontier models still use deterministic frequency-based tokenization algorithms such as byte-pair encoding (BPE), there has been significant recent work to design learned neural tokenizers. However, these schemes generally add to underlying language model complexity and force large changes to architecture, making them hard to implement at large scales. To overcome these challenges, we propose the gated quantized variational autoencoder (GQ-VAE), a novel architecture that can be independently pre-trained to serve as a drop-in replacement for existing tokenizers. The key innovation of the architecture is to learn to encode variable-length discrete tokens. GQ-VAE improves compression and language modeling performance over a standard VQ-VAE tokenizer, and approaches the compression rate and language modeling performance of BPE. Interestingly, if we use BPE with a smaller vocabulary, such that the compression is equivalent between GQ-VAE and BPE, we find that GQ-VAE improves downstream language model learning. We conclude with a discussion of several exciting avenues for future work. Code can be found at https://github.com/Theo-Datta-115/gq-vae.",
      "publishedDate": "2025-12-26T07:59:00Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21913",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21911",
      "title": "Accelerate Speculative Decoding with Sparse Computation in Verification",
      "authors": [
        "Jikai Wang",
        "Jianchao Tan",
        "Yuxuan Hu",
        "Jiayu Qin",
        "Yerui Sun",
        "Yuchen Xie",
        "Xunliang Cai",
        "Juntao Li",
        "Min Zhang"
      ],
      "abstract": "Speculative decoding accelerates autoregressive language model inference by verifying multiple draft tokens in parallel. However, the verification stage often becomes the dominant computational bottleneck, especially for long-context inputs and mixture-of-experts (MoE) models. Existing sparsification methods are designed primarily for standard token-by-token autoregressive decoding to remove substantial computational redundancy in LLMs. This work systematically adopts different sparse methods on the verification stage of the speculative decoding and identifies structured redundancy across multiple dimensions. Based on these observations, we propose a sparse verification framework that jointly sparsifies attention, FFN, and MoE components during the verification stage to reduce the dominant computation cost. The framework further incorporates an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without introducing additional training. Extensive experiments across summarization, question answering, and mathematical reasoning datasets demonstrate that the proposed methods achieve favorable efficiency-accuracy trade-offs, while maintaining stable acceptance length.",
      "publishedDate": "2025-12-26T07:53:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21911",
      "categories": [
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21902",
      "title": "Explainable Statute Prediction via Attention-based Model and LLM Prompting",
      "authors": [
        "Sachin Pawar",
        "Girish Keshav Palshikar",
        "Anindita Sinha Banerjee",
        "Nitin Ramrakhiyani",
        "Basit Ali"
      ],
      "abstract": "In this paper, we explore the problem of automatic statute prediction where for a given case description, a subset of relevant statutes are to be predicted. Here, the term \"statute\" refers to a section, a sub-section, or an article of any specific Act. Addressing this problem would be useful in several applications such as AI-assistant for lawyers and legal question answering system. For better user acceptance of such Legal AI systems, we believe the predictions should also be accompanied by human understandable explanations. We propose two techniques for addressing this problem of statute prediction with explanations -- (i) AoS (Attention-over-Sentences) which uses attention over sentences in a case description to predict statutes relevant for it and (ii) LLMPrompt which prompts an LLM to predict as well as explain relevance of a certain statute. AoS uses smaller language models, specifically sentence transformers and is trained in a supervised manner whereas LLMPrompt uses larger language models in a zero-shot manner and explores both standard as well as Chain-of-Thought (CoT) prompting techniques. Both these models produce explanations for their predictions in human understandable forms. We compare statute prediction performance of both the proposed techniques with each other as well as with a set of competent baselines, across two popular datasets. Also, we evaluate the quality of the generated explanations through an automated counter-factual manner as well as through human evaluation.",
      "publishedDate": "2025-12-26T07:29:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21902",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22309",
      "title": "LLMBoost: Make Large Language Models Stronger with Boosting",
      "authors": [
        "Zehao Chen",
        "Tianxiang Ai",
        "Yifei Li",
        "Gongxun Li",
        "Yuyang Wei",
        "Wang Zhou",
        "Guanghui Li",
        "Bin Yu",
        "Zhijun Chen",
        "Hailong Sun",
        "Fuzhen Zhuang",
        "Jianxin Li",
        "Deqing Wang",
        "Yikun Ban"
      ],
      "abstract": "Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.",
      "publishedDate": "2025-12-26T07:16:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22309",
      "categories": [
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21878",
      "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting",
      "authors": [
        "Marc S. Montalvo",
        "Hamed Yaghoobian"
      ],
      "abstract": "Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance.",
      "publishedDate": "2025-12-26T06:01:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21878",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.21877",
      "title": "CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics",
      "authors": [
        "Vaibhav Devraj",
        "Dhruv Kumar",
        "Jagat Sesh Challa"
      ],
      "abstract": "Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a \"Gold Standard\" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.",
      "publishedDate": "2025-12-26T05:59:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21877",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22306",
      "title": "Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection",
      "authors": [
        "Chinmay Pushkar",
        "Sanchit Kabra",
        "Dhruv Kumar",
        "Jagat Sesh Challa"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated significant potential in automated software security, particularly in vulnerability detection. However, existing benchmarks primarily focus on isolated, single-vulnerability samples or function-level classification, failing to reflect the complexity of real-world software where multiple interacting vulnerabilities often coexist within large files. Recent studies indicate that LLMs suffer from \"count bias\" and \"selection bias\" in multi-label tasks, yet this has not been rigorously quantified in the domain of code security. In this work, we introduce a comprehensive benchmark for Multi-Vulnerability Detection across four major languages: C, C++, Python, and JavaScript. We construct a dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, and 9) into long-context code samples (7.5k-10k tokens) sourced from CodeParrot. We evaluate five state-of-the-art LLMs, including GPT-4o-mini, Llama-3.3-70B, and the Qwen-2.5 series. Our results reveal a sharp degradation in performance as vulnerability density increases. While Llama-3.3-70B achieves near-perfect F1 scores (approximately 0.97) on single-vulnerability C tasks, performance drops by up to 40% in high-density settings. Notably, Python and JavaScript show distinct failure modes compared to C/C++, with models exhibiting severe \"under-counting\" (Recall dropping to less than 0.30) in complex Python files.",
      "publishedDate": "2025-12-26T05:43:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22306",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21871",
      "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?",
      "authors": [
        "Naen Xu",
        "Jinghuai Zhang",
        "Changjiang Li",
        "Hengyu An",
        "Chunyi Zhou",
        "Jun Wang",
        "Boyu Xu",
        "Yuyuan Li",
        "Tianyu Du",
        "Shouling Ji"
      ],
      "abstract": "Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.",
      "publishedDate": "2025-12-26T05:09:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21871",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21859",
      "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
      "authors": [
        "Qi Fan",
        "An Zou",
        "Yehan Ma"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.",
      "publishedDate": "2025-12-26T04:49:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21859",
      "categories": [
        "robotics",
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.21852",
      "title": "A Comedy of Estimators: On KL Regularization in RL Training of LLMs",
      "authors": [
        "Vedant Shah",
        "Johan Obando-Ceron",
        "Vineet Jain",
        "Brian Bartoldson",
        "Bhavya Kailkhura",
        "Sarthak Mittal",
        "Glen Berseth",
        "Pablo Samuel Castro",
        "Yoshua Bengio",
        "Nikolay Malkin",
        "Moksh Jain",
        "Siddarth Venkatraman",
        "Aaron Courville"
      ],
      "abstract": "The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \\texttt{Qwen2.5-7B}, \\texttt{Llama-3.1-8B-Instruct} and \\texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.",
      "publishedDate": "2025-12-26T04:20:58Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21852",
      "categories": [
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.21849",
      "title": "HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs",
      "authors": [
        "Jiaxin Liu",
        "Peiyi Tu",
        "Wenyu Chen",
        "Yihong Zhuang",
        "Xinxia Ling",
        "Anji Zhou",
        "Chenxi Wang",
        "Zhuo Han",
        "Zhengkai Yang",
        "Junbo Zhao",
        "Zenan Huang",
        "Yuanyuan Wang"
      ],
      "abstract": "While Large Language Models (LLMs) have achieved remarkable success in cognitive and reasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence-the capacity to navigate complex social, emotional, and ethical nuances. This gap is particularly acute in the Chinese linguistic and cultural context, where a lack of specialized evaluation frameworks and high-quality socio-emotional data impedes progress. To address these limitations, we present HeartBench, a framework designed to evaluate the integrated emotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic psychological counseling scenarios and developed in collaboration with clinical experts, the benchmark is structured around a theory-driven taxonomy comprising five primary dimensions and 15 secondary capabilities. We implement a case-specific, rubric-based methodology that translates abstract human-like traits into granular, measurable criteria through a ``reasoning-before-scoring'' evaluation protocol. Our assessment of 13 state-of-the-art LLMs indicates a substantial performance ceiling: even leading models achieve only 60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified ``Hard Set'' reveals a significant performance decay in scenarios involving subtle emotional subtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for anthropomorphic AI evaluation and provides a methodological blueprint for constructing high-quality, human-aligned training data.",
      "publishedDate": "2025-12-26T03:54:56Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21849",
      "categories": [
        "evaluation",
        "reasoning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.21837",
      "title": "Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco",
      "authors": [
        "Siyu Li",
        "Chenwei Song",
        "Wan Zhou",
        "Xinyi Liu"
      ],
      "abstract": "This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured information from a domain-specific knowledge graph. Specifically, LLMs are first leveraged to assist in the construction of a tobacco pest and disease knowledge graph, which organizes key entities such as diseases, symptoms, control methods, and their relationships. Based on this graph, relevant knowledge is retrieved and integrated into the reasoning process to support accurate answer generation. The Transformer architecture is adopted as the core inference model, while a graph neural network (GNN) is employed to learn expressive node representations that capture both local and global relational information within the knowledge graph. A ChatGLM-based model serves as the backbone LLM and is fine-tuned using LoRA to achieve parameter-efficient adaptation. Extensive experimental results demonstrate that the proposed approach consistently outperforms baseline methods across multiple evaluation metrics, significantly improving both the accuracy and depth of reasoning, particularly in complex multi-hop and comparative reasoning scenarios.",
      "publishedDate": "2025-12-26T02:48:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21837",
      "categories": [
        "rag",
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.21835",
      "title": "LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices",
      "authors": [
        "Mingyu Sun",
        "Xiao Zhang",
        "Shen Qu",
        "Yan Li",
        "Mengbai Xiao",
        "Yuan Yuan",
        "Dongxiao Yu"
      ],
      "abstract": "Large language models (LLMs) have emerged as a powerful foundation for intelligent reasoning and decision-making, demonstrating substantial impact across a wide range of domains and applications. However, their massive parameter scales and substantial resource demands pose critical challenges for efficient inference on edge devices. These devices are inherently constrained by limited computational power and memory capacity, while bandwidth bottlenecks at the network edge further restrict distributed deployment and real-time responsiveness. Although existing research has explored lightweight optimization techniques to mitigate memory limitations, such approaches often incur significant degradation in model accuracy and performance. To address these challenges, we propose LIME, a collaborative system that enables lossless inference for large models across multiple memory-constrained edge devices under limited network bandwidth. LIME employs an interleaved pipeline parallelism in conjunction with model offloading to dynamically balance computation and communication. Furthermore, a fine-grained offline allocation scheduler and online memory adaptation strategy are introduced to enhance the device's computing and storage resources while minimizing inference latency. Extensive experiments demonstrate that LIME, deployed on four heterogeneous Nvidia Jetson edge devices for LLaMA3.3-70B-Instruct model inference, achieves 1.7$\\times$ and 3.7$\\times$ speedups over state-of-the-art baselines under sporadic and bursty request patterns respectively, without compromising model accuracy.",
      "publishedDate": "2025-12-26T02:41:18Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21835",
      "categories": [
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.23743",
      "title": "Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding",
      "authors": [
        "Yunguo Yu"
      ],
      "abstract": "Clinical coding automation using cloud-based Large Language Models (LLMs) poses privacy risks and latency bottlenecks, rendering them unsuitable for on-premise healthcare deployment. We introduce Hybrid-Code, a hybrid neuro-symbolic multi-agent framework for local clinical coding that ensures production reliability through redundancy and verification. Our system comprises two agents: a Coder that attempts language model-based semantic reasoning using BioMistral-7B but falls back to deterministic keyword matching when model output is unreliable, ensuring pipeline completion; and an Auditor that verifies codes against a 257-code knowledge base and clinical evidence. Evaluating on 1,000 MIMIC-III discharge summaries, we demonstrate no hallucinated codes among accepted outputs within the knowledge base, 24.47% verification rate, and 34.11% coverage (95% CI: 31.2%--37.0%) with 86%+ language model utilization. The Auditor filtered invalid format codes and provided evidence-based quality control (75.53% rejection rate) while ensuring no patient data leaves the hospital firewall. The hybrid architecture -- combining language model semantic understanding (when successful), deterministic fallback (when the model fails), and symbolic verification (always active) -- ensures both reliability and privacy preservation, addressing critical barriers to AI adoption in healthcare. Our key finding is that reliability through redundancy is more valuable than pure model performance in production healthcare systems, where system failures are unacceptable.",
      "publishedDate": "2025-12-26T02:27:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23743",
      "categories": [
        "rag",
        "code-generation",
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.23742",
      "title": "AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization",
      "authors": [
        "Guangxi Fan",
        "Tianliang Ma",
        "Xuguang Sun",
        "Xun Wang",
        "Kain Lu Low",
        "Leilai Shao"
      ],
      "abstract": "With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.",
      "publishedDate": "2025-12-26T01:34:08Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23742",
      "categories": [
        "code-generation",
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.21799",
      "title": "KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering",
      "authors": [
        "Hung-Nghiep Tran",
        "Atsuhiro Takasu"
      ],
      "abstract": "In this paper, we present KG20C and KG20C-QA, two curated datasets for advancing question answering (QA) research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph through targeted selection of venues, quality-based filtering, and schema definition. Although KG20C has been available online in non-peer-reviewed sources such as GitHub repository, this paper provides the first formal, peer-reviewed description of the dataset, including clear documentation of its construction and specifications. KG20C-QA is built upon KG20C to support QA tasks on scholarly data. We define a set of QA templates that convert graph triples into natural language question--answer pairs, producing a benchmark that can be used both with graph-based models such as knowledge graph embeddings and with text-based models such as large language models. We benchmark standard knowledge graph embedding methods on KG20C-QA, analyze performance across relation types, and provide reproducible evaluation protocols. By officially releasing these datasets with thorough documentation, we aim to contribute a reusable, extensible resource for the research community, enabling future work in QA, reasoning, and knowledge-driven applications in the scholarly domain. The full datasets will be released at https://github.com/tranhungnghiep/KG20C/ upon paper publication.",
      "publishedDate": "2025-12-25T22:29:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21799",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.21782",
      "title": "Accelerating Scientific Discovery with Autonomous Goal-evolving Agents",
      "authors": [
        "Yuanqi Du",
        "Botao Yu",
        "Tianyu Liu",
        "Tony Shen",
        "Junwu Chen",
        "Jan G. Rittig",
        "Kunyang Sun",
        "Yikun Zhang",
        "Zhangde Song",
        "Bo Zhou",
        "Cassandra Masschelein",
        "Yingze Wang",
        "Haorui Wang",
        "Haojun Jia",
        "Chao Zhang",
        "Hongyu Zhao",
        "Martin Ester",
        "Teresa Head-Gordon",
        "Carla P. Gomes",
        "Huan Sun",
        "Chenru Duan",
        "Philippe Schwaller",
        "Wengong Jin"
      ],
      "abstract": "There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. We argue that automating objective function design is a central, yet unmet requirement for scientific discovery agents. In this work, we introduce the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. We demonstrate the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.",
      "publishedDate": "2025-12-25T20:54:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21782",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.21778",
      "title": "Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models",
      "authors": [
        "Nimrod Berman",
        "Adam Botach",
        "Emanuel Ben-Baruch",
        "Shunit Haviv Hakimi",
        "Asaf Gendler",
        "Ilan Naiman",
        "Erez Yosef",
        "Igor Kviatkovsky"
      ],
      "abstract": "Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.",
      "publishedDate": "2025-12-25T20:31:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21778",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22293",
      "title": "Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against",
      "authors": [
        "Tsogt-Ochir Enkhbayar"
      ],
      "abstract": "Warning-framed content in training data (e.g., \"DO NOT USE - this code is vulnerable\") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: \"describing X\" and \"performing X\" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call \"stealth slip\", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.",
      "publishedDate": "2025-12-25T20:07:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22293",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21723",
      "title": "HELP: Hierarchical Embodied Language Planner for Household Tasks",
      "authors": [
        "Alexandr V. Korchemnyi",
        "Anatoly O. Onishchenko",
        "Eva A. Bakaeva",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "Embodied agents tasked with complex scenarios, whether in real or simulated environments, rely heavily on robust planning capabilities. When instructions are formulated in natural language, large language models (LLMs) equipped with extensive linguistic knowledge can play this role. However, to effectively exploit the ability of such models to handle linguistic ambiguity, to retrieve information from the environment, and to be based on the available skills of an agent, an appropriate architecture must be designed. We propose a Hierarchical Embodied Language Planner, called HELP, consisting of a set of LLM-based agents, each dedicated to solving a different subtask. We evaluate the proposed approach on a household task and perform real-world experiments with an embodied agent. We also focus on the use of open source LLMs with a relatively small number of parameters, to enable autonomous deployment.",
      "publishedDate": "2025-12-25T15:54:08Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21723",
      "categories": [
        "agents",
        "planning",
        "prompting",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.21722",
      "title": "MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning",
      "authors": [
        "Zishuo Wang",
        "Xinyu Zhang",
        "Zhuonan Liu",
        "Tomohito Kawabata",
        "Daeun Song",
        "Xuesu Xiao",
        "Ling Xiao"
      ],
      "abstract": "Socially compliant navigation requires robots to move safely and appropriately in human-centered environments by respecting social norms. However, social norms are often ambiguous, and in a single scenario, multiple actions may be equally acceptable. Most existing methods simplify this problem by assuming a single correct action, which limits their ability to handle real-world social uncertainty. In this work, we propose MAction-SocialNav, an efficient vision language model for socially compliant navigation that explicitly addresses action ambiguity, enabling generating multiple plausible actions within one scenario. To enhance the model's reasoning capability, we introduce a novel meta-cognitive prompt (MCP) method. Furthermore, to evaluate the proposed method, we curate a multi-action socially compliant navigation dataset that accounts for diverse conditions, including crowd density, indoor and outdoor environments, and dual human annotations. The dataset contains 789 samples, each with three-turn conversation, split into 710 training samples and 79 test samples through random selection. We also design five evaluation metrics to assess high-level decision precision, safety, and diversity. Extensive experiments demonstrate that the proposed MAction-SocialNav achieves strong social reasoning performance while maintaining high efficiency, highlighting its potential for real-world human robot navigation. Compared with zero-shot GPT-4o and Claude, our model achieves substantially higher decision quality (APG: 0.595 vs. 0.000/0.025) and safety alignment (ER: 0.264 vs. 0.642/0.668), while maintaining real-time efficiency (1.524 FPS, over 3x faster).",
      "publishedDate": "2025-12-25T15:52:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21722",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.21720",
      "title": "An Information Theoretic Perspective on Agentic System Design",
      "authors": [
        "Shizhe He",
        "Avanika Narayan",
        "Ishan S. Khare",
        "Scott W. Linderman",
        "Christopher Ré",
        "Dan Biderman"
      ],
      "abstract": "Agentic language model (LM) systems power modern applications like \"Deep Research\" and \"Claude Code,\" and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller \"compressor\" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger \"predictor\" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is $1.6\\times$ more accurate, $4.6\\times$ more concise, and conveys $5.5\\times$ more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover $99\\%$ of frontier-LM accuracy at $26\\%$ of API costs.",
      "publishedDate": "2025-12-25T15:45:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21720",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23739",
      "title": "Break Out the Silverware -- Semantic Understanding of Stored Household Items",
      "authors": [
        "Michaela Levi-Richter",
        "Reuth Mirsky",
        "Oren Glickman"
      ],
      "abstract": "``Bring me a plate.'' For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots' cognitive capabilities: given a household scene and a queried item, predict its most likely storage location. Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants' kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures. To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems. We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments.",
      "publishedDate": "2025-12-25T15:21:49Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23739",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag",
        "prompting",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.21711",
      "title": "Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought",
      "authors": [
        "Yuyi Zhang",
        "Boyu Tang",
        "Tianjie Ju",
        "Sufeng Duan",
        "Gongshen Liu"
      ],
      "abstract": "Latent tokens are gaining attention for enhancing reasoning in large language models (LLMs), yet their internal mechanisms remain unclear. This paper examines the problem from a reliability perspective, uncovering fundamental weaknesses: latent tokens function as uninterpretable placeholders rather than encoding faithful reasoning. While resistant to perturbation, they promote shortcut usage over genuine reasoning. We focus on Chain-of-Continuous-Thought (COCONUT), which claims better efficiency and stability than explicit Chain-of-Thought (CoT) while maintaining performance. We investigate this through two complementary approaches. First, steering experiments perturb specific token subsets, namely COCONUT and explicit CoT. Unlike CoT tokens, COCONUT tokens show minimal sensitivity to steering and lack reasoning-critical information. Second, shortcut experiments evaluate models under biased and out-of-distribution settings. Results on MMLU and HotpotQA demonstrate that COCONUT consistently exploits dataset artifacts, inflating benchmark performance without true reasoning. These findings reposition COCONUT as a pseudo-reasoning mechanism: it generates plausible traces that conceal shortcut dependence rather than faithfully representing reasoning processes.",
      "publishedDate": "2025-12-25T15:14:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21711",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21709",
      "title": "Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers",
      "authors": [
        "Md. Rakibul Islam",
        "Most. Sharmin Sultana Samu",
        "Md. Zahid Hossain",
        "Farhad Uz Zaman",
        "Md. Kamrozzaman Bhuiyan"
      ],
      "abstract": "Large language models (LLMs) can produce text that closely resembles human writing. This capability raises concerns about misuse, including disinformation and content manipulation. Detecting AI-generated text is essential to maintain authenticity and prevent malicious applications. Existing research has addressed detection in multiple languages, but the Bengali language remains largely unexplored. Bengali's rich vocabulary and complex structure make distinguishing human-written and AI-generated text particularly challenging. This study investigates five transformer-based models: XLMRoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base and MultilingualBERT-Base. Zero-shot evaluation shows that all models perform near chance levels (around 50% accuracy) and highlight the need for task-specific fine-tuning. Fine-tuning significantly improves performance, with XLM-RoBERTa, mDeBERTa and MultilingualBERT achieving around 91% on both accuracy and F1-score. IndicBERT demonstrates comparatively weaker performance, indicating limited effectiveness in fine-tuning for this task. This work advances AI-generated text detection in Bengali and establishes a foundation for building robust systems to counter AI-generated content.",
      "publishedDate": "2025-12-25T15:04:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21709",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21708",
      "title": "MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles",
      "authors": [
        "Jing Han",
        "Binwei Yan",
        "Tianyu Guo",
        "Zheyuan Bai",
        "Mengyu Zheng",
        "Hanting Chen",
        "Ying Nie"
      ],
      "abstract": "Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly dominant Reason+Action paradigm, we first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. The reasoner is responsible for comprehending the user's query and determining the next role based on the execution trajectory. The executor is tasked with identifying the appropriate functions and parameters to invoke. The summarizer conveys the distilled information from conversations back to the user. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized Low-Rank Adaptation (LoRA) groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the agent task. 3) To effectively fine-tune the framework, we develop a multi-role data generation pipeline based on publicly available datasets, incorporating role-specific content completion and reliability verification. We conduct extensive experiments and thorough ablation studies on various LLMs and agent benchmarks, demonstrating the effectiveness of the proposed method. This project is publicly available at https://mor-agent.github.io.",
      "publishedDate": "2025-12-25T15:02:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21708",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21699",
      "title": "Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning",
      "authors": [
        "Eranga Bandara",
        "Tharaka Hewa",
        "Ross Gore",
        "Sachin Shetty",
        "Ravi Mukkamala",
        "Peter Foytik",
        "Abdul Rahman",
        "Safdar H. Bouk",
        "Xueping Liang",
        "Amin Hass",
        "Sachini Rajapakse",
        "Ng Wee Keong",
        "Kasun De Zoysa",
        "Aruna Withanage",
        "Nilaan Loganathan"
      ],
      "abstract": "Agentic AI represents a major shift in how autonomous systems reason, plan, and execute multi-step tasks through the coordination of Large Language Models (LLMs), Vision Language Models (VLMs), tools, and external services. While these systems enable powerful new capabilities, increasing autonomy introduces critical challenges related to explainability, accountability, robustness, and governance, especially when agent outputs influence downstream actions or decisions. Existing agentic AI implementations often emphasize functionality and scalability, yet provide limited mechanisms for understanding decision rationale or enforcing responsibility across agent interactions. This paper presents a Responsible(RAI) and Explainable(XAI) AI Agent Architecture for production-grade agentic workflows based on multi-model consensus and reasoning-layer governance. In the proposed design, a consortium of heterogeneous LLM and VLM agents independently generates candidate outputs from a shared input context, explicitly exposing uncertainty, disagreement, and alternative interpretations. A dedicated reasoning agent then performs structured consolidation across these outputs, enforcing safety and policy constraints, mitigating hallucinations and bias, and producing auditable, evidence-backed decisions. Explainability is achieved through explicit cross-model comparison and preserved intermediate outputs, while responsibility is enforced through centralized reasoning-layer control and agent-level constraints. We evaluate the architecture across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains. This work provides practical guidance for designing agentic AI systems that are autonomous and scalable, yet responsible and explainable by construction.",
      "publishedDate": "2025-12-25T14:49:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21699",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.21681",
      "title": "Exploring the Security Threats of Retriever Backdoors in Retrieval-Augmented Code Generation",
      "authors": [
        "Tian Li",
        "Bo Lin",
        "Shangwen Wang",
        "Yusong Tan"
      ],
      "abstract": "Retrieval-Augmented Code Generation (RACG) is increasingly adopted to enhance Large Language Models for software development, yet its security implications remain dangerously underexplored. This paper conducts the first systematic exploration of a critical and stealthy threat: backdoor attacks targeting the retriever component, which represents a significant supply-chain vulnerability. It is infeasible to assess this threat realistically, as existing attack methods are either too ineffective to pose a real danger or are easily detected by state-of-the-art defense mechanisms spanning both latent-space analysis and token-level inspection, which achieve consistently high detection rates. To overcome this barrier and enable a realistic analysis, we first developed VenomRACG, a new class of potent and stealthy attack that serves as a vehicle for our investigation. Its design makes poisoned samples statistically indistinguishable from benign code, allowing the attack to consistently maintain low detectability across all evaluated defense mechanisms. Armed with this capability, our exploration reveals a severe vulnerability: by injecting vulnerable code equivalent to only 0.05% of the entire knowledge base size, an attacker can successfully manipulate the backdoored retriever to rank the vulnerable code in its top-5 results in 51.29% of cases. This translates to severe downstream harm, causing models like GPT-4o to generate vulnerable code in over 40% of targeted scenarios, while leaving the system's general performance intact. Our findings establish that retriever backdooring is not a theoretical concern but a practical threat to the software development ecosystem that current defenses are blind to, highlighting the urgent need for robust security measures.",
      "publishedDate": "2025-12-25T13:53:46Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21681",
      "categories": [
        "code-generation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.21635",
      "title": "Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations",
      "authors": [
        "Chengxu Yang",
        "Jingling Yuan",
        "Siqi Cai",
        "Jiawei Jiang",
        "Chuang Hu"
      ],
      "abstract": "Hallucinations in large language models (LLMs) are commonly regarded as errors to be minimized. However, recent perspectives suggest that some hallucinations may encode creative or epistemically valuable content, a dimension that remains underquantified in current literature. Existing hallucination detection methods primarily focus on factual consistency, struggling to handle heterogeneous scientific tasks and balance creativity with accuracy. To address these challenges, we propose HIC-Bench, a novel evaluation framework that categorizes hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH), enabling systematic investigation of their interplay in LLM creativity. HIC-Bench features three core characteristics: (1) Structured IH/DH Assessment. using a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (TTCT) metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation); (2) Cross-Domain Applicability. spanning ten scientific domains with open-ended innovation tasks; and (3) Dynamic Prompt Optimization. leveraging the Dynamic Hallucination Prompt (DHP) to guide models toward creative and reliable outputs. The evaluation process employs multiple LLM judges, averaging scores to mitigate bias, with human annotators verifying IH/DH classifications. Experimental results reveal a nonlinear relationship between IH and DH, demonstrating that creativity and correctness can be jointly optimized. These insights position IH as a catalyst for creativity and reveal the ability of LLM hallucinations to drive scientific innovation.Additionally, the HIC-Bench offers a valuable platform for advancing research into the creative intelligence of LLM hallucinations.",
      "publishedDate": "2025-12-25T11:33:46Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21635",
      "categories": [
        "evaluation",
        "rag",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21583",
      "title": "A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning",
      "authors": [
        "Zelin Zang",
        "Wenyi Gu",
        "Siqi Ma",
        "Dan Yang",
        "Yue Shen",
        "Zhu Zhang",
        "Guohui Fan",
        "Wing-Kuen Ling",
        "Fuji Yang"
      ],
      "abstract": "With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.",
      "publishedDate": "2025-12-25T09:01:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21583",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21582",
      "title": "LLM-Free Image Captioning Evaluation in Reference-Flexible Settings",
      "authors": [
        "Shinnosuke Hirano",
        "Yuiga Wada",
        "Kazuki Matsuda",
        "Seitaro Otsuki",
        "Komei Sugiura"
      ],
      "abstract": "We focus on the automatic evaluation of image captions in both reference-based and reference-free settings. Existing metrics based on large language models (LLMs) favor their own generations; therefore, the neutrality is in question. Most LLM-free metrics do not suffer from such an issue, whereas they do not always demonstrate high performance. To address these issues, we propose Pearl, an LLM-free supervised metric for image captioning, which is applicable to both reference-based and reference-free settings. We introduce a novel mechanism that learns the representations of image--caption and caption--caption similarities. Furthermore, we construct a human-annotated dataset for image captioning metrics, that comprises approximately 333k human judgments collected from 2,360 annotators across over 75k images. Pearl outperformed other existing LLM-free metrics on the Composite, Flickr8K-Expert, Flickr8K-CF, Nebula, and FOIL datasets in both reference-based and reference-free settings. Our project page is available at https://pearl.kinsta.page/.",
      "publishedDate": "2025-12-25T08:59:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21582",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21578",
      "title": "NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent",
      "authors": [
        "Ali Sahami",
        "Sudhanshu Garg",
        "Andrew Wang",
        "Chaitanya Kulkarni",
        "Farhad Farahani",
        "Sean Yun-Shiuan Chuang",
        "Jian Wan",
        "Srinivasan Manoharan",
        "Uma Kona",
        "Nitin Sharma",
        "Linsey Pang",
        "Prakhar Mehrotra",
        "Jessica Clark",
        "Mark Moyou"
      ],
      "abstract": "We present the development and optimization of PayPal's Commerce Agent, powered by NEMO-4-PAYPAL, a multi-agent system designed to revolutionize agentic commerce on the PayPal platform. Through our strategic partnership with NVIDIA, we leveraged the NeMo Framework for LLM model fine-tuning to enhance agent performance. Specifically, we optimized the Search and Discovery agent by replacing our base model with a fine-tuned Nemotron small language model (SLM). We conducted comprehensive experiments using the llama3.1-nemotron-nano-8B-v1 architecture, training LoRA-based models through systematic hyperparameter sweeps across learning rates, optimizers (Adam, AdamW), cosine annealing schedules, and LoRA ranks. Our contributions include: (1) the first application of NVIDIA's NeMo Framework to commerce-specific agent optimization, (2) LLM powered fine-tuning strategy for retrieval-focused commerce tasks, (3) demonstration of significant improvements in latency and cost while maintaining agent quality, and (4) a scalable framework for multi-agent system optimization in production e-commerce environments. Our results demonstrate that the fine-tuned Nemotron SLM effectively resolves the key performance issue in the retrieval component, which represents over 50\\% of total agent response time, while maintaining or enhancing overall system performance.",
      "publishedDate": "2025-12-25T08:47:32Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21578",
      "categories": [
        "rag",
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.21577",
      "title": "A Unified Definition of Hallucination, Or: It's the World Model, Stupid",
      "authors": [
        "Emmy Liu",
        "Varun Gangal",
        "Chelsea Zou",
        "Xiaoqi Huang",
        "Michael Yu",
        "Alex Chang",
        "Zhuofu Tao",
        "Sachin Kumar",
        "Steven Y. Feng"
      ],
      "abstract": "Despite numerous attempts to solve the issue of hallucination since the inception of neural language models, it remains a problem in even frontier large language models today. Why is this the case? We walk through definitions of hallucination used in the literature from a historical perspective up to the current day, and fold them into a single definition of hallucination, wherein different prior definitions focus on different aspects of our definition. At its core, we argue that hallucination is simply inaccurate (internal) world modeling, in a form where it is observable to the user (e.g., stating a fact which contradicts a knowledge base, or producing a summary which contradicts a known source). By varying the reference world model as well as the knowledge conflict policy (e.g., knowledge base vs. in-context), we arrive at the different existing definitions of hallucination present in the literature. We argue that this unified view is useful because it forces evaluations to make clear their assumed \"world\" or source of truth, clarifies what should and should not be called hallucination (as opposed to planning or reward/incentive-related errors), and provides a common language to compare benchmarks and mitigation techniques. Building on this definition, we outline plans for a family of benchmarks in which hallucinations are defined as mismatches with synthetic but fully specified world models in different environments, and sketch out how these benchmarks can use such settings to stress-test and improve the world modeling components of language models.",
      "publishedDate": "2025-12-25T08:42:18Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21577",
      "categories": [
        "evaluation",
        "planning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.21571",
      "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures",
      "authors": [
        "Hui Guo",
        "Qihang Zheng",
        "Chenghai Huo",
        "Dongliang Guo",
        "Haoqi Yang",
        "Yang Zhang"
      ],
      "abstract": "The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.",
      "publishedDate": "2025-12-25T08:27:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21571",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21567",
      "title": "Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management",
      "authors": [
        "Changzhi Sun",
        "Xiangyu Chen",
        "Jixiang Luo",
        "Dell Zhang",
        "Xuelong Li"
      ],
      "abstract": "External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.",
      "publishedDate": "2025-12-25T08:23:03Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21567",
      "categories": [
        "rag",
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.21560",
      "title": "Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration",
      "authors": [
        "Unnati Saraswat",
        "Tarun Rao",
        "Namah Gupta",
        "Shweta Swami",
        "Shikhar Sharma",
        "Prateek Narang",
        "Dhruv Kumar"
      ],
      "abstract": "Intelligent image editing increasingly relies on advances in computer vision, multimodal reasoning, and generative modeling. While vision-language models (VLMs) and diffusion models enable guided visual manipulation, existing work rarely ensures that inserted objects are \\emph{contextually appropriate}. We introduce two new tasks for advertising and digital media: (1) \\emph{context-aware object insertion}, which requires predicting suitable object categories, generating them, and placing them plausibly within the scene; and (2) \\emph{sponsor-product logo augmentation}, which involves detecting products and inserting correct brand logos, even when items are unbranded or incorrectly branded. To support these tasks, we build two new datasets with category annotations, placement regions, and sponsor-product labels.",
      "publishedDate": "2025-12-25T08:12:27Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21560",
      "categories": [
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.21545",
      "title": "EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal",
      "authors": [
        "Sanghyun Jo",
        "Donghwan Lee",
        "Eunji Jung",
        "Seong Je Oh",
        "Kyungsu Kim"
      ],
      "abstract": "Object removal differs from common inpainting, since it must prevent the masked target from reappearing and reconstruct the occluded background with structural and contextual fidelity, rather than merely filling a hole plausibly. Recent dataset-free approaches that redirect self-attention inside the mask fail in two ways: non-target foregrounds are often misinterpreted as background, which regenerates unwanted objects, and direct attention manipulation disrupts fine details and hinders coherent integration of background cues. We propose EraseLoRA, a novel dataset-free framework that replaces attention surgery with background-aware reasoning and test-time adaptation. First, Background-aware Foreground Exclusion (BFE), uses a multimodal large-language models to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without paired supervision, producing reliable background cues while excluding distractors. Second, Background-aware Reconstruction with Subtype Aggregation (BRSA), performs test-time optimization that treats inferred background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives, preserving local detail and global structure without explicit attention intervention. We validate EraseLoRA as a plug-in to pretrained diffusion models and across benchmarks for object removal, demonstrating consistent improvements over dataset-free baselines and competitive results against dataset-driven methods. The code will be made available upon publication.",
      "publishedDate": "2025-12-25T07:34:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21545",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21543",
      "title": "CEMG: Collaborative-Enhanced Multimodal Generative Recommendation",
      "authors": [
        "Yuzhen Lin",
        "Hongyi Chen",
        "Xuanjing Chen",
        "Shaowen Wang",
        "Ivonne Xu",
        "Dongming Jiang"
      ],
      "abstract": "Generative recommendation models often struggle with two key challenges: (1) the superficial integration of collaborative signals, and (2) the decoupled fusion of multimodal features. These limitations hinder the creation of a truly holistic item representation. To overcome this, we propose CEMG, a novel Collaborative-Enhaned Multimodal Generative Recommendation framework. Our approach features a Multimodal Fusion Layer that dynamically integrates visual and textual features under the guidance of collaborative signals. Subsequently, a Unified Modality Tokenization stage employs a Residual Quantization VAE (RQ-VAE) to convert this fused representation into discrete semantic codes. Finally, in the End-to-End Generative Recommendation stage, a large language model is fine-tuned to autoregressively generate these item codes. Extensive experiments demonstrate that CEMG significantly outperforms state-of-the-art baselines.",
      "publishedDate": "2025-12-25T07:28:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21543",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21526",
      "title": "Selective LLM-Guided Regularization for Enhancing Recommendation Models",
      "authors": [
        "Shanglin Yang",
        "Zhan Shi"
      ],
      "abstract": "Large language models provide rich semantic priors and strong reasoning capabilities, making them promising auxiliary signals for recommendation. However, prevailing approaches either deploy LLMs as standalone recommender or apply global knowledge distillation, both of which suffer from inherent drawbacks. Standalone LLM recommender are costly, biased, and unreliable across large regions of the user item space, while global distillation forces the downstream model to imitate LLM predictions even when such guidance is inaccurate. Meanwhile, recent studies show that LLMs excel particularly in re-ranking and challenging scenarios, rather than uniformly across all contexts.We introduce Selective LLM Guided Regularization, a model-agnostic and computation efficient framework that activates LLM based pairwise ranking supervision only when a trainable gating mechanism informing by user history length, item popularity, and model uncertainty predicts the LLM to be reliable. All LLM scoring is performed offline, transferring knowledge without increasing inference cost. Experiments across multiple datasets show that this selective strategy consistently improves overall accuracy and yields substantial gains in cold start and long tail regimes, outperforming global distillation baselines.",
      "publishedDate": "2025-12-25T06:30:00Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21526",
      "categories": [
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.23738",
      "title": "Enforcing Temporal Constraints for LLM Agents",
      "authors": [
        "Adharsh Kamath",
        "Sishen Zhang",
        "Calvin Xu",
        "Shubham Ugare",
        "Gagandeep Singh",
        "Sasa Misailovic"
      ],
      "abstract": "LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.",
      "publishedDate": "2025-12-25T06:12:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23738",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.22275",
      "title": "The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency",
      "authors": [
        "Dingyu Wang",
        "Zimu Yuan",
        "Jiajun Liu",
        "Shanggui Liu",
        "Nan Zhou",
        "Tianxing Xu",
        "Di Huang",
        "Dong Jiang"
      ],
      "abstract": "Background: The rapid integration of foundation models into clinical practice and public health necessitates a rigorous evaluation of their true clinical reasoning capabilities beyond narrow examination success. Current benchmarks, typically based on medical licensing exams or curated vignettes, fail to capture the integrated, multimodal reasoning essential for real-world patient care. Methods: We developed the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework comprising 1,245 questions derived from real-world patient cases in orthopedics and sports medicine. This benchmark assesses models across 7 tasks that mirror the clinical reasoning pathway, including knowledge recall, text and image interpretation, diagnosis generation, treatment planning, and rationale provision. We evaluated eleven vision-language models (VLMs) and six large language models (LLMs), comparing their performance against expert-derived ground truth. Results: Our results demonstrate a pronounced performance gap between task types. While state-of-the-art models achieved high accuracy, exceeding 90%, on structured multiple-choice questions, their performance markedly declined on open-ended tasks requiring multimodal integration, with accuracy scarcely reaching 60%. VLMs demonstrated substantial limitations in interpreting medical images and frequently exhibited severe text-driven hallucinations, often ignoring contradictory visual evidence. Notably, models specifically fine-tuned for medical applications showed no consistent advantage over general-purpose counterparts. Conclusions: Current artificial intelligence models are not yet clinically competent for complex, multimodal reasoning. Their safe deployment should currently be limited to supportive, text-based roles. Future advancement in core clinical tasks awaits fundamental breakthroughs in multimodal integration and visual understanding.",
      "publishedDate": "2025-12-25T03:33:22Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22275",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.21481",
      "title": "The AI Committee: A Multi-Agent Framework for Automated Validation and Remediation of Web-Sourced Data",
      "authors": [
        "Sunith Vallabhaneni",
        "Thomas Berkane",
        "Maimuna Majumder"
      ],
      "abstract": "Many research areas rely on data from the web to gain insights and test their methods. However, collecting comprehensive research datasets often demands manually reviewing many web pages to identify and record relevant data points, which is labor-intensive and susceptible to error. While the emergence of large language models (LLM)-powered web agents has begun to automate parts of this process, they often struggle to ensure the validity of the data they collect. Indeed, these agents exhibit several recurring failure modes - including hallucinating or omitting values, misinterpreting page semantics, and failing to detect invalid information - which are subtle and difficult to detect and correct manually. To address this, we introduce the AI Committee, a novel model-agnostic multi-agent system that automates the process of validating and remediating web-sourced datasets. Each agent is specialized in a distinct task in the data quality assurance pipeline, from source scrutiny and fact-checking to data remediation and integrity validation. The AI Committee leverages various LLM capabilities - including in-context learning for dataset adaptation, chain-of-thought reasoning for complex semantic validation, and a self-correction loop for data remediation - all without task-specific training. We demonstrate the effectiveness of our system by applying it to three real-world datasets, showing that it generalizes across LLMs and significantly outperforms baseline approaches, achieving data completeness up to 78.7% and precision up to 100%. We additionally conduct an ablation study demonstrating the contribution of each agent to the Committee's performance. This work is released as an open-source tool for the research community.",
      "publishedDate": "2025-12-25T03:00:22Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21481",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.21450",
      "title": "RLLaVA: An RL-central Framework for Language and Vision Assistants",
      "authors": [
        "Lei Zhao",
        "Zihao Ma",
        "Boyu Lin",
        "Yuhe Liu",
        "Wenjun Wu",
        "Lei Huang"
      ],
      "abstract": "We present an RL-central framework for Language and Vision Assistants (RLLaVA) with its formulation of Markov decision process (MDP). RLLaVA decouples RL algorithmic logic from model architecture and distributed execution, supporting researchers in implementing new RL algorithms with minimal code, and to plug in a broad family of RL methods and vision-language models (VLMs) while remaining agnostic to specific training and inference engines. RLLaVA makes resource-efficient training of 1B--7B models feasible on common GPUs; notably, 4B-scale models can be trained end-to-end with full-parameter updates on a single 24GB GPU. Experiments on multi-modal and agentic tasks demonstrate that RLLaVA has task extensibility, and the models trained with it consistently improve performance over base models, competitive with other specially engineered RL frameworks. The code is available at https://github.com/TinyLoopX/RLLaVA.",
      "publishedDate": "2025-12-25T00:09:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21450",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21440",
      "title": "Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing",
      "authors": [
        "Hridya Dhulipala",
        "Xiaokai Rong",
        "Aashish Yadavally",
        "Tien N. Nguyen"
      ],
      "abstract": "In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.",
      "publishedDate": "2025-12-24T22:17:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21440",
      "categories": [
        "agents",
        "rag",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22266",
      "title": "LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs",
      "authors": [
        "Bing Hao",
        "Minglai Shao",
        "Zengyi Wo",
        "Yunlong Chu",
        "Yuhang Liu",
        "Ruijie Wang"
      ],
      "abstract": "The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.",
      "publishedDate": "2025-12-24T18:10:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22266",
      "categories": [
        "agents",
        "prompting",
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.21243",
      "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
      "authors": [
        "Anatoly O. Onishchenko",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .",
      "publishedDate": "2025-12-24T15:36:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21243",
      "categories": [
        "robotics",
        "agents",
        "planning",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.21236",
      "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
      "authors": [
        "Yifan Huang",
        "Xiaojun Jia",
        "Wenbo Guo",
        "Yuqiang Sun",
        "Yihao Huang",
        "Chong Wang",
        "Yang Liu"
      ],
      "abstract": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
      "publishedDate": "2025-12-24T15:25:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21236",
      "categories": [
        "code-generation",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21135",
      "title": "TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation",
      "authors": [
        "Gaoren Lin",
        "Huangxuan Zhao",
        "Yuan Xiong",
        "Lefei Zhang",
        "Bo Du",
        "Wentao Zhu"
      ],
      "abstract": "Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.",
      "publishedDate": "2025-12-24T12:06:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21135",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21066",
      "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation",
      "authors": [
        "Tomoaki Yamaguchi",
        "Yutong Zhou",
        "Masahiro Ryo",
        "Keisuke Katsura"
      ],
      "abstract": "Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.",
      "publishedDate": "2025-12-24T09:19:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21066",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21373",
      "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories",
      "authors": [
        "Titouan Duston",
        "Shuo Xin",
        "Yang Sun",
        "Daoguang Zan",
        "Aoyan Li",
        "Shulin Xin",
        "Kai Shen",
        "Yixiao Chen",
        "Qiming Sun",
        "Ge Zhang",
        "Jiashuo Liu",
        "Huan Zhou",
        "Jingkai Liu",
        "Zhichen Pu",
        "Yuanheng Wang",
        "Bo-Xuan Ge",
        "Xin Tong",
        "Fei Ye",
        "Zhi-Chao Zhao",
        "Wen-Biao Han",
        "Zhoujian Cao",
        "Yueran Zhao",
        "Weiluo Ren",
        "Qingshen Long",
        "Yuxiao Liu",
        "Anni Huang",
        "Yidi Du",
        "Yuanyuan Rong",
        "Jiahao Peng"
      ],
      "abstract": "We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.",
      "publishedDate": "2025-12-24T08:11:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21373",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20997",
      "title": "LLM-Empowered Agentic AI for QoE-Aware Network Slicing Management in Industrial IoT",
      "authors": [
        "Xudong Wang",
        "Lei Feng",
        "Ruichen Zhang",
        "Fanqin Zhou",
        "Hongyang Du",
        "Wenjing Li",
        "Dusit Niyato",
        "Abbas Jamalipour",
        "Ping Zhang"
      ],
      "abstract": "The Industrial Internet of Things (IIoT) requires networks that deliver ultra-low latency, high reliability, and cost efficiency, which traditional optimization methods and deep reinforcement learning (DRL)-based approaches struggle to provide under dynamic and heterogeneous workloads. To address this gap, large language model (LLM)-empowered agentic AI has emerged as a promising paradigm, integrating reasoning, planning, and adaptation to enable QoE-aware network management. In this paper, we explore the integration of agentic AI into QoE-aware network slicing for IIoT. We first review the network slicing management architecture, QoE metrics for IIoT applications, and the challenges of dynamically managing heterogeneous network slices, while highlighting the motivations and advantages of adopting agentic AI. We then present the workflow of agentic AI-based slicing management, illustrating the full lifecycle of AI agents from processing slice requests to constructing slice instances and performing dynamic adjustments. Furthermore, we propose an LLM-empowered agentic AI approach for slicing management, which integrates a retrieval-augmented generation (RAG) module for semantic intent inference, a DRL-based orchestrator for slicing configuration, and an incremental memory mechanism for continual learning and adaptation. Through a case study on heterogeneous slice management, we demonstrate that the proposed approach significantly outperforms other baselines in balancing latency, reliability, and cost, and achieves up to a 19% improvement in slice availability ratio.",
      "publishedDate": "2025-12-24T06:49:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20997",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20986",
      "title": "AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs",
      "authors": [
        "Yihan Wang",
        "Huanqi Yang",
        "Shantanu Pal",
        "Weitao Xu"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.",
      "publishedDate": "2025-12-24T06:29:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20986",
      "categories": [
        "agents",
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20975",
      "title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking",
      "authors": [
        "Yujin Noh",
        "Inho Jake Park",
        "Chigon Hwang"
      ],
      "abstract": "CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.",
      "publishedDate": "2025-12-24T06:04:58Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20975",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.20973",
      "title": "DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination",
      "authors": [
        "Yihan Xia",
        "Taotao Wang",
        "Wenxin Xu",
        "Shengli Zhang"
      ],
      "abstract": "Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.",
      "publishedDate": "2025-12-24T06:00:39Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20973",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.20957",
      "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
      "authors": [
        "Zhaoxi Zhang",
        "Yitong Duan",
        "Yanzhi Zhang",
        "Yiming Xu",
        "Jiyan He",
        "Yunfang Wu"
      ],
      "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
      "publishedDate": "2025-12-24T05:27:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20957",
      "categories": [
        "agents",
        "code-generation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20954",
      "title": "Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models",
      "authors": [
        "Xiang Zhang",
        "Jiaqi Wei",
        "Yuejin Yang",
        "Zijie Qiu",
        "Yuhan Chen",
        "Zhiqiang Gao",
        "Muhammad Abdul-Mageed",
        "Laks V. S. Lakshmanan",
        "Wanli Ouyang",
        "Chenyu You",
        "Siqi Sun"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary \"thinking tokens\" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.",
      "publishedDate": "2025-12-24T05:25:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20954",
      "categories": [
        "reasoning",
        "prompting",
        "planning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20949",
      "title": "Neural Probe-Based Hallucination Detection for Large Language Models",
      "authors": [
        "Shize Liang",
        "Hongzhi Wang"
      ],
      "abstract": "Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.",
      "publishedDate": "2025-12-24T05:10:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20949",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.22250",
      "title": "Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing",
      "authors": [
        "Bo Yang",
        "Yinfen Xia",
        "Weisong Sun",
        "Yang Liu"
      ],
      "abstract": "In Text-to-SQL generation, large language models (LLMs) have shown strong generalization and adaptability. However, LLMs sometimes generate hallucinations, i.e.,unrealistic or illogical content, which leads to incorrect SQL queries and negatively impacts downstream applications. Detecting these hallucinations is particularly challenging. Existing Text-to-SQL error detection methods, which are tailored for traditional deep learning models, face significant limitations when applied to LLMs. This is primarily due to the scarcity of ground-truth data. To address this challenge, we propose SQLHD, a novel hallucination detection method based on metamorphic testing (MT) that does not require standard answers. SQLHD splits the detection task into two sequentiial stages: schema-linking hallucination detection via eight structure-aware Metamorphic Relations (MRs) that perturb comparative words, entities, sentence structure or database schema, and logical-synthesis hallucination detection via nine logic-aware MRs that mutate prefix words, extremum expressions, comparison ranges or the entire database. In each stage the LLM is invoked separately to generate schema mappings or SQL artefacts; the follow-up outputs are cross-checked against their source counterparts through the corresponding MRs, and any violation is flagged as a hallucination without requiring ground-truth SQL. The experimental results demonstrate our method's superior performance in terms of the F1-score, which ranges from 69.36\\% to 82.76\\%. Additionally, SQLHD demonstrates superior performance over LLM Self-Evaluation methods, effectively identifying hallucinations in Text-to-SQL tasks.",
      "publishedDate": "2025-12-24T04:04:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22250",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20916",
      "title": "MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model",
      "authors": [
        "Haoyu Wang",
        "Yitong Wang",
        "Jining Wang"
      ],
      "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant potential in recommendation systems. However, the effective application of MLLMs to multimodal sequential recommendation remains unexplored: A) Existing methods primarily leverage the multimodal semantic understanding capabilities of pre-trained MLLMs to generate item embeddings or semantic IDs, thereby enhancing traditional recommendation models. These approaches generate item representations that exhibit limited interpretability, and pose challenges when transferring to language model-based recommendation systems. B) Other approaches convert user behavior sequence into image-text pairs and perform recommendation through multiple MLLM inference, incurring prohibitive computational and time costs. C) Current MLLM-based recommendation systems generally neglect the integration of collaborative signals. To address these limitations while balancing recommendation performance, interpretability, and computational cost, this paper proposes MultiModal Summarization-and-Retrieval-Augmented Sequential Recommendation. Specifically, we first employ MLLM to summarize items into concise keywords and fine-tune the model using rewards that incorporate summary length, information loss, and reconstruction difficulty, thereby enabling adaptive adjustment of the summarization policy. Inspired by retrieval-augmented generation, we then transform collaborative signals into corresponding keywords and integrate them as supplementary context. Finally, we apply supervised fine-tuning with multi-task learning to align the MLLM with the multimodal sequential recommendation. Extensive evaluations on common recommendation datasets demonstrate the effectiveness of MMSRARec, showcasing its capability to efficiently and interpretably understand user behavior histories and item information for accurate recommendations.",
      "publishedDate": "2025-12-24T03:44:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20916",
      "categories": [
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20845",
      "title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs",
      "authors": [
        "Onat Ozer",
        "Grace Wu",
        "Yuchen Wang",
        "Daniel Dosti",
        "Honghao Zhang",
        "Vivi De La Rue"
      ],
      "abstract": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.",
      "publishedDate": "2025-12-23T23:47:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20845",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20520",
      "title": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units",
      "authors": [
        "Chehak Malhotra",
        "Mehak Gopal",
        "Akshaya Devadiga",
        "Pradeep Singh",
        "Ridam Pal",
        "Ritwik Kashyap",
        "Tavpritesh Sethi"
      ],
      "abstract": "With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.",
      "publishedDate": "2025-12-23T17:08:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20520",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20362",
      "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
      "authors": [
        "V. Kovalev",
        "A. Kuvshinov",
        "A. Buzovkin",
        "D. Pokidov",
        "D. Timonin"
      ],
      "abstract": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping. We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop. Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",
      "publishedDate": "2025-12-23T13:44:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20362",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.20312",
      "title": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
      "authors": [
        "Saisai Yang",
        "Qingyi Huang",
        "Jing Yuan",
        "Liangyu Zha",
        "Kai Tang",
        "Yuhang Yang",
        "Ning Wang",
        "Yucheng Wei",
        "Liyao Li",
        "Wentao Ye",
        "Hao Chen",
        "Tao Zhang",
        "Junlin Zhou",
        "Haobo Wang",
        "Gang Chen",
        "Junbo Zhao"
      ],
      "abstract": "Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.",
      "publishedDate": "2025-12-23T12:30:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20312",
      "categories": [
        "reasoning",
        "evaluation",
        "agents",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20275",
      "title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
      "authors": [
        "Divya Vijay",
        "Vignesh Ethiraj"
      ],
      "abstract": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.",
      "publishedDate": "2025-12-23T11:27:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20275",
      "categories": [
        "agents",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.20237",
      "title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents",
      "authors": [
        "Xingbo Du",
        "Loka Li",
        "Duzhen Zhang",
        "Le Song"
      ],
      "abstract": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.",
      "publishedDate": "2025-12-23T10:49:42Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20237",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20206",
      "title": "TongSIM: A General Platform for Simulating Intelligent Machines",
      "authors": [
        "Zhe Sun",
        "Kunlun Wu",
        "Chuanjian Fu",
        "Zeming Song",
        "Langyong Shi",
        "Zihe Xue",
        "Bohan Jing",
        "Ying Yang",
        "Xiaomeng Gao",
        "Aijia Li",
        "Tianyu Guo",
        "Huiying Li",
        "Xueyuan Yang",
        "Rongkai Liu",
        "Xinyi He",
        "Yuxi Wang",
        "Yue Li",
        "Mingyuan Liu",
        "Yujie Lu",
        "Hongzhao Xie",
        "Shiyun Zhao",
        "Bo Dai",
        "Wei Wang",
        "Tao Yuan",
        "Song-Chun Zhu",
        "Yujia Peng",
        "Zhenliang Zhang"
      ],
      "abstract": "As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.",
      "publishedDate": "2025-12-23T10:00:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20206",
      "categories": [
        "robotics",
        "evaluation",
        "multi-agent",
        "agents",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20182",
      "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
      "authors": [
        "Shuzheng Si",
        "Qingyi Wang",
        "Haozhe Zhao",
        "Yuzhuo Bai",
        "Guanqiao Chen",
        "Kangyang Luo",
        "Gang Chen",
        "Fanchao Qi",
        "Minjia Zhang",
        "Baobao Chang",
        "Maosong Sun"
      ],
      "abstract": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
      "publishedDate": "2025-12-23T09:20:32Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20182",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20164",
      "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
      "authors": [
        "Honglin Mu",
        "Jinghao Liu",
        "Kaiyang Wan",
        "Rui Xing",
        "Xiuying Chen",
        "Timothy Baldwin",
        "Wanxiang Che"
      ],
      "abstract": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.",
      "publishedDate": "2025-12-23T08:42:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20164",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20135",
      "title": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization",
      "authors": [
        "Zhuo Yang",
        "Yeyun Chen",
        "Jiaqing Xie",
        "Ben Gao",
        "Shuaike Shen",
        "Wanhao Liu",
        "Liujia Yang",
        "Beilun Wang",
        "Tianfan Fu",
        "Yuqiang Li"
      ],
      "abstract": "Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.",
      "publishedDate": "2025-12-23T07:53:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20135",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20111",
      "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
      "authors": [
        "Aly Lidayan",
        "Jakob Bjorner",
        "Satvik Golechha",
        "Kartik Goyal",
        "Alane Suhr"
      ],
      "abstract": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.",
      "publishedDate": "2025-12-23T07:11:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20111",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.20084",
      "title": "QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption",
      "authors": [
        "Yanjie Li",
        "Jian Xu",
        "Xueqing Chen",
        "Lina Yu",
        "Shiming Xiang",
        "Weijun Li",
        "Cheng-lin Liu"
      ],
      "abstract": "Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa. To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.",
      "publishedDate": "2025-12-23T06:27:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20084",
      "categories": [
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20002",
      "title": "LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models",
      "authors": [
        "Jiacheng You",
        "Jingcheng Yang",
        "Yuhang Xie",
        "Zhongxuan Wu",
        "Xiucheng Li",
        "Feng Li",
        "Pengjie Wang",
        "Jian Xu",
        "Bo Zheng",
        "Xinyang Chen"
      ],
      "abstract": "Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.",
      "publishedDate": "2025-12-23T02:55:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20002",
      "categories": [
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.19864",
      "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data",
      "authors": [
        "Shashi Kant Gupta",
        "Arijeet Pramanik",
        "Jerrin John Thomas",
        "Regina Schwind",
        "Lauren Wiener",
        "Avi Raju",
        "Jeremy Kornbluth",
        "Yanshan Wang",
        "Zhaohui Su",
        "Hrituraj Singh"
      ],
      "abstract": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale",
      "publishedDate": "2025-12-22T20:38:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19864",
      "categories": [
        "agents",
        "rag",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.19682",
      "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
      "authors": [
        "Jiacheng Guo",
        "Ling Yang",
        "Peter Chen",
        "Qixin Xiao",
        "Yinjie Wang",
        "Xinzhe Juan",
        "Jiahao Qiu",
        "Ke Shen",
        "Mengdi Wang"
      ],
      "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
      "publishedDate": "2025-12-22T18:57:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19682",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19651",
      "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting",
      "authors": [
        "Filippos Ventirozos",
        "Peter Appleby",
        "Matthew Shardlow"
      ],
      "abstract": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",
      "publishedDate": "2025-12-22T18:23:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19651",
      "categories": [
        "prompting",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.19458",
      "title": "An Agentic Framework for Autonomous Materials Computation",
      "authors": [
        "Zeyu Xia",
        "Jinzhe Ma",
        "Congjie Zheng",
        "Shufei Zhang",
        "Yuqiang Li",
        "Hang Su",
        "P. Hu",
        "Changshui Zhang",
        "Xingao Gong",
        "Wanli Ouyang",
        "Lei Bai",
        "Dongzhan Zhou",
        "Mao Su"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
      "publishedDate": "2025-12-22T15:03:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19458",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19247",
      "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
      "authors": [
        "Do Minh Duc",
        "Quan Xuan Truong",
        "Nguyen Tat Dat",
        "Nguyen Van Vinh"
      ],
      "abstract": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
      "publishedDate": "2025-12-22T10:29:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19247",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19228",
      "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
      "authors": [
        "Valentin Schmidberger",
        "Manuel Eberhardinger",
        "Setareh Maghsudi",
        "Johannes Maucher"
      ],
      "abstract": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.",
      "publishedDate": "2025-12-22T10:08:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19228",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19084",
      "title": "$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics",
      "authors": [
        "Mark Burgess"
      ],
      "abstract": "The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $γ(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.",
      "publishedDate": "2025-12-22T06:48:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19084",
      "categories": [
        "agents",
        "robotics",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.19769",
      "title": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
      "authors": [
        "Ivan Daunis"
      ],
      "abstract": "Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises). Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison. We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.",
      "publishedDate": "2025-12-22T05:03:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19769",
      "categories": [
        "agents",
        "rag",
        "code-generation",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19016",
      "title": "DREAM: Dynamic Red-teaming across Environments for AI Models",
      "authors": [
        "Liming Lu",
        "Xiang Gu",
        "Junyu Huang",
        "Jiawei Du",
        "Yunhuai Liu",
        "Yongbin Zhou",
        "Shuchao Pang"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in agentic systems, where their interactions with diverse tools and environments create complex, multi-stage safety challenges. However, existing benchmarks mostly rely on static, single-turn assessments that miss vulnerabilities from adaptive, long-chain attacks. To fill this gap, we introduce DREAM, a framework for systematic evaluation of LLM agents against dynamic, multi-stage attacks. At its core, DREAM uses a Cross-Environment Adversarial Knowledge Graph (CE-AKG) to maintain stateful, cross-domain understanding of vulnerabilities. This graph guides a Contextualized Guided Policy Search (C-GPS) algorithm that dynamically constructs attack chains from a knowledge base of 1,986 atomic actions across 349 distinct digital environments. Our evaluation of 12 leading LLM agents reveals a critical vulnerability: these attack chains succeed in over 70% of cases for most models, showing the power of stateful, cross-environment exploits. Through analysis of these failures, we identify two key weaknesses in current agents: contextual fragility, where safety behaviors fail to transfer across environments, and an inability to track long-term malicious intent. Our findings also show that traditional safety measures, such as initial defense prompts, are largely ineffective against attacks that build context over multiple interactions. To advance agent safety research, we release DREAM as a tool for evaluating vulnerabilities and developing more robust defenses.",
      "publishedDate": "2025-12-22T04:11:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19016",
      "categories": [
        "evaluation",
        "agents",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.19011",
      "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "authors": [
        "Akshaj Prashanth Rao",
        "Advait Singh",
        "Saumya Kumaar Saksena",
        "Dhruv Kumar"
      ],
      "abstract": "Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead. Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators. Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
      "publishedDate": "2025-12-22T04:00:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19011",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22208",
      "title": "Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA",
      "authors": [
        "Pu Zhao",
        "Xuan Shen",
        "Zhenglun Kong",
        "Yixin Shen",
        "Sung-En Chang",
        "Arash Akbari",
        "Timothy Rupprecht",
        "Lei Lu",
        "Enfu Nan",
        "Changdi Yang",
        "Yumei He",
        "Weiyan Shi",
        "Xingchen Xu",
        "Yu Huang",
        "Wei Jiang",
        "Wei Wang",
        "Yue Chen",
        "Yong He",
        "Yanzhi Wang"
      ],
      "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.",
      "publishedDate": "2025-12-22T02:36:42Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22208",
      "categories": [
        "tool-use",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18966",
      "title": "Scrum Sprint Planning: LLM-based and algorithmic solutions",
      "authors": [
        "Yuwon Yoon",
        "Kevin Iwan",
        "Madeleine Zwart",
        "Xiaohan Qin",
        "Hina Lee",
        "Maria Spichkova"
      ],
      "abstract": "Planning for an upcoming project iteration (sprint) is one of the key activities in Scrum planning. In this paper, we present our work in progress on exploring the applicability of Large Language Models (LLMs) for solving this problem. We conducted case studies with manually created data sets to investigate the applicability of OpenAI models for supporting the sprint planning activities. In our experiments, we applied three models provided OpenAI: GPT-3.5 Turbo, GPT-4.0 Turbo, and Val. The experiments demonstrated that the results produced by the models aren't of acceptable quality for direct use in Scrum projects.",
      "publishedDate": "2025-12-22T02:26:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18966",
      "categories": [
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.21354",
      "title": "Reflection-Driven Control for Trustworthy Code Agents",
      "authors": [
        "Bin Wang",
        "Jiazheng Quan",
        "Xingrui Yu",
        "Hansen Hu",
        "Yuhao",
        "Ivor Tsang"
      ],
      "abstract": "Contemporary large language model (LLM) agents are remarkably capable, but they still lack reliable safety controls and can produce unconstrained, unpredictable, and even actively harmful outputs. To address this, we introduce Reflection-Driven Control, a standardized and pluggable control module that can be seamlessly integrated into general agent architectures. Reflection-Driven Control elevates \"self-reflection\" from a post hoc patch into an explicit step in the agent's own reasoning process: during generation, the agent continuously runs an internal reflection loop that monitors and evaluates its own decision path. When potential risks are detected, the system retrieves relevant repair examples and secure coding guidelines from an evolving reflective memory, injecting these evidence-based constraints directly into subsequent reasoning steps. We instantiate Reflection-Driven Control in the setting of secure code generation and systematically evaluate it across eight classes of security-critical programming tasks. Empirical results show that Reflection-Driven Control substantially improves the security and policy compliance of generated code while largely preserving functional correctness, with minimal runtime and token overhead. Taken together, these findings indicate that Reflection-Driven Control is a practical path toward trustworthy AI coding agents: it enables designs that are simultaneously autonomous, safer by construction, and auditable.",
      "publishedDate": "2025-12-22T00:27:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21354",
      "categories": [
        "code-generation",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.18857",
      "title": "CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning",
      "authors": [
        "Zijun Gao",
        "Zhikun Xu",
        "Xiao Ye",
        "Ben Zhou"
      ],
      "abstract": "Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.",
      "publishedDate": "2025-12-21T19:01:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18857",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18733",
      "title": "Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection",
      "authors": [
        "Junjun Pan",
        "Yixin Liu",
        "Rui Miao",
        "Kaize Ding",
        "Yu Zheng",
        "Quoc Viet Hung Nguyen",
        "Alan Wee-Chung Liew",
        "Shirui Pan"
      ],
      "abstract": "Large language model (LLM)-based multi-agent systems (MAS) have shown strong capabilities in solving complex tasks. As MAS become increasingly autonomous in various safety-critical tasks, detecting malicious agents has become a critical security concern. Although existing graph anomaly detection (GAD)-based defenses can identify anomalous agents, they mainly rely on coarse sentence-level information and overlook fine-grained lexical cues, leading to suboptimal performance. Moreover, the lack of interpretability in these methods limits their reliability and real-world applicability. To address these limitations, we propose XG-Guard, an explainable and fine-grained safeguarding framework for detecting malicious agents in MAS. To incorporate both coarse and fine-grained textual information for anomalous agent identification, we utilize a bi-level agent encoder to jointly model the sentence- and token-level representations of each agent. A theme-based anomaly detector further captures the evolving discussion focus in MAS dialogues, while a bi-level score fusion mechanism quantifies token-level contributions for explanation. Extensive experiments across diverse MAS topologies and attack scenarios demonstrate robust detection performance and strong interpretability of XG-Guard.",
      "publishedDate": "2025-12-21T13:46:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18733",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18671",
      "title": "SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse",
      "authors": [
        "Yiming Sun",
        "Mi Zhang",
        "Feifei Li",
        "Geng Hong",
        "Min Yang"
      ],
      "abstract": "Despite Video Large Language Models having rapidly advanced in recent years, perceptual hallucinations pose a substantial safety risk, which severely restricts their real-world applicability. While several methods for hallucination mitigation have been proposed, they often compromise the model's capacity for video understanding and reasoning. In this work, we propose SmartSight, a pioneering step to address this issue in a training-free manner by leveraging the model's own introspective capabilities. Specifically, SmartSight generates multiple candidate responses to uncover low-hallucinated outputs that are often obscured by standard greedy decoding. It assesses the hallucination of each response using the Temporal Attention Collapse score, which measures whether the model over-focuses on trivial temporal regions of the input video when generating the response. To improve efficiency, SmartSight identifies the Visual Attention Vanishing point, enabling more accurate hallucination estimation and early termination of hallucinated responses, leading to a substantial reduction in decoding cost. Experiments show that SmartSight substantially lowers hallucinations for Qwen2.5-VL-7B by 10.59% on VRIPT-HAL, while simultaneously enhancing video understanding and reasoning, boosting performance on VideoMMMU by up to 8.86%. These results highlight SmartSight's effectiveness in improving the reliability of open-source Video-LLMs.",
      "publishedDate": "2025-12-21T10:25:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18671",
      "categories": [
        "tool-use",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18623",
      "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
      "authors": [
        "Jensen Zhang",
        "Ningyuan Liu",
        "Yijia Fan",
        "Zihao Huang",
        "Qinglin Zeng",
        "Kaitong Cai",
        "Jian Wang",
        "Keze Wang"
      ],
      "abstract": "Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting. We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification. Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.",
      "publishedDate": "2025-12-21T06:54:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18623",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.18599",
      "title": "SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback",
      "authors": [
        "Jianglin Lu",
        "Yuanwei Wu",
        "Ziyi Zhao",
        "Hongcheng Wang",
        "Felix Jimenez",
        "Abrar Majeedi",
        "Yun Fu"
      ],
      "abstract": "Complex image restoration aims to recover high-quality images from inputs affected by multiple degradations such as blur, noise, rain, and compression artifacts. Recent restoration agents, powered by vision-language models and large language models, offer promising restoration capabilities but suffer from significant efficiency bottlenecks due to reflection, rollback, and iterative tool searching. Moreover, their performance heavily depends on degradation recognition models that require extensive annotations for training, limiting their applicability in label-free environments. To address these limitations, we propose a policy optimization-based restoration framework that learns an lightweight agent to determine tool-calling sequences. The agent operates in a sequential decision process, selecting the most appropriate restoration operation at each step to maximize final image quality. To enable training within label-free environments, we introduce a novel reward mechanism driven by multimodal large language models, which act as human-aligned evaluator and provide perceptual feedback for policy improvement. Once trained, our agent executes a deterministic restoration plans without redundant tool invocations, significantly accelerating inference while maintaining high restoration quality. Extensive experiments show that despite using no supervision, our method matches SOTA performance on full-reference metrics and surpasses existing approaches on no-reference metrics across diverse degradation scenarios.",
      "publishedDate": "2025-12-21T05:12:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18599",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18582",
      "title": "Wireless Copilot: An AI-Powered Partner for Navigating Next-Generation Wireless Complexity",
      "authors": [
        "Haoxiang Luo",
        "Ruichen Zhang",
        "Yinqiu Liu",
        "Gang Sun",
        "Hongfang Yu",
        "Dusit Niyato",
        "Shiwen Mao",
        "Dong In Kim"
      ],
      "abstract": "The sixth-generation (6G) of wireless networks introduces a level of operational complexity that exceeds the limits of traditional automation and manual oversight. This paper introduces the \"Wireless Copilot\", an AI-powered technical assistant designed to function as a collaborative partner for human network designers, engineers, and operators. We posit that by integrating Large Language Models (LLMs) with a robust cognitive framework. It will surpass the existing AI tools and interact with wireless devices, transmitting the user's intentions into the actual network execution process. Then, Wireless Copilot can translate high-level human intent into precise, optimized, and verifiable network actions. This framework bridges the gap between human expertise and machine-scale complexity, enabling more efficient, intelligent, and trustworthy management of 6G systems. Wireless Copilot will be a novel layer between the wireless infrastructure and the network operators. Moreover, we explore Wireless Copilot's methodology and analyze its application in Low-Altitude Wireless Networks (LAWNets) assisting 6G networking, including network design, configuration, evaluation, and optimization. Additionally, we present a case study on intent-based LAWNets resource allocation, demonstrating its superior adaptability compared to others. Finally, we outline future research directions toward creating a comprehensive human-AI collaborative ecosystem for the 6G era.",
      "publishedDate": "2025-12-21T03:58:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18582",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21352",
      "title": "Multi-Agent LLM Committees for Autonomous Software Beta Testing",
      "authors": [
        "Sumanth Bharadwaj Hachalli Karanam",
        "Dhiwahar Adhithya Kennady"
      ],
      "abstract": "Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.",
      "publishedDate": "2025-12-21T02:06:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21352",
      "categories": [
        "agents",
        "rag",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18552",
      "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
      "authors": [
        "Yuxiang Wei",
        "Zhiqing Sun",
        "Emily McMilin",
        "Jonas Gehring",
        "David Zhang",
        "Gabriel Synnaeve",
        "Daniel Fried",
        "Lingming Zhang",
        "Sida Wang"
      ],
      "abstract": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.",
      "publishedDate": "2025-12-21T00:49:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18552",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21351",
      "title": "CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation",
      "authors": [
        "Santhosh Kumar Ravindran"
      ],
      "abstract": "Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.",
      "publishedDate": "2025-12-20T22:12:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21351",
      "categories": [
        "agents",
        "code-generation",
        "evaluation",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.18436",
      "title": "VeruSAGE: A Study of Agent-Based Verification for Rust Systems",
      "authors": [
        "Chenyuan Yang",
        "Natalie Neamtu",
        "Chris Hawblitzel",
        "Jacob R. Lorch",
        "Shan Lu"
      ],
      "abstract": "Large language models (LLMs) have shown impressive capability to understand and develop code. However, their capability to rigorously reason about and prove code correctness remains in question. This paper offers a comprehensive study of LLMs' capability to develop correctness proofs for system software written in Rust. We curate a new system-verification benchmark suite, VeruSAGE-Bench, which consists of 849 proof tasks extracted from eight open-source Verus-verified Rust systems. Furthermore, we design different agent systems to match the strengths and weaknesses of different LLMs (o4-mini, GPT-5, Sonnet 4, and Sonnet 4.5). Our study shows that different tools and agent settings are needed to stimulate the system-verification capability of different types of LLMs. The best LLM-agent combination in our study completes over 80% of system-verification tasks in VeruSAGE-Bench. It also completes over 90% of a set of system proof tasks not part of VeruSAGE-Bench because they had not yet been finished by human experts. This result shows the great potential for LLM-assisted development of verified system software.",
      "publishedDate": "2025-12-20T17:22:52Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18436",
      "categories": [
        "code-generation",
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18371",
      "title": "Phoneme-based speech recognition driven by large language models and sampling marginalization",
      "authors": [
        "Te Ma",
        "Nanjie Li",
        "Hao Huang",
        "Zhijian Ou"
      ],
      "abstract": "Recently, the Large Language Model-based Phoneme-to-Grapheme (LLM-P2G) method has shown excellent performance in speech recognition tasks and has become a feasible direction to replace the traditional WFST decoding method. This framework takes into account both recognition accuracy and system scalability through two-stage modeling of phoneme prediction and text generation. However, the existing LLM-P2G adopts the Top-K Marginalized (TKM) training strategy, and its candidate phoneme sequences rely on beam search generation, which has problems such as insufficient path diversity, low training efficiency, and high resource overhead. To this end, this paper proposes a sampling marginalized training strategy (Sampling-K Marginalized, SKM), which replaces beam search with random sampling to generate candidate paths, improving marginalized modeling and training efficiency. Experiments were conducted on Polish and German datasets, and the results showed that SKM further improved the model learning convergence speed and recognition performance while maintaining the complexity of the model. Comparative experiments with a speech recognition method that uses a projector combined with a large language model (SpeechLLM) also show that the SKM-driven LLM-P2G has more advantages in recognition accuracy and structural simplicity. The study verified the practical value and application potential of this method in cross-language speech recognition systems.",
      "publishedDate": "2025-12-20T14:13:28Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18371",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18360",
      "title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators",
      "authors": [
        "Mateusz Lango",
        "Ondřej Dušek"
      ],
      "abstract": "We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models",
      "publishedDate": "2025-12-20T13:16:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18360",
      "categories": [
        "agents",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18352",
      "title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent",
      "authors": [
        "Fengzhu Zeng",
        "Qian Shao",
        "Ling Cheng",
        "Wei Gao",
        "Shih-Fen Cheng",
        "Jing Ma",
        "Cheng Niu"
      ],
      "abstract": "Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.",
      "publishedDate": "2025-12-20T12:42:27Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18352",
      "categories": [
        "agents",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.18292",
      "title": "Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy",
      "authors": [
        "Wenkai Li",
        "Lynnette Hui Xian Ng",
        "Andy Liu",
        "Daniel Fried"
      ],
      "abstract": "The study of negotiation styles dates back to Aristotle's ethos-pathos-logos rhetoric. Prior efforts primarily studied the success of negotiation agents. Here, we shift the focus towards the styles of negotiation strategies. Our focus is the strategic dialogue board game Diplomacy, which affords rich natural language negotiation and measures of game success. We used LLM-as-a-judge to annotate a large human-human set of Diplomacy games for fine-grained negotiation tactics from a sociologically-grounded taxonomy. Using a combination of the It Takes Two and WebDiplomacy datasets, we demonstrate the reliability of our LLM-as-a-Judge framework and show strong correlations between negotiation features and success in the Diplomacy setting. Lastly, we investigate the differences between LLM and human negotiation strategies and show that fine-tuning can steer LLM agents toward more human-like negotiation behaviors.",
      "publishedDate": "2025-12-20T09:33:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18292",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.18265",
      "title": "Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration",
      "authors": [
        "Himabindu Thogaru",
        "Saisubramaniam Gopalakrishnan",
        "Zishan Ahmad",
        "Anirudh Deodhar"
      ],
      "abstract": "Manufacturing planners face complex operational challenges that require seamless collaboration between human expertise and intelligent systems to achieve optimal performance in modern production environments. Traditional approaches to analyzing simulation-based manufacturing data often create barriers between human decision-makers and critical operational insights, limiting effective partnership in manufacturing planning. Our framework establishes a collaborative intelligence system integrating Knowledge Graphs and Large Language Model-based agents to bridge this gap, empowering manufacturing professionals through natural language interfaces for complex operational analysis. The system transforms simulation data into semantically rich representations, enabling planners to interact naturally with operational insights without specialized expertise. A collaborative LLM agent works alongside human decision-makers, employing iterative reasoning that mirrors human analytical thinking while generating precise queries for knowledge extraction and providing transparent validation. This partnership approach to manufacturing bottleneck identification, validated through operational scenarios, demonstrates enhanced performance while maintaining human oversight and decision authority. For operational inquiries, the system achieves near-perfect accuracy through natural language interaction. For investigative scenarios requiring collaborative analysis, we demonstrate the framework's effectiveness in supporting human experts to uncover interconnected operational issues that enhance understanding and decision-making. This work advances collaborative manufacturing by creating intuitive methods for actionable insights, reducing cognitive load while amplifying human analytical capabilities in evolving manufacturing ecosystems.",
      "publishedDate": "2025-12-20T08:09:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18265",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.17756",
      "title": "AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora",
      "authors": [
        "Zhihan Zhou",
        "Daqian Shi",
        "Rui Song",
        "Lida Shi",
        "Xiaolei Diao",
        "Hao Xu"
      ],
      "abstract": "Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.",
      "publishedDate": "2025-12-19T16:28:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17756",
      "categories": [
        "evaluation",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.17321",
      "title": "Neuro-Symbolic Control with Large Language Models for Language-Guided Spatial Tasks",
      "authors": [
        "Momina Liaqat Ali",
        "Muhammad Abid"
      ],
      "abstract": "Although large language models (LLMs) have recently become effective tools for language-conditioned control in embodied systems, instability, slow convergence, and hallucinated actions continue to limit their direct application to continuous control. A modular neuro-symbolic control framework that clearly distinguishes between low-level motion execution and high-level semantic reasoning is proposed in this work. While a lightweight neural delta controller performs bounded, incremental actions in continuous space, a locally deployed LLM interprets symbolic tasks. We assess the suggested method in a planar manipulation setting with spatial relations between objects specified by language. Numerous tasks and local language models, such as Mistral, Phi, and LLaMA-3.2, are used in extensive experiments to compare LLM-only control, neural-only control, and the suggested LLM+DL framework. In comparison to LLM-only baselines, the results show that the neuro-symbolic integration consistently increases both success rate and efficiency, achieving average step reductions exceeding 70% and speedups of up to 8.83x while remaining robust to language model quality. The suggested framework enhances interpretability, stability, and generalization without any need of reinforcement learning or costly rollouts by controlling the LLM to symbolic outputs and allocating uninterpreted execution to a neural controller trained on artificial geometric data. These outputs show empirically that neuro-symbolic decomposition offers a scalable and principled way to integrate language understanding with ongoing control, this approach promotes the creation of dependable and effective language-guided embodied systems.",
      "publishedDate": "2025-12-19T08:08:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17321",
      "categories": [
        "reasoning",
        "rag",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.17172",
      "title": "PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases",
      "authors": [
        "Ripan Kumar Kundu",
        "Istiak Ahmed",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "Artificial intelligence (AI)-driven augmented reality (AR) systems are becoming increasingly integrated into daily life, and with this growth comes a greater need for explainability in real-time user interactions. Traditional explainable AI (XAI) methods, which often rely on feature-based or example-based explanations, struggle to deliver dynamic, context-specific, personalized, and human-centric insights for everyday AR users. These methods typically address separate explainability dimensions (e.g., when, what, how) with different explanation techniques, resulting in unrealistic and fragmented experiences for seamless AR interactions. To address this challenge, we propose PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate context-aware, personalized explanations, offering a more intuitive and trustworthy experience in real-time AI-powered AR systems. Unlike traditional methods, which rely on multiple techniques for different aspects of explanation, PILAR employs a unified LLM-based approach that dynamically adapts explanations to the user's needs, fostering greater trust and engagement. We implement the PILAR concept in a real-world AR application (e.g., personalized recipe recommendations), an open-source prototype that integrates real-time object detection, recipe recommendation, and LLM-based personalized explanations of the recommended recipes based on users' dietary preferences. We evaluate the effectiveness of PILAR through a user study with 16 participants performing AR-based recipe recommendation tasks, comparing an LLM-based explanation interface to a traditional template-based one. Results show that the LLM-based interface significantly enhances user performance and experience, with participants completing tasks 40% faster and reporting greater satisfaction, ease of use, and perceived transparency.",
      "publishedDate": "2025-12-19T02:19:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17172",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.17164",
      "title": "TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval",
      "authors": [
        "Yu Yang",
        "Feng Tian",
        "Ping Chen"
      ],
      "abstract": "Query Expansion (QE) enriches queries and Document Expansion (DE) enriches documents, and these two techniques are often applied separately. However, such separate application may lead to semantic misalignment between the expanded queries (or documents) and their relevant documents (or queries). To address this serious issue, we propose TCDE, a dual expansion strategy that leverages large language models (LLMs) for topic-centric enrichment on both queries and documents. In TCDE, we design two distinct prompt templates for processing each query and document. On the query side, an LLM is guided to identify distinct sub-topics within each query and generate a focused pseudo-document for each sub-topic. On the document side, an LLM is guided to distill each document into a set of core topic sentences. The resulting outputs are used to expand the original query and document. This topic-centric dual expansion process establishes semantic bridges between queries and their relevant documents, enabling better alignment for downstream retrieval models. Experiments on two challenging benchmarks, TREC Deep Learning and BEIR, demonstrate that TCDE achieves substantial improvements over strong state-of-the-art expansion baselines. In particular, on dense retrieval tasks, it outperforms several state-of-the-art methods, with a relative improvement of 2.8\\% in NDCG@10 on the SciFact dataset. Experimental results validate the effectiveness of our topic-centric and dual expansion strategy.",
      "publishedDate": "2025-12-19T01:57:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17164",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19753",
      "title": "QMBench: A Research Level Benchmark for Quantum Materials Research",
      "authors": [
        "Yanzhen Wang",
        "Yiyang Jiang",
        "Diana Golovanova",
        "Kamal Das",
        "Hyeonhu Bae",
        "Yufei Zhao",
        "Huu-Thong Le",
        "Abhinava Chatterjee",
        "Yunzhe Liu",
        "Chao-Xing Liu",
        "Felipe H. da Jornada",
        "Binghai Yan",
        "Xiao-Liang Qi"
      ],
      "abstract": "We introduce QMBench, a comprehensive benchmark designed to evaluate the capability of large language model agents in quantum materials research. This specialized benchmark assesses the model's ability to apply condensed matter physics knowledge and computational techniques such as density functional theory to solve research problems in quantum materials science. QMBench encompasses different domains of the quantum material research, including structural properties, electronic properties, thermodynamic and other properties, symmetry principle and computational methodologies. By providing a standardized evaluation framework, QMBench aims to accelerate the development of an AI scientist capable of making creative contributions to quantum materials research. We expect QMBench to be developed and constantly improved by the research community.",
      "publishedDate": "2025-12-19T00:57:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19753",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.17146",
      "title": "Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors",
      "authors": [
        "Huixin Zhan"
      ],
      "abstract": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.",
      "publishedDate": "2025-12-19T00:51:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17146",
      "categories": [
        "evaluation",
        "agents",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.17060",
      "title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues",
      "authors": [
        "Monika Zamojska",
        "Jarosław A. Chudziak"
      ],
      "abstract": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.",
      "publishedDate": "2025-12-18T20:53:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17060",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.17008",
      "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
      "authors": [
        "Junbo Li",
        "Peng Zhou",
        "Rui Meng",
        "Meet P. Vadera",
        "Lihong Li",
        "Yang Li"
      ],
      "abstract": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
      "publishedDate": "2025-12-18T19:07:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17008",
      "categories": [
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.16901",
      "title": "Impacts of Racial Bias in Historical Training Data for News AI",
      "authors": [
        "Rahul Bhargava",
        "Malene Hornstrup Jespersen",
        "Emily Boardman Ndulue",
        "Vivica Dsouza"
      ],
      "abstract": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
      "publishedDate": "2025-12-18T18:56:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16901",
      "categories": [
        "code-generation",
        "tool-use",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.16848",
      "title": "Meta-RL Induces Exploration in Language Agents",
      "authors": [
        "Yulun Jiang",
        "Liangze Jiang",
        "Damien Teney",
        "Michael Moor",
        "Maria Brbic"
      ],
      "abstract": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
      "publishedDate": "2025-12-18T18:22:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16848",
      "categories": [
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.16650",
      "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
      "authors": [
        "Jirui Yang",
        "Hengqi Guo",
        "Zhihui Lu",
        "Yi Zhao",
        "Yuansen Zhang",
        "Shijing Hu",
        "Qiang Duan",
        "Yinggui Wang",
        "Tao Wei"
      ],
      "abstract": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
      "publishedDate": "2025-12-18T15:22:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16650",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.16530",
      "title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics",
      "authors": [
        "Primoz Kocbek",
        "Leon Kopitar",
        "Gregor Stiglic"
      ],
      "abstract": "This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.",
      "publishedDate": "2025-12-18T13:37:58Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16530",
      "categories": [
        "evaluation",
        "agents",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.16970",
      "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework",
      "authors": [
        "Kamer Ali Yuksel"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.",
      "publishedDate": "2025-12-18T12:54:56Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16970",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16453",
      "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
      "authors": [
        "Jiayang Yang",
        "Chunhui Zhao",
        "Martin Guay",
        "Zhixing Cao"
      ],
      "abstract": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
      "publishedDate": "2025-12-18T12:15:52Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16453",
      "categories": [
        "prompting",
        "evaluation",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16381",
      "title": "A Network Arena for Benchmarking AI Agents on Network Troubleshooting",
      "authors": [
        "Zhihao Wang",
        "Alessandro Cornacchia",
        "Alessio Sacco",
        "Franco Galante",
        "Marco Canini",
        "Dingde Jiang"
      ],
      "abstract": "Agentic systems, powered by Large Language Models (LLMs), assist network engineers with network configuration synthesis and network troubleshooting tasks. For network troubleshooting, progress is hindered by the absence of standardized and accessible benchmarks for evaluating LLM agents in dynamic network settings at low operational effort. We present NIKA, the largest public benchmark to date for LLM-driven network incident diagnosis and troubleshooting. NIKA targets both domain experts and especially AI researchers alike, providing zero-effort replay of real-world network scenarios, and establishing well-defined agent-network interfaces for quick agent prototyping. NIKA comprises hundreds of curated network incidents, spanning five network scenarios, from data centers to ISP networks, and covers 54 representative network issues. Lastly, NIKA is modular and extensible by design, offering APIs to facilitate the integration of new network scenarios and failure cases. We evaluate state-of-the-art LLM agents on NIKA and find that while larger models succeed more often in detecting network issues, they still struggle to localize faults and identify root causes. NIKA is open-source and available to the community: https://github.com/sands-lab/nika.",
      "publishedDate": "2025-12-18T10:22:59Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16381",
      "categories": [
        "agents",
        "evaluation",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.16310",
      "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
      "authors": [
        "Yuxuan Qiao",
        "Dongqin Liu",
        "Hongchang Yang",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "abstract": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
      "publishedDate": "2025-12-18T08:50:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16310",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.16962",
      "title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval",
      "authors": [
        "Saksham Sahai Srivastava",
        "Haoyu He"
      ],
      "abstract": "Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.",
      "publishedDate": "2025-12-18T08:34:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16962",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16297",
      "title": "Feature-Selective Representation Misdirection for Machine Unlearning",
      "authors": [
        "Taozhao Chen",
        "Linghan Huang",
        "Kim-Kwang Raymond Choo",
        "Huaming Chen"
      ],
      "abstract": "As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",
      "publishedDate": "2025-12-18T08:31:50Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16297",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16280",
      "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
      "authors": [
        "Gilad Gressel",
        "Rahul Pankajakshan",
        "Shir Rozenfeld",
        "Ling Li",
        "Ivan Franceschini",
        "Krishnashree Achuthan",
        "Yisroel Mirsky"
      ],
      "abstract": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation. We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.",
      "publishedDate": "2025-12-18T07:59:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16280",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16250",
      "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
      "authors": [
        "Sanjoy Chowdhury",
        "Karren D. Yang",
        "Xudong Liu",
        "Fartash Faghri",
        "Pavan Kumar Anasosalu Vasu",
        "Oncel Tuzel",
        "Dinesh Manocha",
        "Chun-Liang Li",
        "Raviteja Vemulapalli"
      ],
      "abstract": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
      "publishedDate": "2025-12-18T07:01:47Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16250",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.16244",
      "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models",
      "authors": [
        "Xueqi Ma",
        "Xingjun Ma",
        "Sarah Monazam Erfani",
        "Danilo Mandic",
        "James Bailey"
      ],
      "abstract": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.",
      "publishedDate": "2025-12-18T06:50:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16244",
      "categories": [
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.16227",
      "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
      "authors": [
        "Qizhou Chen",
        "Chengyu Wang",
        "Taolin Zhang",
        "Xiaofeng He"
      ],
      "abstract": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
      "publishedDate": "2025-12-18T06:21:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16227",
      "categories": [
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16182",
      "title": "DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack",
      "authors": [
        "Hao Li",
        "Yubing Ren",
        "Yanan Cao",
        "Yingjie Li",
        "Fang Fang",
        "Shi Wang",
        "Li Guo"
      ],
      "abstract": "With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.",
      "publishedDate": "2025-12-18T05:08:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16182",
      "categories": [
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.16167",
      "title": "Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services",
      "authors": [
        "Shiduo Yang",
        "Jiye Wang",
        "Jiayu Qin",
        "Jianbin Li",
        "Yu Wang",
        "Yuanhe Zhao",
        "Kenan Guo"
      ],
      "abstract": "The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized \"Request-Response-Payment-Evaluation\" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.",
      "publishedDate": "2025-12-18T04:39:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16167",
      "categories": [
        "agents",
        "multi-agent",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16063",
      "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
      "authors": [
        "Qidi Xu",
        "Nuzha Amjad",
        "Grace Giles",
        "Alexa Cumming",
        "De'angelo Hermesky",
        "Alexander Wen",
        "Min Ji Kwak",
        "Yejin Kim"
      ],
      "abstract": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
      "publishedDate": "2025-12-18T01:13:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16063",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16056",
      "title": "MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services",
      "authors": [
        "Lingfeng Tang",
        "Daoping Zhang",
        "Junjie Chen",
        "Peihao Huang",
        "Feng Jin",
        "Chengguang Xu",
        "Yuxin Chen",
        "Feiqiang Sun",
        "Guo Chen"
      ],
      "abstract": "The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.",
      "publishedDate": "2025-12-18T00:45:00Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16056",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16036",
      "title": "Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education",
      "authors": [
        "Diane Myung-kyung Woodbridge",
        "Allyson Seba",
        "Freddie Seba",
        "Aydin Schwartz"
      ],
      "abstract": "As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.",
      "publishedDate": "2025-12-17T23:39:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16036",
      "categories": [],
      "year": 2025
    },
    {
      "id": "2512.16022",
      "title": "Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting",
      "authors": [
        "Defu Cao",
        "Michael Gee",
        "Jinbo Liu",
        "Hengxuan Wang",
        "Wei Yang",
        "Rui Wang",
        "Yan Liu"
      ],
      "abstract": "The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.",
      "publishedDate": "2025-12-17T23:14:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16022",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.16019",
      "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios",
      "authors": [
        "Qiping Zhang",
        "Nathan Tsoi",
        "Mofeed Nagib",
        "Hao-Tien Lewis Chiang",
        "Marynel Vázquez"
      ],
      "abstract": "Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.",
      "publishedDate": "2025-12-17T23:06:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16019",
      "categories": [
        "prompting",
        "rag",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19742",
      "title": "On-device Large Multi-modal Agent for Human Activity Recognition",
      "authors": [
        "Md Shakhrul Iman Siam",
        "Ishtiaque Ahmed Showmik",
        "Guanqun Song",
        "Ting Zhu"
      ],
      "abstract": "Human Activity Recognition (HAR) has been an active area of research, with applications ranging from healthcare to smart environments. The recent advancements in Large Language Models (LLMs) have opened new possibilities to leverage their capabilities in HAR, enabling not just activity classification but also interpretability and human-like interaction. In this paper, we present a Large Multi-Modal Agent designed for HAR, which integrates the power of LLMs to enhance both performance and user engagement. The proposed framework not only delivers activity classification but also bridges the gap between technical outputs and user-friendly insights through its reasoning and question-answering capabilities. We conduct extensive evaluations using widely adopted HAR datasets, including HHAR, Shoaib, Motionsense to assess the performance of our framework. The results demonstrate that our model achieves high classification accuracy comparable to state-of-the-art methods while significantly improving interpretability through its reasoning and Q&A capabilities.",
      "publishedDate": "2025-12-17T22:05:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19742",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15959",
      "title": "BRAID: Bounded Reasoning for Autonomous Inference and Decisions",
      "authors": [
        "Armağan Amcalar",
        "Eyup Cinar"
      ],
      "abstract": "Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.",
      "publishedDate": "2025-12-17T20:46:44Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15959",
      "categories": [
        "prompting",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15943",
      "title": "Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning",
      "authors": [
        "Polaris Jhandi",
        "Owais Kazi",
        "Shreyas Subramanian",
        "Neel Sendas"
      ],
      "abstract": "As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\\%), ToolLLaMA-DFS (30.18\\%), and ToolLLaMA-CoT (16.27\\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.",
      "publishedDate": "2025-12-17T20:12:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15943",
      "categories": [
        "agents",
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15892",
      "title": "VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces",
      "authors": [
        "Artem Grigor",
        "Christian Schroeder de Witt",
        "Simon Birnbach",
        "Ivan Martinovic"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy. We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs). We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.",
      "publishedDate": "2025-12-17T19:05:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15892",
      "categories": [
        "agents",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.15867",
      "title": "HEPTAPOD: Orchestrating High Energy Physics Workflows Towards Autonomous Agency",
      "authors": [
        "Tony Menzo",
        "Alexander Roman",
        "Sergei Gleyzer",
        "Konstantin Matchev",
        "George T. Fleming",
        "Stefan Höche",
        "Stephen Mrenna",
        "Prasanth Shyamsundar"
      ],
      "abstract": "Many workflows in high-energy-physics (HEP) stand to benefit from recent advances in transformer-based large language models (LLMs). While early applications of LLMs focused on text generation and code completion, modern LLMs now support orchestrated agency: the coordinated execution of complex, multi-step tasks through tool use, structured context, and iterative reasoning. We introduce the HEP Toolkit for Agentic Planning, Orchestration, and Deployment (HEPTAPOD), an orchestration framework designed to bring this emerging paradigm to HEP pipelines. The framework enables LLMs to interface with domain-specific tools, construct and manage simulation workflows, and assist in common utility and data analysis tasks through schema-validated operations and run-card-driven configuration. To demonstrate these capabilities, we consider a representative Beyond the Standard Model (BSM) Monte Carlo validation pipeline that spans model generation, event simulation, and downstream analysis within a unified, reproducible workflow. HEPTAPOD provides a structured and auditable layer between human researchers, LLMs, and computational infrastructure, establishing a foundation for transparent, human-in-the-loop systems.",
      "publishedDate": "2025-12-17T19:00:03Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15867",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15614",
      "title": "Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary",
      "authors": [
        "Xinshun Feng",
        "Mingzhe Liu",
        "Yi Qiao",
        "Tongyu Zhu",
        "Leilei Sun",
        "Shuai Wang"
      ],
      "abstract": "Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.",
      "publishedDate": "2025-12-17T17:24:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15614",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15374",
      "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
      "authors": [
        "Zehua Pei",
        "Hui-Ling Zhen",
        "Shixiong Kai",
        "Sinno Jialin Pan",
        "Yunhe Wang",
        "Mingxuan Yuan",
        "Bei Yu"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \\textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \\textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
      "publishedDate": "2025-12-17T12:25:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15374",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15365",
      "title": "ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata",
      "authors": [
        "Gajendra Doniparthi",
        "Shashank Balu Pandhare",
        "Stefan Deßloch",
        "Timo Mühlhaus"
      ],
      "abstract": "Traditional search applications within Research Data Management (RDM) ecosystems are crucial in helping users discover and explore the structured metadata from the research datasets. Typically, text search engines require users to submit keyword-based queries rather than using natural language. However, using Large Language Models (LLMs) trained on domain-specific content for specialized natural language processing (NLP) tasks is becoming increasingly common. We present ArcBERT, an LLM-based system designed for integrated metadata exploration. ArcBERT understands natural language queries and relies on semantic matching, unlike traditional search applications. Notably, ArcBERT also understands the structure and hierarchies within the metadata, enabling it to handle diverse user querying patterns effectively.",
      "publishedDate": "2025-12-17T12:11:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15365",
      "categories": [],
      "year": 2025
    },
    {
      "id": "2512.15275",
      "title": "Bounty Hunter: Autonomous, Comprehensive Emulation of Multi-Faceted Adversaries",
      "authors": [
        "Louis Hackländer-Jansen",
        "Rafael Uetz",
        "Martin Henze"
      ],
      "abstract": "Adversary emulation is an essential procedure for cybersecurity assessments such as evaluating an organization's security posture or facilitating structured training and research in dedicated environments. To allow for systematic and time-efficient assessments, several approaches from academia and industry have worked towards the automation of adversarial actions. However, they exhibit significant limitations regarding autonomy, tactics coverage, and real-world applicability. Consequently, adversary emulation remains a predominantly manual task requiring substantial human effort and security expertise - even amidst the rise of Large Language Models. In this paper, we present Bounty Hunter, an automated adversary emulation method, designed and implemented as an open-source plugin for the popular adversary emulation platform Caldera, that enables autonomous emulation of adversaries with multi-faceted behavior while providing a wide coverage of tactics. To this end, it realizes diverse adversarial behavior, such as different levels of detectability and varying attack paths across repeated emulations. By autonomously compromising a simulated enterprise network, Bounty Hunter showcases its ability to achieve given objectives without prior knowledge of its target, including pre-compromise, initial compromise, and post-compromise attack tactics. Overall, Bounty Hunter facilitates autonomous, comprehensive, and multi-faceted adversary emulation to help researchers and practitioners in performing realistic and time-efficient security assessments, training exercises, and intrusion detection research.",
      "publishedDate": "2025-12-17T10:27:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15275",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15231",
      "title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications",
      "authors": [
        "Zhengchao Chen",
        "Haoran Wang",
        "Jing Yao",
        "Pedram Ghamisi",
        "Jun Zhou",
        "Peter M. Atkinson",
        "Bing Zhang"
      ],
      "abstract": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).",
      "publishedDate": "2025-12-17T09:31:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15231",
      "categories": [
        "agents",
        "rag",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22165",
      "title": "Marco-ASR: A Principled and Metric-Driven Framework for Fine-Tuning Large-Scale ASR Models for Domain Adaptation",
      "authors": [
        "Xuanfan Ni",
        "Fei Yang",
        "Fengping Tian",
        "Qingjuan Li",
        "Chenyang Lyu",
        "Yichao Du",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "abstract": "Automatic Speech Recognition (ASR) models have achieved remarkable accuracy in general settings, yet their performance often degrades in domain-specific applications due to data mismatch and linguistic variability. This challenge is amplified for modern Large Language Model (LLM)-based ASR systems, whose massive scale and complex training dynamics make effective fine-tuning non-trivial. To address this gap, this paper proposes a principled and metric-driven fine-tuning framework for adapting both traditional and LLM-based ASR models to specialized domains. The framework emphasizes learning rate optimization based on performance metrics, combined with domain-specific data transformation and augmentation. We empirically evaluate our framework on state-of-the-art models, including Whisper, Whisper-Turbo, and Qwen2-Audio, across multi-domain, multilingual, and multi-length datasets. Our results not only validate the proposed framework but also establish practical protocols for improving domain-specific ASR performance while preventing overfitting.",
      "publishedDate": "2025-12-17T07:31:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22165",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15098",
      "title": "Uni-Parser Technical Report",
      "authors": [
        "Xi Fang",
        "Haoyi Tao",
        "Shuwen Yang",
        "Suyang Zhong",
        "Haocheng Lu",
        "Han Lyu",
        "Chaozheng Huang",
        "Xinyu Li",
        "Linfeng Zhang",
        "Guolin Ke"
      ],
      "abstract": "This technical report introduces Uni-Parser, an industrial-grade document parsing engine tailored for scientific literature and patents, delivering high throughput, robust accuracy, and cost efficiency. Unlike pipeline-based document parsing methods, Uni-Parser employs a modular, loosely coupled multi-expert architecture that preserves fine-grained cross-modal alignments across text, equations, tables, figures, and chemical structures, while remaining easily extensible to emerging modalities. The system incorporates adaptive GPU load balancing, distributed inference, dynamic module orchestration, and configurable modes that support either holistic or modality-specific parsing. Optimized for large-scale cloud deployment, Uni-Parser achieves a processing rate of up to 20 PDF pages per second on 8 x NVIDIA RTX 4090D GPUs, enabling cost-efficient inference across billions of pages. This level of scalability facilitates a broad spectrum of downstream applications, ranging from literature retrieval and summarization to the extraction of chemical structures, reaction schemes, and bioactivity data, as well as the curation of large-scale corpora for training next-generation large language models and AI4Science models.",
      "publishedDate": "2025-12-17T05:41:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15098",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.15053",
      "title": "The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops",
      "authors": [
        "Fanzhe Fu"
      ],
      "abstract": "The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based \"prompt engineering,\" fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for \"Observable Software Engineering\" in the era of probabilistic computing.",
      "publishedDate": "2025-12-17T03:32:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15053",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15000",
      "title": "DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding",
      "authors": [
        "Ruiyi Zhang",
        "Peijia Qin",
        "Qi Cao",
        "Pengtao Xie"
      ],
      "abstract": "Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.",
      "publishedDate": "2025-12-17T01:11:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15000",
      "categories": [
        "code-generation",
        "prompting",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.14896",
      "title": "DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline",
      "authors": [
        "Houman Kazemzadeh",
        "Kiarash Mokhtari Dizaji",
        "Seyed Reza Tavakoli",
        "Farbod Davoodi",
        "MohammadReza KarimiNejad",
        "Parham Abed Azad",
        "Ali Sabzi",
        "Armin Khosravi",
        "Siavash Ahmadi",
        "Mohammad Hossein Rohban",
        "Glolamali Aminian",
        "Tahereh Javaheri"
      ],
      "abstract": "Objectives: To evaluate large language model (LLM) performance on pharmacy licensure-style question-answering (QA) tasks and develop an external knowledge integration method to improve their accuracy. Methods: We benchmarked eleven existing LLMs with varying parameter sizes (8 billion to 70+ billion) using a 141-question pharmacy dataset. We measured baseline accuracy for each model without modification. We then developed a three-step retrieval-augmented generation (RAG) pipeline, DrugRAG, that retrieves structured drug knowledge from validated sources and augments model prompts with evidence-based context. This pipeline operates externally to the models, requiring no changes to model architecture or parameters. Results: Baseline accuracy ranged from 46% to 92%, with GPT-5 (92%) and o3 (89%) achieving the highest scores. Models with fewer than 8 billion parameters scored below 50%. DrugRAG improved accuracy across all tested models, with gains ranging from 7 to 21 percentage points (e.g., Gemma 3 27B: 61% to 71%, Llama 3.1 8B: 46% to 67%) on the 141-item benchmark. Conclusion: We demonstrate that external structured drug knowledge integration through DrugRAG measurably improves LLM accuracy on pharmacy tasks without modifying the underlying models. This approach provides a practical pipeline for enhancing pharmacy-focused AI applications with evidence-based information.",
      "publishedDate": "2025-12-16T20:19:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14896",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14895",
      "title": "Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections",
      "authors": [
        "Niklas Lauffer",
        "Xiang Deng",
        "Srivatsa Kundurthy",
        "Brad Kenstler",
        "Jeff Da"
      ],
      "abstract": "A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.",
      "publishedDate": "2025-12-16T20:19:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14895",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14846",
      "title": "MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber",
      "authors": [
        "Arth Bhardwaj",
        "Sia Godika",
        "Yuvam Loonker"
      ],
      "abstract": "Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&CK mappings). For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records. In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.",
      "publishedDate": "2025-12-16T19:08:12Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14846",
      "categories": [
        "agents",
        "evaluation",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.14600",
      "title": "PerProb: Indirectly Evaluating Memorization in Large Language Models",
      "authors": [
        "Yihan Liao",
        "Jacky Keung",
        "Xiaoxue Ma",
        "Jingyu Zhang",
        "Yicheng Sun"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has been driven by extensive datasets that may contain sensitive information, raising serious privacy concerns. One notable threat is the Membership Inference Attack (MIA), where adversaries infer whether a specific sample was used in model training. However, the true impact of MIA on LLMs remains unclear due to inconsistent findings and the lack of standardized evaluation methods, further complicated by the undisclosed nature of many LLM training sets. To address these limitations, we propose PerProb, a unified, label-free framework for indirectly assessing LLM memorization vulnerabilities. PerProb evaluates changes in perplexity and average log probability between data generated by victim and adversary models, enabling an indirect estimation of training-induced memory. Compared with prior MIA methods that rely on member/non-member labels or internal access, PerProb is independent of model and task, and applicable in both black-box and white-box settings. Through a systematic classification of MIA into four attack patterns, we evaluate PerProb's effectiveness across five datasets, revealing varying memory behaviors and privacy risks among LLMs. Additionally, we assess mitigation strategies, including knowledge distillation, early stopping, and differential privacy, demonstrating their effectiveness in reducing data leakage. Our findings offer a practical and generalizable framework for evaluating and improving LLM privacy.",
      "publishedDate": "2025-12-16T17:10:01Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14600",
      "categories": [
        "tool-use",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14565",
      "title": "Pairwise Comparison for Bias Identification and Quantification",
      "authors": [
        "Fabian Haak",
        "Philipp Schaer"
      ],
      "abstract": "Linguistic bias in online news and social media is widespread but difficult to measure. Yet, its identification and quantification remain difficult due to subjectivity, context dependence, and the scarcity of high-quality gold-label datasets. We aim to reduce annotation effort by leveraging pairwise comparison for bias annotation. To overcome the costliness of the approach, we evaluate more efficient implementations of pairwise comparison-based rating. We achieve this by investigating the effects of various rating techniques and the parameters of three cost-aware alternatives in a simulation environment. Since the approach can in principle be applied to both human and large language model annotation, our work provides a basis for creating high-quality benchmark datasets and for quantifying biases and other subjective linguistic aspects. The controlled simulations include latent severity distributions, distance-calibrated noise, and synthetic annotator bias to probe robustness and cost-quality trade-offs. In applying the approach to human-labeled bias benchmark datasets, we then evaluate the most promising setups and compare them to direct assessment by large language models and unmodified pairwise comparison labels as baselines. Our findings support the use of pairwise comparison as a practical foundation for quantifying subjective linguistic aspects, enabling reproducible bias analysis. We contribute an optimization of comparison and matchmaking components, an end-to-end evaluation including simulation and real-data application, and an implementation blueprint for cost-aware large-scale annotation",
      "publishedDate": "2025-12-16T16:36:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14565",
      "categories": [
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.14448",
      "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space",
      "authors": [
        "Xingfu Zhou",
        "Pengfei Wang"
      ],
      "abstract": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.",
      "publishedDate": "2025-12-16T14:34:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14448",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14429",
      "title": "Seismology modeling agent: A smart assistant for geophysical researchers",
      "authors": [
        "Yukun Ren",
        "Siwei Yu",
        "Kai Chen",
        "Jianwei Ma"
      ],
      "abstract": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.",
      "publishedDate": "2025-12-16T14:18:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14429",
      "categories": [
        "agents",
        "code-generation",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.14321",
      "title": "Multi-Agent Medical Decision Consensus Matrix System: An Intelligent Collaborative Framework for Oncology MDT Consultations",
      "authors": [
        "Xudong Han",
        "Xianglun Gao",
        "Xiaoyi Qu",
        "Zhenyu Yu"
      ],
      "abstract": "Multidisciplinary team (MDT) consultations are the gold standard for cancer care decision-making, yet current practice lacks structured mechanisms for quantifying consensus and ensuring decision traceability. We introduce a Multi-Agent Medical Decision Consensus Matrix System that deploys seven specialized large language model agents, including an oncologist, a radiologist, a nurse, a psychologist, a patient advocate, a nutritionist and a rehabilitation therapist, to simulate realistic MDT workflows. The framework incorporates a mathematically grounded consensus matrix that uses Kendall's coefficient of concordance to objectively assess agreement. To further enhance treatment recommendation quality and consensus efficiency, the system integrates reinforcement learning methods, including Q-Learning, PPO and DQN. Evaluation across five medical benchmarks (MedQA, PubMedQA, DDXPlus, MedBullets and SymCat) shows substantial gains over existing approaches, achieving an average accuracy of 87.5% compared with 83.8% for the strongest baseline, a consensus achievement rate of 89.3% and a mean Kendall's W of 0.823. Expert reviewers rated the clinical appropriateness of system outputs at 8.9/10. The system guarantees full evidence traceability through mandatory citations of clinical guidelines and peer-reviewed literature, following GRADE principles. This work advances medical AI by providing structured consensus measurement, role-specialized multi-agent collaboration and evidence-based explainability to improve the quality and efficiency of clinical decision-making.",
      "publishedDate": "2025-12-16T11:35:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14321",
      "categories": [
        "multi-agent",
        "agents",
        "evaluation",
        "tool-use",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.14244",
      "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
      "authors": [
        "Yiqing Zhou",
        "Yu Lei",
        "Shuzheng Si",
        "Qingyan Sun",
        "Wei Wang",
        "Yifei Wu",
        "Hao Wen",
        "Gang Chen",
        "Fanchao Qi",
        "Maosong Sun"
      ],
      "abstract": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.",
      "publishedDate": "2025-12-16T09:52:58Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14244",
      "categories": [
        "agents",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14233",
      "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design",
      "authors": [
        "Ruozhao Yang",
        "Mingfei Cheng",
        "Gelei Deng",
        "Tianwei Zhang",
        "Junjie Wang",
        "Xiaofei Xie"
      ],
      "abstract": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",
      "publishedDate": "2025-12-16T09:37:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14233",
      "categories": [
        "evaluation",
        "agents",
        "prompting",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.14166",
      "title": "IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol",
      "authors": [
        "Yunhao Yao",
        "Zhiqiang Wang",
        "Haoran Cheng",
        "Yihang Cheng",
        "Haohua Du",
        "Xiang-Yang Li"
      ],
      "abstract": "The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.",
      "publishedDate": "2025-12-16T07:52:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14166",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.15790",
      "title": "Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures (XAMT)",
      "authors": [
        "Akhil Sharma",
        "Shaikh Yaser Arafat",
        "Jai Kumar Sharma",
        "Ken Huang"
      ],
      "abstract": "The increasing operational reliance on complex Multi-Agent Systems (MAS) across safety-critical domains necessitates rigorous adversarial robustness assessment. Modern MAS are inherently heterogeneous, integrating conventional Multi-Agent Reinforcement Learning (MARL) with emerging Large Language Model (LLM) agent architectures utilizing Retrieval-Augmented Generation (RAG). A critical shared vulnerability is reliance on centralized memory components: the shared Experience Replay (ER) buffer in MARL and the external Knowledge Base (K) in RAG agents. This paper proposes XAMT (Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures), a novel framework that formalizes attack generation as a bilevel optimization problem. The Upper Level minimizes perturbation magnitude (delta) to enforce covertness while maximizing system behavior divergence toward an adversary-defined target (Lower Level). We provide rigorous mathematical instantiations for CTDE MARL algorithms and RAG-based LLM agents, demonstrating that bilevel optimization uniquely crafts stealthy, minimal-perturbation poisons evading detection heuristics. Comprehensive experimental protocols utilize SMAC and SafeRAG benchmarks to quantify effectiveness at sub-percent poison rates (less than or equal to 1 percent in MARL, less than or equal to 0.1 percent in RAG). XAMT defines a new unified class of training-time threats essential for developing intrinsically secure MAS, with implications for trust, formal verification, and defensive strategies prioritizing intrinsic safety over perimeter-based detection.",
      "publishedDate": "2025-12-15T23:04:48Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15790",
      "categories": [
        "rag",
        "agents",
        "evaluation",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.14762",
      "title": "Workflows vs Agents for Code Translation",
      "authors": [
        "Henry Gray",
        "Tom Yotam",
        "Octavian Udrea"
      ],
      "abstract": "Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \\cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.",
      "publishedDate": "2025-12-15T20:35:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14762",
      "categories": [
        "agents",
        "rag",
        "tool-use",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.13438",
      "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
      "authors": [
        "Dezhi Ran",
        "Zhi Gong",
        "Yuzhe Guo",
        "Mengzhou Wu",
        "Yuan Cao",
        "Haochuan Lu",
        "Hengyu Zhang",
        "Xia Zeng",
        "Gang Cao",
        "Liangchao Yao",
        "Yuetang Deng",
        "Wei Yang",
        "Tao Xie"
      ],
      "abstract": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
      "publishedDate": "2025-12-15T15:34:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.13438",
      "categories": [
        "agents",
        "evaluation",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15784",
      "title": "Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM",
      "authors": [
        "Zibin Liu",
        "Cheng Zhang",
        "Xi Zhao",
        "Yunfei Feng",
        "Bingyu Bai",
        "Dahu Feng",
        "Erhu Feng",
        "Yubin Xia",
        "Haibo Chen"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency. To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors. Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.",
      "publishedDate": "2025-12-15T12:38:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15784",
      "categories": [
        "rag",
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.13278",
      "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning",
      "authors": [
        "Jiaru Zou",
        "Ling Yang",
        "Yunzhe Qi",
        "Sirui Chen",
        "Mengting Ai",
        "Ke Shen",
        "Jingrui He",
        "Mengdi Wang"
      ],
      "abstract": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.",
      "publishedDate": "2025-12-15T12:38:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.13278",
      "categories": [
        "agents",
        "code-generation",
        "tool-use",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.12806",
      "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution",
      "authors": [
        "Boyang Yan"
      ],
      "abstract": "The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows.",
      "publishedDate": "2025-12-14T19:03:59Z",
      "arxivUrl": "https://arxiv.org/abs/2512.12806",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.12791",
      "title": "Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems",
      "authors": [
        "Sreemaee Akshathala",
        "Bassam Adnan",
        "Mahisha Ramesh",
        "Karthik Vaidhyanathan",
        "Basil Muhammed",
        "Kannan Parthasarathy"
      ],
      "abstract": "Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. These challenges emerged during our ongoing industry collaboration with MontyCloud Inc., when we deployed an agentic system in production. These limitations surfaced during deployment, highlighting practical gaps in the current evaluation methods and the need for a systematic assessment of agent behavior beyond task outcomes. Informed by these observations and established definitions of agentic systems, we propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.",
      "publishedDate": "2025-12-14T18:17:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.12791",
      "categories": [
        "agents",
        "evaluation",
        "multi-agent",
        "reasoning",
        "planning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.12716",
      "title": "CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning",
      "authors": [
        "Xuanzhang Liu",
        "Jianglun Feng",
        "Zhuoran Zhuang",
        "Junzhe Zhao",
        "Maofei Que",
        "Jieting Li",
        "Dianlei Wang",
        "Hao Tong",
        "Ye Chen",
        "Pan Li"
      ],
      "abstract": "Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by \"Context Explosion\", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.",
      "publishedDate": "2025-12-14T14:41:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.12716",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.12597",
      "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation",
      "authors": [
        "Miriam Horovicz"
      ],
      "abstract": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.",
      "publishedDate": "2025-12-14T08:31:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.12597",
      "categories": [
        "agents",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.11689",
      "title": "Evaluating Cooperative Resilience in Multiagent Systems: A Comparison Between Humans and LLMs",
      "authors": [
        "Manuela Chacon-Chamorro",
        "Juan Sebastián Pinzón",
        "Rubén Manrique",
        "Luis Felipe Giraldo",
        "Nicanor Quijano"
      ],
      "abstract": "This paper presents a comparative analysis of cooperative resilience in multi-agent systems, defined as the ability to anticipate, resist, recover from, and transform to disruptive events that affect collective well-being. We focus on mixed-motive social dilemmas instantiated as a \\textit{Tragedy of the Commons} environment from the Melting Pot suite, where we systematically compare human groups and Large Language Model (LLM)-based agents, each evaluated with and without explicit communication. Cooperative resilience is assessed under a continuously disruptive condition induced by a persistent unsustainable consumption bot, together with intermittent environmental shocks implemented as stochastic removal of shared resources across scenarios. This experimental design establishes a benchmark for cooperative resilience across agent architectures and interaction modalities, constituting a key step toward systematically comparing humans and LLM-based agents. Using this framework, we find that human groups with communication achieve the highest cooperative resilience compared to all other groups. Communication also improves the resilience of LLM agents, but their performance remains below human levels. Motivated by the performance of humans, we further examine a long-horizon setting with harsher environmental conditions, where humans sustain the shared resource and maintain high resilience in diverse disruption scenarios. Together, these results suggest that human decision-making under adverse social conditions can inform the design of artificial agents that promote prosocial and resilient behaviors.",
      "publishedDate": "2025-12-12T16:11:47Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11689",
      "categories": [
        "agents",
        "rag",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.11303",
      "title": "Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture",
      "authors": [
        "Jiarun Liu",
        "Shiyue Xu",
        "Yang Li",
        "Shangkun Liu",
        "Yongli Yu",
        "Peng Cao"
      ],
      "abstract": "Large Language Model agents face fundamental challenges in adapting to novel tasks due to limitations in tool availability and experience reuse. Existing approaches either rely on predefined tools with limited coverage or build tools from scratch without leveraging past experiences, leading to inefficient exploration and suboptimal performance. We introduce SMITH (Shared Memory Integrated Tool Hub), a unified cognitive architecture that seamlessly integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. SMITH organizes agent memory into procedural, semantic, and episodic components, enabling systematic capability expansion while preserving successful execution patterns. Our approach formalizes tool creation as iterative code generation within controlled sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. We further propose a curriculum learning strategy based on agent-ensemble difficulty re-estimation. Extensive experiments on the GAIA benchmark demonstrate SMITH's effectiveness, achieving 81.8% Pass@1 accuracy and outperforming state-of-the-art baselines including Alita (75.2%) and Memento (70.9%). Our work establishes a foundation for building truly adaptive agents that continuously evolve their capabilities through principled integration of tool creation and experience accumulation.",
      "publishedDate": "2025-12-12T06:00:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11303",
      "categories": [
        "agents",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.11143",
      "title": "Automated Penetration Testing with LLM Agents and Classical Planning",
      "authors": [
        "Lingzhi Wang",
        "Xinyi Shi",
        "Ziyu Li",
        "Yi Jiang",
        "Shiyu Tan",
        "Yuhao Jiang",
        "Junjie Cheng",
        "Wenyuan Chen",
        "Xiangmin Shen",
        "Zhenyuan LI",
        "Yan Chen"
      ],
      "abstract": "While penetration testing plays a vital role in cybersecurity, achieving fully automated, hands-off-the-keyboard execution remains a significant research challenge. In this paper, we introduce the \"Planner-Executor-Perceptor (PEP)\" design paradigm and use it to systematically review existing work and identify the key challenges in this area. We also evaluate existing penetration testing systems, with a particular focus on the use of Large Language Model (LLM) agents for this task. The results show that the out-of-the-box Claude Code and Sonnet 4.5 exhibit superior penetration capabilities observed to date, substantially outperforming all prior systems. However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools. These limitations significantly constrain its overall capability, efficiency, and stability. To address these limitations, we propose CHECKMATE, a framework that integrates enhanced classical planning with LLM agents, providing an external, structured \"brain\" that mitigates the inherent weaknesses of LLM agents. Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%. In addition, it delivers substantially greater stability, cutting both time and monetary costs by more than 50%.",
      "publishedDate": "2025-12-11T22:04:39Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11143",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10931",
      "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs",
      "authors": [
        "George Yakushev",
        "Nataliia Babina",
        "Masoud Vahid Dastgerdi",
        "Vyacheslav Zhdanovskiy",
        "Alina Shutova",
        "Denis Kuznedelev"
      ],
      "abstract": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",
      "publishedDate": "2025-12-11T18:57:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10931",
      "categories": [
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.10696",
      "title": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution",
      "authors": [
        "Zouying Cao",
        "Jiaji Deng",
        "Li Yu",
        "Weikang Zhou",
        "Zhaoyang Liu",
        "Bolin Ding",
        "Hai Zhao"
      ],
      "abstract": "Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research.",
      "publishedDate": "2025-12-11T14:40:01Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10696",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10534",
      "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
      "authors": [
        "Haiteng Zhao",
        "Junhao Shen",
        "Yiming Zhang",
        "Songyang Gao",
        "Kuikun Liu",
        "Tianyou Ma",
        "Fan Zheng",
        "Dahua Lin",
        "Wenwei Zhang",
        "Kai Chen"
      ],
      "abstract": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
      "publishedDate": "2025-12-11T11:05:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10534",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10501",
      "title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation",
      "authors": [
        "Lim Chien Her",
        "Ming Yan",
        "Yunshu Bai",
        "Ruihao Li",
        "Hao Zhang"
      ],
      "abstract": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.",
      "publishedDate": "2025-12-11T10:22:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10501",
      "categories": [
        "agents",
        "prompting",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10393",
      "title": "Cross-modal Retrieval Models for Stripped Binary Analysis",
      "authors": [
        "Guoqiang Chen",
        "Lingyun Ying",
        "Ziyang Song",
        "Daguang Liu",
        "Qiang Wang",
        "Zhiqi Wang",
        "Li Hu",
        "Shaoyin Cheng",
        "Weiming Zhang",
        "Nenghai Yu"
      ],
      "abstract": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.",
      "publishedDate": "2025-12-11T07:58:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10393",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.10195",
      "title": "AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding",
      "authors": [
        "Gyutaek Oh",
        "Sangjoon Park",
        "Byung-Hoon Kim"
      ],
      "abstract": "Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.",
      "publishedDate": "2025-12-11T01:25:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10195",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.11907",
      "title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents",
      "authors": [
        "Daniel Platnick",
        "Marjan Alirezaie",
        "Hossein Rahnama"
      ],
      "abstract": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.",
      "publishedDate": "2025-12-10T20:22:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11907",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.11001",
      "title": "Query Optimization Beyond Data Systems: The Case for Multi-Agent Systems",
      "authors": [
        "Zoi Kaoudi",
        "Ioana Giurgiu"
      ],
      "abstract": "The proliferation of large language models (LLMs) has accelerated the adoption of agent-based workflows, where multiple autonomous agents reason, invoke functions, and collaborate to compose complex data pipelines. However, current approaches to building such agentic architectures remain largely ad hoc, lacking generality, scalability, and systematic optimization. Existing systems often rely on fixed models and single execution engines and are unable to efficiently optimize multiple agents operating over heterogeneous data sources and query engines. This paper presents a vision for a next-generation query optimization framework tailored to multi-agent workflows. We argue that optimizing these workflows can benefit from redesigning query optimization principles to account for new challenges: orchestration of diverse agents, cost efficiency under expensive LLM calls and across heterogeneous engines, and redundancy across tasks. Led by a real-world example and building on an analysis of multi-agent workflows, we outline our envisioned architecture and the main research challenges of building a multi-agent query optimization framework, which aims at enabling automated model selection, workflow composition, and execution across heterogeneous engines. This vision establishes the groundwork for query optimization in emerging multi-agent architectures and opens up a set of future research directions.",
      "publishedDate": "2025-12-10T20:16:20Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11001",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.09543",
      "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
      "authors": [
        "Arihant Tripathy",
        "Ch Pavan Harshit",
        "Karthik Vaidhyanathan"
      ],
      "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood. Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs. Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration. Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency. Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
      "publishedDate": "2025-12-10T11:28:48Z",
      "arxivUrl": "https://arxiv.org/abs/2512.09543",
      "categories": [
        "agents",
        "code-generation",
        "evaluation",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.09440",
      "title": "Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making",
      "authors": [
        "Qingyuan Zhang",
        "Yuxi Wang",
        "Cancan Hua",
        "Yulin Huang",
        "Ning Lyu"
      ],
      "abstract": "This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.",
      "publishedDate": "2025-12-10T09:08:33Z",
      "arxivUrl": "https://arxiv.org/abs/2512.09440",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.09254",
      "title": "The Illusion of Rationality: Tacit Bias and Strategic Dominance in Frontier LLM Negotiation Games",
      "authors": [
        "Manuel S. Ríos",
        "Ruben F. Manrique",
        "Nicanor Quijano",
        "Luis F. Giraldo"
      ],
      "abstract": "Large language models (LLMs) are increasingly being deployed as autonomous agents on behalf of institutions and individuals in economic, political, and social settings that involve negotiation. Yet this trend carries significant risks if their strategic behavior is not well understood. In this work, we revisit the NegotiationArena framework and run controlled simulation experiments on a diverse set of frontier LLMs across three multi turn bargaining games: Buyer Seller, Multi turn Ultimatum, and Resource Exchange. We ask whether improved general reasoning capabilities lead to rational, unbiased, and convergent negotiation strategies. Our results challenge this assumption. We find that models diverge into distinct, model specific strategic equilibria rather than converging to a unified optimal behavior. Moreover, strong numerical and semantic anchoring effects persist: initial offers are highly predictive of final agreements, and models consistently generate biased proposals by collapsing diverse internal valuations into rigid, generic price points. More concerningly, we observe dominance patterns in which some models systematically achieve higher payoffs than their counterparts. These findings underscore an urgent need to develop mechanisms to mitigate these issues before deploying such systems in real-world scenarios.",
      "publishedDate": "2025-12-10T02:17:28Z",
      "arxivUrl": "https://arxiv.org/abs/2512.09254",
      "categories": [
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.09108",
      "title": "Evolving Excellence: Automated Optimization of LLM-based Agents",
      "authors": [
        "Paul Brookes",
        "Vardan Voskanyan",
        "Rafail Giavrimis",
        "Matthew Truscott",
        "Mina Ilieva",
        "Chrystalla Pavlou",
        "Alexandru Staicu",
        "Manal Adham",
        "Will Evers- Hood",
        "Jingzhi Gong",
        "Kejia Zhang",
        "Matvey Fedoseev",
        "Vishal Sharma",
        "Roman Bauer",
        "Zheng Wang",
        "Hema Nair",
        "Wei Jie",
        "Tianhua Xu",
        "Aurora Constantin",
        "Leslie Kanthan",
        "Michail Basios"
      ],
      "abstract": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies. We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications. We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.",
      "publishedDate": "2025-12-09T20:48:45Z",
      "arxivUrl": "https://arxiv.org/abs/2512.09108",
      "categories": [
        "code-generation",
        "agents",
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.08870",
      "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
      "authors": [
        "Xiang Chen",
        "Yuling Shi",
        "Qizhen Lan",
        "Yuchao Qiu",
        "Xiaodong Gu"
      ],
      "abstract": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
      "publishedDate": "2025-12-09T18:04:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08870",
      "categories": [
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.08737",
      "title": "Insured Agents: A Decentralized Trust Insurance Mechanism for Agentic Economy",
      "authors": [
        "Botao 'Amber' Hu",
        "Bangdao Chen"
      ],
      "abstract": "The emerging \"agentic web\" envisions large populations of autonomous agents coordinating, transacting, and delegating across open networks. Yet many agent communication and commerce protocols treat agents as low-cost identities, despite the empirical reality that LLM agents remain unreliable, hallucinated, manipulable, and vulnerable to prompt-injection and tool-abuse. A natural response is \"agents-at-stake\": binding economically meaningful, slashable collateral to persistent identities and adjudicating misbehavior with verifiable evidence. However, heterogeneous tasks make universal verification brittle and centralization-prone, while traditional reputation struggles under rapid model drift and opaque internal states. We propose a protocol-native alternative: insured agents. Specialized insurer agents post stake on behalf of operational agents in exchange for premiums, and receive privileged, privacy-preserving audit access via TEEs to assess claims. A hierarchical insurer market calibrates stake through pricing, decentralizes verification via competitive underwriting, and yields incentive-compatible dispute resolution.",
      "publishedDate": "2025-12-09T15:47:16Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08737",
      "categories": [
        "agents",
        "tool-use",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.08483",
      "title": "NeurIDA: Dynamic Modeling for Effective In-Database Analytics",
      "authors": [
        "Lingze Zeng",
        "Naili Xing",
        "Shaofeng Cai",
        "Peng Lu",
        "Gang Chen",
        "Jian Pei",
        "Beng Chin Ooi"
      ],
      "abstract": "Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics. We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically \"tweaks\" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improvement in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA",
      "publishedDate": "2025-12-09T11:01:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08483",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.08476",
      "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems",
      "authors": [
        "Po-An Shih",
        "Shao-Hua Wang",
        "Yung-Che Li",
        "Chia-Heng Tu",
        "Chih-Han Chang"
      ],
      "abstract": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.",
      "publishedDate": "2025-12-09T10:50:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08476",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "code-generation",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.08366",
      "title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making",
      "authors": [
        "Wentao Zhang",
        "Qunbo Wang",
        "Tao Zhang",
        "Junsheng Wu",
        "Hongping Gan",
        "Yang Liu",
        "Ling Dai",
        "Shizhuang Deng",
        "Shuntong Sun"
      ],
      "abstract": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.",
      "publishedDate": "2025-12-09T08:44:59Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08366",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.14720",
      "title": "SoMe: A Realistic Benchmark for LLM-based Social Media Agents",
      "authors": [
        "Dizhan Xue",
        "Jing Cui",
        "Shengsheng Qian",
        "Chuanrui Hu",
        "Changsheng Xu"
      ],
      "abstract": "Intelligent agents powered by large language models (LLMs) have recently demonstrated impressive capabilities and gained increasing popularity on social media platforms. While LLM agents are reshaping the ecology of social media, there exists a current gap in conducting a comprehensive evaluation of their ability to comprehend media content, understand user behaviors, and make intricate decisions. To address this challenge, we introduce SoMe, a pioneering benchmark designed to evaluate social media agents equipped with various agent tools for accessing and analyzing social media data. SoMe comprises a diverse collection of 8 social media agent tasks, 9,164,284 posts, 6,591 user profiles, and 25,686 reports from various social media platforms and external websites, with 17,869 meticulously annotated task queries. Compared with the existing datasets and benchmarks for social media tasks, SoMe is the first to provide a versatile and realistic platform for LLM-based social media agents to handle diverse social media tasks. By extensive quantitative and qualitative analysis, we provide the first overview insight into the performance of mainstream agentic LLMs in realistic social media environments and identify several limitations. Our evaluation reveals that both the current closed-source and open-source LLMs cannot handle social media agent tasks satisfactorily. SoMe provides a challenging yet meaningful testbed for future social media agents. Our code and data are available at https://github.com/LivXue/SoMe",
      "publishedDate": "2025-12-09T08:36:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14720",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.08290",
      "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem",
      "authors": [
        "Shiva Gaire",
        "Srijan Gyawali",
        "Saroj Mishra",
        "Suman Niroula",
        "Dilip Thakur",
        "Umesh Yadav"
      ],
      "abstract": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the \"USB-C for Agentic AI.\" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how \"context\" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.",
      "publishedDate": "2025-12-09T06:39:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08290",
      "categories": [
        "agents",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.08145",
      "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models",
      "authors": [
        "Haoran Wang",
        "Zhuohang Chen",
        "Guang Li",
        "Bo Ma",
        "Chuanghuang Li"
      ],
      "abstract": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.",
      "publishedDate": "2025-12-09T00:55:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08145",
      "categories": [
        "agents",
        "planning",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.07785",
      "title": "Automating High Energy Physics Data Analysis with LLM-Powered Agents",
      "authors": [
        "Eli Gendreau-Distler",
        "Joshua Ho",
        "Dongwon Kim",
        "Luc Tomas Le Pottier",
        "Haichen Wang",
        "Chengxi Yang"
      ],
      "abstract": "We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.",
      "publishedDate": "2025-12-08T18:13:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.07785",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.07665",
      "title": "Reliable agent engineering should integrate machine-compatible organizational principles",
      "authors": [
        "R. Patrick Xian",
        "Garry A. Gabison",
        "Ahmed Alaa",
        "Christoph Riedl",
        "Grigorios G. Chrysos"
      ],
      "abstract": "As AI agents built on large language models (LLMs) become increasingly embedded in society, issues of coordination, control, delegation, and accountability are entangled with concerns over their reliability. To design and implement LLM agents around reliable operations, we should consider the task complexity in the application settings and reduce their limitations while striving to minimize agent failures and optimize resource efficiency. High-functioning human organizations have faced similar balancing issues, which led to evidence-based theories that seek to understand their functioning strategies. We examine the parallels between LLM agents and the compatible frameworks in organization science, focusing on what the design, scaling, and management of organizations can inform agentic systems towards improving reliability. We offer three preliminary accounts of organizational principles for AI agent engineering to attain reliability and effectiveness, through balancing agency and capabilities in agent design, resource constraints and performance benefits in agent scaling, and internal and external mechanisms in agent management. Our work extends the growing exchanges between the operational and governance principles of AI systems and social systems to facilitate system integration.",
      "publishedDate": "2025-12-08T15:58:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.07665",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.07497",
      "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations",
      "authors": [
        "JV Roig"
      ],
      "abstract": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.",
      "publishedDate": "2025-12-08T12:27:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.07497",
      "categories": [
        "agents",
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.07287",
      "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents",
      "authors": [
        "Sijia Li",
        "Yuchen Huang",
        "Zifan Liu",
        "Zijian Li",
        "Jingjing fu",
        "Lei Song",
        "Jiang Bian",
        "Jun Zhang",
        "Rui Wang"
      ],
      "abstract": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.",
      "publishedDate": "2025-12-08T08:27:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.07287",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.13713",
      "title": "LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms",
      "authors": [
        "Ali Parsaee",
        "Yashar Talebirad",
        "Csongor Szepesvári",
        "Vishwajeet Ohal",
        "Eden Redman"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \\textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.",
      "publishedDate": "2025-12-07T22:26:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.13713",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.06914",
      "title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions",
      "authors": [
        "Guanquan Shi",
        "Haohua Du",
        "Zhiqiang Wang",
        "Xiaoyu Liang",
        "Weiwenpei Liu",
        "Song Bian",
        "Zhenyu Guan"
      ],
      "abstract": "Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security. We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.",
      "publishedDate": "2025-12-07T16:41:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06914",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.06721",
      "title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems",
      "authors": [
        "Bufang Yang",
        "Lilin Xu",
        "Liekang Zeng",
        "Yunqi Guo",
        "Siyang Jiang",
        "Wenrui Lu",
        "Kaiwei Liu",
        "Hancheng Xiang",
        "Xiaofan Jiang",
        "Guoliang Xing",
        "Zhenyu Yan"
      ],
      "abstract": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.",
      "publishedDate": "2025-12-07T08:21:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06721",
      "categories": [
        "agents",
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.06716",
      "title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents",
      "authors": [
        "Zhibo Liang",
        "Tianze Hu",
        "Zaiye Chen",
        "Mingjie Tang"
      ],
      "abstract": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated \"Intent Graph\"; and (ii) an innovative \"Tiered Adjudicator\" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.",
      "publishedDate": "2025-12-07T08:11:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06716",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.06590",
      "title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems",
      "authors": [
        "Tendai Mukande",
        "Esraa Ali",
        "Annalina Caputo",
        "Ruihai Dong",
        "Noel OConnor"
      ],
      "abstract": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.",
      "publishedDate": "2025-12-06T23:04:49Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06590",
      "categories": [
        "agents",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.06227",
      "title": "Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety",
      "authors": [
        "Junyu Mao",
        "Anthony Hills",
        "Talia Tseriotou",
        "Maria Liakata",
        "Aya Shamir",
        "Dan Sayda",
        "Dana Atzil-Slonim",
        "Natalie Djohari",
        "Arpan Mandal",
        "Silke Roth",
        "Pamela Ugwudike",
        "Mahesan Niranjan",
        "Stuart E. Middleton"
      ],
      "abstract": "Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.",
      "publishedDate": "2025-12-06T00:21:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06227",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.06060",
      "title": "Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring",
      "authors": [
        "Mohanakrishnan Hariharan"
      ],
      "abstract": "This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.",
      "publishedDate": "2025-12-05T17:52:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06060",
      "categories": [
        "agents",
        "rag",
        "evaluation",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.05462",
      "title": "Model Gateway: Model Management Platform for Model-Driven Drug Discovery",
      "authors": [
        "Yan-Shiun Wu",
        "Nathan A. Morin"
      ],
      "abstract": "This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools.",
      "publishedDate": "2025-12-05T06:39:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.05462",
      "categories": [
        "agents",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.05374",
      "title": "Please Don't Kill My Vibe: Empowering Agents with Data Flow Control",
      "authors": [
        "Charlie Summers",
        "Haneen Mohammed",
        "Eugene Wu"
      ],
      "abstract": "The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.",
      "publishedDate": "2025-12-05T02:24:27Z",
      "arxivUrl": "https://arxiv.org/abs/2512.05374",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.04988",
      "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets",
      "authors": [
        "Christopher Chiu",
        "Simpson Zhang",
        "Mihaela van der Schaar"
      ],
      "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.",
      "publishedDate": "2025-12-04T16:57:28Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04988",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.04987",
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "authors": [
        "Nex-AGI Team",
        ":",
        "Yuxuan Cai",
        "Lu Chen",
        "Qiaoling Chen",
        "Yuyang Ding",
        "Liwen Fan",
        "Wenjie Fu",
        "Yufei Gao",
        "Honglin Guo",
        "Pinxue Guo",
        "Zhenhua Han",
        "Zhengfu He",
        "Hanglei Hu",
        "Kai Hu",
        "Shengjia Hua",
        "Tianyu Huai",
        "Baodai Huang",
        "Li Ji",
        "Zhen Jiang",
        "Zhikai Lei",
        "Bufan Li",
        "Jiahang Lin",
        "Lizhi Lin",
        "Jinxiu Liu",
        "Shichun Liu",
        "Ziming Liu",
        "Yuchen Ni",
        "Pengfang Qian",
        "Yujiong Shen",
        "Qingyun Shi",
        "Wentao Shu",
        "Peng Sun",
        "Yiran Suo",
        "Tian Tang",
        "Boyu Tian",
        "Guoteng Wang",
        "Junzhe Wang",
        "Peixin Wang",
        "Zhiheng Xi",
        "Hang Yan",
        "Jie Yang",
        "Zhixiong Yang",
        "Tianchu Yao",
        "Guangze Ye",
        "Qianxi Yu",
        "Shuo Zhang",
        "Xinyue Zhang",
        "Yiqi Zhang",
        "Jiarong Zhao",
        "Miao Zheng",
        "Rui Zheng",
        "Enyu Zhou",
        "Jiazheng Zhou",
        "Maosen Zhou",
        "Yuhao Zhou",
        "Tao Gui",
        "Yining Zheng",
        "Xinchi Chen",
        "Jie Zhou",
        "Siyuan Feng",
        "Qin Chen",
        "Liang He",
        "Qi Zhang",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "publishedDate": "2025-12-04T16:57:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04987",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.04785",
      "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications",
      "authors": [
        "Eranga Bandara",
        "Amin Hass",
        "Ross Gore",
        "Sachin Shetty",
        "Ravi Mukkamala",
        "Safdar H. Bouk",
        "Xueping Liang",
        "Ng Wee Keong",
        "Kasun De Zoysa",
        "Aruna Withanage",
        "Nilaan Loganathan"
      ],
      "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.",
      "publishedDate": "2025-12-04T13:32:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04785",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.04691",
      "title": "Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective",
      "authors": [
        "Jae Hee Lee",
        "Anne Lauscher",
        "Stefano V. Albrecht"
      ],
      "abstract": "Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.",
      "publishedDate": "2025-12-04T11:41:44Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04691",
      "categories": [
        "agents",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.04601",
      "title": "Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space",
      "authors": [
        "Joey Hong",
        "Kang Liu",
        "Zhan Ling",
        "Jiecao Chen",
        "Sergey Levine"
      ],
      "abstract": "Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.",
      "publishedDate": "2025-12-04T09:21:44Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04601",
      "categories": [
        "agents",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.04535",
      "title": "GTM: Simulating the World of Tools for AI Agents",
      "authors": [
        "Zhenzhen Ren",
        "Xinpeng Zhang",
        "Zhenxing Qian",
        "Yan Gao",
        "Yu Shi",
        "Shuxin Zheng",
        "Jiyan He"
      ],
      "abstract": "The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.",
      "publishedDate": "2025-12-04T07:33:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04535",
      "categories": [
        "robotics",
        "agents",
        "tool-use",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.03466",
      "title": "AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation",
      "authors": [
        "Xavier Cadet",
        "Edward Koh",
        "Peter Chin"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.",
      "publishedDate": "2025-12-03T05:42:01Z",
      "arxivUrl": "https://arxiv.org/abs/2512.03466",
      "categories": [
        "multi-agent",
        "evaluation",
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.03318",
      "title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia",
      "authors": [
        "Chandler Smith",
        "Marwa Abdulhai",
        "Manfred Diaz",
        "Marko Tesic",
        "Rakshit S. Trivedi",
        "Alexander Sasha Vezhnevets",
        "Lewis Hammond",
        "Jesse Clifton",
        "Minsuk Chang",
        "Edgar A. Duéñez-Guzmán",
        "John P. Agapiou",
        "Jayd Matyas",
        "Danny Karmon",
        "Akash Kundu",
        "Aliaksei Korshuk",
        "Ananya Ananya",
        "Arrasy Rahman",
        "Avinaash Anand Kulandaivel",
        "Bain McHale",
        "Beining Zhang",
        "Buyantuev Alexander",
        "Carlos Saith Rodriguez Rojas",
        "Caroline Wang",
        "Chetan Talele",
        "Chenao Liu",
        "Chichen Lin",
        "Diana Riazi",
        "Di Yang Shi",
        "Emanuel Tewolde",
        "Elizaveta Tennant",
        "Fangwei Zhong",
        "Fuyang Cui",
        "Gang Zhao",
        "Gema Parreño Piqueras",
        "Hyeonggeun Yun",
        "Ilya Makarov",
        "Jiaxun Cui",
        "Jebish Purbey",
        "Jim Dilkes",
        "Jord Nguyen",
        "Lingyun Xiao",
        "Luis Felipe Giraldo",
        "Manuela Chacon-Chamorro",
        "Manuel Sebastian Rios Beltran",
        "Marta Emili García Segura",
        "Mengmeng Wang",
        "Mogtaba Alim",
        "Nicanor Quijano",
        "Nico Schiavone",
        "Olivia Macmillan-Scott",
        "Oswaldo Peña",
        "Peter Stone",
        "Ram Mohan Rao Kadiyala",
        "Rolando Fernandez",
        "Ruben Manrique",
        "Sunjia Lu",
        "Sheila A. McIlraith",
        "Shamika Dhuri",
        "Shuqing Shi",
        "Siddhant Gupta",
        "Sneheel Sarangi",
        "Sriram Ganapathi Subramanian",
        "Taehun Cha",
        "Toryn Q. Klassen",
        "Wenming Tu",
        "Weijian Fan",
        "Wu Ruiyang",
        "Xue Feng",
        "Yali Du",
        "Yang Liu",
        "Yiding Wang",
        "Yipeng Kang",
        "Yoonchang Sung",
        "Yuxuan Chen",
        "Zhaowei Zhang",
        "Zhihan Wang",
        "Zhiqiang Wu",
        "Ziang Chen",
        "Zilong Zheng",
        "Zixia Jia",
        "Ziyan Wang",
        "Dylan Hadfield-Menell",
        "Natasha Jaques",
        "Tim Baarslag",
        "Jose Hernandez-Orallo",
        "Joel Z. Leibo"
      ],
      "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.",
      "publishedDate": "2025-12-03T00:11:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.03318",
      "categories": [
        "agents",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.03262",
      "title": "Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks",
      "authors": [
        "Songwen Zhao",
        "Danqing Wang",
        "Kexun Zhang",
        "Jiaxuan Luo",
        "Zhuo Li",
        "Lei Li"
      ],
      "abstract": "Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.",
      "publishedDate": "2025-12-02T22:11:56Z",
      "arxivUrl": "https://arxiv.org/abs/2512.03262",
      "categories": [
        "code-generation",
        "evaluation",
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.02543",
      "title": "In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs",
      "authors": [
        "Vishnu Sarukkai",
        "Asanshay Gupta",
        "James Hong",
        "Michaël Gharbi",
        "Kayvon Fatahalian"
      ],
      "abstract": "The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\\textbf{2.5$\\times$ lower cost}$, reducing per-episode costs from \\$0.059 to \\$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \\$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.",
      "publishedDate": "2025-12-02T09:11:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02543",
      "categories": [
        "prompting",
        "agents",
        "tool-use",
        "reasoning",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.02445",
      "title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents",
      "authors": [
        "Tsimur Hadeliya",
        "Mohammad Ali Jauhar",
        "Nidhi Sakpal",
        "Diogo Cruz"
      ],
      "abstract": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.",
      "publishedDate": "2025-12-02T06:12:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02445",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.02410",
      "title": "Decentralized Multi-Agent System with Trust-Aware Communication",
      "authors": [
        "Yepeng Ding",
        "Ahmed Twabi",
        "Junwei Yu",
        "Lingfeng Zhang",
        "Tohru Kondo",
        "Hiroyuki Sato"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.",
      "publishedDate": "2025-12-02T04:39:12Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02410",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.02329",
      "title": "Towards autonomous normative multi-agent systems for Human-AI software engineering teams",
      "authors": [
        "Hoa Khanh Dam",
        "Geeta Mahala",
        "Rashina Hoda",
        "Xi Zheng",
        "Cristina Conati"
      ],
      "abstract": "This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.",
      "publishedDate": "2025-12-02T01:57:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02329",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "code-generation",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.02289",
      "title": "Multi-Objective Agentic Rewrites for Unstructured Data Processing",
      "authors": [
        "Lindsey Linxi Wei",
        "Shreya Shankar",
        "Sepanta Zeighami",
        "Yeounoh Chung",
        "Fatma Ozcan",
        "Aditya G. Parameswaran"
      ],
      "abstract": "One year ago, we open-sourced DocETL, a declarative system for LLM-powered data processing that, as of November 2025, has 3.2K GitHub stars and users across domains (e.g., journalism, law, medicine, policy, finance, and urban planning). In DocETL, users build pipelines by composing operators described in natural language, also known as semantic operators, with an LLM executing each operator's logic. However, due to complexity in the operator or the data it operates on, LLMs often give inaccurate results. To address this challenge, DocETL introduced rewrite directives, or abstract rules that guide LLM agents in rewriting pipelines by decomposing operators or data. For example, decomposing a single filter(\"is this email sent from an executive and discussing fraud?\") into the conjunction of two separate semantic filters may improve accuracy. However, DocETL only optimizes for accuracy, not cost. How do we optimize for both? We present MOAR (Multi-Objective Agentic Rewrites), a new optimizer for DocETL. To target cost optimization, we introduce two new categories of directives and extend all three existing categories with new ones, bringing the total to over 30 directives -- more than doubling what DocETL originally had. Moreover, since operators can interact with each other unpredictably due to LLM behavior, optimizing operators or sub-pipelines individually can yield suboptimal overall plans. Recognizing this, we design a new global search algorithm that explores rewrites in the context of entire pipelines. Since the space of rewrites is infinite -- pipelines can be rewritten in many ways, and each rewritten pipeline can itself be rewritten -- our algorithm adapts a multi-armed bandit framework to prioritize which pipelines to rewrite. Across six workloads, MOAR achieves 27% higher accuracy than ABACUS, the next-best optimizer, while matching its best accuracy at 55% of its cost.",
      "publishedDate": "2025-12-02T00:08:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02289",
      "categories": [
        "agents",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.02230",
      "title": "Benchmarking LLM Agents for Wealth-Management Workflows",
      "authors": [
        "Rory Milsom"
      ],
      "abstract": "Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.",
      "publishedDate": "2025-12-01T21:56:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02230",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.02228",
      "title": "STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls",
      "authors": [
        "Shubhi Asthana",
        "Bing Zhang",
        "Chad DeLuca",
        "Ruchi Mahindru",
        "Hima Patel"
      ],
      "abstract": "The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk. We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context. Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.",
      "publishedDate": "2025-12-01T21:54:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02228",
      "categories": [
        "agents",
        "reasoning",
        "tool-use",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.01945",
      "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
      "authors": [
        "Han Zhou",
        "Xingchen Wan",
        "Ivan Vulić",
        "Anna Korhonen"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.",
      "publishedDate": "2025-12-01T17:56:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.01945",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.01270",
      "title": "Egent: An Autonomous Agent for Equivalent Width Measurement",
      "authors": [
        "Yuan-Sen Ting",
        "Serat Mahmud Saad",
        "Fan Liu",
        "Yuting Shen"
      ],
      "abstract": "We present Egent, an autonomous agent that combines classical multi-Voigt profile fitting with large language model (LLM) visual inspection and iterative refinement. The fitting engine is built from scratch with minimal dependencies, creating an ecosystem where the LLM can reason about fits through function calls-adjusting wavelength windows, adding blend components, modifying continuum treatment, and flagging problematic cases. Egent operates directly on raw flux spectra without requiring pre-normalized continua. We validate against manual measurements from human experts using 18,615 lines from the C3PO program across 84 Magellan/MIKE spectra at SNR~50-250. We find per-spectrum systematic offsets between Egent and expert measurements, likely arising from differences in global continuum placement prior to manual fitting; after accounting for these offsets, the agreement is 5-7 milliangstrom. The LLM's primary role is quality control: it confirms good fits (~60-65% of lines are LLM-refined and accepted), flags problematic cases (~10-20%), and occasionally rescues edge cases where tool use improves fits. Agreement between GPT-5 and GPT-5-mini confirms reproducibility, with GPT-5-mini enabling low-cost analysis at ~200 lines per US dollar. Every fit stores complete Voigt parameters, continuum coefficients, and LLM reasoning chains, enabling exact reconstruction without re-running. Egent compresses what traditionally requires months of expert effort into days of automated analysis, enabling survey-scale EW measurement. We provide open-source code at https://github.com/tingyuansen/Egent, including a web interface for drag-and-drop analysis and a local LLM backend for fully offline operation on consumer hardware.",
      "publishedDate": "2025-12-01T04:32:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.01270",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10971",
      "title": "AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets",
      "authors": [
        "Tianyu Fan",
        "Yuhao Yang",
        "Yangqin Jiang",
        "Yifei Zhang",
        "Yuxuan Chen",
        "Chao Huang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential as autonomous agents, approaching human-expert performance through advanced reasoning and tool orchestration. However, decision-making in fully dynamic and live environments remains highly challenging, requiring real-time information integration and adaptive responses. While existing efforts have explored live evaluation mechanisms in structured tasks, a critical gap remains in systematic benchmarking for real-world applications, particularly in finance where stringent requirements exist for live strategic responsiveness. To address this gap, we introduce AI-Trader, the first fully-automated, live, and data-uncontaminated evaluation benchmark for LLM agents in financial decision-making. AI-Trader spans three major financial markets: U.S. stocks, A-shares, and cryptocurrencies, with multiple trading granularities to simulate live financial environments. Our benchmark implements a revolutionary fully autonomous minimal information paradigm where agents receive only essential context and must independently search, verify, and synthesize live market information without human intervention. We evaluate six mainstream LLMs across three markets and multiple trading frequencies. Our analysis reveals striking findings: general intelligence does not automatically translate to effective trading capability, with most agents exhibiting poor returns and weak risk management. We demonstrate that risk control capability determines cross-market robustness, and that AI trading strategies achieve excess returns more readily in highly liquid markets than policy-driven environments. These findings expose critical limitations in current autonomous agents and provide clear directions for future improvements. The code and evaluation data are open-sourced to foster community research: https://github.com/HKUDS/AI-Trader.",
      "publishedDate": "2025-12-01T04:25:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10971",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.01078",
      "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
      "authors": [
        "Jiawei Ren",
        "Yan Zhuang",
        "Xiaokang Ye",
        "Lingjun Mao",
        "Xuhong He",
        "Jianzhi Shen",
        "Mrinaal Dogra",
        "Yiming Liang",
        "Ruixuan Zhang",
        "Tianai Yue",
        "Yiqing Yang",
        "Eric Liu",
        "Ryan Wu",
        "Kevin Benavente",
        "Rajiv Mandya Nagaraju",
        "Muhammad Faayez",
        "Xiyan Zhang",
        "Dhruv Vivek Sharma",
        "Xianrui Zhong",
        "Ziqiao Ma",
        "Tianmin Shu",
        "Zhiting Hu",
        "Lianhui Qin"
      ],
      "abstract": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.",
      "publishedDate": "2025-11-30T20:58:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.01078",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "multi-agent",
        "code-generation",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.00831",
      "title": "ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning",
      "authors": [
        "Yuchen Zeng",
        "Shuibai Zhang",
        "Wonjun Kang",
        "Shutong Wu",
        "Lynnix Zou",
        "Ying Fan",
        "Heeju Kim",
        "Ziqian Lin",
        "Jungtaek Kim",
        "Hyung Il Koo",
        "Dimitris Papailiopoulos",
        "Kangwook Lee"
      ],
      "abstract": "Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning \"algorithms\" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.",
      "publishedDate": "2025-11-30T10:39:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00831",
      "categories": [
        "agents",
        "reasoning",
        "code-generation",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.00617",
      "title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization",
      "authors": [
        "Omer Jauhar Khan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R^2 values exceeding 0.96 in ELO rating convergence.",
      "publishedDate": "2025-11-29T20:16:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00617",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.00520",
      "title": "Toward a Safe Internet of Agents",
      "authors": [
        "Juan A. Wibowo",
        "George C. Polyzos"
      ],
      "abstract": "Background: Autonomous agents powered by Large Language Models (LLMs) are driving a paradigm shift toward an \"Internet of Agents\" (IoA). While offering immense potential, this vision also introduces novel and systemic risks to safety and security. Objectives: Unlike common threat-centric taxonomies, our survey provides a principled, architectural framework for engineering safe and reliable agentic systems. We aim to identify the architectural sources of vulnerabilities to establish a foundation for secure design. Methods: We perform a bottom-up deconstruction of agentic systems, treating each component as a dual-use interface. The analysis spans three levels of complexity: the foundational Single Agent, the collaborative Multi-Agent System (MAS), and the visionary Interoperable Multi-Agent System (IMAS). At each level, we identify core architectural components and their inherent security risks. Results & Conclusions: Our central finding is that agentic safety is an architectural principle, not an add-on. By identifying specific vulnerabilities and deriving mitigation principles at each level of the agentic stack, this survey serves as a foundational guide for building the capable, safe, and trustworthy AI needed to realize a secure Internet of Agents.",
      "publishedDate": "2025-11-29T15:31:16Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00520",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.00491",
      "title": "Smart-TCP: An Agentic AI-based Autonomous and Adaptive TCP Protocol",
      "authors": [
        "Yule Han",
        "Kezhi Wang",
        "Kun Yang"
      ],
      "abstract": "The Transmission Control Protocol (TCP) relies on a state machine and deterministic arithmetic to ensure reliable connections. However, traditional protocol logic driven by hard-coded state machines struggles to meet the demands of intelligent and autonomous network architectures. Here, we adopt the agentic AI-based paradigm, driven by Large Language Models (LLMs), characterized by context perception, autonomous reasoning, and tool use. Based on this, we propose Smart-TCP, which re-imagines TCP's core control logic as an autonomous agent. Specifically, the proposed architecture employs a context aggregation mechanism to synthesize the protocol context, utilizes the LLM for autonomous logical reasoning, and invokes an Arithmetic Logic Unit (ALU) as a tool for computation. Furthermore, we establish a dual-agent interaction framework based on this architecture and implement TCP protocol interactions. Experiments demonstrate that the Smart-TCP agent excels in static prediction and error detection, achieving a 93.33% success rate in end-to-end sessions. These results strongly validate the technical feasibility of an agentic AI-based TCP protocol.",
      "publishedDate": "2025-11-29T13:55:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00491",
      "categories": [
        "agents",
        "reasoning",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.00417",
      "title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency",
      "authors": [
        "Jiacheng Guo",
        "Suozhi Huang",
        "Zixin Yao",
        "Yifan Zhang",
        "Yifu Lu",
        "Jiashuo Liu",
        "Zihao Li",
        "Nicholas Deng",
        "Qixin Xiao",
        "Jia Tian",
        "Kanghong Zhan",
        "Tianyi Li",
        "Xiaochen Liu",
        "Jason Ge",
        "Chaoyang He",
        "Kaixuan Huang",
        "Lin Yang",
        "Wenhao Huang",
        "Mengdi Wang"
      ],
      "abstract": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills. Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.",
      "publishedDate": "2025-11-29T09:52:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00417",
      "categories": [
        "evaluation",
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.00371",
      "title": "Evaluating LLMs in Open-Source Games",
      "authors": [
        "Swadesh Sistla",
        "Max Kleiman-Weiner"
      ],
      "abstract": "Large Language Models' (LLMs) programming capabilities enable their participation in open-source games: a game-theoretic setting in which players submit computer programs in lieu of actions. These programs offer numerous advantages, including interpretability, inter-agent transparency, and formal verifiability; additionally, they enable program equilibria, solutions that leverage the transparency of code and are inaccessible within normal-form settings. We evaluate the capabilities of leading open- and closed-weight LLMs to predict and classify program strategies and evaluate features of the approximate program equilibria reached by LLM agents in dyadic and evolutionary settings. We identify the emergence of payoff-maximizing, cooperative, and deceptive strategies, characterize the adaptation of mechanisms within these programs over repeated open-source games, and analyze their comparative evolutionary fitness. We find that open-source games serve as a viable environment to study and steer the emergence of cooperative strategy in multi-agent dilemmas.",
      "publishedDate": "2025-11-29T07:46:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00371",
      "categories": [
        "agents",
        "code-generation",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.11819",
      "title": "A Modular LLM-Agent System for Transparent Multi-Parameter Weather Interpretation",
      "authors": [
        "Daniil Sukhorukov",
        "Andrei Zakharov",
        "Nikita Glazkov",
        "Katsiaryna Yanchanka",
        "Vladimir Kirilin",
        "Maxim Dubovitsky",
        "Roman Sultimov",
        "Yuri Maksimov",
        "Ilya Makarov"
      ],
      "abstract": "Weather forecasting is not only a predictive task but an interpretive scientific process requiring explanation, contextualization, and hypothesis generation. This paper introduces AI-Meteorologist, an explainable LLM-agent framework that converts raw numerical forecasts into scientifically grounded narrative reports with transparent reasoning steps. Unlike conventional forecast outputs presented as dense tables or unstructured time series, our system performs agent-based analysis across multiple meteorological variables, integrates historical climatological context, and generates structured explanations that identify weather fronts, anomalies, and localized dynamics. The architecture relies entirely on in-context prompting, without fine-tuning, demonstrating that interpretability can be achieved through reasoning rather than parameter updates. Through case studies on multi-location forecast data, we show how AI-Meteorologist not only communicates weather events but also reveals the underlying atmospheric drivers, offering a pathway toward AI systems that augment human meteorological expertise and support scientific discovery in climate analytics.",
      "publishedDate": "2025-11-28T22:24:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11819",
      "categories": [
        "prompting",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2511.23476",
      "title": "Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction",
      "authors": [
        "Bao Shu",
        "Yan Cai",
        "Jianjian Sun",
        "Chunrui Han",
        "En Yu",
        "Liang Zhao",
        "Jingcheng Hu",
        "Yinmin Zhang",
        "Haoran Lv",
        "Yuang Peng",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang",
        "Xiangyu Yue"
      ],
      "abstract": "Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.",
      "publishedDate": "2025-11-28T18:59:47Z",
      "arxivUrl": "https://arxiv.org/abs/2511.23476",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.23387",
      "title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting",
      "authors": [
        "Daniil Sukhorukov",
        "Andrei Zakharov",
        "Nikita Glazkov",
        "Katsiaryna Yanchanka",
        "Vladimir Kirilin",
        "Maxim Dubovitsky",
        "Roman Sultimov",
        "Yuri Maksimov",
        "Ilya Makarov"
      ],
      "abstract": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.",
      "publishedDate": "2025-11-28T17:27:06Z",
      "arxivUrl": "https://arxiv.org/abs/2511.23387",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.23281",
      "title": "MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)",
      "authors": [
        "Aaron Steiner",
        "Ralph Peeters",
        "Christian Bizer"
      ],
      "abstract": "Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks. To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.",
      "publishedDate": "2025-11-28T15:32:15Z",
      "arxivUrl": "https://arxiv.org/abs/2511.23281",
      "categories": [
        "agents",
        "rag",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.22138",
      "title": "TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices",
      "authors": [
        "Mohd Ariful Haque",
        "Fahad Rahman",
        "Kishor Datta Gupta",
        "Khalil Shujaee",
        "Roy George"
      ],
      "abstract": "This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and describe parameter-driven optimization strategies that include supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL)-based optimization, preference alignment via Direct Preference Optimization (DPO), and hybrid methods. We report results for models including TinyAgent, TinyLlama, Qwen, and xLAM across BFCL categories (simple, multiple, parallel, parallel-multiple, and relevance detection), both in live and non-live settings, and in multi-turn evaluations. We additionally detail a DPO training pipeline constructed from AgentBank data (e.g., ALFRED), including our conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation. Our results demonstrate clear accuracy differences across model scales where medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy, and 55.62% multi-turn accuracy with hybrid optimization. This study highlights the importance of hybrid optimization strategies that enable small language models to deliver accurate, efficient, and stable agentic AI on edge devices, making privacy-preserving, low-latency autonomous agents practical beyond the cloud.",
      "publishedDate": "2025-11-27T06:09:54Z",
      "arxivUrl": "https://arxiv.org/abs/2511.22138",
      "categories": [
        "agents",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.21572",
      "title": "BAMAS: Structuring Budget-Aware Multi-Agent Systems",
      "authors": [
        "Liming Yang",
        "Junyu Luo",
        "Xuanzhe Liu",
        "Yiling Lou",
        "Zhenpeng Chen"
      ],
      "abstract": "Large language model (LLM)-based multi-agent systems have emerged as a powerful paradigm for enabling autonomous agents to solve complex tasks. As these systems scale in complexity, cost becomes an important consideration for practical deployment. However, existing work rarely addresses how to structure multi-agent systems under explicit budget constraints. In this paper, we propose BAMAS, a novel approach for building multi-agent systems with budget awareness. BAMAS first selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem that balances performance and cost. It then determines how these LLMs should collaborate by leveraging a reinforcement learning-based method to select the interaction topology. Finally, the system is instantiated and executed based on the selected agents and their collaboration topology. We evaluate BAMAS on three representative tasks and compare it with state-of-the-art agent construction methods. Results show that BAMAS achieves comparable performance while reducing cost by up to 86%.",
      "publishedDate": "2025-11-26T16:48:18Z",
      "arxivUrl": "https://arxiv.org/abs/2511.21572",
      "categories": [
        "agents",
        "multi-agent",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2511.19368",
      "title": "LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems",
      "authors": [
        "Tianyang Duan",
        "Zongyuan Zhang",
        "Zheng Lin",
        "Songxiao Guo",
        "Xiuxian Guan",
        "Guangyu Wu",
        "Zihan Fang",
        "Haotian Meng",
        "Xia Du",
        "Ji-Zhe Zhou",
        "Heming Cui",
        "Jun Luo",
        "Yue Gao"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.",
      "publishedDate": "2025-11-24T18:03:59Z",
      "arxivUrl": "https://arxiv.org/abs/2511.19368",
      "categories": [
        "agents",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2511.19536",
      "title": "AttackPilot: Autonomous Inference Attacks Against ML Services With LLM-Based Agents",
      "authors": [
        "Yixin Wu",
        "Rui Wen",
        "Chi Cui",
        "Michael Backes",
        "Yang Zhang"
      ],
      "abstract": "Inference attacks have been widely studied and offer a systematic risk assessment of ML services; however, their implementation and the attack parameters for optimal estimation are challenging for non-experts. The emergence of advanced large language models presents a promising yet largely unexplored opportunity to develop autonomous agents as inference attack experts, helping address this challenge. In this paper, we propose AttackPilot, an autonomous agent capable of independently conducting inference attacks without human intervention. We evaluate it on 20 target services. The evaluation shows that our agent, using GPT-4o, achieves a 100.0% task completion rate and near-expert attack performance, with an average token cost of only $0.627 per run. The agent can also be powered by many other representative LLMs and can adaptively optimize its strategy under service constraints. We further perform trace analysis, demonstrating that design choices, such as a multi-agent framework and task-specific action spaces, effectively mitigate errors such as bad plans, inability to follow instructions, task context loss, and hallucinations. We anticipate that such agents could empower non-expert ML service providers, auditors, or regulators to systematically assess the risks of ML services without requiring deep domain expertise.",
      "publishedDate": "2025-11-24T10:14:14Z",
      "arxivUrl": "https://arxiv.org/abs/2511.19536",
      "categories": [
        "agents",
        "evaluation",
        "rag",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2511.18114",
      "title": "ASTRA: Agentic Steerability and Risk Assessment Framework",
      "authors": [
        "Itay Hazan",
        "Yael Mathov",
        "Guy Shtar",
        "Ron Bitton",
        "Itsik Mantin"
      ],
      "abstract": "Securing AI agents powered by Large Language Models (LLMs) represents one of the most critical challenges in AI security today. Unlike traditional software, AI agents leverage LLMs as their \"brain\" to autonomously perform actions via connected tools. This capability introduces significant risks that go far beyond those of harmful text presented in a chatbot that was the main application of LLMs. A compromised AI agent can deliberately abuse powerful tools to perform malicious actions, in many cases irreversible, and limited solely by the guardrails on the tools themselves and the LLM ability to enforce them. This paper presents ASTRA, a first-of-its-kind framework designed to evaluate the effectiveness of LLMs in supporting the creation of secure agents that enforce custom guardrails defined at the system-prompt level (e.g., \"Do not send an email out of the company domain,\" or \"Never extend the robotic arm in more than 2 meters\"). Our holistic framework simulates 10 diverse autonomous agents varying between a coding assistant and a delivery drone equipped with 37 unique tools. We test these agents against a suite of novel attacks developed specifically for agentic threats, inspired by the OWASP Top 10 but adapted to challenge the ability of the LLM for policy enforcement during multi-turn planning and execution of strict tool activation. By evaluating 13 open-source, tool-calling LLMs, we uncovered surprising and significant differences in their ability to remain secure and keep operating within their boundaries. The purpose of this work is to provide the community with a robust and unified methodology to build and validate better LLMs, ultimately pushing for more secure and reliable agentic AI systems.",
      "publishedDate": "2025-11-22T16:32:29Z",
      "arxivUrl": "https://arxiv.org/abs/2511.18114",
      "categories": [
        "agents",
        "code-generation",
        "planning",
        "rag",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.17190",
      "title": "AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale",
      "authors": [
        "Ziyang Wang",
        "Yuanlei Zheng",
        "Zhenbiao Cao",
        "Xiaojin Zhang",
        "Zhongyu Wei",
        "Pei Fu",
        "Zhenbo Luo",
        "Wei Chen",
        "Xiang Bai"
      ],
      "abstract": "For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \\textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \\textbf{97.4\\%} on Bird-Dev and \\textbf{91.2\\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \\textbf{68.7\\%} EX on Bird-Dev (better than CHESS) and \\textbf{34.9\\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \\textbf{exceptional scalability}, \\textbf{maintaining high recall}, \\textbf{efficient token consumption}, and \\textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.",
      "publishedDate": "2025-11-21T12:12:17Z",
      "arxivUrl": "https://arxiv.org/abs/2511.17190",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2511.16709",
      "title": "AutoBackdoor: Automating Backdoor Attacks via LLM Agents",
      "authors": [
        "Yige Li",
        "Zhe Li",
        "Wei Zhao",
        "Nay Myat Min",
        "Hanxun Huang",
        "Xingjun Ma",
        "Jun Sun"
      ],
      "abstract": "Backdoor attacks pose a serious threat to the secure deployment of large language models (LLMs), enabling adversaries to implant hidden behaviors triggered by specific inputs. However, existing methods often rely on manually crafted triggers and static data pipelines, which are rigid, labor-intensive, and inadequate for systematically evaluating modern defense robustness. As AI agents become increasingly capable, there is a growing need for more rigorous, diverse, and scalable \\textit{red-teaming frameworks} that can realistically simulate backdoor threats and assess model resilience under adversarial conditions. In this work, we introduce \\textsc{AutoBackdoor}, a general framework for automating backdoor injection, encompassing trigger generation, poisoned data construction, and model fine-tuning via an autonomous agent-driven pipeline. Unlike prior approaches, AutoBackdoor uses a powerful language model agent to generate semantically coherent, context-aware trigger phrases, enabling scalable poisoning across arbitrary topics with minimal human effort. We evaluate AutoBackdoor under three realistic threat scenarios, including \\textit{Bias Recommendation}, \\textit{Hallucination Injection}, and \\textit{Peer Review Manipulation}, to simulate a broad range of attacks. Experiments on both open-source and commercial models, including LLaMA-3, Mistral, Qwen, and GPT-4o, demonstrate that our method achieves over 90\\% attack success with only a small number of poisoned samples. More importantly, we find that existing defenses often fail to mitigate these attacks, underscoring the need for more rigorous and adaptive evaluation techniques against agent-driven threats as explored in this work. All code, datasets, and experimental configurations will be merged into our primary repository at https://github.com/bboylyg/BackdoorLLM.",
      "publishedDate": "2025-11-20T03:58:54Z",
      "arxivUrl": "https://arxiv.org/abs/2511.16709",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.13998",
      "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering",
      "authors": [
        "Jielin Qiu",
        "Zuxin Liu",
        "Zhiwei Liu",
        "Rithesh Murthy",
        "Jianguo Zhang",
        "Haolin Chen",
        "Shiyu Wang",
        "Ming Zhu",
        "Liangwei Yang",
        "Juntao Tan",
        "Roshan Ram",
        "Akshara Prabhakar",
        "Tulika Awalgaonkar",
        "Zixiang Chen",
        "Zhepeng Cen",
        "Cheng Qian",
        "Shelby Heinecke",
        "Weiran Yao",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang"
      ],
      "abstract": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.",
      "publishedDate": "2025-11-17T23:57:24Z",
      "arxivUrl": "https://arxiv.org/abs/2511.13998",
      "categories": [
        "evaluation",
        "agents",
        "code-generation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2511.13087",
      "title": "MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements",
      "authors": [
        "SeokJoo Kwak",
        "Jihoon Kim",
        "Boyoun Kim",
        "Jung Jae Yoon",
        "Wooseok Jang",
        "Jeonghoon Hong",
        "Jaeho Yang",
        "Yeong-Dae Kwon"
      ],
      "abstract": "Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.",
      "publishedDate": "2025-11-17T07:38:05Z",
      "arxivUrl": "https://arxiv.org/abs/2511.13087",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.10395",
      "title": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
      "authors": [
        "Yunpeng Zhai",
        "Shuchang Tao",
        "Cheng Chen",
        "Anni Zou",
        "Ziqian Chen",
        "Qingxu Fu",
        "Shinji Mai",
        "Li Yu",
        "Jiaji Deng",
        "Zouying Cao",
        "Zhaoyang Liu",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.",
      "publishedDate": "2025-11-13T15:14:47Z",
      "arxivUrl": "https://arxiv.org/abs/2511.10395",
      "categories": [
        "agents",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2511.08649",
      "title": "Bio AI Agent: A Multi-Agent Artificial Intelligence System for Autonomous CAR-T Cell Therapy Development with Integrated Target Discovery, Toxicity Prediction, and Rational Molecular Design",
      "authors": [
        "Yi Ni",
        "Liwei Zhu",
        "Shuai Li"
      ],
      "abstract": "Chimeric antigen receptor T-cell (CAR-T) therapy represents a paradigm shift in cancer treatment, yet development timelines of 8-12 years and clinical attrition rates exceeding 40-60% highlight critical inefficiencies in target selection, safety assessment, and molecular optimization. We present Bio AI Agent, a multi-agent artificial intelligence system powered by large language models that enables autonomous CAR-T development through collaborative specialized agents. The system comprises six autonomous agents: Target Selection Agent for multi-parametric antigen prioritization across >10,000 cancer-associated targets, Toxicity Prediction Agent for comprehensive safety profiling integrating tissue expression atlases and pharmacovigilance databases, Molecular Design Agent for rational CAR engineering, Patent Intelligence Agent for freedom-to-operate analysis, Clinical Translation Agent for regulatory compliance, and Decision Orchestration Agent for multi-agent coordination. Retrospective validation demonstrated autonomous identification of high-risk targets including FcRH5 (hepatotoxicity) and CD229 (off-tumor toxicity), patent infringement risks for CD38+SLAMF7 combinations, and generation of comprehensive development roadmaps. By enabling parallel processing, specialized reasoning, and autonomous decision-making superior to monolithic AI systems, Bio AI Agent addresses critical gaps in precision oncology development and has potential to accelerate translation of next-generation immunotherapies from discovery to clinic.",
      "publishedDate": "2025-11-11T02:41:08Z",
      "arxivUrl": "https://arxiv.org/abs/2511.08649",
      "categories": [
        "agents",
        "multi-agent",
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2511.06449",
      "title": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
      "authors": [
        "Zhicheng Cai",
        "Xinyuan Guo",
        "Yu Pei",
        "Jiangtao Feng",
        "Jinsong Su",
        "Jiangjie Chen",
        "Ya-Qin Zhang",
        "Wei-Ying Ma",
        "Mingxuan Wang",
        "Hao Zhou"
      ],
      "abstract": "Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.",
      "publishedDate": "2025-11-09T16:31:39Z",
      "arxivUrl": "https://arxiv.org/abs/2511.06449",
      "categories": [
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2511.05359",
      "title": "ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations",
      "authors": [
        "Amr Gomaa",
        "Ahmed Salem",
        "Sahar Abdelnabi"
      ],
      "abstract": "As language models evolve into autonomous agents that act and communicate on behalf of users, ensuring safety in multi-agent ecosystems becomes a central challenge. Interactions between personal assistants and external service providers expose a core tension between utility and protection: effective collaboration requires information sharing, yet every exchange creates new attack surfaces. We introduce ConVerse, a dynamic benchmark for evaluating privacy and security risks in agent-agent interactions. ConVerse spans three practical domains (travel, real estate, insurance) with 12 user personas and over 864 contextually grounded attacks (611 privacy, 253 security). Unlike prior single-agent settings, it models autonomous, multi-turn agent-to-agent conversations where malicious requests are embedded within plausible discourse. Privacy is tested through a three-tier taxonomy assessing abstraction quality, while security attacks target tool use and preference manipulation. Evaluating seven state-of-the-art models reveals persistent vulnerabilities; privacy attacks succeed in up to 88% of cases and security breaches in up to 60%, with stronger models leaking more. By unifying privacy and security within interactive multi-agent contexts, ConVerse reframes safety as an emergent property of communication.",
      "publishedDate": "2025-11-07T15:49:49Z",
      "arxivUrl": "https://arxiv.org/abs/2511.05359",
      "categories": [
        "agents",
        "multi-agent",
        "evaluation",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2511.05269",
      "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems",
      "authors": [
        "Ishan Kavathekar",
        "Hemang Jain",
        "Ameya Rathod",
        "Ponnurangam Kumaraguru",
        "Tanuja Ganu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities as autonomous agents through tool use, planning, and decision-making abilities, leading to their widespread adoption across diverse tasks. As task complexity grows, multi-agent LLM systems are increasingly used to solve problems collaboratively. However, safety and security of these systems remains largely under-explored. Existing benchmarks and datasets predominantly focus on single-agent settings, failing to capture the unique vulnerabilities of multi-agent dynamics and co-ordination. To address this gap, we introduce $\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the robustness and safety of multi-agent LLM systems. TAMAS includes five distinct scenarios comprising 300 adversarial instances across six attack types and 211 tools, along with 100 harmless tasks. We assess system performance across ten backbone LLMs and three agent interaction configurations from Autogen and CrewAI frameworks, highlighting critical challenges and failure modes in current multi-agent deployments. Furthermore, we introduce Effective Robustness Score (ERS) to assess the tradeoff between safety and task effectiveness of these frameworks. Our findings show that multi-agent systems are highly vulnerable to adversarial attacks, underscoring the urgent need for stronger defenses. TAMAS provides a foundation for systematically studying and improving the safety of multi-agent LLM systems.",
      "publishedDate": "2025-11-07T14:30:26Z",
      "arxivUrl": "https://arxiv.org/abs/2511.05269",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "planning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2511.02424",
      "title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning",
      "authors": [
        "Jae-Woo Choi",
        "Hyungmin Kim",
        "Hyobin Ong",
        "Minsu Jang",
        "Dohyung Kim",
        "Jaehong Kim",
        "Youngwoo Yoon"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled significant progress in decision-making and task planning for embodied autonomous agents. However, most existing methods still struggle with complex, long-horizon tasks because they rely on a monolithic trajectory that entangles all past decisions and observations, attempting to solve the entire task in a single unified process. To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Each subgoal is handled by an LLM agent node capable of reasoning, acting, and further expanding the tree, while control flow nodes coordinate the execution strategies of agent nodes. In addition, we integrate two complementary memory systems: each agent node retrieves goal-specific, subgoal-level examples from episodic memory and shares environment-specific observations through working memory. Experiments on the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently outperforms strong task-planning baselines such as ReAct across diverse LLMs. Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.",
      "publishedDate": "2025-11-04T09:55:40Z",
      "arxivUrl": "https://arxiv.org/abs/2511.02424",
      "categories": [
        "agents",
        "planning",
        "reasoning",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2510.25320",
      "title": "GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning",
      "authors": [
        "Jiaqi Wu",
        "Qinlao Zhao",
        "Zefeng Chen",
        "Kai Qin",
        "Yifei Zhao",
        "Xueqian Wang",
        "Yuhang Yao"
      ],
      "abstract": "Autonomous agents powered by large language models (LLMs) have shown impressive capabilities in tool manipulation for complex task-solving. However, existing paradigms such as ReAct rely on sequential reasoning and execution, failing to exploit the inherent parallelism among independent sub-tasks. This sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios. We introduce Graph-based Agent Planning (GAP), a novel framework that explicitly models inter-task dependencies through graph-based planning to enable adaptive parallel and serial tool execution. Our approach trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. This dependency-aware orchestration achieves substantial improvements in both execution efficiency and task accuracy. To train GAP, we construct a high-quality dataset of graph-based planning traces derived from the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage training strategy: supervised fine-tuning (SFT) on the curated dataset, followed by reinforcement learning (RL) with a correctness-based reward function on strategically sampled queries where tool-based reasoning provides maximum value. Experimental results on MHQA datasets demonstrate that GAP significantly outperforms traditional ReAct baselines, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization. The project page is available at: https://github.com/WJQ7777/Graph-Agent-Planning.",
      "publishedDate": "2025-10-29T09:35:55Z",
      "arxivUrl": "https://arxiv.org/abs/2510.25320",
      "categories": [
        "agents",
        "reasoning",
        "tool-use",
        "planning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.24949",
      "title": "SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving",
      "authors": [
        "Anil Yildiz",
        "Sarah M. Thornton",
        "Carl Hildebrandt",
        "Sreeja Roy-Singh",
        "Mykel J. Kochenderfer"
      ],
      "abstract": "Assessing scenario coverage is crucial for evaluating the robustness of autonomous agents, yet existing methods rely on expensive human annotations or computationally intensive Large Vision-Language Models (LVLMs). These approaches are impractical for large-scale deployment due to cost and efficiency constraints. To address these shortcomings, we propose SCOUT (Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate model designed to predict scenario coverage labels directly from an agent's latent sensor representations. SCOUT is trained through a distillation process, learning to approximate LVLM-generated coverage labels while eliminating the need for continuous LVLM inference or human annotation. By leveraging precomputed perception features, SCOUT avoids redundant computations and enables fast, scalable scenario coverage estimation. We evaluate our method across a large dataset of real-life autonomous navigation scenarios, demonstrating that it maintains high accuracy while significantly reducing computational cost. Our results show that SCOUT provides an effective and practical alternative for large-scale coverage analysis. While its performance depends on the quality of LVLM-generated training labels, SCOUT represents a major step toward efficient scenario coverage oversight in autonomous systems.",
      "publishedDate": "2025-10-28T20:31:19Z",
      "arxivUrl": "https://arxiv.org/abs/2510.24949",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.24645",
      "title": "FunReason-MT Technical Report: Advanced Data Synthesis Solution for Real-world Multi-Turn Tool-use",
      "authors": [
        "Zengzhuang Xu",
        "Bingguang Hao",
        "Zechuan Wang",
        "Yuntao Wen",
        "Xinyi Xu",
        "Yang Liu",
        "Long Chen",
        "Dong Wang",
        "Maolin Wang",
        "Tong Zhao",
        "Yicheng Chen",
        "Cunyin Peng",
        "Jinjie Gu",
        "Leilei Gan",
        "Xiangyu Zhao",
        "Chenyi Zhuang",
        "Shi Gu"
      ],
      "abstract": "Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted data synthesis, hard query construction, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories with targeted tool, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.",
      "publishedDate": "2025-10-28T17:15:26Z",
      "arxivUrl": "https://arxiv.org/abs/2510.24645",
      "categories": [
        "tool-use",
        "agents",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.23824",
      "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models",
      "authors": [
        "Murad Ismayilov",
        "Edwin Meriaux",
        "Shuo Wen",
        "Gregory Dudek"
      ],
      "abstract": "Coordinating multiple autonomous agents in shared environments under decentralized conditions is a long-standing challenge in robotics and artificial intelligence. This work addresses the problem of decentralized goal assignment for multi-agent path planning, where agents independently generate ranked preferences over goals based on structured representations of the environment, including grid visualizations and scenario data. After this reasoning phase, agents exchange their goal rankings, and assignments are determined by a fixed, deterministic conflict-resolution rule (e.g., agent index ordering), without negotiation or iterative coordination. We systematically compare greedy heuristics, optimal assignment, and large language model (LLM)-based agents in fully observable grid-world settings. Our results show that LLM-based agents, when provided with well-designed prompts and relevant quantitative information, can achieve near-optimal makespans and consistently outperform traditional heuristics. These findings underscore the potential of language models for decentralized goal assignment in multi-agent path planning and highlight the importance of information structure in such systems.",
      "publishedDate": "2025-10-27T20:05:56Z",
      "arxivUrl": "https://arxiv.org/abs/2510.23824",
      "categories": [
        "agents",
        "multi-agent",
        "robotics",
        "reasoning",
        "planning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2510.23682",
      "title": "Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents",
      "authors": [
        "Gokturk Aytug Akarlar"
      ],
      "abstract": "Large language models show promise as autonomous decision-making agents, yet their deployment in high-stakes domains remains fraught with risk. Without architectural safeguards, LLM agents exhibit catastrophic brittleness: identical capabilities produce wildly different outcomes depending solely on prompt framing. We present Chimera, a neuro-symbolic-causal architecture that integrates three complementary components - an LLM strategist, a formally verified symbolic constraint engine, and a causal inference module for counterfactual reasoning. We benchmark Chimera against baseline architectures (LLM-only, LLM with symbolic constraints) across 52-week simulations in a realistic e-commerce environment featuring price elasticity, trust dynamics, and seasonal demand. Under organizational biases toward either volume or margin optimization, LLM-only agents fail catastrophically (total loss of \\$99K in volume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding symbolic constraints prevents disasters but achieves only 43-87% of Chimera's profit. Chimera consistently delivers the highest returns (\\$1.52M and \\$1.96M respectively, some cases +\\$2.2M) while improving brand trust (+1.8% and +10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+ formal verification proves zero constraint violations across all scenarios. These results establish that architectural design not prompt engineering determines the reliability of autonomous agents in production environments. We provide open-source implementations and interactive demonstrations for reproducibility.",
      "publishedDate": "2025-10-27T15:25:35Z",
      "arxivUrl": "https://arxiv.org/abs/2510.23682",
      "categories": [
        "agents",
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.23182",
      "title": "SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations",
      "authors": [
        "Shuai Huang",
        "Wenxuan Zhao",
        "Jun Gao"
      ],
      "abstract": "As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at https://github.com/SI-Bench/SI-Bench.git.",
      "publishedDate": "2025-10-27T10:21:46Z",
      "arxivUrl": "https://arxiv.org/abs/2510.23182",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.20333",
      "title": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?",
      "authors": [
        "Chiyu Chen",
        "Xinhao Song",
        "Yunkai Chai",
        "Yang Yao",
        "Haodong Zhao",
        "Lijun Li",
        "Jie Li",
        "Yan Teng",
        "Gongshen Liu",
        "Yingchun Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.",
      "publishedDate": "2025-10-23T08:33:24Z",
      "arxivUrl": "https://arxiv.org/abs/2510.20333",
      "categories": [
        "agents",
        "prompting",
        "evaluation",
        "reasoning",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2510.19747",
      "title": "Review of Tools for Zero-Code LLM Based Application Development",
      "authors": [
        "Priyaranjan Pattnayak",
        "Hussain Bohra"
      ],
      "abstract": "Large Language Models (LLMs) are transforming software creation by enabling zero code development platforms. Our survey reviews recent platforms that let users build applications without writing code, by leveraging LLMs as the brains of the development process. We adopt a broad survey methodology, categorizing platforms based on key dimensions such as interface style, backend integration, output type, and extensibility. We analyze both dedicated LLM based app builders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and general no code platforms (e.g., Bubble, Glide) that integrate LLM capabilities. We present a taxonomy categorizing these platforms by their interface (conversational, visual, etc.), supported LLM backends, output type (chatbot, full application, workflow), and degree of extensibility. Core features such as autonomous agents, memory management, workflow orchestration, and API integrations are in scope of the survey. We provide a detailed comparison, highlighting each platform's strengths and limitations. Trade offs (customizability, scalability, vendor lock-in) are discussed in comparison with traditional and low code development approaches. Finally, we outline future directions, including multimodal interfaces, on device LLMs, and improved orchestration for democratizing app creation with AI. Our findings indicate that while zero code LLM platforms greatly reduce the barrier to creating AI powered applications, they still face challenges in flexibility and reliability. Overall, the landscape is rapidly evolving, offering exciting opportunities to empower non programmers to create sophisticated software.",
      "publishedDate": "2025-10-22T16:41:16Z",
      "arxivUrl": "https://arxiv.org/abs/2510.19747",
      "categories": [
        "agents",
        "code-generation",
        "tool-use",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2510.20849",
      "title": "Cultural Alien Sampler: Open-ended art generation balancing originality and coherence",
      "authors": [
        "Alejandro H. Artiles",
        "Hiromu Yakura",
        "Levin Brinkmann",
        "Mar Canet Sola",
        "Hassan Abu Alhaija",
        "Ignacio Serna",
        "Nasim Rahaman",
        "Bernhard Schölkopf",
        "Iyad Rahwan"
      ],
      "abstract": "In open-ended domains like art, autonomous agents must generate ideas that are both original and internally coherent, yet current Large Language Models (LLMs) either default to familiar cultural patterns or sacrifice coherence when pushed toward novelty. We address this by introducing the Cultural Alien Sampler (CAS), a concept-selection method that explicitly separates compositional fit from cultural typicality. CAS uses two GPT-2 models fine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether concepts plausibly co-occur within artworks, and a Cultural Context Model that estimates how typical those combinations are within individual artists' bodies of work. CAS targets combinations that are high in coherence and low in typicality, yielding ideas that maintain internal consistency while deviating from learned conventions and embedded cultural context. In a human evaluation (N = 100), our approach outperforms random selection and GPT-4o baselines and achieves performance comparable to human art students in both perceived originality and harmony. Additionally, a quantitative study shows that our method produces more diverse outputs and explores a broader conceptual space than its GPT-4o counterpart, demonstrating that artificial cultural alienness can unlock creative potential in autonomous agents.",
      "publishedDate": "2025-10-21T09:32:46Z",
      "arxivUrl": "https://arxiv.org/abs/2510.20849",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.16492",
      "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety",
      "authors": [
        "Vamshi Krishna Bonagiri",
        "Ponnurangam Kumaragurum",
        "Khanh Nguyen",
        "Benjamin Plaut"
      ],
      "abstract": "As Large Language Model (LLM) agents increasingly operate in complex environments with real-world consequences, their safety becomes critical. While uncertainty quantification is well-studied for single-turn tasks, multi-turn agentic scenarios with real-world tool access present unique challenges where uncertainties and ambiguities compound, leading to severe or catastrophic risks beyond traditional text generation failures. We propose using \"quitting\" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence. Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Our results demonstrate a highly favorable safety-helpfulness trade-off: agents prompted to quit with explicit instructions improve safety by an average of +0.39 on a 0-3 scale across all models (+0.64 for proprietary models), while maintaining a negligible average decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding explicit quit instructions proves to be a highly effective safety mechanism that can immediately be deployed in existing agent systems, and establishes quitting as an effective first-line defense mechanism for autonomous agents in high-stakes applications.",
      "publishedDate": "2025-10-18T13:22:19Z",
      "arxivUrl": "https://arxiv.org/abs/2510.16492",
      "categories": [
        "agents",
        "prompting",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.16381",
      "title": "ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents",
      "authors": [
        "David Peer",
        "Sebastian Stabinger"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities, yet their deployment in high-stakes domains is hindered by inherent limitations in trustworthiness, including hallucinations, instability, and a lack of transparency. To address these challenges, we introduce a generic neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The core of our approach lies in decoupling tasks into two distinct phases: Offline knowledge ingestion and online task processing. During knowledge ingestion, an LLM translates an informal problem specification into a formal, symbolic knowledge base. This formal representation is crucial as it can be verified and refined by human experts, ensuring its correctness and alignment with domain requirements. In the subsequent task processing phase, each incoming input is encoded into the same formal language. A symbolic decision engine then utilizes this encoded input in conjunction with the formal knowledge base to derive a reliable result. Through an extensive evaluation on a complex reasoning task, we demonstrate that a concrete implementation of ATA is competitive with state-of-the-art end-to-end reasoning models in a fully automated setup while maintaining trustworthiness. Crucially, with a human-verified and corrected knowledge base, our approach significantly outperforms even larger models, while exhibiting perfect determinism, enhanced stability against input perturbations, and inherent immunity to prompt injection attacks. By generating decisions grounded in symbolic reasoning, ATA offers a practical and controllable architecture for building the next generation of transparent, auditable, and reliable autonomous agents.",
      "publishedDate": "2025-10-18T07:35:54Z",
      "arxivUrl": "https://arxiv.org/abs/2510.16381",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.15863",
      "title": "PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction",
      "authors": [
        "Simon Yu",
        "Gang Li",
        "Weiyan Shi",
        "Peng Qi"
      ],
      "abstract": "Large language models (LLMs) are moving beyond static uses and are now powering agents that learn continually during their interaction with external environments. For example, agents can learn reusable skills while navigating web pages or toggling new tools. However, existing methods for skill learning often create skills that are over-specialized to a single website and fail to generalize. We introduce PolySkill, a new framework that enables agents to learn generalizable and compositional skills. The core idea, inspired by polymorphism in software engineering, is to decouple a skill's abstract goal (what it accomplishes) and its concrete implementation (how it is executed). Experiments show that our method (1) improves skill reuse by 1.7x on seen websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on unseen websites, while reducing steps by over 20%. (3) In self-exploration settings without specified tasks, our framework improves the quality of proposed tasks and enables agents to learn generalizable skills that work across different sites. By enabling the agent to identify and refine its own goals, the PolySkill enhances the agent's ability to learn a better curriculum, leading to the acquisition of more generalizable skills compared to baseline methods. This work provides a practical path toward building agents capable of continual learning in adaptive environments. Our findings show that separating a skill's goal from its execution is a crucial step toward developing autonomous agents that can learn and generalize across the open web continuously.",
      "publishedDate": "2025-10-17T17:56:00Z",
      "arxivUrl": "https://arxiv.org/abs/2510.15863",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.16051",
      "title": "GUIrilla: A Scalable Framework for Automated Desktop UI Exploration",
      "authors": [
        "Sofiya Garkot",
        "Maksym Shamrai",
        "Ivan Synytsia",
        "Mariya Hirna"
      ],
      "abstract": "Autonomous agents capable of operating complex graphical user interfaces (GUIs) have the potential to transform desktop automation. While recent advances in large language models (LLMs) have significantly improved UI understanding, navigating full-window, multi-application desktop environments remains a major challenge. Data availability is limited by costly manual annotation, closed-source datasets and surface-level synthetic pipelines. We introduce GUIrilla, an automated scalable framework that systematically explores applications via native accessibility APIs to address the critical data collection challenge in GUI automation. Our framework focuses on macOS - an ecosystem with limited representation in current UI datasets - though many of its components are designed for broader cross-platform applicability. GUIrilla organizes discovered interface elements and crawler actions into hierarchical GUI graphs and employs specialized interaction handlers to achieve comprehensive application coverage. Using the application graphs from GUIrilla crawler, we construct and release GUIrilla-Task, a large-scale dataset of 27,171 functionally grounded tasks across 1,108 macOS applications, each annotated with full-desktop and window-level screenshots, accessibility metadata, and semantic action traces. Empirical results show that tuning LLM-based agents on GUIrilla-Task significantly improves performance on downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro benchmark while using 97% less data. We also release macapptree, an open-source library for reproducible collection of structured accessibility metadata, along with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold benchmark, and the framework code to support open research in desktop autonomy.",
      "publishedDate": "2025-10-16T19:03:45Z",
      "arxivUrl": "https://arxiv.org/abs/2510.16051",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.14881",
      "title": "The Gatekeeper Knows Enough",
      "authors": [
        "Fikresilase Wondmeneh Abebayew"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs' stateless nature and inefficient context management. These limitations lead to unreliable output, unpredictable behavior, and inefficient resource usage, particularly when interacting with large, structured, and sensitive knowledge systems such as codebases and documents. To address these challenges, we introduce the Gatekeeper Protocol, a novel, domain-agnostic framework that governs agent-system interactions. Our protocol mandates that the agent first operate and reason on a minimalist, low-fidelity \"latent state\" representation of the system to strategically request high-fidelity context on demand. All interactions are mediated through a unified JSON format that serves as a declarative, state-synchronized protocol, ensuring the agent's model of the system remains verifiably grounded in the system's reality. We demonstrate the efficacy of this protocol with Sage, a reference implementation of the Gatekeeper Protocol for software development. Our results show that this approach significantly increases agent reliability, improves computational efficiency by minimizing token consumption, and enables scalable interaction with complex systems, creating a foundational methodology for building more robust, predictable, and grounded AI agents for any structured knowledge domain.",
      "publishedDate": "2025-10-16T17:00:42Z",
      "arxivUrl": "https://arxiv.org/abs/2510.14881",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.14703",
      "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling",
      "authors": [
        "Jianghao Lin",
        "Yuanyuan Shi",
        "Xin Peng",
        "Renjie Ding",
        "Hairui Wang",
        "Yuxuan Peng",
        "Bizhe Bai",
        "Weixi Song",
        "Fengshuo Bai",
        "Huacan Chai",
        "Weinan Zhang",
        "Fei Huang",
        "Ying Wen"
      ],
      "abstract": "Large language models (LLMs) are increasingly demonstrating strong capabilities as autonomous agents, with function calling serving as a core mechanism for interaction with the environment. Meanwhile, inference scaling has become a cutting-edge technique to enhance LLM performance by allocating more computational resources during the inference process. However, current research on inference scaling primarily focuses on unstructured output generation tasks, leaving its application in structured outputs, like function calling, largely underexplored. To bridge this gap, we propose an inference scaling framework that combines fine-grained beam search with a process reward model, ToolPRM, which scores the internal steps of each single function call. To train ToolPRM, we construct the first fine-grained intra-call process supervision dataset, automatically annotated with function-masking techniques to provide step-level rewards for structured tool-use reasoning. Extensive experiments demonstrate that ToolPRM beats the coarse-grained and outcome reward models in terms of predictive accuracy, indicating its stronger capability in supervising the function calling inference process. Inference scaling technique equipped with ToolPRM also significantly improves the backbone model performance across various function calling tasks and benchmarks. More importantly, we reveal a key principle for applying inference scaling techniques to structured outputs: \"explore more but retain less\" due to the unrecoverability characteristics of structured function calling generation.",
      "publishedDate": "2025-10-16T14:06:03Z",
      "arxivUrl": "https://arxiv.org/abs/2510.14703",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.14133",
      "title": "Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems",
      "authors": [
        "Edoardo Allegrini",
        "Ananth Shreekumar",
        "Z. Berkay Celik"
      ],
      "abstract": "Agentic AI systems, which leverage multiple autonomous agents and Large Language Models (LLMs), are increasingly used to address complex, multi-step tasks. The safety, security, and functionality of these systems are critical, especially in high-stakes applications. However, the current ecosystem of inter-agent communication is fragmented, with protocols such as the Model Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol for coordination being analyzed in isolation. This fragmentation creates a semantic gap that prevents the rigorous analysis of system properties and introduces risks such as architectural misalignment and exploitable coordination issues. To address these challenges, we introduce a modeling framework for agentic AI systems composed of two foundational models. The first, the host agent model, formalizes the top-level entity that interacts with the user, decomposes tasks, and orchestrates their execution by leveraging external agents and tools. The second, the task lifecycle model, details the states and transitions of individual sub-tasks from creation to completion, providing a fine-grained view of task management and error handling. Together, these models provide a unified semantic framework for reasoning about the behavior of multi-AI agent systems. Grounded in this framework, we define 17 properties for the host agent and 14 for the task lifecycle, categorized into liveness, safety, completeness, and fairness. Expressed in temporal logic, these properties enable formal verification of system behavior, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities. Through this effort, we introduce the first rigorously grounded, domain-agnostic framework for the systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.",
      "publishedDate": "2025-10-15T22:02:30Z",
      "arxivUrl": "https://arxiv.org/abs/2510.14133",
      "categories": [
        "agents",
        "multi-agent",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2510.12864",
      "title": "From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models",
      "authors": [
        "Imran Khan"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being deployed as the reasoning engines for agentic AI systems, yet they exhibit a critical flaw: a rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent. This \"rule-rigidity\" is a significant barrier to building trustworthy autonomous agents. While prior work has shown that supervised fine-tuning (SFT) with human explanations can mitigate this issue, SFT is computationally expensive and inaccessible to many practitioners. To address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a novel, low-compute meta-prompting technique designed to elicit human-aligned exception handling in LLMs in a zero-shot manner. The RID framework provides the model with a structured cognitive schema for deconstructing tasks, classifying rules, weighing conflicting outcomes, and justifying its final decision. We evaluated the RID framework against baseline and Chain-of-Thought (CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced judgment across diverse domains. Our human-verified results demonstrate that the RID framework significantly improves performance, achieving a 95% Human Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT. Furthermore, it consistently produces higher-quality, intent-driven reasoning. This work presents a practical, accessible, and effective method for steering LLMs from literal instruction-following to liberal, goal-oriented reasoning, paving the way for more reliable and pragmatic AI agents.",
      "publishedDate": "2025-10-14T16:42:52Z",
      "arxivUrl": "https://arxiv.org/abs/2510.12864",
      "categories": [
        "prompting",
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.10991",
      "title": "A Survey on Agentic Multimodal Large Language Models",
      "authors": [
        "Huanjin Yao",
        "Ruifei Zhang",
        "Jiaxing Huang",
        "Jingyi Zhang",
        "Yibo Wang",
        "Bo Fang",
        "Ruolin Zhu",
        "Yongcheng Jing",
        "Shunyu Liu",
        "Guanbin Li",
        "Dacheng Tao"
      ],
      "abstract": "With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.",
      "publishedDate": "2025-10-13T04:07:01Z",
      "arxivUrl": "https://arxiv.org/abs/2510.10991",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.10461",
      "title": "MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision",
      "authors": [
        "Hongjie Zheng",
        "Zesheng Shi",
        "Ping Yi"
      ],
      "abstract": "Autonomous agents utilizing Large Language Models (LLMs) have demonstrated remarkable capabilities in isolated medical tasks like diagnosis and image analysis, but struggle with integrated clinical workflows that connect diagnostic reasoning and medication decisions. We identify a core limitation: existing medical AI systems process tasks in isolation without the cross-validation and knowledge integration found in clinical teams, reducing their effectiveness in real-world healthcare scenarios. To transform the isolation paradigm into a collaborative approach, we propose MedCoAct, a confidence-aware multi-agent framework that simulates clinical collaboration by integrating specialized doctor and pharmacist agents, and present a benchmark, DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\\% diagnostic accuracy and 67.58\\% medication recommendation accuracy, outperforming single agent framework by 7.04\\% and 7.08\\% respectively. This collaborative approach generalizes well across diverse medical domains, proving especially effective for telemedicine consultations and routine clinical scenarios, while providing interpretable decision-making pathways.",
      "publishedDate": "2025-10-12T05:52:31Z",
      "arxivUrl": "https://arxiv.org/abs/2510.10461",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.10049",
      "title": "ALLOY: Generating Reusable Agent Workflows from User Demonstration",
      "authors": [
        "Jiawen Li",
        "Zheng Ning",
        "Yuan Tian",
        "Toby Jia-jun Li"
      ],
      "abstract": "Large language models (LLMs) enable end-users to delegate complex tasks to autonomous agents through natural language. However, prompt-based interaction faces critical limitations: Users often struggle to specify procedural requirements for tasks, especially those that don't have a factually correct solution but instead rely on personal preferences, such as posting social media content or planning a trip. Additionally, a ''successful'' prompt for one task may not be reusable or generalizable across similar tasks. We present ALLOY, a system inspired by classical HCI theories on Programming by Demonstration (PBD), but extended to enhance adaptability in creating LLM-based web agents. ALLOY enables users to express procedural preferences through natural demonstrations rather than prompts, while making these procedures transparent and editable through visualized workflows that can be generalized across task variations. In a study with 12 participants, ALLOY's demonstration--based approach outperformed prompt-based agents and manual workflows in capturing user intent and procedural preferences in complex web tasks. Insights from the study also show how demonstration--based interaction complements the traditional prompt-based approach.",
      "publishedDate": "2025-10-11T06:30:34Z",
      "arxivUrl": "https://arxiv.org/abs/2510.10049",
      "categories": [
        "agents",
        "planning",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.09901",
      "title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
      "authors": [
        "Lianhao Zhou",
        "Hongyi Ling",
        "Cong Fu",
        "Yepeng Huang",
        "Michael Sun",
        "Wendi Yu",
        "Xiaoxuan Wang",
        "Xiner Li",
        "Xingyu Su",
        "Junkai Zhang",
        "Xiusi Chen",
        "Chenxing Liang",
        "Xiaofeng Qian",
        "Heng Ji",
        "Wei Wang",
        "Marinka Zitnik",
        "Shuiwang Ji"
      ],
      "abstract": "Computing has long served as a cornerstone of scientific discovery. Recently, a paradigm shift has emerged with the rise of large language models (LLMs), introducing autonomous systems, referred to as agents, that accelerate discovery across varying levels of autonomy. These language agents provide a flexible and versatile framework that orchestrates interactions with human scientists, natural language, computer language and code, and physics. This paper presents our view and vision of LLM-based scientific agents and their growing role in transforming the scientific discovery lifecycle, from hypothesis discovery, experimental design and execution, to result analysis and refinement. We critically examine current methodologies, emphasizing key innovations, practical achievements, and outstanding limitations. Additionally, we identify open research challenges and outline promising directions for building more robust, generalizable, and adaptive scientific agents. Our analysis highlights the transformative potential of autonomous agents to accelerate scientific discovery across diverse domains.",
      "publishedDate": "2025-10-10T22:26:26Z",
      "arxivUrl": "https://arxiv.org/abs/2510.09901",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.09608",
      "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
      "authors": [
        "Ruyi Xu",
        "Guangxuan Xiao",
        "Yukang Chen",
        "Liuning He",
        "Kelly Peng",
        "Yao Lu",
        "Song Han"
      ],
      "abstract": "Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
      "publishedDate": "2025-10-10T17:59:58Z",
      "arxivUrl": "https://arxiv.org/abs/2510.09608",
      "categories": [
        "agents",
        "evaluation",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.09721",
      "title": "A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System",
      "authors": [
        "Jiale Guo",
        "Suizhi Huang",
        "Mei Li",
        "Dong Huang",
        "Xingsheng Chen",
        "Regina Zhang",
        "Zhijiang Guo",
        "Han Yu",
        "Siu-Ming Yiu",
        "Pietro Lio",
        "Kwok-Yan Lam"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into software engineering has driven a transition from traditional rule-based systems to autonomous agentic systems capable of solving complex problems. However, systematic progress is hindered by a lack of comprehensive understanding of how benchmarks and solutions interconnect. This survey addresses this gap by providing the first holistic analysis of LLM-powered software engineering, offering insights into evaluation methodologies and solution paradigms. We review over 150 recent papers and propose a taxonomy along two key dimensions: (1) Solutions, categorized into prompt-based, fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, including tasks such as code generation, translation, and repair. Our analysis highlights the evolution from simple prompt engineering to sophisticated agentic systems incorporating capabilities like planning, reasoning, memory mechanisms, and tool augmentation. To contextualize this progress, we present a unified pipeline illustrating the workflow from task specification to deliverables, detailing how different solution paradigms address various complexity levels. Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. We also identify critical research gaps and propose future directions, including multi-agent collaboration, self-evolving systems, and formal verification integration. This survey serves as a foundational guide for advancing LLM-driven software engineering. We maintain a GitHub repository that continuously updates the reviewed and related papers at https://github.com/lisaGuojl/LLM-Agent-SE-Survey.",
      "publishedDate": "2025-10-10T06:56:50Z",
      "arxivUrl": "https://arxiv.org/abs/2510.09721",
      "categories": [
        "multi-agent",
        "code-generation",
        "agents",
        "prompting",
        "evaluation",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2510.08255",
      "title": "Opponent Shaping in LLM Agents",
      "authors": [
        "Marta Emili Garcia Segura",
        "Stephen Hailes",
        "Mirco Musolesi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.",
      "publishedDate": "2025-10-09T14:13:24Z",
      "arxivUrl": "https://arxiv.org/abs/2510.08255",
      "categories": [
        "agents",
        "multi-agent",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2510.06903",
      "title": "When Machines Meet Each Other: Network Effects and the Strategic Role of History in Multi-Agent AI",
      "authors": [
        "Yu Liu",
        "Wenwen Li",
        "Yifan Dou",
        "Guangnan Ye"
      ],
      "abstract": "As artificial intelligence (AI) enters the agentic era, large language models (LLMs) are increasingly deployed as autonomous agents that interact with one another rather than operate in isolation. This shift raises a fundamental question: how do machine agents behave in interdependent environments where outcomes depend not only on their own choices but also on the coordinated expectations of peers? To address this question, we study LLM agents in a canonical network-effect game, where economic theory predicts convergence to a fulfilled expectation equilibrium (FEE). We design an experimental framework in which 50 heterogeneous GPT-5-based agents repeatedly interact under systematically varied network-effect strengths, price trajectories, and decision-history lengths. The results reveal that LLM agents systematically diverge from FEE: they underestimate participation at low prices, overestimate at high prices, and sustain persistent dispersion. Crucially, the way history is structured emerges as a design lever. Simple monotonic histories-where past outcomes follow a steady upward or downward trend-help stabilize coordination, whereas nonmonotonic histories amplify divergence and path dependence. Regression analyses at the individual level further show that price is the dominant driver of deviation, history moderates this effect, and network effects amplify contextual distortions. Together, these findings advance machine behavior research by providing the first systematic evidence on multi-agent AI systems under network effects and offer guidance for configuring such systems in practice.",
      "publishedDate": "2025-10-08T11:39:16Z",
      "arxivUrl": "https://arxiv.org/abs/2510.06903",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2510.05596",
      "title": "From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions",
      "authors": [
        "Changyuan Zhao",
        "Ruichen Zhang",
        "Jiacheng Wang",
        "Dusit Niyato",
        "Geng Sun",
        "Xianbin Wang",
        "Shiwen Mao",
        "Abbas Jamalipour"
      ],
      "abstract": "Self-evolving agentic artificial intelligence (AI) offers a new paradigm for future wireless systems by enabling autonomous agents to continually adapt and improve without human intervention. Unlike static AI models, self-evolving agents embed an autonomous evolution cycle that updates models, tools, and workflows in response to environmental dynamics. This paper presents a comprehensive overview of self-evolving agentic AI, highlighting its layered architecture, life cycle, and key techniques, including tool intelligence, workflow optimization, self-reflection, and evolutionary learning. We further propose a multi-agent cooperative self-evolving agentic AI framework, where multiple large language models (LLMs) are assigned role-specialized prompts under the coordination of a supervisor agent. Through structured dialogue, iterative feedback, and systematic validation, the system autonomously executes the entire life cycle without human intervention. A case study on antenna evolution in low-altitude wireless networks (LAWNs) demonstrates how the framework autonomously upgrades fixed antenna optimization into movable antenna optimization. Experimental results show that the proposed self-evolving agentic AI autonomously improves beam gain and restores degraded performance by up to 52.02%, consistently surpassing the fixed baseline with little to no human intervention and validating its adaptability and robustness for next-generation wireless intelligence.",
      "publishedDate": "2025-10-07T05:45:25Z",
      "arxivUrl": "https://arxiv.org/abs/2510.05596",
      "categories": [
        "agents",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2510.05414",
      "title": "A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis",
      "authors": [
        "Ziheng Geng",
        "Jiachen Liu",
        "Ran Cao",
        "Lu Cheng",
        "Haifeng Wang",
        "Minghui Cheng"
      ],
      "abstract": "Large language models (LLMs) have recently been used to empower autonomous agents in engineering, significantly improving automation and efficiency in labor-intensive workflows. However, their potential remains underexplored in structural engineering, particularly for finite element modeling tasks requiring geometric modeling, complex reasoning, and domain knowledge. To bridge this gap, this paper develops a LLM-based multi-agent system to automate finite element modeling of 2D frames. The system decomposes structural analysis into subtasks, each managed by a specialized agent powered by the lightweight Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis Agent, which extracts geometry, boundary, and material parameters from the user input. Next, a Geometry Agent incrementally derives node coordinates and element connectivity by applying expert-defined rules. These structured outputs are converted into executable OpenSeesPy code by a Translation Agent and refined by a Model Validation Agent through consistency checks. Then, a Load Agent applies load conditions into the assembled structural model. Experimental evaluations on 20 benchmark problems demonstrate that the system achieves accuracy over 80% in most cases across 10 repeated trials, outperforming Gemini-2.5 Pro and ChatGPT-4o models.",
      "publishedDate": "2025-10-06T22:12:52Z",
      "arxivUrl": "https://arxiv.org/abs/2510.05414",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.02209",
      "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?",
      "authors": [
        "Yanxu Chen",
        "Zijun Yao",
        "Yantao Liu",
        "Jin Ye",
        "Jianing Yu",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.",
      "publishedDate": "2025-10-02T16:54:57Z",
      "arxivUrl": "https://arxiv.org/abs/2510.02209",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.01869",
      "title": "TACOS: Task Agnostic COordinator of a multi-drone System",
      "authors": [
        "Alessandro Nazzari",
        "Roberto Rubinacci",
        "Marco Lovera"
      ],
      "abstract": "When a single pilot is responsible for managing a multi-drone system, the task may demand varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks. Enabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces. In this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models (LLMs). TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system on a real-world multi-drone system, and conduct an ablation study to assess the contribution of each module.",
      "publishedDate": "2025-10-02T10:21:35Z",
      "arxivUrl": "https://arxiv.org/abs/2510.01869",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "multi-agent",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2510.01645",
      "title": "Position: Privacy Is Not Just Memorization!",
      "authors": [
        "Niloofar Mireshghallah",
        "Tianshi Li"
      ],
      "abstract": "The discourse on privacy risks in Large Language Models (LLMs) has disproportionately focused on verbatim memorization of training data, while a constellation of more immediate and scalable privacy threats remain underexplored. This position paper argues that the privacy landscape of LLM systems extends far beyond training data extraction, encompassing risks from data collection practices, inference-time context leakage, autonomous agent capabilities, and the democratization of surveillance through deep inference attacks. We present a comprehensive taxonomy of privacy risks across the LLM lifecycle -- from data collection through deployment -- and demonstrate through case studies how current privacy frameworks fail to address these multifaceted threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers published at leading conferences over the past decade (2016--2025), we reveal that while memorization receives outsized attention in technical research, the most pressing privacy harms lie elsewhere, where current technical approaches offer little traction and viable paths forward remain unclear. We call for a fundamental shift in how the research community approaches LLM privacy, moving beyond the narrow focus of current technical solutions and embracing interdisciplinary approaches that address the sociotechnical nature of these emerging threats.",
      "publishedDate": "2025-10-02T04:02:06Z",
      "arxivUrl": "https://arxiv.org/abs/2510.01645",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2510.00857",
      "title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs",
      "authors": [
        "Adi Simhi",
        "Jonathan Herzig",
        "Martin Tutek",
        "Itay Itzhak",
        "Idan Szpektor",
        "Yonatan Belinkov"
      ],
      "abstract": "As large language models (LLMs) evolve from conversational assistants into autonomous agents, evaluating the safety of their actions becomes critical. Prior safety benchmarks have primarily focused on preventing generation of harmful content, such as toxic text. However, they overlook the challenge of agents taking harmful actions when the most effective path to an operational goal conflicts with human safety. To address this gap, we introduce ManagerBench, a benchmark that evaluates LLM decision-making in realistic, human-validated managerial scenarios. Each scenario forces a choice between a pragmatic but harmful action that achieves an operational goal, and a safe action that leads to worse operational performance. A parallel control set, where potential harm is directed only at inanimate objects, measures a model's pragmatism and identifies its tendency to be overly safe. Our findings indicate that the frontier LLMs perform poorly when navigating this safety-pragmatism trade-off. Many consistently choose harmful options to advance their operational goals, while others avoid harm only to become overly safe and ineffective. Critically, we find this misalignment does not stem from an inability to perceive harm, as models' harm assessments align with human judgments, but from flawed prioritization. ManagerBench is a challenging benchmark for a core component of agentic behavior: making safe choices when operational goals and alignment values incentivize conflicting actions. Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.",
      "publishedDate": "2025-10-01T13:08:33Z",
      "arxivUrl": "https://arxiv.org/abs/2510.00857",
      "categories": [
        "agents",
        "evaluation",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.01295",
      "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation",
      "authors": [
        "Zarreen Reza"
      ],
      "abstract": "As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. These methods fail to capture the emergent social and cognitive dynamics that arise when agents communicate, persuade, and collaborate in interactive environments. To address this gap, we introduce a novel evaluation framework that uses multi-agent debate as a controlled \"social laboratory\" to discover and quantify these behaviors. In our framework, LLM-based agents, instantiated with distinct personas and incentives, deliberate on a wide range of challenging topics under the supervision of an LLM moderator. Our analysis, enabled by a new suite of psychometric and semantic metrics, reveals several key findings. Across hundreds of debates, we uncover a powerful and robust emergent tendency for agents to seek consensus, consistently reaching high semantic agreement (μ > 0.88) even without explicit instruction and across sensitive topics. We show that assigned personas induce stable, measurable psychometric profiles, particularly in cognitive effort, and that the moderators persona can significantly alter debate outcomes by structuring the environment, a key finding for external AI alignment. This work provides a blueprint for a new class of dynamic, psychometrically grounded evaluation protocols designed for the agentic setting, offering a crucial methodology for understanding and shaping the social behaviors of the next generation of AI agents. We have released the code and results at https://github.com/znreza/multi-agent-LLM-eval-for-debate.",
      "publishedDate": "2025-10-01T07:10:28Z",
      "arxivUrl": "https://arxiv.org/abs/2510.01295",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "multi-agent",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.00510",
      "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
      "authors": [
        "Jiarun Liu",
        "Shiyue Xu",
        "Shangkun Liu",
        "Yang Li",
        "Wen Liu",
        "Min Liu",
        "Xiaoqing Zhou",
        "Hanmin Wang",
        "Shilin Jia",
        "zhen Wang",
        "Shaohua Tian",
        "Hanhao Li",
        "Junbo Zhang",
        "Yongli Yu",
        "Peng Cao",
        "Haofen Wang"
      ],
      "abstract": "Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.",
      "publishedDate": "2025-10-01T04:41:58Z",
      "arxivUrl": "https://arxiv.org/abs/2510.00510",
      "categories": [
        "agents",
        "planning",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.25873",
      "title": "Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs",
      "authors": [
        "Hankun Dai",
        "Maoquan Wang",
        "Mengnan Qi",
        "Yikai Zhang",
        "Zijian Jin",
        "Yongqiang Yao",
        "Yufan Huang",
        "Shengyu Fu",
        "Elsie Nallipogu"
      ],
      "abstract": "Large language models (LLMs) are increasingly being applied to programming tasks, ranging from single-turn code completion to autonomous agents. Current code agent designs frequently depend on complex, hand-crafted workflows and tool sets. However, this reliance on elaborate scaffolding presents several challenges: agent performance becomes overly dependent on prompt tuning and custom design choices, heavy human intervention obscures a model's true underlying capabilities, and intricate pipelines are costly to build and maintain. Furthermore, optimizing complex task prompts increases the risk of data leakage. Currently, when introducing new models, LLM providers like OpenAI and Anthropic often publish benchmark scores to demonstrate their models' coding proficiency, but keep their proprietary evaluation frameworks confidential. To address these limitations, we introduce Lita (Lite Agent), which operationalizes liteness, a principle of minimizing manual design while retaining the essential elements of a fully autonomous agent. Lita enables a more faithful and unified evaluation without elaborate scaffolding. Experiments on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines. Crucially, Lita also consumes fewer tokens and requires significantly less design effort. Our results suggest that Lita is sufficient to reveal the underlying coding competence of modern LLMs. Finally, we propose the Agent Complexity Law: the performance gap between agents of varying complexity, from simple to sophisticated designs, will shrink as the core model improves, ultimately converging to a negligible difference.",
      "publishedDate": "2025-09-30T07:07:32Z",
      "arxivUrl": "https://arxiv.org/abs/2509.25873",
      "categories": [
        "code-generation",
        "agents",
        "evaluation",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2509.24923",
      "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training",
      "authors": [
        "Sanxing Chen",
        "Xiaoyin Chen",
        "Yukun Huang",
        "Roy Xie",
        "Bhuwan Dhingra"
      ],
      "abstract": "While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.",
      "publishedDate": "2025-09-29T15:25:42Z",
      "arxivUrl": "https://arxiv.org/abs/2509.24923",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.03253",
      "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents",
      "authors": [
        "Heyang Gao",
        "Zexu Sun",
        "Erxue Min",
        "Hengyi Cai",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Xu Chen"
      ],
      "abstract": "Large Language Models (LLMs) as autonomous agents are increasingly tasked with solving complex, long-horizon problems. Aligning these agents via preference-based offline methods like Direct Preference Optimization (DPO) is a promising direction, yet it faces a critical granularity mismatch. Trajectory-level DPO provides a signal that is too coarse for precise credit assignment, while step-level DPO is often too myopic to capture the value of multi-step behaviors. To resolve this challenge, we introduce Hierarchical Preference Learning (HPL), a hierarchical framework that optimizes LLM agents by leveraging preference signals at multiple, synergistic granularities. While HPL incorporates trajectory- and step-level DPO for global and local policy stability, its core innovation lies in group-level preference optimization guided by a dual-layer curriculum. Our approach first decomposes expert trajectories into semantically coherent action groups and then generates contrasting suboptimal groups to enable preference learning at a fine-grained, sub-task level. Then, instead of treating all preference pairs equally, HPL introduces a curriculum scheduler that organizes the learning process from simple to complex. This curriculum is structured along two axes: the group length, representing sub-task complexity, and the sample difficulty, defined by the reward gap between preferred and dispreferred action groups. Experiments on three challenging agent benchmarks show that HPL outperforms existing state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO loss effectively integrates preference signals across multiple granularities, while the dual-layer curriculum is crucial for enabling the agent to solve a wide range of tasks, from simple behaviors to complex multi-step sequences.",
      "publishedDate": "2025-09-26T08:43:39Z",
      "arxivUrl": "https://arxiv.org/abs/2510.03253",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.21981",
      "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration",
      "authors": [
        "Zhimin Wang",
        "Shaokang He",
        "Duo Wu",
        "Jinghe Wang",
        "Linjia Kang",
        "Jing Yu",
        "Zhi Wang"
      ],
      "abstract": "Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.",
      "publishedDate": "2025-09-26T07:03:52Z",
      "arxivUrl": "https://arxiv.org/abs/2509.21981",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "planning",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.20616",
      "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning",
      "authors": [
        "Hanjiang Hu",
        "Changliu Liu",
        "Na Li",
        "Yebin Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.",
      "publishedDate": "2025-09-24T23:47:36Z",
      "arxivUrl": "https://arxiv.org/abs/2509.20616",
      "categories": [
        "agents",
        "planning",
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2510.05107",
      "title": "Structured Cognitive Loop for Behavioral Intelligence in Large Language Model Agents",
      "authors": [
        "Myung Ho Kim"
      ],
      "abstract": "Large language models have advanced natural language understanding and generation, but their use as autonomous agents introduces architectural challenges for multi-step tasks. Existing frameworks often mix cognition, memory, and control in a single prompt, reducing coherence and predictability. The Structured Cognitive Loop (SCL) is proposed as an alternative architecture that separates these functions. In SCL, the language model handles cognition, memory is stored externally, and execution is guided by a lightweight controller within a goal-directed loop. This design allows intermediate results to be recorded and verified before actions are taken, improving traceability and evaluation. SCL is evaluated against prompt-based baselines such as ReAct and LangChain agents across three tasks: travel planning, conditional email drafting, and constraint-guided image generation. Under matched settings, SCL achieves an average task success rate of 86.3 percent, compared with 70.5 to 76.8 percent for baselines. It also shows higher goal fidelity, fewer redundant calls, and reduced unsupported assertions. These results indicate that separating cognition, memory, and control can enhance reliability and interpretability without relying on larger models or heavier prompts. The findings should be regarded as preliminary evidence, with broader tests across model families and task domains planned for future work.",
      "publishedDate": "2025-09-23T17:43:17Z",
      "arxivUrl": "https://arxiv.org/abs/2510.05107",
      "categories": [
        "agents",
        "planning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.19199",
      "title": "Agentic Reinforcement Learning with Implicit Step Rewards",
      "authors": [
        "Xiaoqian Liu",
        "Ke Wang",
        "Yuchuan Wu",
        "Fei Huang",
        "Yongbin Li",
        "Junge Zhang",
        "Jianbin Jiao"
      ],
      "abstract": "Large language models (LLMs) are increasingly developed as autonomous agents using reinforcement learning (agentic RL) that reason and act in interactive environments. However, sparse and sometimes unverifiable rewards make it extremely challenging to assign credit when training LLM agents that serve as a policy. Recent work attempts to integrate process supervision into RL but suffers from biased annotation, reward hacking, high-variance from overly fine-grained rewards or failtures when state overlap is rare. We therefore introduce implicit step rewards for agentic RL (iStar), a general credit-assignment strategy that integrates seamlessly with standard RL algorithms without relying on additional rollouts or explicit step labels. Particularly, we alternatively optimize an implicit process reward model (PRM) with the policy model to generate implicit step rewards via a trajectory-based DPO objective. Theoretical analysis shows that this learning objective produces a step-wise reward function. Then the implicit step rewards are used to compute step-level advantages, which are combined with trajectory (or episode)-level advantages for policy updates, creating a self-reinforcing training loop. We evaluate our method on three challenging agent benchmarks, including WebShop and VisualSokoban, as well as open-ended social interactions with unverifiable rewards in SOTOPIA. Crucially, iStar shows superior performance over frontier LLMs and strong RL baselines across domains, achieving state-of-the-art results with higher sample-efficiency and training stability. Further analysis also demonstrates efficient exploration by iStar with increased rewards in both step- and episode-level while maintaining fewer steps to achieve task success. Code will be available soon.",
      "publishedDate": "2025-09-23T16:15:42Z",
      "arxivUrl": "https://arxiv.org/abs/2509.19199",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.17567",
      "title": "LIMI: Less is More for Agency",
      "authors": [
        "Yang Xiao",
        "Mohan Jiang",
        "Jie Sun",
        "Keyu Li",
        "Jifan Lin",
        "Yumin Zhuang",
        "Ji Zeng",
        "Shijie Xia",
        "Qishuo Hua",
        "Xuefeng Li",
        "Xiaojie Cai",
        "Tongyu Wang",
        "Yue Zhang",
        "Liming Liu",
        "Xia Wu",
        "Jinlong Hou",
        "Yuan Cheng",
        "Wenjie Li",
        "Xiang Wang",
        "Dequan Wang",
        "Pengfei Liu"
      ],
      "abstract": "We define Agency as the emergent capacity of AI systems to function as autonomous agents actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of the Age of AI Agency, driven by a critical industry shift: the urgent need for AI systems that don't just think, but work. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount. Current approaches assume that more data yields better agency, following traditional scaling laws from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%). Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples. Our findings establish the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.",
      "publishedDate": "2025-09-22T10:59:32Z",
      "arxivUrl": "https://arxiv.org/abs/2509.17567",
      "categories": [
        "agents",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.17328",
      "title": "UIPro: Unleashing Superior Interaction Capability For GUI Agents",
      "authors": [
        "Hongxin Li",
        "Jingran Su",
        "Jingfan Chen",
        "Zheng Ju",
        "Yuntao Chen",
        "Qing Li",
        "Zhaoxiang Zhang"
      ],
      "abstract": "Building autonomous agents that perceive and operate graphical user interfaces (GUIs) like humans has long been a vision in the field of artificial intelligence. Central to these agents is the capability for GUI interaction, which involves GUI understanding and planning capabilities. Existing methods have tried developing GUI agents based on the multi-modal comprehension ability of vision-language models (VLMs). However, the limited scenario, insufficient size, and heterogeneous action spaces hinder the progress of building generalist GUI agents. To resolve these issues, this paper proposes \\textbf{UIPro}, a novel generalist GUI agent trained with extensive multi-platform and multi-task GUI interaction data, coupled with a unified action space. We first curate a comprehensive dataset encompassing 20.6 million GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding capability, which is key to downstream GUI agent tasks. Subsequently, we establish a unified action space to harmonize heterogeneous GUI agent task datasets and produce a merged dataset to foster the action prediction ability of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's superior performance across multiple GUI task benchmarks on various platforms, highlighting the effectiveness of our approach.",
      "publishedDate": "2025-09-22T03:04:53Z",
      "arxivUrl": "https://arxiv.org/abs/2509.17328",
      "categories": [
        "agents",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.16736",
      "title": "Towards Transparent and Incentive-Compatible Collaboration in Decentralized LLM Multi-Agent Systems: A Blockchain-Driven Approach",
      "authors": [
        "Minfeng Qi",
        "Tianqing Zhu",
        "Lefeng Zhang",
        "Ningran Li",
        "Wanlei Zhou"
      ],
      "abstract": "Large Language Models (LLMs) have enabled the emergence of autonomous agents capable of complex reasoning, planning, and interaction. However, coordinating such agents at scale remains a fundamental challenge, particularly in decentralized environments where communication lacks transparency and agent behavior cannot be shaped through centralized incentives. We propose a blockchain-based framework that enables transparent agent registration, verifiable task allocation, and dynamic reputation tracking through smart contracts. The core of our design lies in two mechanisms: a matching score-based task allocation protocol that evaluates agents by reputation, capability match, and workload; and a behavior-shaping incentive mechanism that adjusts agent behavior via feedback on performance and reward. Our implementation integrates GPT-4 agents with Solidity contracts and demonstrates, through 50-round simulations, strong task success rates, stable utility distribution, and emergent agent specialization. The results underscore the potential for trustworthy, incentive-compatible multi-agent coordination in open environments.",
      "publishedDate": "2025-09-20T16:00:24Z",
      "arxivUrl": "https://arxiv.org/abs/2509.16736",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2509.12987",
      "title": "Toward PDDL Planning Copilot",
      "authors": [
        "Yarin Benyamin",
        "Argaman Mordoch",
        "Shahaf S. Shperberg",
        "Roni Stern"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being used as autonomous agents capable of performing complicated tasks. However, they lack the ability to perform reliable long-horizon planning on their own. This paper bridges this gap by introducing the Planning Copilot, a chatbot that integrates multiple planning tools and allows users to invoke them through instructions in natural language. The Planning Copilot leverages the Model Context Protocol (MCP), a recently developed standard for connecting LLMs with external tools and systems. This approach allows using any LLM that supports MCP without domain-specific fine-tuning. Our Planning Copilot supports common planning tasks such as checking the syntax of planning problems, selecting an appropriate planner, calling it, validating the plan it generates, and simulating their execution. We empirically evaluate the ability of our Planning Copilot to perform these tasks using three open-source LLMs. The results show that the Planning Copilot highly outperforms using the same LLMs without the planning tools. We also conducted a limited qualitative comparison of our tool against Chat GPT-5, a very recent commercial LLM. Our results shows that our Planning Copilot significantly outperforms GPT-5 despite relying on a much smaller LLM. This suggests dedicated planning tools may be an effective way to enable LLMs to perform planning tasks.",
      "publishedDate": "2025-09-16T11:51:07Z",
      "arxivUrl": "https://arxiv.org/abs/2509.12987",
      "categories": [
        "agents",
        "tool-use",
        "planning",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2509.11943",
      "title": "Agentic System with Modal Logic for Autonomous Diagnostics",
      "authors": [
        "Antonin Sulc",
        "Thorsten Hellert"
      ],
      "abstract": "The development of intelligent agents, particularly those powered by language models (LMs), has shown a critical role in various environments that require intelligent and autonomous decision-making. Environments are not passive testing grounds, and they represent the data required for agents to learn and exhibit in very challenging conditions that require adaptive, complex, and autonomous capacity to make decisions. While the paradigm of scaling models and datasets has led to remarkable emergent capabilities, we argue that scaling the structure, fidelity, and logical consistency of agent reasoning within these environments is a crucial, yet underexplored, dimension of AI research. This paper introduces a neuro-symbolic multi-agent architecture where the belief states of individual agents are formally represented as Kripke models. This foundational choice enables them to reason about known concepts of \\emph{possibility} and \\emph{necessity} using the formal language of modal logic. In this work, we use immutable, domain-specific knowledge to make an informed root cause diagnosis, which is encoded as logical constraints essential for proper, reliable, and explainable diagnosis. In the proposed model, we show constraints that actively guide the hypothesis generation of LMs, effectively preventing them from reaching physically or logically untenable conclusions. In a high-fidelity simulated particle accelerator environment, our system successfully diagnoses complex, cascading failures by combining the powerful semantic intuition of LMs with the rigorous, verifiable validation of modal logic and a factual world model and showcasing a viable path toward more robust, reliable, and verifiable autonomous agents.",
      "publishedDate": "2025-09-15T14:03:06Z",
      "arxivUrl": "https://arxiv.org/abs/2509.11943",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2509.09906",
      "title": "Tackling One Health Risks: How Large Language Models are leveraged for Risk Negotiation and Consensus-building",
      "authors": [
        "Alexandra Fetsch",
        "Iurii Savvateev",
        "Racem Ben Romdhane",
        "Martin Wiedmann",
        "Artemiy Dimov",
        "Maciej Durkalec",
        "Josef Teichmann",
        "Jakob Zinsstag",
        "Konstantinos Koutsoumanis",
        "Andreja Rajkovic",
        "Jason Mann",
        "Mauro Tonolla",
        "Monika Ehling-Schulz",
        "Matthias Filter",
        "Sophia Johler"
      ],
      "abstract": "Key global challenges of our times are characterized by complex interdependencies and can only be effectively addressed through an integrated, participatory effort. Conventional risk analysis frameworks often reduce complexity to ensure manageability, creating silos that hinder comprehensive solutions. A fundamental shift towards holistic strategies is essential to enable effective negotiations between different sectors and to balance the competing interests of stakeholders. However, achieving this balance is often hindered by limited time, vast amounts of information, and the complexity of integrating diverse perspectives. This study presents an AI-assisted negotiation framework that incorporates large language models (LLMs) and AI-based autonomous agents into a negotiation-centered risk analysis workflow. The framework enables stakeholders to simulate negotiations, systematically model dynamics, anticipate compromises, and evaluate solution impacts. By leveraging LLMs' semantic analysis capabilities we could mitigate information overload and augment decision-making process under time constraints. Proof-of-concept implementations were conducted in two real-world scenarios: (i) prudent use of a biopesticide, and (ii) targeted wild animal population control. Our work demonstrates the potential of AI-assisted negotiation to address the current lack of tools for cross-sectoral engagement. Importantly, the solution's open source, web based design, suits for application by a broader audience with limited resources and enables users to tailor and develop it for their own needs.",
      "publishedDate": "2025-09-12T00:25:20Z",
      "arxivUrl": "https://arxiv.org/abs/2509.09906",
      "categories": [
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2509.09356",
      "title": "Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning",
      "authors": [
        "Abdel Hakim Drid",
        "Vincenzo Suriani",
        "Daniele Nardi",
        "Abderrezzak Debilou"
      ],
      "abstract": "Navigating and understanding complex and unknown environments autonomously demands more than just basic perception and movement from embodied agents. Truly effective exploration requires agents to possess higher-level cognitive abilities, the ability to reason about their surroundings, and make more informed decisions regarding exploration strategies. However, traditional RL approaches struggle to balance efficient exploration and semantic understanding due to limited cognitive capabilities embedded in the small policies for the agents, leading often to human drivers when dealing with semantic exploration. In this paper, we address this challenge by presenting a novel Deep Reinforcement Learning (DRL) architecture that is specifically designed for resource efficient semantic exploration. A key methodological contribution is the integration of a Vision-Language Model (VLM) common-sense through a layered reward function. The VLM query is modeled as a dedicated action, allowing the agent to strategically query the VLM only when deemed necessary for gaining external guidance, thereby conserving resources. This mechanism is combined with a curriculum learning strategy designed to guide learning at different levels of complexity to ensure robust and stable learning. Our experimental evaluation results convincingly demonstrate that our agent achieves significantly enhanced object discovery rates and develops a learned capability to effectively navigate towards semantically rich regions. Furthermore, it also shows a strategic mastery of when to prompt for external environmental information. By demonstrating a practical and scalable method for embedding common-sense semantic reasoning with autonomous agents, this research provides a novel approach to pursuing a fully intelligent and self-guided exploration in robotics.",
      "publishedDate": "2025-09-11T11:10:08Z",
      "arxivUrl": "https://arxiv.org/abs/2509.09356",
      "categories": [
        "robotics",
        "agents",
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.09215",
      "title": "Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions",
      "authors": [
        "Qinnan Hu",
        "Yuntao Wang",
        "Yuan Gao",
        "Zhou Su",
        "Linkang Du"
      ],
      "abstract": "Large language models (LLMs)-empowered autonomous agents are transforming both digital and physical environments by enabling adaptive, multi-agent collaboration. While these agents offer significant opportunities across domains such as finance, healthcare, and smart manufacturing, their unpredictable behaviors and heterogeneous capabilities pose substantial governance and accountability challenges. In this paper, we propose a blockchain-enabled layered architecture for regulatory agent collaboration, comprising an agent layer, a blockchain data layer, and a regulatory application layer. Within this framework, we design three key modules: (i) an agent behavior tracing and arbitration module for automated accountability, (ii) a dynamic reputation evaluation module for trust assessment in collaborative scenarios, and (iii) a malicious behavior forecasting module for early detection of adversarial activities. Our approach establishes a systematic foundation for trustworthy, resilient, and scalable regulatory mechanisms in large-scale agent ecosystems. Finally, we discuss the future research directions for blockchain-enabled regulatory frameworks in multi-agent systems.",
      "publishedDate": "2025-09-11T07:46:00Z",
      "arxivUrl": "https://arxiv.org/abs/2509.09215",
      "categories": [
        "multi-agent",
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.08380",
      "title": "Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives",
      "authors": [
        "Prathamesh Vasudeo Naik",
        "Naresh Kumar Dintakurthi",
        "Zhanghao Hu",
        "Yue Wang",
        "Robby Qiu"
      ],
      "abstract": "Generating regulatorily compliant Suspicious Activity Report (SAR) remains a high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows. While large language models (LLMs) offer promising fluency, they suffer from factual hallucination, limited crime typology alignment, and poor explainability -- posing unacceptable risks in compliance-critical domains. This paper introduces Co-Investigator AI, an agentic framework optimized to produce Suspicious Activity Reports (SARs) significantly faster and with greater accuracy than traditional methods. Drawing inspiration from recent advances in autonomous agent architectures, such as the AI Co-Scientist, our approach integrates specialized agents for planning, crime type detection, external intelligence gathering, and compliance validation. The system features dynamic memory management, an AI-Privacy Guard layer for sensitive data handling, and a real-time validation agent employing the Agent-as-a-Judge paradigm to ensure continuous narrative quality assurance. Human investigators remain firmly in the loop, empowered to review and refine drafts in a collaborative workflow that blends AI efficiency with domain expertise. We demonstrate the versatility of Co-Investigator AI across a range of complex financial crime scenarios, highlighting its ability to streamline SAR drafting, align narratives with regulatory expectations, and enable compliance teams to focus on higher-order analytical work. This approach marks the beginning of a new era in compliance reporting -- bringing the transformative benefits of AI agents to the core of regulatory processes and paving the way for scalable, reliable, and transparent SAR generation.",
      "publishedDate": "2025-09-10T08:16:04Z",
      "arxivUrl": "https://arxiv.org/abs/2509.08380",
      "categories": [
        "agents",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2509.08088",
      "title": "EnvX: Agentize Everything with Agentic AI",
      "authors": [
        "Linyao Chen",
        "Zimian Peng",
        "Yingxuan Yang",
        "Yikun Wang",
        "Wenzheng Tom Tang",
        "Hiroki H. Kobayashi",
        "Weinan Zhang"
      ],
      "abstract": "The widespread availability of open-source repositories has led to a vast collection of reusable software components, yet their utilization remains manual, error-prone, and disconnected. Developers must navigate documentation, understand APIs, and write integration code, creating significant barriers to efficient software reuse. To address this, we present EnvX, a framework that leverages Agentic AI to agentize GitHub repositories, transforming them into intelligent, autonomous agents capable of natural language interaction and inter-agent collaboration. Unlike existing approaches that treat repositories as static code resources, EnvX reimagines them as active agents through a three-phase process: (1) TODO-guided environment initialization, which sets up the necessary dependencies, data, and validation datasets; (2) human-aligned agentic automation, allowing repository-specific agents to autonomously perform real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple agents to collaborate. By combining large language model capabilities with structured tool integration, EnvX automates not just code generation, but the entire process of understanding, initializing, and operationalizing repository functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18 repositories across domains such as image processing, speech recognition, document analysis, and video manipulation. Our results show that EnvX achieves a 74.07% execution completion rate and 51.85% task pass rate, outperforming existing frameworks. Case studies further demonstrate EnvX's ability to enable multi-repository collaboration via the A2A protocol. This work marks a shift from treating repositories as passive code resources to intelligent, interactive agents, fostering greater accessibility and collaboration within the open-source ecosystem.",
      "publishedDate": "2025-09-09T18:51:36Z",
      "arxivUrl": "https://arxiv.org/abs/2509.08088",
      "categories": [
        "code-generation",
        "agents",
        "tool-use",
        "multi-agent",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2412.12345",
      "title": "Sample LLM Agent Paper: Task Planning with Large Language Models",
      "authors": [
        "John Doe",
        "Jane Smith"
      ],
      "abstract": "This paper presents a novel approach to task planning using large language models. We demonstrate that LLM agents can effectively decompose complex tasks into manageable subtasks through chain-of-thought reasoning.",
      "publishedDate": "2024-12-15T00:00:00Z",
      "arxivUrl": "https://arxiv.org/abs/2412.12345",
      "categories": [
        "agents",
        "planning",
        "reasoning"
      ],
      "year": 2024
    },
    {
      "id": "2411.98765",
      "title": "Retrieval-Augmented Generation for Domain-Specific Applications",
      "authors": [
        "Alice Johnson",
        "Bob Williams"
      ],
      "abstract": "We explore the use of retrieval-augmented generation (RAG) systems for improving large language model performance on domain-specific tasks. Our approach combines dense retrieval with prompting strategies.",
      "publishedDate": "2024-11-20T00:00:00Z",
      "arxivUrl": "https://arxiv.org/abs/2411.98765",
      "categories": [
        "rag",
        "prompting"
      ],
      "year": 2024
    },
    {
      "id": "2410.55555",
      "title": "Multi-Agent Systems for Code Generation with LLMs",
      "authors": [
        "Charlie Brown"
      ],
      "abstract": "This work investigates multi-agent collaboration for automated code generation. We show that multiple LLM agents working together can produce higher quality code than single-agent systems.",
      "publishedDate": "2024-10-10T00:00:00Z",
      "arxivUrl": "https://arxiv.org/abs/2410.55555",
      "categories": [
        "multi-agent",
        "code-generation",
        "agents"
      ],
      "year": 2024
    }
  ]
}