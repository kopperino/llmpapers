{
  "year": "2021",
  "count": 79,
  "papers": [
    {
      "id": "2104.06737",
      "title": "Natural-Language Multi-Agent Simulations of Argumentative Opinion Dynamics",
      "authors": [
        {
          "name": "Gregor Betz",
          "affiliation": null
        }
      ],
      "abstract": "This paper develops a natural-language agent-based model of argumentation (ABMA). Its artificial deliberative agents (ADAs) are constructed with the help of so-called neural language models recently developed in AI and computational linguistics. ADAs are equipped with a minimalist belief system and may generate and submit novel contributions to a conversation. The natural-language ABMA allows us to simulate collective deliberation in English, i.e. with arguments, reasons, and claims themselves -- rather than with their mathematical representations (as in formal models). This paper uses the natural-language ABMA to test the robustness of formal reason-balancing models of argumentation [Maes & Flache 2013, Singer et al. 2019]: First of all, as long as ADAs remain passive, confirmation bias and homophily updating trigger polarization, which is consistent with results from formal models. However, once ADAs start to actively generate new contributions, the evolution of a conservation is dominated by properties of the agents *as authors*. This suggests that the creation of new arguments, reasons, and claims critically affects a conversation and is of pivotal importance for understanding the dynamics of collective deliberation. The paper closes by pointing out further fruitful applications of the model and challenges for future research.",
      "publishedDate": "2021-04-14T09:45:22Z",
      "updatedDate": "2021-04-14T09:45:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.MA",
        "cs.SI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2104.06737v1",
      "arxivUrl": "https://arxiv.org/abs/2104.06737",
      "comment": null,
      "journalRef": null,
      "doi": "10.18564/jasss.4725",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2103.09656",
      "title": "Set-to-Sequence Methods in Machine Learning: a Review",
      "authors": [
        {
          "name": "Mateusz Jurewicz",
          "affiliation": null
        },
        {
          "name": "Leon Strømberg-Derczynski",
          "affiliation": null
        }
      ],
      "abstract": "Machine learning on sets towards sequential output is an important and ubiquitous task, with applications ranging from language modeling and meta-learning to multi-agent strategy games and power grid optimization. Combining elements of representation learning and structured prediction, its two primary challenges include obtaining a meaningful, permutation invariant set representation and subsequently utilizing this representation to output a complex target permutation. This paper provides a comprehensive introduction to the field as well as an overview of important machine learning methods tackling both of these key challenges, with a detailed qualitative comparison of selected model architectures.",
      "publishedDate": "2021-03-17T13:52:33Z",
      "updatedDate": "2021-08-16T12:32:05Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2103.09656v2",
      "arxivUrl": "https://arxiv.org/abs/2103.09656",
      "comment": "46 pages of text, with 10 pages of references. Contains 2 tables and 4 figures. Updated version includes expanded notes on method comparison",
      "journalRef": "Journal of Artificial Intelligence Research 71 (2021): 885 - 924",
      "doi": "10.1613/jair.1.12839",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2112.08493",
      "title": "StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation",
      "authors": [
        {
          "name": "Umut Kocasari",
          "affiliation": null
        },
        {
          "name": "Alara Dirik",
          "affiliation": null
        },
        {
          "name": "Mert Tiftikci",
          "affiliation": null
        },
        {
          "name": "Pinar Yanardag",
          "affiliation": null
        }
      ],
      "abstract": "Discovering meaningful directions in the latent space of GANs to manipulate semantic attributes typically requires large amounts of labeled data. Recent work aims to overcome this limitation by leveraging the power of Contrastive Language-Image Pre-training (CLIP), a joint text-image model. While promising, these methods require several hours of preprocessing or training to achieve the desired manipulations. In this paper, we present StyleMC, a fast and efficient method for text-driven image generation and manipulation. StyleMC uses a CLIP-based loss and an identity loss to manipulate images via a single text prompt without significantly affecting other attributes. Unlike prior work, StyleMC requires only a few seconds of training per text prompt to find stable global directions, does not require prompt engineering and can be used with any pre-trained StyleGAN2 model. We demonstrate the effectiveness of our method and compare it to state-of-the-art methods. Our code can be found at http://catlab-team.github.io/stylemc.",
      "publishedDate": "2021-12-15T21:37:10Z",
      "updatedDate": "2021-12-15T21:37:10Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2112.08493v1",
      "arxivUrl": "https://arxiv.org/abs/2112.08493",
      "comment": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2022)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2111.08267",
      "title": "Solving Probability and Statistics Problems by Program Synthesis",
      "authors": [
        {
          "name": "Leonard Tang",
          "affiliation": null
        },
        {
          "name": "Elizabeth Ke",
          "affiliation": null
        },
        {
          "name": "Nikhil Singh",
          "affiliation": null
        },
        {
          "name": "Nakul Verma",
          "affiliation": null
        },
        {
          "name": "Iddo Drori",
          "affiliation": null
        }
      ],
      "abstract": "We solve university level probability and statistics questions by program synthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned on code. We transform course problems from MIT's 18.05 Introduction to Probability and Statistics and Harvard's STAT110 Probability into programming tasks. We then execute the generated code to get a solution. Since these course questions are grounded in probability, we often aim to have Codex generate probabilistic programs that simulate a large number of probabilistic dependencies to compute its solution. Our approach requires prompt engineering to transform the question from its original form to an explicit, tractable form that results in a correct program and solution. To estimate the amount of work needed to translate an original question into its tractable form, we measure the similarity between original and transformed questions. Our work is the first to introduce a new dataset of university-level probability and statistics problems and solve these problems in a scalable fashion using the program synthesis capabilities of large language models.",
      "publishedDate": "2021-11-16T07:34:25Z",
      "updatedDate": "2021-11-16T07:34:25Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2111.08267v1",
      "arxivUrl": "https://arxiv.org/abs/2111.08267",
      "comment": "33 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.07178",
      "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models",
      "authors": [
        {
          "name": "Peter West",
          "affiliation": null
        },
        {
          "name": "Chandra Bhagavatula",
          "affiliation": null
        },
        {
          "name": "Jack Hessel",
          "affiliation": null
        },
        {
          "name": "Jena D. Hwang",
          "affiliation": null
        },
        {
          "name": "Liwei Jiang",
          "affiliation": null
        },
        {
          "name": "Ronan Le Bras",
          "affiliation": null
        },
        {
          "name": "Ximing Lu",
          "affiliation": null
        },
        {
          "name": "Sean Welleck",
          "affiliation": null
        },
        {
          "name": "Yejin Choi",
          "affiliation": null
        }
      ],
      "abstract": "The common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.",
      "publishedDate": "2021-10-14T06:50:19Z",
      "updatedDate": "2022-11-28T23:28:35Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.07178v2",
      "arxivUrl": "https://arxiv.org/abs/2110.07178",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.04544",
      "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters",
      "authors": [
        {
          "name": "Peng Gao",
          "affiliation": null
        },
        {
          "name": "Shijie Geng",
          "affiliation": null
        },
        {
          "name": "Renrui Zhang",
          "affiliation": null
        },
        {
          "name": "Teli Ma",
          "affiliation": null
        },
        {
          "name": "Rongyao Fang",
          "affiliation": null
        },
        {
          "name": "Yongfeng Zhang",
          "affiliation": null
        },
        {
          "name": "Hongsheng Li",
          "affiliation": null
        },
        {
          "name": "Yu Qiao",
          "affiliation": null
        }
      ],
      "abstract": "Large-scale contrastive vision-language pre-training has shown significant progress in visual representation learning. Unlike traditional visual systems trained by a fixed set of discrete labels, a new paradigm was introduced in \\cite{radford2021learning} to directly learn to align images with raw texts in an open-vocabulary setting. On downstream tasks, a carefully chosen text prompt is employed to make zero-shot predictions.~To avoid non-trivial prompt engineering, context optimization \\cite{zhou2021coop} has been proposed to learn continuous vectors as task-specific prompts with few-shot training examples.~In this paper, we show that there is an alternative path to achieve better vision-language models other than prompt tuning.~While prompt tuning is for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with feature adapters on either visual or language branch. Specifically, CLIP-Adapter adopts an additional bottleneck layer to learn new features and performs residual-style feature blending with the original pre-trained features.~As a consequence, CLIP-Adapter is able to outperform context optimization while maintains a simple design. Experiments and extensive ablation studies on various visual classification tasks demonstrate the effectiveness of our approach. Code is released at t https://github.com/gaopengcuhk/CLIP-Adapter.",
      "publishedDate": "2021-10-09T11:39:30Z",
      "updatedDate": "2025-03-25T14:34:04Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.04544v2",
      "arxivUrl": "https://arxiv.org/abs/2110.04544",
      "comment": "Accepted by IJCV",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.03111",
      "title": "Cut the CARP: Fishing for zero-shot story evaluation",
      "authors": [
        {
          "name": "Shahbuland Matiana",
          "affiliation": null
        },
        {
          "name": "JR Smith",
          "affiliation": null
        },
        {
          "name": "Ryan Teehan",
          "affiliation": null
        },
        {
          "name": "Louis Castricato",
          "affiliation": null
        },
        {
          "name": "Stella Biderman",
          "affiliation": null
        },
        {
          "name": "Leo Gao",
          "affiliation": null
        },
        {
          "name": "Spencer Frazier",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large-scale language models (Raffel et al., 2019; Brown et al., 2020) have brought significant qualitative and quantitative improvements in machine-driven text generation. Despite this, generation and evaluation of machine-generated narrative text remains a challenging problem. Objective evaluation of computationally-generated stories may be prohibitively expensive, require meticulously annotated datasets, or may not adequately measure the logical coherence of a generated story's narratological structure. Informed by recent advances in contrastive learning (Radford et al., 2021), we present Contrastive Authoring and Reviewing Pairing (CARP): a scalable, efficient method for performing qualitatively superior, zero-shot evaluation of stories. We show a strong correlation between human evaluation of stories and those of CARP. Model outputs more significantly correlate with corresponding human input than those language-model based methods which utilize finetuning or prompt engineering approaches. We also present and analyze the Story-Critique Dataset, a new corpora composed of 1.3 million aligned story-critique pairs derived from over 80,000 stories. We expect this corpus to be of interest to NLP researchers.",
      "publishedDate": "2021-10-06T23:50:46Z",
      "updatedDate": "2021-10-26T16:37:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.03111v3",
      "arxivUrl": "https://arxiv.org/abs/2110.03111",
      "comment": "9 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.08472",
      "title": "ActionCLIP: A New Paradigm for Video Action Recognition",
      "authors": [
        {
          "name": "Mengmeng Wang",
          "affiliation": null
        },
        {
          "name": "Jiazheng Xing",
          "affiliation": null
        },
        {
          "name": "Yong Liu",
          "affiliation": null
        }
      ],
      "abstract": "The canonical approach to video action recognition dictates a neural model to do a classic and standard 1-of-N majority vote task. They are trained to predict a fixed set of predefined categories, limiting their transferable ability on new datasets with unseen concepts. In this paper, we provide a new perspective on action recognition by attaching importance to the semantic information of label texts rather than simply mapping them into numbers. Specifically, we model this task as a video-text matching problem within a multimodal learning framework, which strengthens the video representation with more semantic language supervision and enables our model to do zero-shot action recognition without any further labeled data or parameters requirements. Moreover, to handle the deficiency of label texts and make use of tremendous web data, we propose a new paradigm based on this multimodal learning framework for action recognition, which we dub \"pre-train, prompt and fine-tune\". This paradigm first learns powerful representations from pre-training on a large amount of web image-text or video-text data. Then it makes the action recognition task to act more like pre-training problems via prompt engineering. Finally, it end-to-end fine-tunes on target datasets to obtain strong performance. We give an instantiation of the new paradigm, ActionCLIP, which not only has superior and flexible zero-shot/few-shot transfer ability but also reaches a top performance on general action recognition task, achieving 83.8% top-1 accuracy on Kinetics-400 with a ViT-B/16 as the backbone. Code is available at https://github.com/sallymmx/ActionCLIP.git",
      "publishedDate": "2021-09-17T11:21:34Z",
      "updatedDate": "2021-09-17T11:21:34Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.08472v1",
      "arxivUrl": "https://arxiv.org/abs/2109.08472",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.06977",
      "title": "Design Guidelines for Prompt Engineering Text-to-Image Generative Models",
      "authors": [
        {
          "name": "Vivian Liu",
          "affiliation": null
        },
        {
          "name": "Lydia B. Chilton",
          "affiliation": null
        }
      ],
      "abstract": "Text-to-image generative models are a new and powerful way to generate visual artwork. However, the open-ended nature of text as interaction is double-edged; while users can input anything and have access to an infinite range of generations, they also must engage in brute-force trial and error with the text prompt when the result quality is poor. We conduct a study exploring what prompt keywords and model hyperparameters can help produce coherent outputs. In particular, we study prompts structured to include subject and style keywords and investigate success and failure modes of these prompts. Our evaluation of 5493 generations over the course of five experiments spans 51 abstract and concrete subjects as well as 51 abstract and figurative styles. From this evaluation, we present design guidelines that can help people produce better outcomes from text-to-image generative models.",
      "publishedDate": "2021-09-14T21:31:24Z",
      "updatedDate": "2023-09-28T16:20:46Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.06977v3",
      "arxivUrl": "https://arxiv.org/abs/2109.06977",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.04650",
      "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers",
      "authors": [
        {
          "name": "Boseop Kim",
          "affiliation": null
        },
        {
          "name": "HyoungSeok Kim",
          "affiliation": null
        },
        {
          "name": "Sang-Woo Lee",
          "affiliation": null
        },
        {
          "name": "Gichang Lee",
          "affiliation": null
        },
        {
          "name": "Donghyun Kwak",
          "affiliation": null
        },
        {
          "name": "Dong Hyeon Jeon",
          "affiliation": null
        },
        {
          "name": "Sunghyun Park",
          "affiliation": null
        },
        {
          "name": "Sungju Kim",
          "affiliation": null
        },
        {
          "name": "Seonhoon Kim",
          "affiliation": null
        },
        {
          "name": "Dongpil Seo",
          "affiliation": null
        },
        {
          "name": "Heungsub Lee",
          "affiliation": null
        },
        {
          "name": "Minyoung Jeong",
          "affiliation": null
        },
        {
          "name": "Sungjae Lee",
          "affiliation": null
        },
        {
          "name": "Minsub Kim",
          "affiliation": null
        },
        {
          "name": "Suk Hyun Ko",
          "affiliation": null
        },
        {
          "name": "Seokhun Kim",
          "affiliation": null
        },
        {
          "name": "Taeyong Park",
          "affiliation": null
        },
        {
          "name": "Jinuk Kim",
          "affiliation": null
        },
        {
          "name": "Soyoung Kang",
          "affiliation": null
        },
        {
          "name": "Na-Hyeon Ryu",
          "affiliation": null
        },
        {
          "name": "Kang Min Yoo",
          "affiliation": null
        },
        {
          "name": "Minsuk Chang",
          "affiliation": null
        },
        {
          "name": "Soobin Suh",
          "affiliation": null
        },
        {
          "name": "Sookyo In",
          "affiliation": null
        },
        {
          "name": "Jinseong Park",
          "affiliation": null
        },
        {
          "name": "Kyungduk Kim",
          "affiliation": null
        },
        {
          "name": "Hiun Kim",
          "affiliation": null
        },
        {
          "name": "Jisu Jeong",
          "affiliation": null
        },
        {
          "name": "Yong Goo Yeo",
          "affiliation": null
        },
        {
          "name": "Donghoon Ham",
          "affiliation": null
        },
        {
          "name": "Dongju Park",
          "affiliation": null
        },
        {
          "name": "Min Young Lee",
          "affiliation": null
        },
        {
          "name": "Jaewook Kang",
          "affiliation": null
        },
        {
          "name": "Inho Kang",
          "affiliation": null
        },
        {
          "name": "Jung-Woo Ha",
          "affiliation": null
        },
        {
          "name": "Woomyoung Park",
          "affiliation": null
        },
        {
          "name": "Nako Sung",
          "affiliation": null
        }
      ],
      "abstract": "GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",
      "publishedDate": "2021-09-10T03:32:19Z",
      "updatedDate": "2021-11-28T10:56:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.04650v2",
      "arxivUrl": "https://arxiv.org/abs/2109.04650",
      "comment": "Accepted to EMNLP2021 as a long paper. Fixed some typos",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.02772",
      "title": "An Empirical Study on Few-shot Knowledge Probing for Pretrained Language Models",
      "authors": [
        {
          "name": "Tianxing He",
          "affiliation": null
        },
        {
          "name": "Kyunghyun Cho",
          "affiliation": null
        },
        {
          "name": "James Glass",
          "affiliation": null
        }
      ],
      "abstract": "Prompt-based knowledge probing for 1-hop relations has been used to measure how much world knowledge is stored in pretrained language models. Existing work uses considerable amounts of data to tune the prompts for better performance. In this work, we compare a variety of approaches under a few-shot knowledge probing setting, where only a small number (e.g., 10 or 20) of example triples are available. In addition, we create a new dataset named TREx-2p, which contains 2-hop relations. We report that few-shot examples can strongly boost the probing performance for both 1-hop and 2-hop relations. In particular, we find that a simple-yet-effective approach of finetuning the bias vectors in the model outperforms existing prompt-engineering methods. Our dataset and code are available at \\url{https://github.com/cloudygoose/fewshot_lama}.",
      "publishedDate": "2021-09-06T23:29:36Z",
      "updatedDate": "2021-09-11T22:31:32Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.02772v2",
      "arxivUrl": "https://arxiv.org/abs/2109.02772",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.01134",
      "title": "Learning to Prompt for Vision-Language Models",
      "authors": [
        {
          "name": "Kaiyang Zhou",
          "affiliation": null
        },
        {
          "name": "Jingkang Yang",
          "affiliation": null
        },
        {
          "name": "Chen Change Loy",
          "affiliation": null
        },
        {
          "name": "Ziwei Liu",
          "affiliation": null
        }
      ],
      "abstract": "Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming -- one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt's context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.",
      "publishedDate": "2021-09-02T17:57:31Z",
      "updatedDate": "2022-10-06T11:36:09Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.01134v6",
      "arxivUrl": "https://arxiv.org/abs/2109.01134",
      "comment": "International Journal of Computer Vision (IJCV), 2022. Update: Adds results on the DOSCO (DOmain Shift in COntext) benchmark",
      "journalRef": null,
      "doi": "10.1007/s11263-022-01653-1",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.13161",
      "title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners",
      "authors": [
        {
          "name": "Ningyu Zhang",
          "affiliation": null
        },
        {
          "name": "Luoqiu Li",
          "affiliation": null
        },
        {
          "name": "Xiang Chen",
          "affiliation": null
        },
        {
          "name": "Shumin Deng",
          "affiliation": null
        },
        {
          "name": "Zhen Bi",
          "affiliation": null
        },
        {
          "name": "Chuanqi Tan",
          "affiliation": null
        },
        {
          "name": "Fei Huang",
          "affiliation": null
        },
        {
          "name": "Huajun Chen",
          "affiliation": null
        }
      ],
      "abstract": "Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners without any prompt engineering. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance. Code is available in https://github.com/zjunlp/DART.",
      "publishedDate": "2021-08-30T12:29:25Z",
      "updatedDate": "2022-05-04T22:41:50Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.13161v7",
      "arxivUrl": "https://arxiv.org/abs/2108.13161",
      "comment": "Accepted by ICLR 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2106.13353",
      "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models",
      "authors": [
        {
          "name": "Robert L. Logan",
          "affiliation": null
        },
        {
          "name": "Ivana Balažević",
          "affiliation": null
        },
        {
          "name": "Eric Wallace",
          "affiliation": null
        },
        {
          "name": "Fabio Petroni",
          "affiliation": null
        },
        {
          "name": "Sameer Singh",
          "affiliation": null
        },
        {
          "name": "Sebastian Riedel",
          "affiliation": null
        }
      ],
      "abstract": "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.",
      "publishedDate": "2021-06-24T23:38:10Z",
      "updatedDate": "2021-07-01T06:30:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2106.13353v2",
      "arxivUrl": "https://arxiv.org/abs/2106.13353",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2112.15594",
      "title": "A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level",
      "authors": [
        {
          "name": "Iddo Drori",
          "affiliation": null
        },
        {
          "name": "Sarah Zhang",
          "affiliation": null
        },
        {
          "name": "Reece Shuttleworth",
          "affiliation": null
        },
        {
          "name": "Leonard Tang",
          "affiliation": null
        },
        {
          "name": "Albert Lu",
          "affiliation": null
        },
        {
          "name": "Elizabeth Ke",
          "affiliation": null
        },
        {
          "name": "Kevin Liu",
          "affiliation": null
        },
        {
          "name": "Linda Chen",
          "affiliation": null
        },
        {
          "name": "Sunny Tran",
          "affiliation": null
        },
        {
          "name": "Newman Cheng",
          "affiliation": null
        },
        {
          "name": "Roman Wang",
          "affiliation": null
        },
        {
          "name": "Nikhil Singh",
          "affiliation": null
        },
        {
          "name": "Taylor L. Patti",
          "affiliation": null
        },
        {
          "name": "Jayson Lynch",
          "affiliation": null
        },
        {
          "name": "Avi Shporer",
          "affiliation": null
        },
        {
          "name": "Nakul Verma",
          "affiliation": null
        },
        {
          "name": "Eugene Wu",
          "affiliation": null
        },
        {
          "name": "Gilbert Strang",
          "affiliation": null
        }
      ],
      "abstract": "We demonstrate that a neural network pre-trained on text and fine-tuned on code solves mathematics course problems, explains solutions, and generates new questions at a human level. We automatically synthesize programs using few-shot learning and OpenAI's Codex transformer and execute them to solve course problems at 81% automatic accuracy. We curate a new dataset of questions from MIT's largest mathematics courses (Single Variable and Multivariable Calculus, Differential Equations, Introduction to Probability and Statistics, Linear Algebra, and Mathematics for Computer Science) and Columbia University's Computational Linear Algebra. We solve questions from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Intermediate Algebra, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems designed to assess mathematical reasoning. We randomly sample questions and generate solutions with multiple modalities, including numbers, equations, and plots. The latest GPT-3 language model pre-trained on text automatically solves only 18.8% of these university questions using zero-shot learning and 30.8% using few-shot learning and the most recent chain of thought prompting. In contrast, program synthesis with few-shot learning using Codex fine-tuned on code generates programs that automatically solve 81% of these questions. Our approach improves the previous state-of-the-art automatic solution accuracy on the benchmark topics from 8.8% to 81.1%. We perform a survey to evaluate the quality and difficulty of generated questions. This work is the first to automatically solve university-level mathematics course questions at a human level and the first work to explain and generate university-level mathematics course questions at scale, a milestone for higher education.",
      "publishedDate": "2021-12-31T18:57:31Z",
      "updatedDate": "2022-05-30T20:21:46Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2112.15594v4",
      "arxivUrl": "https://arxiv.org/abs/2112.15594",
      "comment": "181 pages, 8 figures, 280 tables",
      "journalRef": null,
      "doi": "10.1073/pnas.2123433119",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2112.10668",
      "title": "Few-shot Learning with Multilingual Language Models",
      "authors": [
        {
          "name": "Xi Victoria Lin",
          "affiliation": null
        },
        {
          "name": "Todor Mihaylov",
          "affiliation": null
        },
        {
          "name": "Mikel Artetxe",
          "affiliation": null
        },
        {
          "name": "Tianlu Wang",
          "affiliation": null
        },
        {
          "name": "Shuohui Chen",
          "affiliation": null
        },
        {
          "name": "Daniel Simig",
          "affiliation": null
        },
        {
          "name": "Myle Ott",
          "affiliation": null
        },
        {
          "name": "Naman Goyal",
          "affiliation": null
        },
        {
          "name": "Shruti Bhosale",
          "affiliation": null
        },
        {
          "name": "Jingfei Du",
          "affiliation": null
        },
        {
          "name": "Ramakanth Pasunuru",
          "affiliation": null
        },
        {
          "name": "Sam Shleifer",
          "affiliation": null
        },
        {
          "name": "Punit Singh Koura",
          "affiliation": null
        },
        {
          "name": "Vishrav Chaudhary",
          "affiliation": null
        },
        {
          "name": "Brian O'Horo",
          "affiliation": null
        },
        {
          "name": "Jeff Wang",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "Zornitsa Kozareva",
          "affiliation": null
        },
        {
          "name": "Mona Diab",
          "affiliation": null
        },
        {
          "name": "Veselin Stoyanov",
          "affiliation": null
        },
        {
          "name": "Xian Li",
          "affiliation": null
        }
      ],
      "abstract": "Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.",
      "publishedDate": "2021-12-20T16:52:35Z",
      "updatedDate": "2022-11-10T07:01:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2112.10668v3",
      "arxivUrl": "https://arxiv.org/abs/2112.10668",
      "comment": "Accepted to EMNLP 2022; 34 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2112.08633",
      "title": "Learning To Retrieve Prompts for In-Context Learning",
      "authors": [
        {
          "name": "Ohad Rubin",
          "affiliation": null
        },
        {
          "name": "Jonathan Herzig",
          "affiliation": null
        },
        {
          "name": "Jonathan Berant",
          "affiliation": null
        }
      ],
      "abstract": "In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompt). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and a LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.",
      "publishedDate": "2021-12-16T05:17:56Z",
      "updatedDate": "2022-05-08T05:17:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2112.08633v2",
      "arxivUrl": "https://arxiv.org/abs/2112.08633",
      "comment": "NAACL-HLT 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2112.11909",
      "title": "Few-shot Multi-hop Question Answering over Knowledge Base",
      "authors": [
        {
          "name": "Meihao Fan",
          "affiliation": null
        },
        {
          "name": "Lei Zhang",
          "affiliation": null
        },
        {
          "name": "Siyao Xiao",
          "affiliation": null
        },
        {
          "name": "Yuru Liang",
          "affiliation": null
        }
      ],
      "abstract": "KBQA is a task that requires to answer questions by using semantic structured information in knowledge base. Previous work in this area has been restricted due to the lack of large semantic parsing dataset and the exponential growth of searching space with the increasing hops of relation paths. In this paper, we propose an efficient pipeline method equipped with a pre-trained language model. By adopting Beam Search algorithm, the searching space will not be restricted in subgraph of 3 hops. Besides, we propose a data generation strategy, which enables our model to generalize well from few training samples. We evaluate our model on an open-domain complex Chinese Question Answering task CCKS2019 and achieve F1-score of 62.55% on the test dataset. In addition, in order to test the few-shot learning capability of our model, we ramdomly select 10% of the primary data to train our model, the result shows that our model can still achieves F1-score of 58.54%, which verifies the capability of our model to process KBQA task and the advantage in few-shot Learning.",
      "publishedDate": "2021-12-14T00:56:54Z",
      "updatedDate": "2022-01-27T04:04:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2112.11909v2",
      "arxivUrl": "https://arxiv.org/abs/2112.11909",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2112.06905",
      "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
      "authors": [
        {
          "name": "Nan Du",
          "affiliation": null
        },
        {
          "name": "Yanping Huang",
          "affiliation": null
        },
        {
          "name": "Andrew M. Dai",
          "affiliation": null
        },
        {
          "name": "Simon Tong",
          "affiliation": null
        },
        {
          "name": "Dmitry Lepikhin",
          "affiliation": null
        },
        {
          "name": "Yuanzhong Xu",
          "affiliation": null
        },
        {
          "name": "Maxim Krikun",
          "affiliation": null
        },
        {
          "name": "Yanqi Zhou",
          "affiliation": null
        },
        {
          "name": "Adams Wei Yu",
          "affiliation": null
        },
        {
          "name": "Orhan Firat",
          "affiliation": null
        },
        {
          "name": "Barret Zoph",
          "affiliation": null
        },
        {
          "name": "Liam Fedus",
          "affiliation": null
        },
        {
          "name": "Maarten Bosma",
          "affiliation": null
        },
        {
          "name": "Zongwei Zhou",
          "affiliation": null
        },
        {
          "name": "Tao Wang",
          "affiliation": null
        },
        {
          "name": "Yu Emma Wang",
          "affiliation": null
        },
        {
          "name": "Kellie Webster",
          "affiliation": null
        },
        {
          "name": "Marie Pellat",
          "affiliation": null
        },
        {
          "name": "Kevin Robinson",
          "affiliation": null
        },
        {
          "name": "Kathleen Meier-Hellstern",
          "affiliation": null
        },
        {
          "name": "Toju Duke",
          "affiliation": null
        },
        {
          "name": "Lucas Dixon",
          "affiliation": null
        },
        {
          "name": "Kun Zhang",
          "affiliation": null
        },
        {
          "name": "Quoc V Le",
          "affiliation": null
        },
        {
          "name": "Yonghui Wu",
          "affiliation": null
        },
        {
          "name": "Zhifeng Chen",
          "affiliation": null
        },
        {
          "name": "Claire Cui",
          "affiliation": null
        }
      ],
      "abstract": "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",
      "publishedDate": "2021-12-13T18:58:19Z",
      "updatedDate": "2022-08-01T21:07:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2112.06905v2",
      "arxivUrl": "https://arxiv.org/abs/2112.06905",
      "comment": "Accepted to ICML 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2112.05253",
      "title": "MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning",
      "authors": [
        {
          "name": "Constantin Eichenberg",
          "affiliation": null
        },
        {
          "name": "Sidney Black",
          "affiliation": null
        },
        {
          "name": "Samuel Weinbach",
          "affiliation": null
        },
        {
          "name": "Letitia Parcalabescu",
          "affiliation": null
        },
        {
          "name": "Anette Frank",
          "affiliation": null
        }
      ],
      "abstract": "Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language models with additional modalities using adapter-based finetuning. Building on Frozen, we train a series of VL models that autoregressively generate text from arbitrary combinations of visual and textual input. The pretraining is entirely end-to-end using a single language modeling objective, simplifying optimization compared to previous approaches. Importantly, the language model weights remain unchanged during training, allowing for transfer of encyclopedic knowledge and in-context learning abilities from language pretraining. MAGMA outperforms Frozen on open-ended generative tasks, achieving state of the art results on the OKVQA benchmark and competitive results on a range of other popular VL benchmarks, while pretraining on 0.2% of the number of samples used to train SimVLM.",
      "publishedDate": "2021-12-09T23:58:45Z",
      "updatedDate": "2022-10-24T21:35:42Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2112.05253v2",
      "arxivUrl": "https://arxiv.org/abs/2112.05253",
      "comment": "13 pages, 6 figures, 2 tables. Minor improvements. Accepted at EMNLP 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2111.12421",
      "title": "Few-shot Named Entity Recognition with Cloze Questions",
      "authors": [
        {
          "name": "Valerio La Gatta",
          "affiliation": null
        },
        {
          "name": "Vincenzo Moscato",
          "affiliation": null
        },
        {
          "name": "Marco Postiglione",
          "affiliation": null
        },
        {
          "name": "Giancarlo Sperlì",
          "affiliation": null
        }
      ],
      "abstract": "Despite the huge and continuous advances in computational linguistics, the lack of annotated data for Named Entity Recognition (NER) is still a challenging issue, especially in low-resource languages and when domain knowledge is required for high-quality annotations. Recent findings in NLP show the effectiveness of cloze-style questions in enabling language models to leverage the knowledge they acquired during the pre-training phase. In our work, we propose a simple and intuitive adaptation of Pattern-Exploiting Training (PET), a recent approach which combines the cloze-questions mechanism and fine-tuning for few-shot learning: the key idea is to rephrase the NER task with patterns. Our approach achieves considerably better performance than standard fine-tuning and comparable or improved results with respect to other few-shot baselines without relying on manually annotated data or distant supervision on three benchmark datasets: NCBI-disease, BC2GM and a private Italian biomedical corpus.",
      "publishedDate": "2021-11-24T11:08:59Z",
      "updatedDate": "2021-11-24T11:08:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2111.12421v1",
      "arxivUrl": "https://arxiv.org/abs/2111.12421",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2111.02080",
      "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
      "authors": [
        {
          "name": "Sang Michael Xie",
          "affiliation": null
        },
        {
          "name": "Aditi Raghunathan",
          "affiliation": null
        },
        {
          "name": "Percy Liang",
          "affiliation": null
        },
        {
          "name": "Tengyu Ma",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",
      "publishedDate": "2021-11-03T09:12:33Z",
      "updatedDate": "2022-07-21T07:44:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2111.02080v6",
      "arxivUrl": "https://arxiv.org/abs/2111.02080",
      "comment": "ICLR 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.15943",
      "title": "MetaICL: Learning to Learn In Context",
      "authors": [
        {
          "name": "Sewon Min",
          "affiliation": null
        },
        {
          "name": "Mike Lewis",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "Hannaneh Hajishirzi",
          "affiliation": null
        }
      ],
      "abstract": "We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task, and outperforms much bigger models with nearly 8x parameters. Finally, we show that MetaICL is complementary to human-written instructions, and the best performance can be achieved by combining both approaches.",
      "publishedDate": "2021-10-29T17:42:08Z",
      "updatedDate": "2022-05-03T10:36:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.15943v2",
      "arxivUrl": "https://arxiv.org/abs/2110.15943",
      "comment": "19 pages, 2 figures. Published as a conference paper at NAACL 2022 (long). Code available at https://github.com/facebookresearch/MetaICL",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.08551",
      "title": "HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression",
      "authors": [
        {
          "name": "Chenhe Dong",
          "affiliation": null
        },
        {
          "name": "Yaliang Li",
          "affiliation": null
        },
        {
          "name": "Ying Shen",
          "affiliation": null
        },
        {
          "name": "Minghui Qiu",
          "affiliation": null
        }
      ],
      "abstract": "On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at https://github.com/cheneydon/hrkd.",
      "publishedDate": "2021-10-16T11:23:02Z",
      "updatedDate": "2021-10-16T11:23:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.08551v1",
      "arxivUrl": "https://arxiv.org/abs/2110.08551",
      "comment": "EMNLP 2021",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.08118",
      "title": "Few-Shot Bot: Prompt-Based Learning for Dialogue Systems",
      "authors": [
        {
          "name": "Andrea Madotto",
          "affiliation": null
        },
        {
          "name": "Zhaojiang Lin",
          "affiliation": null
        },
        {
          "name": "Genta Indra Winata",
          "affiliation": null
        },
        {
          "name": "Pascale Fung",
          "affiliation": null
        }
      ],
      "abstract": "Learning to converse using only a few examples is a great challenge in conversational AI. The current best conversational models, which are either good chit-chatters (e.g., BlenderBot) or goal-oriented systems (e.g., MinTL), are language models (LMs) fine-tuned on large conversational datasets. Training these models is expensive, both in terms of computational resources and time, and it is hard to keep them up to date with new conversational skills. A simple yet unexplored solution is prompt-based few-shot learning (Brown et al. 2020) which does not require gradient-based fine-tuning but instead uses a few examples in the LM context as the only source of learning. In this paper, we explore prompt-based few-shot learning in dialogue tasks. We benchmark LMs of different sizes in nine response generation tasks, which include four knowledge-grounded tasks, a task-oriented generations task, three open-chat tasks, and controlled stylistic generation, and five conversational parsing tasks, which include dialogue state tracking, graph path generation, persona information extraction, document retrieval, and internet query generation. The current largest released LM (GPT-J-6B) using prompt-based few-shot learning, and thus requiring no training, achieves competitive performance to fully trained state-of-the-art models. Moreover, we propose a novel prompt-based few-shot classifier, that also does not require any fine-tuning, to select the most appropriate prompt given a dialogue history. Finally, by combining the power of prompt-based few-shot learning and a Skill Selector, we create an end-to-end chatbot named the Few-Shot Bot (FSB), which automatically selects the most appropriate conversational skill, queries different knowledge bases or the internet, and uses the retrieved knowledge to generate a human-like response, all using only few dialogue examples per skill.",
      "publishedDate": "2021-10-15T14:36:45Z",
      "updatedDate": "2021-10-15T14:36:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.08118v1",
      "arxivUrl": "https://arxiv.org/abs/2110.08118",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.07814",
      "title": "Meta-learning via Language Model In-context Tuning",
      "authors": [
        {
          "name": "Yanda Chen",
          "affiliation": null
        },
        {
          "name": "Ruiqi Zhong",
          "affiliation": null
        },
        {
          "name": "Sheng Zha",
          "affiliation": null
        },
        {
          "name": "George Karypis",
          "affiliation": null
        },
        {
          "name": "He He",
          "affiliation": null
        }
      ],
      "abstract": "The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. To tackle this problem in NLP, we propose $\\textit{in-context tuning}$, which recasts adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, the labeled examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label from the input sequences on a collection of tasks. We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to first-order MAML which adapts the model with gradient descent, our method better leverages the inductive bias of LMs to perform pattern matching, and outperforms MAML by an absolute $6\\%$ AUC ROC score on BinaryClfs, with increasing advantage w.r.t. model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning directly learns to learn from in-context examples. On BinaryClfs, in-context tuning improves the average AUC-ROC score by an absolute $10\\%$, and reduces the variance with respect to example ordering by 6x and example choices by 2x.",
      "publishedDate": "2021-10-15T02:29:09Z",
      "updatedDate": "2022-04-12T04:00:50Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.07814v2",
      "arxivUrl": "https://arxiv.org/abs/2110.07814",
      "comment": "ACL 2022 camera-ready version",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.06274",
      "title": "LiST: Lite Prompted Self-training Makes Parameter-Efficient Few-shot Learners",
      "authors": [
        {
          "name": "Yaqing Wang",
          "affiliation": null
        },
        {
          "name": "Subhabrata Mukherjee",
          "affiliation": null
        },
        {
          "name": "Xiaodong Liu",
          "affiliation": null
        },
        {
          "name": "Jing Gao",
          "affiliation": null
        },
        {
          "name": "Ahmed Hassan Awadallah",
          "affiliation": null
        },
        {
          "name": "Jianfeng Gao",
          "affiliation": null
        }
      ],
      "abstract": "We present a new method LiST is short for Lite Prompted Self-Training for parameter-efficient fine-tuning of large pre-trained language models (PLMs) for few-shot learning. LiST improves over recent methods that adopt prompt-based fine-tuning (FN) using two key techniques. The first is the use of self-training to leverage large amounts of unlabeled data for prompt-based FN in few-shot settings. We use self-training in conjunction with meta-learning for re-weighting noisy pseudo-prompt labels. Self-training is expensive as it requires updating all the model parameters repetitively. Therefore, we use a second technique for light-weight fine-tuning where we introduce a small number of task-specific parameters that are fine-tuned during self-training while keeping the PLM encoder frozen. Our experiments show that LiST can effectively leverage unlabeled data to improve the model performance for few-shot learning. Additionally, the fine-tuning is efficient as it only updates a small percentage of parameters and the overall model footprint is reduced since several tasks can share a common PLM encoder as backbone. A comprehensive study on six NLU tasks demonstrate LiST to improve by 35% over classic fine-tuning and 6% over prompt-based FN with 96% reduction in number of trainable parameters when fine-tuned with no more than 30 labeled examples from each task. With only 14M tunable parameters, LiST outperforms GPT-3 in-context learning by 33% on few-shot NLU tasks.",
      "publishedDate": "2021-10-12T18:47:18Z",
      "updatedDate": "2022-05-18T19:01:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.06274v2",
      "arxivUrl": "https://arxiv.org/abs/2110.06274",
      "comment": "Accepted by NAACL findings. Code is https://github.com/microsoft/LiST",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.04725",
      "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning",
      "authors": [
        {
          "name": "Shaohua Wu",
          "affiliation": null
        },
        {
          "name": "Xudong Zhao",
          "affiliation": null
        },
        {
          "name": "Tong Yu",
          "affiliation": null
        },
        {
          "name": "Rongguo Zhang",
          "affiliation": null
        },
        {
          "name": "Chong Shen",
          "affiliation": null
        },
        {
          "name": "Hongli Liu",
          "affiliation": null
        },
        {
          "name": "Feng Li",
          "affiliation": null
        },
        {
          "name": "Hong Zhu",
          "affiliation": null
        },
        {
          "name": "Jiangang Luo",
          "affiliation": null
        },
        {
          "name": "Liang Xu",
          "affiliation": null
        },
        {
          "name": "Xuanwei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",
      "publishedDate": "2021-10-10T07:40:22Z",
      "updatedDate": "2021-10-12T02:25:35Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.04725v2",
      "arxivUrl": "https://arxiv.org/abs/2110.04725",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.04541",
      "title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design",
      "authors": [
        {
          "name": "Yoav Levine",
          "affiliation": null
        },
        {
          "name": "Noam Wies",
          "affiliation": null
        },
        {
          "name": "Daniel Jannai",
          "affiliation": null
        },
        {
          "name": "Dan Navon",
          "affiliation": null
        },
        {
          "name": "Yedid Hoshen",
          "affiliation": null
        },
        {
          "name": "Amnon Shashua",
          "affiliation": null
        }
      ],
      "abstract": "Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose \"kNN-Pretraining\": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities. This theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations.",
      "publishedDate": "2021-10-09T11:05:16Z",
      "updatedDate": "2022-03-21T17:57:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.04541v3",
      "arxivUrl": "https://arxiv.org/abs/2110.04541",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.01256",
      "title": "Revisiting Self-Training for Few-Shot Learning of Language Model",
      "authors": [
        {
          "name": "Yiming Chen",
          "affiliation": null
        },
        {
          "name": "Yan Zhang",
          "affiliation": null
        },
        {
          "name": "Chen Zhang",
          "affiliation": null
        },
        {
          "name": "Grandee Lee",
          "affiliation": null
        },
        {
          "name": "Ran Cheng",
          "affiliation": null
        },
        {
          "name": "Haizhou Li",
          "affiliation": null
        }
      ],
      "abstract": "As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model. The question is how to effectively make use of such data. In this work, we revisit the self-training technique for language model fine-tuning and present a state-of-the-art prompt-based few-shot learner, SFLM. Given two views of a text sample via weak and strong augmentation techniques, SFLM generates a pseudo label on the weakly augmented version. Then, the model predicts the same pseudo label when fine-tuned with the strongly augmented version. This simple approach is shown to outperform other state-of-the-art supervised and semi-supervised counterparts on six sentence classification and six sentence-pair classification benchmarking tasks. In addition, SFLM only relies on a few in-domain unlabeled data. We conduct a comprehensive analysis to demonstrate the robustness of our proposed approach under various settings, including augmentation techniques, model scale, and few-shot knowledge transfer across tasks.",
      "publishedDate": "2021-10-04T08:51:36Z",
      "updatedDate": "2021-10-04T08:51:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.01256v1",
      "arxivUrl": "https://arxiv.org/abs/2110.01256",
      "comment": "Accepted to EMNLP 2021",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.14076",
      "title": "RAFT: A Real-World Few-Shot Text Classification Benchmark",
      "authors": [
        {
          "name": "Neel Alex",
          "affiliation": null
        },
        {
          "name": "Eli Lifland",
          "affiliation": null
        },
        {
          "name": "Lewis Tunstall",
          "affiliation": null
        },
        {
          "name": "Abhishek Thakur",
          "affiliation": null
        },
        {
          "name": "Pegah Maham",
          "affiliation": null
        },
        {
          "name": "C. Jess Riedel",
          "affiliation": null
        },
        {
          "name": "Emmie Hine",
          "affiliation": null
        },
        {
          "name": "Carolyn Ashurst",
          "affiliation": null
        },
        {
          "name": "Paul Sedille",
          "affiliation": null
        },
        {
          "name": "Alexis Carlier",
          "affiliation": null
        },
        {
          "name": "Michael Noetel",
          "affiliation": null
        },
        {
          "name": "Andreas Stuhlmüller",
          "affiliation": null
        }
      ],
      "abstract": "Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific examples. Will models soon solve classification tasks that have so far been reserved for human research assistants? Existing benchmarks are not designed to measure progress in applied settings, and so don't directly answer this question. The RAFT benchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring tasks and uses an evaluation setup that mirrors deployment. Baseline evaluations on RAFT reveal areas current techniques struggle with: reasoning over long texts and tasks with many classes. Human baselines show that some classification tasks are difficult for non-expert humans, reflecting that real-world value sometimes depends on domain expertise. Yet even non-expert human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets and leaderboard will track which model improvements translate into real-world benefits at https://raft.elicit.org .",
      "publishedDate": "2021-09-28T22:35:31Z",
      "updatedDate": "2022-01-18T21:40:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.14076v3",
      "arxivUrl": "https://arxiv.org/abs/2109.14076",
      "comment": "Dataset, submission instructions, code and leaderboard available at https://raft.elicit.org",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.06513",
      "title": "Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation",
      "authors": [
        {
          "name": "Chujie Zheng",
          "affiliation": null
        },
        {
          "name": "Minlie Huang",
          "affiliation": null
        }
      ],
      "abstract": "Dialog models can be greatly strengthened through grounding on various external information, but grounded dialog corpora are usually not naturally accessible. In this work, we focus on the few-shot learning for grounded dialog generation (GDG). We first propose a simple prompting method for GDG tasks, where different constructs of model input, such as the grounding source and the conversation context, are distinguished through continuous or discrete prompts. On three typical GDG tasks, we empirically demonstrate and analyze in-depth the effectiveness of our method. We then conduct extensive experiments to thoroughly investigate how our prompting method works with different pre-trained models. We show that prompted language models perform superiorly to conversational models, and further analyze various factors that influence the effects of prompting. Overall, our work introduces a prompt-based perspective to the few-shot learning for GDG tasks, and provides valuable findings and insights for future research.",
      "publishedDate": "2021-09-14T08:17:57Z",
      "updatedDate": "2022-01-14T13:05:18Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.06513v2",
      "arxivUrl": "https://arxiv.org/abs/2109.06513",
      "comment": "Work in progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.06270",
      "title": "STraTA: Self-Training with Task Augmentation for Better Few-shot Learning",
      "authors": [
        {
          "name": "Tu Vu",
          "affiliation": null
        },
        {
          "name": "Minh-Thang Luong",
          "affiliation": null
        },
        {
          "name": "Quoc V. Le",
          "affiliation": null
        },
        {
          "name": "Grady Simon",
          "affiliation": null
        },
        {
          "name": "Mohit Iyyer",
          "affiliation": null
        }
      ],
      "abstract": "Despite their recent successes in tackling many NLP tasks, large-scale pre-trained language models do not perform as well in few-shot settings where only a handful of training examples are available. To address this shortcoming, we propose STraTA, which stands for Self-Training with Task Augmentation, an approach that builds on two key ideas for effective leverage of unlabeled data. First, STraTA uses task augmentation, a novel technique that synthesizes a large amount of data for auxiliary-task fine-tuning from target-task unlabeled texts. Second, STraTA performs self-training by further fine-tuning the strong base model created by task augmentation on a broad distribution of pseudo-labeled data. Our experiments demonstrate that STraTA can substantially improve sample efficiency across 12 few-shot benchmarks. Remarkably, on the SST-2 sentiment dataset, STraTA, with only 8 training examples per class, achieves comparable results to standard fine-tuning with 67K training examples. Our analyses reveal that task augmentation and self-training are both complementary and independently effective.",
      "publishedDate": "2021-09-13T19:14:01Z",
      "updatedDate": "2022-04-12T16:44:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.06270v2",
      "arxivUrl": "https://arxiv.org/abs/2109.06270",
      "comment": "Accepted as a main conference paper at EMNLP 2021, 17 pages, 3 figures, 11 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.05357",
      "title": "Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework",
      "authors": [
        {
          "name": "Yaqing Wang",
          "affiliation": null
        },
        {
          "name": "Haoda Chu",
          "affiliation": null
        },
        {
          "name": "Chao Zhang",
          "affiliation": null
        },
        {
          "name": "Jing Gao",
          "affiliation": null
        }
      ],
      "abstract": "In this work, we study the problem of named entity recognition (NER) in a low resource scenario, focusing on few-shot and zero-shot settings. Built upon large-scale pre-trained language models, we propose a novel NER framework, namely SpanNER, which learns from natural language supervision and enables the identification of never-seen entity classes without using in-domain labeled data. We perform extensive experiments on 5 benchmark datasets and evaluate the proposed method in the few-shot learning, domain transfer and zero-shot learning settings. The experimental results show that the proposed method can bring 10%, 23% and 26% improvements in average over the best baselines in few-shot learning, domain transfer and zero-shot learning settings respectively.",
      "publishedDate": "2021-09-11T19:52:09Z",
      "updatedDate": "2021-09-11T19:52:09Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.05357v1",
      "arxivUrl": "https://arxiv.org/abs/2109.05357",
      "comment": "EMNLP 2021 Findings",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.04645",
      "title": "CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems",
      "authors": [
        {
          "name": "Fei Mi",
          "affiliation": null
        },
        {
          "name": "Yitong Li",
          "affiliation": null
        },
        {
          "name": "Yasheng Wang",
          "affiliation": null
        },
        {
          "name": "Xin Jiang",
          "affiliation": null
        },
        {
          "name": "Qun Liu",
          "affiliation": null
        }
      ],
      "abstract": "As labeling cost for different modules in task-oriented dialog (ToD) systems is high, a major challenge in practice is to learn different tasks with the least amount of labeled data. Recently, prompting methods over pre-trained language models (PLMs) have shown promising results for few-shot learning in ToD. To better utilize the power of PLMs, this paper proposes Comprehensive Instruction (CINS) that exploits PLMs with extra task-specific instructions. We design a schema (definition, constraint, prompt) of instructions and their customized realizations for three important downstream tasks in ToD, i.e. intent classification, dialog state tracking, and natural language generation. A sequence-to-sequence model (T5) is adopted to solve these three tasks in a unified framework. Extensive experiments are conducted on these ToD tasks in realistic few-shot learning scenarios with small validation data. Empirical results demonstrate that the proposed CINS approach consistently improves techniques that finetune PLMs with raw input or short prompts.",
      "publishedDate": "2021-09-10T03:23:06Z",
      "updatedDate": "2022-03-21T14:24:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.04645v4",
      "arxivUrl": "https://arxiv.org/abs/2109.04645",
      "comment": "Accepted at AAAI2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.04332",
      "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning",
      "authors": [
        {
          "name": "Yuxian Gu",
          "affiliation": null
        },
        {
          "name": "Xu Han",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Liu",
          "affiliation": null
        },
        {
          "name": "Minlie Huang",
          "affiliation": null
        }
      ],
      "abstract": "Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model fine-tuning when downstream data are sufficient, whereas it performs much worse under few-shot learning settings, which may hinder the application of prompt tuning in practice. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.",
      "publishedDate": "2021-09-09T15:11:04Z",
      "updatedDate": "2022-03-14T02:05:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.04332v3",
      "arxivUrl": "https://arxiv.org/abs/2109.04332",
      "comment": "Accepted by ACL2022 (main conference)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.03630",
      "title": "Discrete and Soft Prompting for Multilingual Models",
      "authors": [
        {
          "name": "Mengjie Zhao",
          "affiliation": null
        },
        {
          "name": "Hinrich Schütze",
          "affiliation": null
        }
      ],
      "abstract": "It has been shown for English that discrete and soft prompting perform strongly in few-shot learning with pretrained language models (PLMs). In this paper, we show that discrete and soft prompting perform better than finetuning in multilingual cases: Crosslingual transfer and in-language training of multilingual natural language inference. For example, with 48 English training examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely surpassing the majority baseline (33.33%). In contrast, discrete and soft prompting outperform finetuning, achieving 36.43% and 38.79%. We also demonstrate good performance of prompting with training data in multiple languages other than English.",
      "publishedDate": "2021-09-08T13:22:59Z",
      "updatedDate": "2021-09-08T13:22:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.03630v1",
      "arxivUrl": "https://arxiv.org/abs/2109.03630",
      "comment": "EMNLP 2021",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.02555",
      "title": "GPT-3 Models are Poor Few-Shot Learners in the Biomedical Domain",
      "authors": [
        {
          "name": "Milad Moradi",
          "affiliation": null
        },
        {
          "name": "Kathrin Blagec",
          "affiliation": null
        },
        {
          "name": "Florian Haberl",
          "affiliation": null
        },
        {
          "name": "Matthias Samwald",
          "affiliation": null
        }
      ],
      "abstract": "Deep neural language models have set new breakthroughs in many tasks of Natural Language Processing (NLP). Recent work has shown that deep transformer language models (pretrained on large amounts of texts) can achieve high levels of task-specific few-shot performance comparable to state-of-the-art models. However, the ability of these large language models in few-shot transfer learning has not yet been explored in the biomedical domain. We investigated the performance of two powerful transformer language models, i.e. GPT-3 and BioBERT, in few-shot settings on various biomedical NLP tasks. The experimental results showed that, to a great extent, both the models underperform a language model fine-tuned on the full training data. Although GPT-3 had already achieved near state-of-the-art results in few-shot knowledge transfer on open-domain NLP tasks, it could not perform as effectively as BioBERT, which is orders of magnitude smaller than GPT-3. Regarding that BioBERT was already pretrained on large biomedical text corpora, our study suggests that language models may largely benefit from in-domain pretraining in task-specific few-shot learning. However, in-domain pretraining seems not to be sufficient; novel pretraining and few-shot learning strategies are required in the biomedical NLP domain.",
      "publishedDate": "2021-09-06T15:50:37Z",
      "updatedDate": "2022-06-01T13:30:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.02555v2",
      "arxivUrl": "https://arxiv.org/abs/2109.02555",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.00729",
      "title": "ConQX: Semantic Expansion of Spoken Queries for Intent Detection based on Conditioned Text Generation",
      "authors": [
        {
          "name": "Eyup Halit Yilmaz",
          "affiliation": null
        },
        {
          "name": "Cagri Toraman",
          "affiliation": null
        }
      ],
      "abstract": "Intent detection of spoken queries is a challenging task due to their noisy structure and short length. To provide additional information regarding the query and enhance the performance of intent detection, we propose a method for semantic expansion of spoken queries, called ConQX, which utilizes the text generation ability of an auto-regressive language model, GPT-2. To avoid off-topic text generation, we condition the input query to a structured context with prompt mining. We then apply zero-shot, one-shot, and few-shot learning. We lastly use the expanded queries to fine-tune BERT and RoBERTa for intent detection. The experimental results show that the performance of intent detection can be improved by our semantic expansion method.",
      "publishedDate": "2021-09-02T05:57:07Z",
      "updatedDate": "2021-09-02T05:57:07Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.00729v1",
      "arxivUrl": "https://arxiv.org/abs/2109.00729",
      "comment": "5 pages, 1 figure, conference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.13934",
      "title": "Robust Retrieval Augmented Generation for Zero-shot Slot Filling",
      "authors": [
        {
          "name": "Michael Glass",
          "affiliation": null
        },
        {
          "name": "Gaetano Rossiello",
          "affiliation": null
        },
        {
          "name": "Md Faisal Mahbub Chowdhury",
          "affiliation": null
        },
        {
          "name": "Alfio Gliozzo",
          "affiliation": null
        }
      ],
      "abstract": "Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to fill the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models.",
      "publishedDate": "2021-08-31T15:51:27Z",
      "updatedDate": "2021-09-14T01:06:04Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.13934v2",
      "arxivUrl": "https://arxiv.org/abs/2108.13934",
      "comment": "Accepted at EMNLP 2021. arXiv admin note: substantial text overlap with arXiv:2104.08610",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.13487",
      "title": "Want To Reduce Labeling Cost? GPT-3 Can Help",
      "authors": [
        {
          "name": "Shuohang Wang",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        },
        {
          "name": "Yichong Xu",
          "affiliation": null
        },
        {
          "name": "Chenguang Zhu",
          "affiliation": null
        },
        {
          "name": "Michael Zeng",
          "affiliation": null
        }
      ],
      "abstract": "Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 175 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that, to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance with limited labeling budget. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.",
      "publishedDate": "2021-08-30T19:18:24Z",
      "updatedDate": "2021-08-30T19:18:24Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.13487v1",
      "arxivUrl": "https://arxiv.org/abs/2108.13487",
      "comment": "Findings of EMNLP 2021, 11 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.13349",
      "title": "On the Multilingual Capabilities of Very Large-Scale English Language Models",
      "authors": [
        {
          "name": "Jordi Armengol-Estapé",
          "affiliation": null
        },
        {
          "name": "Ona de Gibert Bonet",
          "affiliation": null
        },
        {
          "name": "Maite Melero",
          "affiliation": null
        }
      ],
      "abstract": "Generative Pre-trained Transformers (GPTs) have recently been scaled to unprecedented sizes in the history of machine learning. These models, solely trained on the language modeling objective, have been shown to exhibit outstanding few-shot learning capabilities in a number of different tasks. Nevertheless, aside from anecdotal experiences, little is known regarding their multilingual capabilities, given the fact that the pre-training corpus is almost entirely composed of English text. In this work, we investigate the multilingual skills of GPT-3, focusing on one language that barely appears in the pre-training corpus, Catalan, which makes the results especially meaningful; we assume that our results may be relevant for other languages as well. We find that the model shows an outstanding performance, particularly in generative tasks, with predictable limitations mostly in language understanding tasks but still with remarkable results given the zero-shot scenario. We investigate its potential and limits in extractive question-answering and natural language generation, as well as the effect of scale in terms of model size.",
      "publishedDate": "2021-08-30T16:18:50Z",
      "updatedDate": "2021-08-30T16:18:50Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.13349v1",
      "arxivUrl": "https://arxiv.org/abs/2108.13349",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.12589",
      "title": "Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems",
      "authors": [
        {
          "name": "Fei Mi",
          "affiliation": null
        },
        {
          "name": "Wanhao Zhou",
          "affiliation": null
        },
        {
          "name": "Fengyu Cai",
          "affiliation": null
        },
        {
          "name": "Lingjing Kong",
          "affiliation": null
        },
        {
          "name": "Minlie Huang",
          "affiliation": null
        },
        {
          "name": "Boi Faltings",
          "affiliation": null
        }
      ],
      "abstract": "As the labeling cost for different modules in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different modules with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this paper, we devise a self-training approach to utilize the abundant unlabeled dialog data to further improve state-of-the-art pre-trained models in few-shot learning scenarios for ToD systems. Specifically, we propose a self-training approach that iteratively labels the most confident unlabeled data to train a stronger Student model. Moreover, a new text augmentation technique (GradAug) is proposed to better train the Student by replacing non-crucial tokens using a masked language model. We conduct extensive experiments and present analyses on four downstream tasks in ToD, including intent classification, dialog state tracking, dialog act prediction, and response selection. Empirical results demonstrate that the proposed self-training approach consistently improves state-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number of labeled data are available.",
      "publishedDate": "2021-08-28T07:22:06Z",
      "updatedDate": "2021-08-28T07:22:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.12589v1",
      "arxivUrl": "https://arxiv.org/abs/2108.12589",
      "comment": "Accepted as Long Paper at \"EMNLP, 2021\"",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.08103",
      "title": "AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters",
      "authors": [
        {
          "name": "Tilman Beck",
          "affiliation": null
        },
        {
          "name": "Bela Bohlender",
          "affiliation": null
        },
        {
          "name": "Christina Viehmann",
          "affiliation": null
        },
        {
          "name": "Vincent Hane",
          "affiliation": null
        },
        {
          "name": "Yanik Adamson",
          "affiliation": null
        },
        {
          "name": "Jaber Khuri",
          "affiliation": null
        },
        {
          "name": "Jonas Brossmann",
          "affiliation": null
        },
        {
          "name": "Jonas Pfeiffer",
          "affiliation": null
        },
        {
          "name": "Iryna Gurevych",
          "affiliation": null
        }
      ],
      "abstract": "The open-access dissemination of pretrained language models through online repositories has led to a democratization of state-of-the-art natural language processing (NLP) research. This also allows people outside of NLP to use such models and adapt them to specific use-cases. However, a certain amount of technical proficiency is still required which is an entry barrier for users who want to apply these models to a certain task but lack the necessary knowledge or resources. In this work, we aim to overcome this gap by providing a tool which allows researchers to leverage pretrained models without writing a single line of code. Built upon the parameter-efficient adapter modules for transfer learning, our AdapterHub Playground provides an intuitive interface, allowing the usage of adapters for prediction, training and analysis of textual data for a variety of NLP tasks. We present the tool's architecture and demonstrate its advantages with prototypical use-cases, where we show that predictive performance can easily be increased in a few-shot learning scenario. Finally, we evaluate its usability in a user study. We provide the code and a live interface at https://adapter-hub.github.io/playground.",
      "publishedDate": "2021-08-18T11:56:01Z",
      "updatedDate": "2022-04-19T07:51:38Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.08103v3",
      "arxivUrl": "https://arxiv.org/abs/2108.08103",
      "comment": "ACL 2022 System Demonstrations",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.07732",
      "title": "Program Synthesis with Large Language Models",
      "authors": [
        {
          "name": "Jacob Austin",
          "affiliation": null
        },
        {
          "name": "Augustus Odena",
          "affiliation": null
        },
        {
          "name": "Maxwell Nye",
          "affiliation": null
        },
        {
          "name": "Maarten Bosma",
          "affiliation": null
        },
        {
          "name": "Henryk Michalewski",
          "affiliation": null
        },
        {
          "name": "David Dohan",
          "affiliation": null
        },
        {
          "name": "Ellen Jiang",
          "affiliation": null
        },
        {
          "name": "Carrie Cai",
          "affiliation": null
        },
        {
          "name": "Michael Terry",
          "affiliation": null
        },
        {
          "name": "Quoc Le",
          "affiliation": null
        },
        {
          "name": "Charles Sutton",
          "affiliation": null
        }
      ],
      "abstract": "This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.",
      "publishedDate": "2021-08-16T03:57:30Z",
      "updatedDate": "2021-08-16T03:57:30Z",
      "primaryCategory": "cs.PL",
      "arxivCategories": [
        "cs.PL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.07732v1",
      "arxivUrl": "https://arxiv.org/abs/2108.07732",
      "comment": "Jacob and Augustus contributed equally",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.04106",
      "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification",
      "authors": [
        {
          "name": "Sewon Min",
          "affiliation": null
        },
        {
          "name": "Mike Lewis",
          "affiliation": null
        },
        {
          "name": "Hannaneh Hajishirzi",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        }
      ],
      "abstract": "We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive methods (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.",
      "publishedDate": "2021-08-09T15:06:26Z",
      "updatedDate": "2022-03-15T06:53:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.04106v3",
      "arxivUrl": "https://arxiv.org/abs/2108.04106",
      "comment": "15 pages, 6 figures. Published as a conference paper at ACL 2022 (long). Code available at https://github.com/shmsw25/Channel-LM-Prompting",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.00356",
      "title": "Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning",
      "authors": [
        {
          "name": "Chiyu Zhang",
          "affiliation": null
        },
        {
          "name": "Muhammad Abdul-Mageed",
          "affiliation": null
        }
      ],
      "abstract": "Masked language models (MLMs) are pre-trained with a denoising objective that is in a mismatch with the objective of downstream fine-tuning. We propose pragmatic masking and surrogate fine-tuning as two complementing strategies that exploit social cues to drive pre-trained representations toward a broad set of concepts useful for a wide class of social meaning tasks. We test our models on $15$ different Twitter datasets for social meaning detection. Our methods achieve $2.34\\%$ $F_1$ over a competitive baseline, while outperforming domain-specific language models pre-trained on large datasets. Our methods also excel in few-shot learning: with only $5\\%$ of training data (severely few-shot), our methods enable an impressive $68.54\\%$ average $F_1$. The methods are also language agnostic, as we show in a zero-shot setting involving six datasets from three different languages.",
      "publishedDate": "2021-08-01T03:32:21Z",
      "updatedDate": "2022-05-31T23:58:24Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.00356v4",
      "arxivUrl": "https://arxiv.org/abs/2108.00356",
      "comment": "12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis at ACL 2022 (corrected typos)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2107.10064",
      "title": "Few Shots Are All You Need: A Progressive Few Shot Learning Approach for Low Resource Handwritten Text Recognition",
      "authors": [
        {
          "name": "Mohamed Ali Souibgui",
          "affiliation": null
        },
        {
          "name": "Alicia Fornés",
          "affiliation": null
        },
        {
          "name": "Yousri Kessentini",
          "affiliation": null
        },
        {
          "name": "Beáta Megyesi",
          "affiliation": null
        }
      ],
      "abstract": "Handwritten text recognition in low resource scenarios, such as manuscripts with rare alphabets, is a challenging problem. The main difficulty comes from the very few annotated data and the limited linguistic information (e.g. dictionaries and language models). Thus, we propose a few-shot learning-based handwriting recognition approach that significantly reduces the human labor annotation process, requiring only few images of each alphabet symbol. The method consists in detecting all the symbols of a given alphabet in a textline image and decoding the obtained similarity scores to the final sequence of transcribed symbols. Our model is first pretrained on synthetic line images generated from any alphabet, even though different from the target domain. A second training step is then applied to diminish the gap between the source and target data. Since this retraining would require annotation of thousands of handwritten symbols together with their bounding boxes, we propose to avoid such human effort through an unsupervised progressive learning approach that automatically assigns pseudo-labels to the non-annotated data. The evaluation on different manuscript datasets show that our model can lead to competitive results with a significant reduction in human effort. The code will be publicly available in this repository: \\url{https://github.com/dali92002/HTRbyMatching}",
      "publishedDate": "2021-07-21T13:18:21Z",
      "updatedDate": "2022-06-13T11:22:21Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2107.10064v3",
      "arxivUrl": "https://arxiv.org/abs/2107.10064",
      "comment": "Accepted in Pattern Recognition Letters",
      "journalRef": null,
      "doi": "10.1016/j.patrec.2022.06.003",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2107.07498",
      "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark",
      "authors": [
        {
          "name": "Liang Xu",
          "affiliation": null
        },
        {
          "name": "Xiaojing Lu",
          "affiliation": null
        },
        {
          "name": "Chenyang Yuan",
          "affiliation": null
        },
        {
          "name": "Xuanwei Zhang",
          "affiliation": null
        },
        {
          "name": "Huilin Xu",
          "affiliation": null
        },
        {
          "name": "Hu Yuan",
          "affiliation": null
        },
        {
          "name": "Guoao Wei",
          "affiliation": null
        },
        {
          "name": "Xiang Pan",
          "affiliation": null
        },
        {
          "name": "Xin Tian",
          "affiliation": null
        },
        {
          "name": "Libo Qin",
          "affiliation": null
        },
        {
          "name": "Hu Hai",
          "affiliation": null
        }
      ],
      "abstract": "Pretrained Language Models (PLMs) have achieved tremendous success in natural language understanding tasks. While different learning schemes -- fine-tuning, zero-shot, and few-shot learning -- have been widely explored and compared for languages such as English, there is comparatively little work in Chinese to fairly and comprehensively evaluate and compare these methods and thus hinders cumulative progress. In this paper, we introduce the Chinese Few-shot Learning Evaluation Benchmark (FewCLUE), the first comprehensive few-shot evaluation benchmark in Chinese. It includes nine tasks, ranging from single-sentence and sentence-pair classification tasks to machine reading comprehension tasks. We systematically evaluate five state-of-the-art (SOTA) few-shot learning methods (including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their performance with fine-tuning and zero-shot learning schemes on the newly constructed FewCLUE benchmark. Experimental results reveal that: 1) The effect of different few-shot learning methods is sensitive to the pre-trained model to which the methods are applied; 2) PET and P-tuning achieve the best overall performance with RoBERTa and ERNIE respectively. Our benchmark is used in the few-shot learning contest of NLPCC 2021. In addition, we provide a user-friendly toolkit, as well as an online leaderboard to help facilitate further progress on Chinese few-shot learning. We provide a baseline performance on different learning methods, a reference for future research.",
      "publishedDate": "2021-07-15T17:51:25Z",
      "updatedDate": "2021-09-29T17:09:40Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2107.07498v2",
      "arxivUrl": "https://arxiv.org/abs/2107.07498",
      "comment": "10 pages, 3 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2107.07430",
      "title": "Wordcraft: a Human-AI Collaborative Editor for Story Writing",
      "authors": [
        {
          "name": "Andy Coenen",
          "affiliation": null
        },
        {
          "name": "Luke Davis",
          "affiliation": null
        },
        {
          "name": "Daphne Ippolito",
          "affiliation": null
        },
        {
          "name": "Emily Reif",
          "affiliation": null
        },
        {
          "name": "Ann Yuan",
          "affiliation": null
        }
      ],
      "abstract": "As neural language models grow in effectiveness, they are increasingly being applied in real-world settings. However these applications tend to be limited in the modes of interaction they support. In this extended abstract, we propose Wordcraft, an AI-assisted editor for story writing in which a writer and a dialog system collaborate to write a story. Our novel interface uses few-shot learning and the natural affordances of conversation to support a variety of interactions. Our editor provides a sandbox for writers to probe the boundaries of transformer-based language models and paves the way for future human-in-the-loop training pipelines and novel evaluation methods.",
      "publishedDate": "2021-07-15T16:18:27Z",
      "updatedDate": "2021-07-15T16:18:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2107.07430v1",
      "arxivUrl": "https://arxiv.org/abs/2107.07430",
      "comment": null,
      "journalRef": "First Workshop on Bridging Human-Computer Interaction and Natural Language Processing at EACL 2021",
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2107.07170",
      "title": "FLEX: Unifying Evaluation for Few-Shot NLP",
      "authors": [
        {
          "name": "Jonathan Bragg",
          "affiliation": null
        },
        {
          "name": "Arman Cohan",
          "affiliation": null
        },
        {
          "name": "Kyle Lo",
          "affiliation": null
        },
        {
          "name": "Iz Beltagy",
          "affiliation": null
        }
      ],
      "abstract": "Few-shot NLP research is highly active, yet conducted in disjoint research threads with evaluation suites that lack challenging-yet-realistic testing setups and fail to employ careful experimental design. Consequently, the community does not know which techniques perform best or even if they outperform simple baselines. In response, we formulate the FLEX Principles, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation. These principles include Sample Size Design, a novel approach to benchmark design that optimizes statistical accuracy and precision while keeping evaluation costs manageable. Following the principles, we release the FLEX benchmark, which includes four few-shot transfer settings, zero-shot evaluation, and a public leaderboard that covers diverse NLP tasks. In addition, we present UniFew, a prompt-based model for few-shot learning that unifies pretraining and finetuning prompt formats, eschewing complex machinery of recent prompt-based approaches in adapting downstream task formats to language model pretraining objectives. We demonstrate that despite simplicity, UniFew achieves results competitive with both popular meta-learning and prompt-based approaches.",
      "publishedDate": "2021-07-15T07:37:06Z",
      "updatedDate": "2021-11-08T18:33:32Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2107.07170v2",
      "arxivUrl": "https://arxiv.org/abs/2107.07170",
      "comment": "NeurIPS 2021. First two authors contributed equally. Code and leaderboard available at: https://github.com/allenai/flex",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2107.02137",
      "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
      "authors": [
        {
          "name": "Yu Sun",
          "affiliation": null
        },
        {
          "name": "Shuohuan Wang",
          "affiliation": null
        },
        {
          "name": "Shikun Feng",
          "affiliation": null
        },
        {
          "name": "Siyu Ding",
          "affiliation": null
        },
        {
          "name": "Chao Pang",
          "affiliation": null
        },
        {
          "name": "Junyuan Shang",
          "affiliation": null
        },
        {
          "name": "Jiaxiang Liu",
          "affiliation": null
        },
        {
          "name": "Xuyi Chen",
          "affiliation": null
        },
        {
          "name": "Yanbin Zhao",
          "affiliation": null
        },
        {
          "name": "Yuxiang Lu",
          "affiliation": null
        },
        {
          "name": "Weixin Liu",
          "affiliation": null
        },
        {
          "name": "Zhihua Wu",
          "affiliation": null
        },
        {
          "name": "Weibao Gong",
          "affiliation": null
        },
        {
          "name": "Jianzhong Liang",
          "affiliation": null
        },
        {
          "name": "Zhizhou Shang",
          "affiliation": null
        },
        {
          "name": "Peng Sun",
          "affiliation": null
        },
        {
          "name": "Wei Liu",
          "affiliation": null
        },
        {
          "name": "Xuan Ouyang",
          "affiliation": null
        },
        {
          "name": "Dianhai Yu",
          "affiliation": null
        },
        {
          "name": "Hao Tian",
          "affiliation": null
        },
        {
          "name": "Hua Wu",
          "affiliation": null
        },
        {
          "name": "Haifeng Wang",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).",
      "publishedDate": "2021-07-05T16:54:59Z",
      "updatedDate": "2021-07-05T16:54:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2107.02137v1",
      "arxivUrl": "https://arxiv.org/abs/2107.02137",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2106.14720",
      "title": "What's in a Measurement? Using GPT-3 on SemEval 2021 Task 8 -- MeasEval",
      "authors": [
        {
          "name": "Curt Kohler",
          "affiliation": null
        },
        {
          "name": "Ron Daniel",
          "affiliation": null
        }
      ],
      "abstract": "In the summer of 2020 OpenAI released its GPT-3 autoregressive language model to much fanfare. While the model has shown promise on tasks in several areas, it has not always been clear when the results were cherry-picked or when they were the unvarnished output. We were particularly interested in what benefits GPT-3 could bring to the SemEval 2021 MeasEval task - identifying measurements and their associated attributes in scientific literature. We had already experimented with multi-turn questions answering as a solution to this task. We wanted to see if we could use GPT-3's few-shot learning capabilities to more easily develop a solution that would have better performance than our prior work. Unfortunately, we have not been successful in that effort. This paper discusses the approach we used, challenges we encountered, and results we observed. Some of the problems we encountered were simply due to the state of the art. For example, the limits on the size of the prompt and answer limited the amount of the training signal that could be offered. Others are more fundamental. We are unaware of generative models that excel in retaining factual information. Also, the impact of changes in the prompts is unpredictable, making it hard to reliably improve performance.",
      "publishedDate": "2021-06-28T13:48:25Z",
      "updatedDate": "2021-06-28T13:48:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2106.14720v1",
      "arxivUrl": "https://arxiv.org/abs/2106.14720",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2106.13884",
      "title": "Multimodal Few-Shot Learning with Frozen Language Models",
      "authors": [
        {
          "name": "Maria Tsimpoukelli",
          "affiliation": null
        },
        {
          "name": "Jacob Menick",
          "affiliation": null
        },
        {
          "name": "Serkan Cabi",
          "affiliation": null
        },
        {
          "name": "S. M. Ali Eslami",
          "affiliation": null
        },
        {
          "name": "Oriol Vinyals",
          "affiliation": null
        },
        {
          "name": "Felix Hill",
          "affiliation": null
        }
      ],
      "abstract": "When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.",
      "publishedDate": "2021-06-25T21:07:09Z",
      "updatedDate": "2021-07-03T10:04:41Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2106.13884v2",
      "arxivUrl": "https://arxiv.org/abs/2106.13884",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "tool-use",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "tool-use",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2106.06168",
      "title": "Generate, Annotate, and Learn: NLP with Synthetic Text",
      "authors": [
        {
          "name": "Xuanli He",
          "affiliation": null
        },
        {
          "name": "Islam Nassar",
          "affiliation": null
        },
        {
          "name": "Jamie Kiros",
          "affiliation": null
        },
        {
          "name": "Gholamreza Haffari",
          "affiliation": null
        },
        {
          "name": "Mohammad Norouzi",
          "affiliation": null
        }
      ],
      "abstract": "This paper studies the use of language models as a source of synthetic unlabeled text for NLP. We formulate a general framework called ``generate, annotate, and learn (GAL)'' to take advantage of synthetic text within knowledge distillation, self-training, and few-shot learning applications. To generate high-quality task-specific text, we either fine-tune LMs on inputs from the task of interest, or prompt large LMs with few examples. We use the best available classifier to annotate synthetic text with soft pseudo labels for knowledge distillation and self-training, and use LMs to obtain hard labels for few-shot learning. We train new supervised models on the combination of labeled and pseudo-labeled data, which results in significant gains across several applications. We investigate key components of GAL and present theoretical and empirical arguments against the use of class-conditional LMs to generate synthetic labeled text instead of unlabeled text. GAL achieves new state-of-the-art knowledge distillation results for 6-layer transformers on the GLUE leaderboard.",
      "publishedDate": "2021-06-11T05:01:24Z",
      "updatedDate": "2022-05-31T15:06:16Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2106.06168v3",
      "arxivUrl": "https://arxiv.org/abs/2106.06168",
      "comment": "accepted to TACL2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2106.01751",
      "title": "Reordering Examples Helps during Priming-based Few-Shot Learning",
      "authors": [
        {
          "name": "Sawan Kumar",
          "affiliation": null
        },
        {
          "name": "Partha Talukdar",
          "affiliation": null
        }
      ],
      "abstract": "The ability to learn from limited data, or few-shot learning, is a desirable and often critical requirement for NLP systems. While many existing methods do poorly at learning from a handful of examples, large pretrained language models have recently been shown to be efficient few-shot learners. One approach to few-shot learning, which does not require finetuning of model parameters, is to augment the language model's input with priming text which is typically constructed using task specific descriptions and examples. In this work, we further explore priming-based few-shot learning, with focus on using examples as prompts. We show that presenting examples in the right order is key for generalization. We introduce PERO (Prompting with Examples in the Right Order), where we formulate few-shot learning as search over the set of permutations of the training examples. We show that PERO can learn to generalize efficiently using as few as 10 examples, in contrast to existing approaches. While the newline token is a natural choice for separating the examples in the prompt, we show that learning a new separator token can potentially provide further gains in performance. We demonstrate the effectiveness of the proposed method on the tasks of sentiment classification, natural language inference and fact retrieval. Finally, we analyze the learned prompts to reveal novel insights, including the idea that two training examples in the right order alone can provide competitive performance for sentiment classification and natural language inference.",
      "publishedDate": "2021-06-03T11:02:36Z",
      "updatedDate": "2021-06-03T11:02:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2106.01751v1",
      "arxivUrl": "https://arxiv.org/abs/2106.01751",
      "comment": "12 pages, 1 figure, Accepted to Findings of ACL 2021",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2106.01465",
      "title": "Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?",
      "authors": [
        {
          "name": "Jieyu Zhao",
          "affiliation": null
        },
        {
          "name": "Daniel Khashabi",
          "affiliation": null
        },
        {
          "name": "Tushar Khot",
          "affiliation": null
        },
        {
          "name": "Ashish Sabharwal",
          "affiliation": null
        },
        {
          "name": "Kai-Wei Chang",
          "affiliation": null
        }
      ],
      "abstract": "Is it possible to use natural language to intervene in a model's behavior and alter its prediction in a desired way? We investigate the effectiveness of natural language interventions for reading-comprehension systems, studying this in the context of social stereotypes. Specifically, we propose a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a question-answering (QA) model's unethical behavior by communicating context-specific principles of ethics and equity to it. To this end, we build upon recent methods for quantifying a system's social stereotypes, augmenting them with different kinds of ethical interventions and the desired model behavior under such interventions. Our zero-shot evaluation finds that even today's powerful neural language models are extremely poor ethical-advice takers, that is, they respond surprisingly little to ethical interventions even though these interventions are stated as simple sentences. Few-shot learning improves model behavior but remains far from the desired outcome, especially when evaluated for various types of generalization. Our new task thus poses a novel language understanding challenge for the community.",
      "publishedDate": "2021-06-02T20:57:58Z",
      "updatedDate": "2021-06-02T20:57:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2106.01465v1",
      "arxivUrl": "https://arxiv.org/abs/2106.01465",
      "comment": "9 pages, Findings of ACL-IJCNLP 2021",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2105.11447",
      "title": "True Few-Shot Learning with Language Models",
      "authors": [
        {
          "name": "Ethan Perez",
          "affiliation": null
        },
        {
          "name": "Douwe Kiela",
          "affiliation": null
        },
        {
          "name": "Kyunghyun Cho",
          "affiliation": null
        }
      ],
      "abstract": "Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (\"prompts\"). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.",
      "publishedDate": "2021-05-24T17:55:51Z",
      "updatedDate": "2021-05-24T17:55:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2105.11447v1",
      "arxivUrl": "https://arxiv.org/abs/2105.11447",
      "comment": "Code at https://github.com/ethanjperez/true_few_shot",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2105.10195",
      "title": "Aligning Visual Prototypes with BERT Embeddings for Few-Shot Learning",
      "authors": [
        {
          "name": "Kun Yan",
          "affiliation": null
        },
        {
          "name": "Zied Bouraoui",
          "affiliation": null
        },
        {
          "name": "Ping Wang",
          "affiliation": null
        },
        {
          "name": "Shoaib Jameel",
          "affiliation": null
        },
        {
          "name": "Steven Schockaert",
          "affiliation": null
        }
      ],
      "abstract": "Few-shot learning (FSL) is the task of learning to recognize previously unseen categories of images from a small number of training examples. This is a challenging task, as the available examples may not be enough to unambiguously determine which visual features are most characteristic of the considered categories. To alleviate this issue, we propose a method that additionally takes into account the names of the image classes. While the use of class names has already been explored in previous work, our approach differs in two key aspects. First, while previous work has aimed to directly predict visual prototypes from word embeddings, we found that better results can be obtained by treating visual and text-based prototypes separately. Second, we propose a simple strategy for learning class name embeddings using the BERT language model, which we found to substantially outperform the GloVe vectors that were used in previous work. We furthermore propose a strategy for dealing with the high dimensionality of these vectors, inspired by models for aligning cross-lingual word embeddings. We provide experiments on miniImageNet, CUB and tieredImageNet, showing that our approach consistently improves the state-of-the-art in metric-based FSL.",
      "publishedDate": "2021-05-21T08:08:28Z",
      "updatedDate": "2021-05-21T08:08:28Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2105.10195v1",
      "arxivUrl": "https://arxiv.org/abs/2105.10195",
      "comment": "Accepted by ICMR2021",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2104.14690",
      "title": "Entailment as Few-Shot Learner",
      "authors": [
        {
          "name": "Sinong Wang",
          "affiliation": null
        },
        {
          "name": "Han Fang",
          "affiliation": null
        },
        {
          "name": "Madian Khabsa",
          "affiliation": null
        },
        {
          "name": "Hanzi Mao",
          "affiliation": null
        },
        {
          "name": "Hao Ma",
          "affiliation": null
        }
      ],
      "abstract": "Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12\\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.",
      "publishedDate": "2021-04-29T22:52:26Z",
      "updatedDate": "2021-04-29T22:52:26Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2104.14690v1",
      "arxivUrl": "https://arxiv.org/abs/2104.14690",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2104.12369",
      "title": "PanGu-$α$: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation",
      "authors": [
        {
          "name": "Wei Zeng",
          "affiliation": null
        },
        {
          "name": "Xiaozhe Ren",
          "affiliation": null
        },
        {
          "name": "Teng Su",
          "affiliation": null
        },
        {
          "name": "Hui Wang",
          "affiliation": null
        },
        {
          "name": "Yi Liao",
          "affiliation": null
        },
        {
          "name": "Zhiwei Wang",
          "affiliation": null
        },
        {
          "name": "Xin Jiang",
          "affiliation": null
        },
        {
          "name": "ZhenZhang Yang",
          "affiliation": null
        },
        {
          "name": "Kaisheng Wang",
          "affiliation": null
        },
        {
          "name": "Xiaoda Zhang",
          "affiliation": null
        },
        {
          "name": "Chen Li",
          "affiliation": null
        },
        {
          "name": "Ziyan Gong",
          "affiliation": null
        },
        {
          "name": "Yifan Yao",
          "affiliation": null
        },
        {
          "name": "Xinjing Huang",
          "affiliation": null
        },
        {
          "name": "Jun Wang",
          "affiliation": null
        },
        {
          "name": "Jianfeng Yu",
          "affiliation": null
        },
        {
          "name": "Qi Guo",
          "affiliation": null
        },
        {
          "name": "Yue Yu",
          "affiliation": null
        },
        {
          "name": "Yan Zhang",
          "affiliation": null
        },
        {
          "name": "Jin Wang",
          "affiliation": null
        },
        {
          "name": "Hengtao Tao",
          "affiliation": null
        },
        {
          "name": "Dasen Yan",
          "affiliation": null
        },
        {
          "name": "Zexuan Yi",
          "affiliation": null
        },
        {
          "name": "Fang Peng",
          "affiliation": null
        },
        {
          "name": "Fangqing Jiang",
          "affiliation": null
        },
        {
          "name": "Han Zhang",
          "affiliation": null
        },
        {
          "name": "Lingfeng Deng",
          "affiliation": null
        },
        {
          "name": "Yehong Zhang",
          "affiliation": null
        },
        {
          "name": "Zhe Lin",
          "affiliation": null
        },
        {
          "name": "Chao Zhang",
          "affiliation": null
        },
        {
          "name": "Shaojie Zhang",
          "affiliation": null
        },
        {
          "name": "Mingyue Guo",
          "affiliation": null
        },
        {
          "name": "Shanzhi Gu",
          "affiliation": null
        },
        {
          "name": "Gaojun Fan",
          "affiliation": null
        },
        {
          "name": "Yaowei Wang",
          "affiliation": null
        },
        {
          "name": "Xuefeng Jin",
          "affiliation": null
        },
        {
          "name": "Qun Liu",
          "affiliation": null
        },
        {
          "name": "Yonghong Tian",
          "affiliation": null
        }
      ],
      "abstract": "Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \\textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-$α$, with up to 200 billion parameters. PanGu-$α$ is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-$α$, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-$α$ in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-$α$ in performing various tasks under few-shot or zero-shot settings.",
      "publishedDate": "2021-04-26T06:59:36Z",
      "updatedDate": "2021-04-26T06:59:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2104.12369v1",
      "arxivUrl": "https://arxiv.org/abs/2104.12369",
      "comment": "The technique report for PanGu-$α$",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2104.08691",
      "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
      "authors": [
        {
          "name": "Brian Lester",
          "affiliation": null
        },
        {
          "name": "Rami Al-Rfou",
          "affiliation": null
        },
        {
          "name": "Noah Constant",
          "affiliation": null
        }
      ],
      "abstract": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.",
      "publishedDate": "2021-04-18T03:19:26Z",
      "updatedDate": "2021-09-02T17:34:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2104.08691v2",
      "arxivUrl": "https://arxiv.org/abs/2104.08691",
      "comment": "Accepted to EMNLP 2021",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2105.00828",
      "title": "Memorisation versus Generalisation in Pre-trained Language Models",
      "authors": [
        {
          "name": "Michael Tänzer",
          "affiliation": null
        },
        {
          "name": "Sebastian Ruder",
          "affiliation": null
        },
        {
          "name": "Marek Rei",
          "affiliation": null
        }
      ],
      "abstract": "State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets. However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose an extension based on prototypical networks that improves performance in low-resource named entity recognition tasks.",
      "publishedDate": "2021-04-16T18:53:19Z",
      "updatedDate": "2022-03-15T01:14:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2105.00828v2",
      "arxivUrl": "https://arxiv.org/abs/2105.00828",
      "comment": "15 pages, 25 figures. To be published in ACL2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2103.16911",
      "title": "Few-shot learning through contextual data augmentation",
      "authors": [
        {
          "name": "Farid Arthaud",
          "affiliation": null
        },
        {
          "name": "Rachel Bawden",
          "affiliation": null
        },
        {
          "name": "Alexandra Birch",
          "affiliation": null
        }
      ],
      "abstract": "Machine translation (MT) models used in industries with constantly changing topics, such as translation or news agencies, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pre-trained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining data augmentation with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples.",
      "publishedDate": "2021-03-31T09:05:43Z",
      "updatedDate": "2021-03-31T09:05:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2103.16911v1",
      "arxivUrl": "https://arxiv.org/abs/2103.16911",
      "comment": "14 pages includince 3 of appendices",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "evaluation",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2103.12407",
      "title": "Detecting Hate Speech with GPT-3",
      "authors": [
        {
          "name": "Ke-Li Chiu",
          "affiliation": null
        },
        {
          "name": "Annie Collins",
          "affiliation": null
        },
        {
          "name": "Rohan Alexander",
          "affiliation": null
        }
      ],
      "abstract": "Sophisticated language models such as OpenAI's GPT-3 can generate hateful text that targets marginalized groups. Given this capacity, we are interested in whether large language models can be used to identify hate speech and classify text as sexist or racist. We use GPT-3 to identify sexist and racist text passages with zero-, one-, and few-shot learning. We find that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an average accuracy between 55 per cent and 67 per cent, depending on the category of text and type of learning. With few-shot learning, the model's accuracy can be as high as 85 per cent. Large language models have a role to play in hate speech detection, and with further development they could eventually be used to counter hate speech.",
      "publishedDate": "2021-03-23T09:17:22Z",
      "updatedDate": "2022-03-24T16:24:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2103.12407v4",
      "arxivUrl": "https://arxiv.org/abs/2103.12407",
      "comment": "29 pages, 1 figure, 23 tables 24 March 2022: Re-submission changes the modelling to occur multiple times and adds standard errors",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2103.11955",
      "title": "Improving and Simplifying Pattern Exploiting Training",
      "authors": [
        {
          "name": "Derek Tam",
          "affiliation": null
        },
        {
          "name": "Rakesh R Menon",
          "affiliation": null
        },
        {
          "name": "Mohit Bansal",
          "affiliation": null
        },
        {
          "name": "Shashank Srivastava",
          "affiliation": null
        },
        {
          "name": "Colin Raffel",
          "affiliation": null
        }
      ],
      "abstract": "Recently, pre-trained language models (LMs) have achieved strong performance when fine-tuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that leverages patterns for few-shot learning. However, PET uses task-specific unlabeled data. In this paper, we focus on few-shot learning without any unlabeled data and introduce ADAPET, which modifies PET's objective to provide denser supervision during fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any task-specific unlabeled data. Our code can be found at https://github.com/rrmenon10/ADAPET.",
      "publishedDate": "2021-03-22T15:52:45Z",
      "updatedDate": "2021-09-28T13:56:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2103.11955v3",
      "arxivUrl": "https://arxiv.org/abs/2103.11955",
      "comment": "EMNLP 2021 (12 pages, 2 figures)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2103.09535",
      "title": "Towards Few-Shot Fact-Checking via Perplexity",
      "authors": [
        {
          "name": "Nayeon Lee",
          "affiliation": null
        },
        {
          "name": "Yejin Bang",
          "affiliation": null
        },
        {
          "name": "Andrea Madotto",
          "affiliation": null
        },
        {
          "name": "Madian Khabsa",
          "affiliation": null
        },
        {
          "name": "Pascale Fung",
          "affiliation": null
        }
      ],
      "abstract": "Few-shot learning has drawn researchers' attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in few-shot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19.",
      "publishedDate": "2021-03-17T09:43:19Z",
      "updatedDate": "2021-03-17T09:43:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2103.09535v1",
      "arxivUrl": "https://arxiv.org/abs/2103.09535",
      "comment": "Accpeted to NAACL'21",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2103.07102",
      "title": "Inductive Relation Prediction by BERT",
      "authors": [
        {
          "name": "Hanwen Zha",
          "affiliation": null
        },
        {
          "name": "Zhiyu Chen",
          "affiliation": null
        },
        {
          "name": "Xifeng Yan",
          "affiliation": null
        }
      ],
      "abstract": "Relation prediction in knowledge graphs is dominated by embedding based methods which mainly focus on the transductive setting. Unfortunately, they are not able to handle inductive learning where unseen entities and relations are present and cannot take advantage of prior knowledge. Furthermore, their inference process is not easily explainable. In this work, we propose an all-in-one solution, called BERTRL (BERT-based Relational Learning), which leverages pre-trained language model and fine-tunes it by taking relation instances and their possible reasoning paths as training samples. BERTRL outperforms the SOTAs in 15 out of 18 cases in both inductive and transductive settings. Meanwhile, it demonstrates strong generalization capability in few-shot learning and is explainable.",
      "publishedDate": "2021-03-12T06:27:11Z",
      "updatedDate": "2021-03-12T06:27:11Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2103.07102v1",
      "arxivUrl": "https://arxiv.org/abs/2103.07102",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2103.01075",
      "title": "OmniNet: Omnidirectional Representations from Transformers",
      "authors": [
        {
          "name": "Yi Tay",
          "affiliation": null
        },
        {
          "name": "Mostafa Dehghani",
          "affiliation": null
        },
        {
          "name": "Vamsi Aribandi",
          "affiliation": null
        },
        {
          "name": "Jai Gupta",
          "affiliation": null
        },
        {
          "name": "Philip Pham",
          "affiliation": null
        },
        {
          "name": "Zhen Qin",
          "affiliation": null
        },
        {
          "name": "Dara Bahri",
          "affiliation": null
        },
        {
          "name": "Da-Cheng Juan",
          "affiliation": null
        },
        {
          "name": "Donald Metzler",
          "affiliation": null
        }
      ],
      "abstract": "This paper proposes Omnidirectional Representations from Transformers (OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive field, each token is allowed to attend to all tokens in the entire network. This process can also be interpreted as a form of extreme or intensive attention mechanism that has the receptive field of the entire width and depth of the network. To this end, the omnidirectional attention is learned via a meta-learner, which is essentially another self-attention based model. In order to mitigate the computationally expensive costs of full receptive field attention, we leverage efficient self-attention models such as kernel-based (Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer et al.) as the meta-learner. Extensive experiments are conducted on autoregressive language modeling (LM1B, C4), Machine Translation, Long Range Arena (LRA), and Image Recognition. The experiments show that OmniNet achieves considerable improvements across these tasks, including achieving state-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena. Moreover, using omnidirectional representation in Vision Transformers leads to significant improvements on image recognition tasks on both few-shot learning and fine-tuning setups.",
      "publishedDate": "2021-03-01T15:31:54Z",
      "updatedDate": "2021-03-01T15:31:54Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2103.01075v1",
      "arxivUrl": "https://arxiv.org/abs/2103.01075",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2102.10956",
      "title": "Few Shot Learning for Information Verification",
      "authors": [
        {
          "name": "Usama Khalid",
          "affiliation": null
        },
        {
          "name": "Mirza Omer Beg",
          "affiliation": null
        }
      ],
      "abstract": "Information verification is quite a challenging task, this is because many times verifying a claim can require picking pieces of information from multiple pieces of evidence which can have a hierarchy of complex semantic relations. Previously a lot of researchers have mainly focused on simply concatenating multiple evidence sentences to accept or reject claims. These approaches are limited as evidence can contain hierarchical information and dependencies. In this research, we aim to verify facts based on evidence selected from a list of articles taken from Wikipedia. Pretrained language models such as XLNET are used to generate meaningful representations and graph-based attention and convolutions are used in such a way that the system requires little additional training to learn to verify facts.",
      "publishedDate": "2021-02-22T12:56:12Z",
      "updatedDate": "2021-02-22T12:56:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2102.10956v1",
      "arxivUrl": "https://arxiv.org/abs/2102.10956",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2102.09690",
      "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
      "authors": [
        {
          "name": "Tony Z. Zhao",
          "affiliation": null
        },
        {
          "name": "Eric Wallace",
          "affiliation": null
        },
        {
          "name": "Shi Feng",
          "affiliation": null
        },
        {
          "name": "Dan Klein",
          "affiliation": null
        },
        {
          "name": "Sameer Singh",
          "affiliation": null
        }
      ],
      "abstract": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.",
      "publishedDate": "2021-02-19T00:23:59Z",
      "updatedDate": "2021-06-10T18:20:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2102.09690v2",
      "arxivUrl": "https://arxiv.org/abs/2102.09690",
      "comment": "ICML 2021",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2111.12028",
      "title": "Romanian Speech Recognition Experiments from the ROBIN Project",
      "authors": [
        {
          "name": "Andrei-Marius Avram",
          "affiliation": null
        },
        {
          "name": "Vasile Păiş",
          "affiliation": null
        },
        {
          "name": "Dan Tufiş",
          "affiliation": null
        }
      ],
      "abstract": "One of the fundamental functionalities for accepting a socially assistive robot is its communication capabilities with other agents in the environment. In the context of the ROBIN project, situational dialogue through voice interaction with a robot was investigated. This paper presents different speech recognition experiments with deep neural networks focusing on producing fast (under 100ms latency from the network itself), while still reliable models. Even though one of the key desired characteristics is low latency, the final deep neural network model achieves state of the art results for recognizing Romanian language, obtaining a 9.91% word error rate (WER), when combined with a language model, thus improving over the previous results while offering at the same time an improved runtime performance. Additionally, we explore two modules for correcting the ASR output (hyphen and capitalization restoration and unknown words correction), targeting the ROBIN project's goals (dialogue in closed micro-worlds). We design a modular architecture based on APIs allowing an integration engine (either in the robot or external) to chain together the available modules as needed. Finally, we test the proposed design by integrating it in the RELATE platform and making the ASR service available to web users by either uploading a file or recording new speech.",
      "publishedDate": "2021-11-23T17:35:00Z",
      "updatedDate": "2021-11-23T17:35:00Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2111.12028v1",
      "arxivUrl": "https://arxiv.org/abs/2111.12028",
      "comment": "12 pages, 3 figures, ConsILR2020",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "tool-use",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2111.07975",
      "title": "Semantically Grounded Object Matching for Robust Robotic Scene Rearrangement",
      "authors": [
        {
          "name": "Walter Goodwin",
          "affiliation": null
        },
        {
          "name": "Sagar Vaze",
          "affiliation": null
        },
        {
          "name": "Ioannis Havoutis",
          "affiliation": null
        },
        {
          "name": "Ingmar Posner",
          "affiliation": null
        }
      ],
      "abstract": "Object rearrangement has recently emerged as a key competency in robot manipulation, with practical solutions generally involving object detection, recognition, grasping and high-level planning. Goal-images describing a desired scene configuration are a promising and increasingly used mode of instruction. A key outstanding challenge is the accurate inference of matches between objects in front of a robot, and those seen in a provided goal image, where recent works have struggled in the absence of object-specific training data. In this work, we explore the deterioration of existing methods' ability to infer matches between objects as the visual shift between observed and goal scenes increases. We find that a fundamental limitation of the current setting is that source and target images must contain the same $\\textit{instance}$ of every object, which restricts practical deployment. We present a novel approach to object matching that uses a large pre-trained vision-language model to match objects in a cross-instance setting by leveraging semantics together with visual features as a more robust, and much more general, measure of similarity. We demonstrate that this provides considerably improved matching performance in cross-instance settings, and can be used to guide multi-object rearrangement with a robot manipulator from an image that shares no object $\\textit{instances}$ with the robot's scene.",
      "publishedDate": "2021-11-15T18:39:43Z",
      "updatedDate": "2021-11-15T18:39:43Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2111.07975v1",
      "arxivUrl": "https://arxiv.org/abs/2111.07975",
      "comment": "8 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "planning",
        "rag",
        "prompting",
        "robotics"
      ],
      "tags": {
        "auto": [
          "planning",
          "rag",
          "prompting",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2110.05603",
      "title": "Generalizing to New Domains by Mapping Natural Language to Lifted LTL",
      "authors": [
        {
          "name": "Eric Hsiung",
          "affiliation": null
        },
        {
          "name": "Hiloni Mehta",
          "affiliation": null
        },
        {
          "name": "Junchi Chu",
          "affiliation": null
        },
        {
          "name": "Xinyu Liu",
          "affiliation": null
        },
        {
          "name": "Roma Patel",
          "affiliation": null
        },
        {
          "name": "Stefanie Tellex",
          "affiliation": null
        },
        {
          "name": "George Konidaris",
          "affiliation": null
        }
      ],
      "abstract": "Recent work on using natural language to specify commands to robots has grounded that language to LTL. However, mapping natural language task specifications to LTL task specifications using language models require probability distributions over finite vocabulary. Existing state-of-the-art methods have extended this finite vocabulary to include unseen terms from the input sequence to improve output generalization. However, novel out-of-vocabulary atomic propositions cannot be generated using these methods. To overcome this, we introduce an intermediate contextual query representation which can be learned from single positive task specification examples, associating a contextual query with an LTL template. We demonstrate that this intermediate representation allows for generalization over unseen object references, assuming accurate groundings are available. We compare our method of mapping natural language task specifications to intermediate contextual queries against state-of-the-art CopyNet models capable of translating natural language to LTL, by evaluating whether correct LTL for manipulation and navigation task specifications can be output, and show that our method outperforms the CopyNet model on unseen object references. We demonstrate that the grounded LTL our method outputs can be used for planning in a simulated OO-MDP environment. Finally, we discuss some common failure modes encountered when translating natural language task specifications to grounded LTL.",
      "publishedDate": "2021-10-11T20:49:26Z",
      "updatedDate": "2022-03-09T22:10:03Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2110.05603v2",
      "arxivUrl": "https://arxiv.org/abs/2110.05603",
      "comment": "7 pages (6 + 1 references page), 3 figures, 2 tables. Accepted to ICRA 2022. To appear in Proceedings of the 2022 International Conference on Robotics and Automation, May 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "planning",
        "robotics"
      ],
      "tags": {
        "auto": [
          "planning",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2109.08270",
      "title": "Language Models as a Knowledge Source for Cognitive Agents",
      "authors": [
        {
          "name": "Robert E. Wray,",
          "affiliation": null
        },
        {
          "name": "James R. Kirk",
          "affiliation": null
        },
        {
          "name": "John E. Laird",
          "affiliation": null
        }
      ],
      "abstract": "Language models (LMs) are sentence-completion engines trained on massive corpora. LMs have emerged as a significant breakthrough in natural-language processing, providing capabilities that go far beyond sentence completion including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, exploiting language models as a source of task knowledge, especially for task learning, offers significant, near-term benefits. We introduce language models and the various tasks to which they have been applied and then review methods of knowledge extraction from language models. The resulting analysis outlines both the challenges and opportunities for using language models as a new knowledge source for cognitive systems. It also identifies possible ways to improve knowledge extraction from language models using the capabilities provided by cognitive systems. Central to success will be the ability of a cognitive agent to itself learn an abstract model of the knowledge implicit in the LM as well as methods to extract high-quality knowledge effectively and efficiently. To illustrate, we introduce a hypothetical robot agent and describe how language models could extend its task knowledge and improve its performance and the kinds of knowledge and methods the agent can use to exploit the knowledge within a language model.",
      "publishedDate": "2021-09-17T01:12:34Z",
      "updatedDate": "2021-10-23T21:26:58Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2109.08270v3",
      "arxivUrl": "https://arxiv.org/abs/2109.08270",
      "comment": "16 pages, 2 figures; accepted for 2021 Advances in Cognitive Systems Conference (revised based on reviews)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2108.00159",
      "title": "Learning Embeddings that Capture Spatial Semantics for Indoor Navigation",
      "authors": [
        {
          "name": "Vidhi Jain",
          "affiliation": null
        },
        {
          "name": "Prakhar Agarwal",
          "affiliation": null
        },
        {
          "name": "Shishir Patil",
          "affiliation": null
        },
        {
          "name": "Katia Sycara",
          "affiliation": null
        }
      ],
      "abstract": "Incorporating domain-specific priors in search and navigation tasks has shown promising results in improving generalization and sample complexity over end-to-end trained policies. In this work, we study how object embeddings that capture spatial semantic priors can guide search and navigation tasks in a structured environment. We know that humans can search for an object like a book, or a plate in an unseen house, based on the spatial semantics of bigger objects detected. For example, a book is likely to be on a bookshelf or a table, whereas a plate is likely to be in a cupboard or dishwasher. We propose a method to incorporate such spatial semantic awareness in robots by leveraging pre-trained language models and multi-relational knowledge bases as object embeddings. We demonstrate using these object embeddings to search a query object in an unseen indoor environment. We measure the performance of these embeddings in an indoor simulator (AI2Thor). We further evaluate different pre-trained embedding onSuccess Rate(SR) and success weighted by Path Length(SPL).",
      "publishedDate": "2021-07-31T06:12:40Z",
      "updatedDate": "2021-07-31T06:12:40Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2108.00159v1",
      "arxivUrl": "https://arxiv.org/abs/2108.00159",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "robotics"
      ],
      "tags": {
        "auto": [
          "rag",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2107.03438",
      "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding",
      "authors": [
        {
          "name": "Junha Roh",
          "affiliation": null
        },
        {
          "name": "Karthik Desingh",
          "affiliation": null
        },
        {
          "name": "Ali Farhadi",
          "affiliation": null
        },
        {
          "name": "Dieter Fox",
          "affiliation": null
        }
      ],
      "abstract": "For robots to understand human instructions and perform meaningful tasks in the near future, it is important to develop learned models that comprehend referential language to identify common objects in real-world 3D scenes. In this paper, we introduce a spatial-language model for a 3D visual grounding problem. Specifically, given a reconstructed 3D scene in the form of point clouds with 3D bounding boxes of potential object candidates, and a language utterance referring to a target object in the scene, our model successfully identifies the target object from a set of potential candidates. Specifically, LanguageRefer uses a transformer-based architecture that combines spatial embedding from bounding boxes with fine-tuned language embeddings from DistilBert to predict the target object. We show that it performs competitively on visio-linguistic datasets proposed by ReferIt3D. Further, we analyze its spatial reasoning task performance decoupled from perception noise, the accuracy of view-dependent utterances, and viewpoint annotations for potential robotics applications.",
      "publishedDate": "2021-07-07T18:55:03Z",
      "updatedDate": "2021-11-04T19:38:48Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.CL",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2107.03438v3",
      "arxivUrl": "https://arxiv.org/abs/2107.03438",
      "comment": "11 pages, 3 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "robotics",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2105.04633",
      "title": "Language Acquisition is Embodied, Interactive, Emotive: a Research Proposal",
      "authors": [
        {
          "name": "Casey Kennington",
          "affiliation": null
        }
      ],
      "abstract": "Humans' experience of the world is profoundly multimodal from the beginning, so why do existing state-of-the-art language models only use text as a modality to learn and represent semantic meaning? In this paper we review the literature on the role of embodiment and emotion in the interactive setting of spoken dialogue as necessary prerequisites for language learning for human children, including how words in child vocabularies are largely concrete, then shift to become more abstract as the children get older. We sketch a model of semantics that leverages current transformer-based models and a word-level grounded model, then explain the robot-dialogue system that will make use of our semantic model, the setting for the system to learn language, and existing benchmarks for evaluation.",
      "publishedDate": "2021-05-10T19:40:17Z",
      "updatedDate": "2021-05-10T19:40:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2105.04633v1",
      "arxivUrl": "https://arxiv.org/abs/2105.04633",
      "comment": "6 pages, ICLR 2021 Embodied Multimodal Learning Workshop",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "robotics",
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2104.12145",
      "title": "LLM4Laser: Large Language Models Automate the Design of Lasers",
      "authors": [
        {
          "name": "Renjie Li",
          "affiliation": null
        },
        {
          "name": "Ceyao Zhang",
          "affiliation": null
        },
        {
          "name": "Sixuan Mao",
          "affiliation": null
        },
        {
          "name": "Xiyuan Zhou",
          "affiliation": null
        },
        {
          "name": "Feng Yin",
          "affiliation": null
        },
        {
          "name": "Sergios Theodoridis",
          "affiliation": null
        },
        {
          "name": "Zhaoyu Zhang",
          "affiliation": null
        }
      ],
      "abstract": "With the rapid evolution of global autonomous driving technology, the demand for its core sensing hardware, Light Detection and Ranging (LiDAR), is escalating. As the light source part of the LiDAR system, lasers, particularly the cutting-edge Photonic Crystal Surface Emitting Lasers (PCSEL), have correspondingly attracted extensive research attention. The conventional manual design and optimization of PCSEL typically require expertise in semiconductor physics and months of dedicated effort to achieve satisfactory results. While AI-driven approaches can expedite this process, laser designers still need to invest time in learning the AI algorithms involved. Meanwhile Large Language Models (LLMs), leveraging their powerful reasoning abilities, can effectively comprehend natural language and provide constructive feedback in multi-turn dialogues. They have already demonstrated potential to assist humans in scientific fields such as robotics design and chemical discovery. A question naturally arises is: Can LLMs transform the lasers design process? This paper proposes a novel human-AI co-design paradigm to show that LLMs can guide the laser design and optimization process both conceptually and technically. Specifically, by simply having conversations, GPT assisted us with writing both Finite Difference Time Domain (FDTD) simulation code and deep reinforcement learning (RL) code to acquire the optimized PCSEL solution, spanning from the proposition of ideas to the realization of algorithms. Given that GPT will perform better when given detailed and specific prompts, we break down the PCSEL design problem into a series of sub-problems and converse with GPT by posing open-ended heuristic questions rather than definitive commands. We achieved a significant milestone towards self-driving laboratories, that is, a fully automated AI-driven pipeline, for laser design and production.",
      "publishedDate": "2021-04-25T12:55:22Z",
      "updatedDate": "2025-11-23T03:30:50Z",
      "primaryCategory": "physics.optics",
      "arxivCategories": [
        "physics.optics"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2104.12145v3",
      "arxivUrl": "https://arxiv.org/abs/2104.12145",
      "comment": "14 pages, 10 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "agents",
          "tool-use",
          "reasoning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    }
  ]
}