{
  "year": "2026",
  "count": 162,
  "papers": [
    {
      "id": "2601.00791",
      "title": "Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning",
      "authors": [
        {
          "name": "Valentin Noël",
          "affiliation": null
        }
      ],
      "abstract": "We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\\text{MW}} = 1.16 \\times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.",
      "publishedDate": "2026-01-02T18:49:37Z",
      "updatedDate": "2026-01-02T18:49:37Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00791v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00791",
      "comment": "58 pages, 19 figures, Under Review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00756",
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "authors": [
        {
          "name": "Thomas Katraouras",
          "affiliation": null
        },
        {
          "name": "Dimitrios Rafailidis",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.",
      "publishedDate": "2026-01-02T17:22:34Z",
      "updatedDate": "2026-01-02T17:22:34Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00756v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00756",
      "comment": "Accepted to the 41st ACM/SIGAPP Symposium on Applied Computing (SAC '26)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00694",
      "title": "A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference",
      "authors": [
        {
          "name": "Qingwen Pu",
          "affiliation": null
        },
        {
          "name": "Kun Xie",
          "affiliation": null
        },
        {
          "name": "Hong Yang",
          "affiliation": null
        },
        {
          "name": "Guocong Zhai",
          "affiliation": null
        }
      ],
      "abstract": "Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.",
      "publishedDate": "2026-01-02T14:13:28Z",
      "updatedDate": "2026-01-02T14:13:28Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00694v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00694",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00635",
      "title": "SEMODS: A Validated Dataset of Open-Source Software Engineering Models",
      "authors": [
        {
          "name": "Alexandra González",
          "affiliation": null
        },
        {
          "name": "Xavier Franch",
          "affiliation": null
        },
        {
          "name": "Silverio Martínez-Fernández",
          "affiliation": null
        }
      ],
      "abstract": "Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.",
      "publishedDate": "2026-01-02T10:38:24Z",
      "updatedDate": "2026-01-02T10:38:24Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00635v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00635",
      "comment": "Accepted at the 3rd ACM international conference on AI Foundation Models and Software Engineering (FORGE 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00497",
      "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
      "authors": [
        {
          "name": "Lev Sorokin",
          "affiliation": null
        },
        {
          "name": "Ivan Vasilev",
          "affiliation": null
        },
        {
          "name": "Ken E. Friedl",
          "affiliation": null
        },
        {
          "name": "Andrea Stocco",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
      "publishedDate": "2026-01-01T22:30:15Z",
      "updatedDate": "2026-01-05T18:03:57Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00497v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00497",
      "comment": "Accepted for publication at the 33th International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "evaluation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00348",
      "title": "Robust Uncertainty Quantification for Factual Generation of Large Language Models",
      "authors": [
        {
          "name": "Yuhao Zhang",
          "affiliation": null
        },
        {
          "name": "Zhongliang Yang",
          "affiliation": null
        },
        {
          "name": "Linna Zhou",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.",
      "publishedDate": "2026-01-01T14:06:58Z",
      "updatedDate": "2026-01-01T14:06:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00348v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00348",
      "comment": "9 pages, 5 tables, 5 figures, accepted to IJCNN 2025",
      "journalRef": "2025 International Joint Conference on Neural Networks (IJCNN), Rome, Italy, 2025, pp. 1-9",
      "doi": "10.1109/IJCNN64981.2025.11227634",
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00282",
      "title": "Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations",
      "authors": [
        {
          "name": "Qianli Wang",
          "affiliation": null
        },
        {
          "name": "Nils Feldhus",
          "affiliation": null
        },
        {
          "name": "Pepa Atanasova",
          "affiliation": null
        },
        {
          "name": "Fedor Splitt",
          "affiliation": null
        },
        {
          "name": "Simon Ostermann",
          "affiliation": null
        },
        {
          "name": "Sebastian Möller",
          "affiliation": null
        },
        {
          "name": "Vera Schmitt",
          "affiliation": null
        }
      ],
      "abstract": "Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\\%) and faithfulness (up to 2.38\\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.",
      "publishedDate": "2026-01-01T09:50:01Z",
      "updatedDate": "2026-01-01T09:50:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00282v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00282",
      "comment": "In submission",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00274",
      "title": "Making Theft Useless: Adulteration-Based Protection of Proprietary Knowledge Graphs in GraphRAG Systems",
      "authors": [
        {
          "name": "Weijie Wang",
          "affiliation": null
        },
        {
          "name": "Peizhuo Lv",
          "affiliation": null
        },
        {
          "name": "Yan Wang",
          "affiliation": null
        },
        {
          "name": "Rujie Dai",
          "affiliation": null
        },
        {
          "name": "Guokun Xu",
          "affiliation": null
        },
        {
          "name": "Qiujian Lv",
          "affiliation": null
        },
        {
          "name": "Hangcheng Liu",
          "affiliation": null
        },
        {
          "name": "Weiqing Huang",
          "affiliation": null
        },
        {
          "name": "Wei Dong",
          "affiliation": null
        },
        {
          "name": "Jiaheng Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) has emerged as a key technique for enhancing Large Language Models (LLMs) with proprietary Knowledge Graphs (KGs) in knowledge-intensive applications. As these KGs often represent an organization's highly valuable intellectual property (IP), they face a significant risk of theft for private use. In this scenario, attackers operate in isolated environments. This private-use threat renders passive defenses like watermarking ineffective, as they require output access for detection. Simultaneously, the low-latency demands of GraphRAG make strong encryption which incurs prohibitive overhead impractical. To address these challenges, we propose AURA, a novel framework based on Data Adulteration designed to make any stolen KG unusable to an adversary. Our framework pre-emptively injects plausible but false adulterants into the KG. For an attacker, these adulterants deteriorate the retrieved context and lead to factually incorrect responses. Conversely, for authorized users, a secret key enables the efficient filtering of all adulterants via encrypted metadata tags before they are passed to the LLM, ensuring query results remain completely accurate. Our evaluation demonstrates the effectiveness of this approach: AURA degrades the performance of unauthorized systems to an accuracy of just 5.3%, while maintaining 100% fidelity for authorized users with negligible overhead. Furthermore, AURA proves robust against various sanitization attempts, retaining 80.2% of its adulterants.",
      "publishedDate": "2026-01-01T09:27:24Z",
      "updatedDate": "2026-01-01T09:27:24Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00274v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00274",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00254",
      "title": "An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems",
      "authors": [
        {
          "name": "Md Hasan Saju",
          "affiliation": null
        },
        {
          "name": "Maher Muhtadi",
          "affiliation": null
        },
        {
          "name": "Akramul Azim",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.",
      "publishedDate": "2026-01-01T08:05:51Z",
      "updatedDate": "2026-01-01T08:05:51Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00254v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00254",
      "comment": null,
      "journalRef": "https://conf.researchr.org/home/cascon-2025",
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "code-generation",
        "agents",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "agents",
          "tool-use",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00213",
      "title": "Overlooked Safety Vulnerability in LLMs: Malicious Intelligent Optimization Algorithm Request and its Jailbreak",
      "authors": [
        {
          "name": "Haoran Gu",
          "affiliation": null
        },
        {
          "name": "Handing Wang",
          "affiliation": null
        },
        {
          "name": "Yi Mei",
          "affiliation": null
        },
        {
          "name": "Mengjie Zhang",
          "affiliation": null
        },
        {
          "name": "Yaochu Jin",
          "affiliation": null
        }
      ],
      "abstract": "The widespread deployment of large language models (LLMs) has raised growing concerns about their misuse risks and associated safety issues. While prior studies have examined the safety of LLMs in general usage, code generation, and agent-based applications, their vulnerabilities in automated algorithm design remain underexplored. To fill this gap, this study investigates this overlooked safety vulnerability, with a particular focus on intelligent optimization algorithm design, given its prevalent use in complex decision-making scenarios. We introduce MalOptBench, a benchmark consisting of 60 malicious optimization algorithm requests, and propose MOBjailbreak, a jailbreak method tailored for this scenario. Through extensive evaluation of 13 mainstream LLMs including the latest GPT-5 and DeepSeek-V3.1, we reveal that most models remain highly susceptible to such attacks, with an average attack success rate of 83.59% and an average harmfulness score of 4.28 out of 5 on original harmful prompts, and near-complete failure under MOBjailbreak. Furthermore, we assess state-of-the-art plug-and-play defenses that can be applied to closed-source models, and find that they are only marginally effective against MOBjailbreak and prone to exaggerated safety behaviors. These findings highlight the urgent need for stronger alignment techniques to safeguard LLMs against misuse in algorithm design.",
      "publishedDate": "2026-01-01T05:14:32Z",
      "updatedDate": "2026-01-01T05:14:32Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00213v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00213",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00202",
      "title": "Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models",
      "authors": [
        {
          "name": "Wang Xing",
          "affiliation": null
        },
        {
          "name": "Wei Song",
          "affiliation": null
        },
        {
          "name": "Siyu Lin",
          "affiliation": null
        },
        {
          "name": "Chen Wu",
          "affiliation": null
        },
        {
          "name": "Zhesi Li",
          "affiliation": null
        },
        {
          "name": "Man Wang",
          "affiliation": null
        }
      ],
      "abstract": "Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.",
      "publishedDate": "2026-01-01T04:38:00Z",
      "updatedDate": "2026-01-01T04:38:00Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00202v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00202",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00770",
      "title": "LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization",
      "authors": [
        {
          "name": "Simon Paquette-Greenbaum",
          "affiliation": null
        },
        {
          "name": "Jiangbo Yu",
          "affiliation": null
        }
      ],
      "abstract": "Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.",
      "publishedDate": "2026-01-02T18:02:13Z",
      "updatedDate": "2026-01-02T18:02:13Z",
      "primaryCategory": "cs.CE",
      "arxivCategories": [
        "cs.CE",
        "cs.AI",
        "econ.GN"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00770v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00770",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00596",
      "title": "Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence",
      "authors": [
        {
          "name": "Sumanth Balaji",
          "affiliation": null
        },
        {
          "name": "Piyush Mishra",
          "affiliation": null
        },
        {
          "name": "Aashraya Sachdeva",
          "affiliation": null
        },
        {
          "name": "Suraj Agrawal",
          "affiliation": null
        }
      ],
      "abstract": "Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.",
      "publishedDate": "2026-01-02T07:21:23Z",
      "updatedDate": "2026-01-02T07:21:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00596v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00596",
      "comment": "17 pages, 3 figures, preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00516",
      "title": "Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI",
      "authors": [
        {
          "name": "Laksh Advani",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both \"wrong plan for this task\" and \"malformed plan structure.\" On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.",
      "publishedDate": "2026-01-02T00:27:11Z",
      "updatedDate": "2026-01-02T00:27:11Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00516v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00516",
      "comment": "Accepted to AAAI Trustagent 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00268",
      "title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity",
      "authors": [
        {
          "name": "Doyoung Kim",
          "affiliation": null
        },
        {
          "name": "Zhiwei Ren",
          "affiliation": null
        },
        {
          "name": "Jie Hao",
          "affiliation": null
        },
        {
          "name": "Zhongkai Sun",
          "affiliation": null
        },
        {
          "name": "Lichao Wang",
          "affiliation": null
        },
        {
          "name": "Xiyao Ma",
          "affiliation": null
        },
        {
          "name": "Zack Ye",
          "affiliation": null
        },
        {
          "name": "Xu Han",
          "affiliation": null
        },
        {
          "name": "Jun Yin",
          "affiliation": null
        },
        {
          "name": "Heng Ji",
          "affiliation": null
        },
        {
          "name": "Wei Shen",
          "affiliation": null
        },
        {
          "name": "Xing Fan",
          "affiliation": null
        },
        {
          "name": "Benjamin Yao",
          "affiliation": null
        },
        {
          "name": "Chenlei Guo",
          "affiliation": null
        }
      ],
      "abstract": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.",
      "publishedDate": "2026-01-01T09:19:20Z",
      "updatedDate": "2026-01-01T09:19:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00268v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00268",
      "comment": "26 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00227",
      "title": "FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems",
      "authors": [
        {
          "name": "Shanli Xing",
          "affiliation": null
        },
        {
          "name": "Yiyan Zhai",
          "affiliation": null
        },
        {
          "name": "Alexander Jiang",
          "affiliation": null
        },
        {
          "name": "Yixin Dong",
          "affiliation": null
        },
        {
          "name": "Yong Wu",
          "affiliation": null
        },
        {
          "name": "Zihao Ye",
          "affiliation": null
        },
        {
          "name": "Charlie Ruan",
          "affiliation": null
        },
        {
          "name": "Yingyi Huang",
          "affiliation": null
        },
        {
          "name": "Yineng Zhang",
          "affiliation": null
        },
        {
          "name": "Liangsheng Yin",
          "affiliation": null
        },
        {
          "name": "Aksara Bayyapu",
          "affiliation": null
        },
        {
          "name": "Luis Ceze",
          "affiliation": null
        },
        {
          "name": "Tianqi Chen",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.",
      "publishedDate": "2026-01-01T06:18:53Z",
      "updatedDate": "2026-01-01T06:18:53Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00227v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00227",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00513",
      "title": "When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents",
      "authors": [
        {
          "name": "Laksh Advani",
          "affiliation": null
        }
      ],
      "abstract": "Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.",
      "publishedDate": "2026-01-01T23:54:15Z",
      "updatedDate": "2026-01-01T23:54:15Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00513v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00513",
      "comment": "Accepted to Trustagent workshop AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00777",
      "title": "Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection",
      "authors": [
        {
          "name": "Akanksha Chuchra",
          "affiliation": null
        },
        {
          "name": "Shukesh Reddy",
          "affiliation": null
        },
        {
          "name": "Sudeepta Mishra",
          "affiliation": null
        },
        {
          "name": "Abhijit Das",
          "affiliation": null
        },
        {
          "name": "Abhinav Dhall",
          "affiliation": null
        }
      ],
      "abstract": "While Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have shown strong generalisation in detecting image and video deepfakes, their use for audio deepfake detection remains largely unexplored. In this work, we aim to explore the potential of MLLMs for audio deepfake detection. Combining audio inputs with a range of text prompts as queries to find out the viability of MLLMs to learn robust representations across modalities for audio deepfake detection. Therefore, we attempt to explore text-aware and context-rich, question-answer based prompts with binary decisions. We hypothesise that such a feature-guided reasoning will help in facilitating deeper multimodal understanding and enable robust feature learning for audio deepfake detection. We evaluate the performance of two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, in two evaluation modes: (a) zero-shot and (b) fine-tuned. Our experiments demonstrate that combining audio with a multi-prompt approach could be a viable way forward for audio deepfake detection. Our experiments show that the models perform poorly without task-specific training and struggle to generalise to out-of-domain data. However, they achieve good performance on in-domain data with minimal supervision, indicating promising potential for audio deepfake detection.",
      "publishedDate": "2026-01-02T18:17:22Z",
      "updatedDate": "2026-01-02T18:17:22Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00777v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00777",
      "comment": "Accepted at IJCB 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00747",
      "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
      "authors": [
        {
          "name": "Max Ruiz Luyten",
          "affiliation": null
        },
        {
          "name": "Mihaela van der Schaar",
          "affiliation": null
        }
      ],
      "abstract": "State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.",
      "publishedDate": "2026-01-02T17:10:31Z",
      "updatedDate": "2026-01-02T17:10:31Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00747v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00747",
      "comment": "56 pages, 9 figures, submitted to Twenty-Ninth Annual Conference on Artificial Intelligence and Statistics",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00736",
      "title": "Exploring the Performance of Large Language Models on Subjective Span Identification Tasks",
      "authors": [
        {
          "name": "Alphaeus Dmonte",
          "affiliation": null
        },
        {
          "name": "Roland Oruche",
          "affiliation": null
        },
        {
          "name": "Tharindu Ranasinghe",
          "affiliation": null
        },
        {
          "name": "Marcos Zampieri",
          "affiliation": null
        },
        {
          "name": "Prasad Calyam",
          "affiliation": null
        }
      ],
      "abstract": "Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.",
      "publishedDate": "2026-01-02T16:30:14Z",
      "updatedDate": "2026-01-02T16:30:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00736v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00736",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00730",
      "title": "Grading Handwritten Engineering Exams with Multimodal Large Language Models",
      "authors": [
        {
          "name": "Janez Perš",
          "affiliation": null
        },
        {
          "name": "Jon Muhovič",
          "affiliation": null
        },
        {
          "name": "Andrej Košir",
          "affiliation": null
        },
        {
          "name": "Boštjan Murovec",
          "affiliation": null
        }
      ],
      "abstract": "Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\\approx$17% at $D_{\\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.",
      "publishedDate": "2026-01-02T16:10:08Z",
      "updatedDate": "2026-01-02T16:10:08Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00730v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00730",
      "comment": "10 pages, 5 figures, 2 tables. Supplementary material available at https://lmi.fe.uni-lj.si/en/janez-pers-2/supplementary-material/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00680",
      "title": "Sigmoid Head for Quality Estimation under Language Ambiguity",
      "authors": [
        {
          "name": "Tu Anh Dinh",
          "affiliation": null
        },
        {
          "name": "Jan Niehues",
          "affiliation": null
        }
      ],
      "abstract": "Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.",
      "publishedDate": "2026-01-02T13:12:28Z",
      "updatedDate": "2026-01-02T13:12:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00680v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00680",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00675",
      "title": "RoboReward: General-Purpose Vision-Language Reward Models for Robotics",
      "authors": [
        {
          "name": "Tony Lee",
          "affiliation": null
        },
        {
          "name": "Andrew Wagenmaker",
          "affiliation": null
        },
        {
          "name": "Karl Pertsch",
          "affiliation": null
        },
        {
          "name": "Percy Liang",
          "affiliation": null
        },
        {
          "name": "Sergey Levine",
          "affiliation": null
        },
        {
          "name": "Chelsea Finn",
          "affiliation": null
        }
      ],
      "abstract": "A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \\textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \\emph{negative examples data augmentation} pipeline that generates calibrated \\emph{negatives} and \\emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.",
      "publishedDate": "2026-01-02T12:47:34Z",
      "updatedDate": "2026-01-02T12:47:34Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00675v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00675",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "robotics",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "robotics",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00575",
      "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
      "authors": [
        {
          "name": "Ishir Garg",
          "affiliation": null
        },
        {
          "name": "Neel Kolhe",
          "affiliation": null
        },
        {
          "name": "Xuandong Zhao",
          "affiliation": null
        },
        {
          "name": "Dawn Song",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/",
      "publishedDate": "2026-01-02T05:26:27Z",
      "updatedDate": "2026-01-02T05:26:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00575v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00575",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00559",
      "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
      "authors": [
        {
          "name": "Jason Quantrill",
          "affiliation": null
        },
        {
          "name": "Noura Khajehnouri",
          "affiliation": null
        },
        {
          "name": "Zihan Guo",
          "affiliation": null
        },
        {
          "name": "Manar H. Alalfi",
          "affiliation": null
        }
      ],
      "abstract": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
      "publishedDate": "2026-01-02T04:17:36Z",
      "updatedDate": "2026-01-02T04:17:36Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00559v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00559",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00506",
      "title": "Rule-Based Approaches to Atomic Sentence Extraction",
      "authors": [
        {
          "name": "Lineesha Kamana",
          "affiliation": null
        },
        {
          "name": "Akshita Ananda Subramanian",
          "affiliation": null
        },
        {
          "name": "Mehuli Ghosh",
          "affiliation": null
        },
        {
          "name": "Suman Saha",
          "affiliation": null
        }
      ],
      "abstract": "Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the \"split-and-rephrase\" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.",
      "publishedDate": "2026-01-01T23:19:51Z",
      "updatedDate": "2026-01-01T23:19:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00506v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00506",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00501",
      "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
      "authors": [
        {
          "name": "Ahmad Rezaei",
          "affiliation": null
        },
        {
          "name": "Mohsen Gholami",
          "affiliation": null
        },
        {
          "name": "Saeed Ranjbar Alvar",
          "affiliation": null
        },
        {
          "name": "Kevin Cannons",
          "affiliation": null
        },
        {
          "name": "Mohammad Asiful Hossain",
          "affiliation": null
        },
        {
          "name": "Zhou Weimin",
          "affiliation": null
        },
        {
          "name": "Shunbo Zhou",
          "affiliation": null
        },
        {
          "name": "Yong Zhang",
          "affiliation": null
        },
        {
          "name": "Mohammad Akbari",
          "affiliation": null
        }
      ],
      "abstract": "We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.",
      "publishedDate": "2026-01-01T22:48:26Z",
      "updatedDate": "2026-01-01T22:48:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00501v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00501",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00482",
      "title": "Multi-Agent Coordinated Rename Refactoring",
      "authors": [
        {
          "name": "Abhiram Bellur",
          "affiliation": null
        },
        {
          "name": "Mohammed Raihan Ullah",
          "affiliation": null
        },
        {
          "name": "Fraol Batole",
          "affiliation": null
        },
        {
          "name": "Mohit Kansara",
          "affiliation": null
        },
        {
          "name": "Masaharu Morimoto",
          "affiliation": null
        },
        {
          "name": "Kai Ishikawa",
          "affiliation": null
        },
        {
          "name": "Haifeng Chen",
          "affiliation": null
        },
        {
          "name": "Yaroslav Zharov",
          "affiliation": null
        },
        {
          "name": "Timofey Bryksin",
          "affiliation": null
        },
        {
          "name": "Tien N. Nguyen",
          "affiliation": null
        },
        {
          "name": "Hridesh Rajan",
          "affiliation": null
        },
        {
          "name": "Danny Dig",
          "affiliation": null
        }
      ],
      "abstract": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat. We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...",
      "publishedDate": "2026-01-01T21:29:43Z",
      "updatedDate": "2026-01-01T21:29:43Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00482v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00482",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "multi-agent",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00448",
      "title": "Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games",
      "authors": [
        {
          "name": "Dimitris Vartziotis",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.",
      "publishedDate": "2026-01-01T19:15:17Z",
      "updatedDate": "2026-01-01T19:15:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00448v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00448",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00388",
      "title": "Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach",
      "authors": [
        {
          "name": "Biao Wu",
          "affiliation": null
        },
        {
          "name": "Meng Fang",
          "affiliation": null
        },
        {
          "name": "Ling Chen",
          "affiliation": null
        },
        {
          "name": "Ke Xu",
          "affiliation": null
        },
        {
          "name": "Tao Cheng",
          "affiliation": null
        },
        {
          "name": "Jun Wang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.",
      "publishedDate": "2026-01-01T16:51:41Z",
      "updatedDate": "2026-01-05T18:27:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00388v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00388",
      "comment": "Accepted to AAAI 2026. Project Page: https://github.com/aialt/geo-r",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00364",
      "title": "The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining",
      "authors": [
        {
          "name": "Jiandong Shao",
          "affiliation": null
        },
        {
          "name": "Raphael Tang",
          "affiliation": null
        },
        {
          "name": "Crystina Zhang",
          "affiliation": null
        },
        {
          "name": "Karin Sevegnani",
          "affiliation": null
        },
        {
          "name": "Pontus Stenetorp",
          "affiliation": null
        },
        {
          "name": "Jianfei Yang",
          "affiliation": null
        },
        {
          "name": "Yao Lu",
          "affiliation": null
        }
      ],
      "abstract": "Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.",
      "publishedDate": "2026-01-01T14:52:06Z",
      "updatedDate": "2026-01-01T14:52:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00364v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00364",
      "comment": "under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00339",
      "title": "Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems",
      "authors": [
        {
          "name": "Alaa Saleh",
          "affiliation": null
        },
        {
          "name": "Praveen Kumar Donta",
          "affiliation": null
        },
        {
          "name": "Roberto Morabito",
          "affiliation": null
        },
        {
          "name": "Sasu Tarkoma",
          "affiliation": null
        },
        {
          "name": "Anders Lindgren",
          "affiliation": null
        },
        {
          "name": "Qiyang Zhang",
          "affiliation": null
        },
        {
          "name": "Schahram Dustdar Susanna Pirttikangas",
          "affiliation": null
        },
        {
          "name": "Lauri Lovén",
          "affiliation": null
        }
      ],
      "abstract": "Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.",
      "publishedDate": "2026-01-01T13:30:38Z",
      "updatedDate": "2026-01-01T13:30:38Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.DC",
        "cs.ET",
        "cs.MA",
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00339v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00339",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00269",
      "title": "FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering",
      "authors": [
        {
          "name": "Chaodong Tong",
          "affiliation": null
        },
        {
          "name": "Qi Zhang",
          "affiliation": null
        },
        {
          "name": "Chen Li",
          "affiliation": null
        },
        {
          "name": "Lei Jiang",
          "affiliation": null
        },
        {
          "name": "Yanbing Liu",
          "affiliation": null
        }
      ],
      "abstract": "Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.",
      "publishedDate": "2026-01-01T09:19:39Z",
      "updatedDate": "2026-01-01T09:19:39Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00269v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00269",
      "comment": "14 pages, 9 figures, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00215",
      "title": "From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning",
      "authors": [
        {
          "name": "Omar Sharif",
          "affiliation": null
        },
        {
          "name": "Eftekhar Hossain",
          "affiliation": null
        },
        {
          "name": "Patrick Ng",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7. To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.",
      "publishedDate": "2026-01-01T05:19:28Z",
      "updatedDate": "2026-01-01T05:19:28Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00215v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00215",
      "comment": "23 pages, 15 Figures, 10 Tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00166",
      "title": "Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description",
      "authors": [
        {
          "name": "Yongmin Yoo",
          "affiliation": null
        },
        {
          "name": "Kris W Pan",
          "affiliation": null
        }
      ],
      "abstract": "Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.",
      "publishedDate": "2026-01-01T02:10:26Z",
      "updatedDate": "2026-01-01T02:10:26Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00166v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00166",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00150",
      "title": "FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications",
      "authors": [
        {
          "name": "Yehui Yang",
          "affiliation": null
        },
        {
          "name": "Dalu Yang",
          "affiliation": null
        },
        {
          "name": "Wenshuo Zhou",
          "affiliation": null
        },
        {
          "name": "Fangxin Shang",
          "affiliation": null
        },
        {
          "name": "Yifan Liu",
          "affiliation": null
        },
        {
          "name": "Jie Ren",
          "affiliation": null
        },
        {
          "name": "Haojun Fei",
          "affiliation": null
        },
        {
          "name": "Qing Yang",
          "affiliation": null
        },
        {
          "name": "Tao Chen",
          "affiliation": null
        }
      ],
      "abstract": "As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.",
      "publishedDate": "2026-01-01T00:42:54Z",
      "updatedDate": "2026-01-01T00:42:54Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.MM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00150v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00150",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00685",
      "title": "Human-like AI-based Auto-Field-in-Field Whole-Brain Radiotherapy Treatment Planning With Conversation Large Language Model Feedback",
      "authors": [
        {
          "name": "Adnan Jafar",
          "affiliation": null
        },
        {
          "name": "An Qin",
          "affiliation": null
        },
        {
          "name": "Gavin Atkins",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Hu",
          "affiliation": null
        },
        {
          "name": "Yin Gao",
          "affiliation": null
        },
        {
          "name": "Xun Jia",
          "affiliation": null
        }
      ],
      "abstract": "Whole-brain radiotherapy (WBRT) is a common treatment due to its simplicity and effectiveness. While automated Field-in-Field (Auto-FiF) functions assist WBRT planning in modern treatment planning systems, it still requires manual approaches for optimal plan generation including patient-specific hyperparameters definition and plan refinement based on quality feedback. This study introduces an automated WBRT planning pipeline that integrates a deep learning (DL) Hyperparameter Prediction model for patient-specific parameter generation and a large-language model (LLM)-based conversational interface for interactive plan refinement. The Hyperparameter Prediction module was trained on 55 WBRT cases using geometric features of clinical target volume (CTV) and organs at risk (OARs) to determine optimal Auto-FiF settings in RayStation treatment planning system. Plans were generated under predicted hyperparameters. For cases in which the generated plan was suboptimal, quality feedback via voice input was captured by a Conversation module, transcribed using Whisper, and interpreted by GPT-4o to adjust planning settings. Plan quality was evaluated in 15 independent cases using clinical metrics and expert review, and model explainability was supported through analysis of feature importance. Fourteen of 15 DL-generated plans were clinically acceptable. Normalized to identical CTV D95% as the clinical plans, the DL-generated and clinical plans showed no statistically significant differences in doses to the eyes, lenses, or CTV dose metrics D1% and D99%. The DL-based planning required under 1 minute of computation and achieved total workflow execution in approximately 7 minutes with a single mouse click, compared to 15 minutes for manual planning. In cases requiring adjustment, the Conversational module successfully improved dose conformity and hotspot reduction.",
      "publishedDate": "2026-01-02T13:23:12Z",
      "updatedDate": "2026-01-02T13:23:12Z",
      "primaryCategory": "physics.med-ph",
      "arxivCategories": [
        "physics.med-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00685v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00685",
      "comment": "22 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00742",
      "title": "Materials Informatics: Emergence To Autonomous Discovery In The Age Of AI",
      "authors": [
        {
          "name": "Turab Lookman",
          "affiliation": null
        },
        {
          "name": "YuJie Liu",
          "affiliation": null
        },
        {
          "name": "Zhibin Gao",
          "affiliation": null
        }
      ],
      "abstract": "This perspective explores the evolution of materials informatics, from its foundational roots in physics and information theory to its maturation through artificial intelligence (AI). We trace the field's trajectory from early milestones to the transformative impact of the Materials Genome Initiative and the recent advent of large language models (LLMs). Rather than a mere toolkit, we present materials informatics as an evolving ecosystem, reviewing key methodologies such as Bayesian Optimization, Reinforcement Learning, and Transformers that drive inverse design and autonomous self-driving laboratories. We specifically address the practical challenges of LLM integration, comparing specialist versus generalist models and discussing solutions for uncertainty quantification. Looking forward, we assess the transition of AI from a predictive tool to a collaborative research partner. By leveraging active learning and retrieval-augmented generation (RAG), the field is moving toward a new era of autonomous materials science, increasingly characterized by \"human-out-of-the-loop\" discovery processes.",
      "publishedDate": "2026-01-02T16:53:19Z",
      "updatedDate": "2026-01-02T16:53:19Z",
      "primaryCategory": "physics.comp-ph",
      "arxivCategories": [
        "physics.comp-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00742v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00742",
      "comment": "44 pages, 14 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "agents"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00536",
      "title": "Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends",
      "authors": [
        {
          "name": "Yuelyu Ji",
          "affiliation": null
        },
        {
          "name": "Zhuochun Li",
          "affiliation": null
        },
        {
          "name": "Rui Meng",
          "affiliation": null
        },
        {
          "name": "Daqing He",
          "affiliation": null
        }
      ],
      "abstract": "Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \\emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.",
      "publishedDate": "2026-01-02T02:38:01Z",
      "updatedDate": "2026-01-02T02:38:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00536v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00536",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00526",
      "title": "Federated Customization of Large Models: Approaches, Experiments, and Insights",
      "authors": [
        {
          "name": "Yuchuan Ye",
          "affiliation": null
        },
        {
          "name": "Ming Ding",
          "affiliation": null
        },
        {
          "name": "Youjia Chen",
          "affiliation": null
        },
        {
          "name": "Peng Cheng",
          "affiliation": null
        },
        {
          "name": "Dusit Niyato",
          "affiliation": null
        }
      ],
      "abstract": "In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.",
      "publishedDate": "2026-01-02T01:45:52Z",
      "updatedDate": "2026-01-02T01:45:52Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.DC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00526v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00526",
      "comment": "8 pages, 1 figure",
      "journalRef": null,
      "doi": "10.1109/MNET.2025.3648812",
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00509",
      "title": "Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback",
      "authors": [
        {
          "name": "Vidyut Sriram",
          "affiliation": null
        },
        {
          "name": "Sawan Pandita",
          "affiliation": null
        },
        {
          "name": "Achintya Lakshmanan",
          "affiliation": null
        },
        {
          "name": "Aneesh Shamraj",
          "affiliation": null
        },
        {
          "name": "Suman Saha",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on \"stubborn\" models.",
      "publishedDate": "2026-01-01T23:34:00Z",
      "updatedDate": "2026-01-01T23:34:00Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00509v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00509",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00376",
      "title": "In Line with Context: Repository-Level Code Generation via Context Inlining",
      "authors": [
        {
          "name": "Chao Hu",
          "affiliation": null
        },
        {
          "name": "Wenhao Zeng",
          "affiliation": null
        },
        {
          "name": "Yuling Shi",
          "affiliation": null
        },
        {
          "name": "Beijun Shen",
          "affiliation": null
        },
        {
          "name": "Xiaodong Gu",
          "affiliation": null
        }
      ],
      "abstract": "Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.",
      "publishedDate": "2026-01-01T15:56:24Z",
      "updatedDate": "2026-01-01T15:56:24Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00376v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00376",
      "comment": "Accepted to FSE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "rag",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00216",
      "title": "From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark",
      "authors": [
        {
          "name": "Jinning Zhang",
          "affiliation": null
        },
        {
          "name": "Jie Song",
          "affiliation": null
        },
        {
          "name": "Wenhui Tu",
          "affiliation": null
        },
        {
          "name": "Zecheng Li",
          "affiliation": null
        },
        {
          "name": "Jingxuan Li",
          "affiliation": null
        },
        {
          "name": "Jin Li",
          "affiliation": null
        },
        {
          "name": "Xuan Liu",
          "affiliation": null
        },
        {
          "name": "Taole Sha",
          "affiliation": null
        },
        {
          "name": "Zichen Wei",
          "affiliation": null
        },
        {
          "name": "Yan Li",
          "affiliation": null
        }
      ],
      "abstract": "In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.",
      "publishedDate": "2026-01-01T05:20:54Z",
      "updatedDate": "2026-01-01T05:20:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00216v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00216",
      "comment": "35 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00641",
      "title": "Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs",
      "authors": [
        {
          "name": "Nils Rautenberg",
          "affiliation": null
        },
        {
          "name": "Sven Schippkus",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting. We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer. Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.",
      "publishedDate": "2026-01-02T10:52:33Z",
      "updatedDate": "2026-01-02T10:52:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00641v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00641",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00624",
      "title": "Do Chatbot LLMs Talk Too Much? The YapBench Benchmark",
      "authors": [
        {
          "name": "Vadim Borisov",
          "affiliation": null
        },
        {
          "name": "Michael Gröger",
          "affiliation": null
        },
        {
          "name": "Mina Mikhael",
          "affiliation": null
        },
        {
          "name": "Richard H. Schreiber",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality. We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores. YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.",
      "publishedDate": "2026-01-02T09:43:52Z",
      "updatedDate": "2026-01-02T09:43:52Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00624v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00624",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "tool-use",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00504",
      "title": "MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation",
      "authors": [
        {
          "name": "Miaowei Wang",
          "affiliation": null
        },
        {
          "name": "Jakub Zadrożny",
          "affiliation": null
        },
        {
          "name": "Oisin Mac Aodha",
          "affiliation": null
        },
        {
          "name": "Amir Vaxman",
          "affiliation": null
        }
      ],
      "abstract": "Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.",
      "publishedDate": "2026-01-01T22:56:37Z",
      "updatedDate": "2026-01-01T22:56:37Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00504v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00504",
      "comment": "AAAI2026 Accepted",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00469",
      "title": "DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis",
      "authors": [
        {
          "name": "Negin Ayoughi",
          "affiliation": null
        },
        {
          "name": "David Dewar",
          "affiliation": null
        },
        {
          "name": "Shiva Nejati",
          "affiliation": null
        },
        {
          "name": "Mehrdad Sabetzadeh",
          "affiliation": null
        }
      ],
      "abstract": "Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.",
      "publishedDate": "2026-01-01T20:48:15Z",
      "updatedDate": "2026-01-05T17:09:37Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00469v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00469",
      "comment": "Accepted for publication in ICSE-SEIP 2026",
      "journalRef": null,
      "doi": "10.1145/3786583.3786879",
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00224",
      "title": "Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback",
      "authors": [
        {
          "name": "Yan Sun",
          "affiliation": null
        },
        {
          "name": "Ming Cai",
          "affiliation": null
        },
        {
          "name": "Stanley Kok",
          "affiliation": null
        }
      ],
      "abstract": "As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.",
      "publishedDate": "2026-01-01T06:10:06Z",
      "updatedDate": "2026-01-01T06:10:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00224v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00224",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02298",
      "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)",
      "authors": [
        {
          "name": "Mahmoud Elgenedy",
          "affiliation": null
        }
      ],
      "abstract": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.",
      "publishedDate": "2026-01-05T17:33:16Z",
      "updatedDate": "2026-01-05T17:33:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "eess.SP"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02298v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02298",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.02186",
      "title": "Toward Global Large Language Models in Medicine",
      "authors": [
        {
          "name": "Rui Yang",
          "affiliation": null
        },
        {
          "name": "Huitao Li",
          "affiliation": null
        },
        {
          "name": "Weihao Xuan",
          "affiliation": null
        },
        {
          "name": "Heli Qi",
          "affiliation": null
        },
        {
          "name": "Xin Li",
          "affiliation": null
        },
        {
          "name": "Kunyu Yu",
          "affiliation": null
        },
        {
          "name": "Yingjian Chen",
          "affiliation": null
        },
        {
          "name": "Rongrong Wang",
          "affiliation": null
        },
        {
          "name": "Jacques Behmoaras",
          "affiliation": null
        },
        {
          "name": "Tianxi Cai",
          "affiliation": null
        },
        {
          "name": "Bibhas Chakraborty",
          "affiliation": null
        },
        {
          "name": "Qingyu Chen",
          "affiliation": null
        },
        {
          "name": "Lionel Tim-Ee Cheng",
          "affiliation": null
        },
        {
          "name": "Marie-Louise Damwanza",
          "affiliation": null
        },
        {
          "name": "Chido Dzinotyiwei",
          "affiliation": null
        },
        {
          "name": "Aosong Feng",
          "affiliation": null
        },
        {
          "name": "Chuan Hong",
          "affiliation": null
        },
        {
          "name": "Yusuke Iwasawa",
          "affiliation": null
        },
        {
          "name": "Yuhe Ke",
          "affiliation": null
        },
        {
          "name": "Linah Kitala",
          "affiliation": null
        },
        {
          "name": "Taehoon Ko",
          "affiliation": null
        },
        {
          "name": "Jisan Lee",
          "affiliation": null
        },
        {
          "name": "Irene Li",
          "affiliation": null
        },
        {
          "name": "Jonathan Chong Kai Liew",
          "affiliation": null
        },
        {
          "name": "Hongfang Liu",
          "affiliation": null
        },
        {
          "name": "Lian Leng Low",
          "affiliation": null
        },
        {
          "name": "Edison Marrese-Taylor",
          "affiliation": null
        },
        {
          "name": "Yutaka Matsuo",
          "affiliation": null
        },
        {
          "name": "Isheanesu Misi",
          "affiliation": null
        },
        {
          "name": "Yilin Ning",
          "affiliation": null
        },
        {
          "name": "Jasmine Chiat Ling Ong",
          "affiliation": null
        },
        {
          "name": "Marcus Eng Hock Ong",
          "affiliation": null
        },
        {
          "name": "Enrico Petretto",
          "affiliation": null
        },
        {
          "name": "Hossein Rouhizadeh",
          "affiliation": null
        },
        {
          "name": "Abiram Sandralegar",
          "affiliation": null
        },
        {
          "name": "Oren Schreier",
          "affiliation": null
        },
        {
          "name": "Iain Bee Huat Tan",
          "affiliation": null
        },
        {
          "name": "Patrick Tan",
          "affiliation": null
        },
        {
          "name": "Daniel Shu Wei Ting",
          "affiliation": null
        },
        {
          "name": "Junjue Wang",
          "affiliation": null
        },
        {
          "name": "Chunhua Weng",
          "affiliation": null
        },
        {
          "name": "Matthew Yu Heng Wong",
          "affiliation": null
        },
        {
          "name": "Fang Wu",
          "affiliation": null
        },
        {
          "name": "Yunze Xiao",
          "affiliation": null
        },
        {
          "name": "Xuhai Xu",
          "affiliation": null
        },
        {
          "name": "Qingcheng Zeng",
          "affiliation": null
        },
        {
          "name": "Zhuo Zheng",
          "affiliation": null
        },
        {
          "name": "Yifan Peng",
          "affiliation": null
        },
        {
          "name": "Douglas Teodoro",
          "affiliation": null
        },
        {
          "name": "Nan Liu",
          "affiliation": null
        }
      ],
      "abstract": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.",
      "publishedDate": "2026-01-05T15:05:49Z",
      "updatedDate": "2026-01-05T15:05:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02186v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02186",
      "comment": "182 pages, 65 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02179",
      "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
      "authors": [
        {
          "name": "Caiqi Zhang",
          "affiliation": null
        },
        {
          "name": "Ruihan Yang",
          "affiliation": null
        },
        {
          "name": "Xiaochen Zhu",
          "affiliation": null
        },
        {
          "name": "Chengzu Li",
          "affiliation": null
        },
        {
          "name": "Tiancheng Hu",
          "affiliation": null
        },
        {
          "name": "Yijiang River Dong",
          "affiliation": null
        },
        {
          "name": "Deqing Yang",
          "affiliation": null
        },
        {
          "name": "Nigel Collier",
          "affiliation": null
        }
      ],
      "abstract": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
      "publishedDate": "2026-01-05T14:58:04Z",
      "updatedDate": "2026-01-05T14:58:04Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02179v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02179",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02078",
      "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot",
      "authors": [
        {
          "name": "Chenghao Yin",
          "affiliation": null
        },
        {
          "name": "Da Huang",
          "affiliation": null
        },
        {
          "name": "Di Yang",
          "affiliation": null
        },
        {
          "name": "Jichao Wang",
          "affiliation": null
        },
        {
          "name": "Nanshu Zhao",
          "affiliation": null
        },
        {
          "name": "Chen Xu",
          "affiliation": null
        },
        {
          "name": "Wenjun Sun",
          "affiliation": null
        },
        {
          "name": "Linjie Hou",
          "affiliation": null
        },
        {
          "name": "Zhijun Li",
          "affiliation": null
        },
        {
          "name": "Junhui Wu",
          "affiliation": null
        },
        {
          "name": "Zhaobo Liu",
          "affiliation": null
        },
        {
          "name": "Zhen Xiao",
          "affiliation": null
        },
        {
          "name": "Sheng Zhang",
          "affiliation": null
        },
        {
          "name": "Lei Bao",
          "affiliation": null
        },
        {
          "name": "Rui Feng",
          "affiliation": null
        },
        {
          "name": "Zhenquan Pang",
          "affiliation": null
        },
        {
          "name": "Jiayu Li",
          "affiliation": null
        },
        {
          "name": "Qian Wang",
          "affiliation": null
        },
        {
          "name": "Maoqing Yao",
          "affiliation": null
        }
      ],
      "abstract": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.",
      "publishedDate": "2026-01-05T12:59:39Z",
      "updatedDate": "2026-01-05T12:59:39Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02078v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02078",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "prompting",
        "tool-use",
        "rag",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "tool-use",
          "rag",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02071",
      "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations",
      "authors": [
        {
          "name": "Adeshola Okubena",
          "affiliation": null
        },
        {
          "name": "Yusuf Ali Mohammed",
          "affiliation": null
        },
        {
          "name": "Moe Elbadawi",
          "affiliation": null
        }
      ],
      "abstract": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.",
      "publishedDate": "2026-01-05T12:50:50Z",
      "updatedDate": "2026-01-05T12:50:50Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02071v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02071",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01925",
      "title": "AR-MOT: Autoregressive Multi-object Tracking",
      "authors": [
        {
          "name": "Lianjie Jia",
          "affiliation": null
        },
        {
          "name": "Yuhan Wu",
          "affiliation": null
        },
        {
          "name": "Binghao Ran",
          "affiliation": null
        },
        {
          "name": "Yifan Wang",
          "affiliation": null
        },
        {
          "name": "Lijun Wang",
          "affiliation": null
        },
        {
          "name": "Huchuan Lu",
          "affiliation": null
        }
      ],
      "abstract": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
      "publishedDate": "2026-01-05T09:17:28Z",
      "updatedDate": "2026-01-05T09:17:28Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01925v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01925",
      "comment": "12 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01868",
      "title": "DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs",
      "authors": [
        {
          "name": "Jinghan Ru",
          "affiliation": null
        },
        {
          "name": "Siyuan Yan",
          "affiliation": null
        },
        {
          "name": "Yuguo Yin",
          "affiliation": null
        },
        {
          "name": "Yuexian Zou",
          "affiliation": null
        },
        {
          "name": "Zongyuan Ge",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.",
      "publishedDate": "2026-01-05T07:55:36Z",
      "updatedDate": "2026-01-05T07:55:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01868v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01868",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01836",
      "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "authors": [
        {
          "name": "Dasol Choi",
          "affiliation": null
        },
        {
          "name": "DongGeon Lee",
          "affiliation": null
        },
        {
          "name": "Brigitta Jesica Kartono",
          "affiliation": null
        },
        {
          "name": "Helena Berndt",
          "affiliation": null
        },
        {
          "name": "Taeyoun Kwon",
          "affiliation": null
        },
        {
          "name": "Joonwon Jang",
          "affiliation": null
        },
        {
          "name": "Haon Park",
          "affiliation": null
        },
        {
          "name": "Hwanjo Yu",
          "affiliation": null
        },
        {
          "name": "Minsuk Kahng",
          "affiliation": null
        }
      ],
      "abstract": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
      "publishedDate": "2026-01-05T06:57:45Z",
      "updatedDate": "2026-01-05T06:57:45Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01836v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01836",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01768",
      "title": "Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation",
      "authors": [
        {
          "name": "Meiman Xiao",
          "affiliation": null
        },
        {
          "name": "Ante Wang",
          "affiliation": null
        },
        {
          "name": "Qingguo Hu",
          "affiliation": null
        },
        {
          "name": "Zhongjian Miao",
          "affiliation": null
        },
        {
          "name": "Huangjun Shen",
          "affiliation": null
        },
        {
          "name": "Longyue Wang",
          "affiliation": null
        },
        {
          "name": "Weihua Luo",
          "affiliation": null
        },
        {
          "name": "Jinsong Su",
          "affiliation": null
        }
      ],
      "abstract": "Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.",
      "publishedDate": "2026-01-05T03:49:14Z",
      "updatedDate": "2026-01-05T03:49:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01768v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01768",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01718",
      "title": "Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications",
      "authors": [
        {
          "name": "YuanLab. ai",
          "affiliation": null
        },
        {
          "name": ":",
          "affiliation": null
        },
        {
          "name": "Shawn Wu",
          "affiliation": null
        },
        {
          "name": "Sean Wang",
          "affiliation": null
        },
        {
          "name": "Louie Li",
          "affiliation": null
        },
        {
          "name": "Darcy Chen",
          "affiliation": null
        },
        {
          "name": "Allen Wang",
          "affiliation": null
        },
        {
          "name": "Jiangang Luo",
          "affiliation": null
        },
        {
          "name": "Xudong Zhao",
          "affiliation": null
        },
        {
          "name": "Joseph Shen",
          "affiliation": null
        },
        {
          "name": "Gawain Ma",
          "affiliation": null
        },
        {
          "name": "Jasper Jia",
          "affiliation": null
        },
        {
          "name": "Marcus Mao",
          "affiliation": null
        },
        {
          "name": "Claire Wang",
          "affiliation": null
        },
        {
          "name": "Hunter He",
          "affiliation": null
        },
        {
          "name": "Carol Wang",
          "affiliation": null
        },
        {
          "name": "Zera Zhang",
          "affiliation": null
        },
        {
          "name": "Jason Wang",
          "affiliation": null
        },
        {
          "name": "Chonly Shen",
          "affiliation": null
        },
        {
          "name": "Leo Zhang",
          "affiliation": null
        },
        {
          "name": "Logan Chen",
          "affiliation": null
        },
        {
          "name": "Qasim Meng",
          "affiliation": null
        },
        {
          "name": "James Gong",
          "affiliation": null
        },
        {
          "name": "Danied Zhao",
          "affiliation": null
        },
        {
          "name": "Penn Zheng",
          "affiliation": null
        },
        {
          "name": "Owen Zhu",
          "affiliation": null
        },
        {
          "name": "Tong Yu",
          "affiliation": null
        }
      ],
      "abstract": "We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.",
      "publishedDate": "2026-01-05T01:44:09Z",
      "updatedDate": "2026-01-05T01:44:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01718v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01718",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01609",
      "title": "Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration",
      "authors": [
        {
          "name": "Albert Sadowski",
          "affiliation": null
        },
        {
          "name": "Jarosław A. Chudziak",
          "affiliation": null
        }
      ],
      "abstract": "Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.",
      "publishedDate": "2026-01-04T17:19:20Z",
      "updatedDate": "2026-01-04T17:19:20Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01609v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01609",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01592",
      "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
      "authors": [
        {
          "name": "Xin Wang",
          "affiliation": null
        },
        {
          "name": "Yunhao Chen",
          "affiliation": null
        },
        {
          "name": "Juncheng Li",
          "affiliation": null
        },
        {
          "name": "Yixu Wang",
          "affiliation": null
        },
        {
          "name": "Yang Yao",
          "affiliation": null
        },
        {
          "name": "Tianle Gu",
          "affiliation": null
        },
        {
          "name": "Jie Li",
          "affiliation": null
        },
        {
          "name": "Yan Teng",
          "affiliation": null
        },
        {
          "name": "Xingjun Ma",
          "affiliation": null
        },
        {
          "name": "Yingchun Wang",
          "affiliation": null
        },
        {
          "name": "Xia Hu",
          "affiliation": null
        }
      ],
      "abstract": "The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.",
      "publishedDate": "2026-01-04T16:41:33Z",
      "updatedDate": "2026-01-04T16:41:33Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01592v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01592",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "reasoning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01477",
      "title": "Can Legislation Be Made Machine-Readable in PROLEG?",
      "authors": [
        {
          "name": "May-Myo Zin",
          "affiliation": null
        },
        {
          "name": "Sabine Wehnert",
          "affiliation": null
        },
        {
          "name": "Yuntao Kong",
          "affiliation": null
        },
        {
          "name": "Ha-Thanh Nguyen",
          "affiliation": null
        },
        {
          "name": "Wachara Fungwacharakorn",
          "affiliation": null
        },
        {
          "name": "Jieying Xue",
          "affiliation": null
        },
        {
          "name": "Michał Araszkiewicz",
          "affiliation": null
        },
        {
          "name": "Randy Goebel",
          "affiliation": null
        },
        {
          "name": "Ken Satoh",
          "affiliation": null
        },
        {
          "name": "Le-Minh Nguyen",
          "affiliation": null
        }
      ],
      "abstract": "The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to \"compile\" natural language text to if-then rules, then to further \"compile\" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.",
      "publishedDate": "2026-01-04T10:53:16Z",
      "updatedDate": "2026-01-04T10:53:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01477v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01477",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01432",
      "title": "Personalizing black-box models for nonparametric regression with minimax optimality",
      "authors": [
        {
          "name": "Sai Li",
          "affiliation": null
        },
        {
          "name": "Linjun Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models.",
      "publishedDate": "2026-01-04T08:32:28Z",
      "updatedDate": "2026-01-04T08:32:28Z",
      "primaryCategory": "stat.ME",
      "arxivCategories": [
        "stat.ME",
        "stat.ML"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01432v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01432",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01378",
      "title": "Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification",
      "authors": [
        {
          "name": "Han Yuan",
          "affiliation": null
        },
        {
          "name": "Yilin Wu",
          "affiliation": null
        },
        {
          "name": "Li Zhang",
          "affiliation": null
        },
        {
          "name": "Zheng Ma",
          "affiliation": null
        }
      ],
      "abstract": "Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.",
      "publishedDate": "2026-01-04T05:09:11Z",
      "updatedDate": "2026-01-04T05:09:11Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01378v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01378",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01321",
      "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
      "authors": [
        {
          "name": "Rong Zhou",
          "affiliation": null
        },
        {
          "name": "Dongping Chen",
          "affiliation": null
        },
        {
          "name": "Zihan Jia",
          "affiliation": null
        },
        {
          "name": "Yao Su",
          "affiliation": null
        },
        {
          "name": "Yixin Liu",
          "affiliation": null
        },
        {
          "name": "Yiwen Lu",
          "affiliation": null
        },
        {
          "name": "Dongwei Shi",
          "affiliation": null
        },
        {
          "name": "Yue Huang",
          "affiliation": null
        },
        {
          "name": "Tianyang Xu",
          "affiliation": null
        },
        {
          "name": "Yi Pan",
          "affiliation": null
        },
        {
          "name": "Xinliang Li",
          "affiliation": null
        },
        {
          "name": "Yohannes Abate",
          "affiliation": null
        },
        {
          "name": "Qingyu Chen",
          "affiliation": null
        },
        {
          "name": "Zhengzhong Tu",
          "affiliation": null
        },
        {
          "name": "Yu Yang",
          "affiliation": null
        },
        {
          "name": "Yu Zhang",
          "affiliation": null
        },
        {
          "name": "Qingsong Wen",
          "affiliation": null
        },
        {
          "name": "Gengchen Mai",
          "affiliation": null
        },
        {
          "name": "Sunyang Fu",
          "affiliation": null
        },
        {
          "name": "Jiachen Li",
          "affiliation": null
        },
        {
          "name": "Xuyu Wang",
          "affiliation": null
        },
        {
          "name": "Ziran Wang",
          "affiliation": null
        },
        {
          "name": "Jing Huang",
          "affiliation": null
        },
        {
          "name": "Tianming Liu",
          "affiliation": null
        },
        {
          "name": "Yong Chen",
          "affiliation": null
        },
        {
          "name": "Lichao Sun",
          "affiliation": null
        },
        {
          "name": "Lifang He",
          "affiliation": null
        }
      ],
      "abstract": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.",
      "publishedDate": "2026-01-04T01:17:09Z",
      "updatedDate": "2026-01-04T01:17:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01321v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01321",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "robotics",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "robotics",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01260",
      "title": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance",
      "authors": [
        {
          "name": "Hamad Khan",
          "affiliation": null
        },
        {
          "name": "Saddam Hussain Khan",
          "affiliation": null
        }
      ],
      "abstract": "The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.",
      "publishedDate": "2026-01-03T19:01:33Z",
      "updatedDate": "2026-01-03T19:01:33Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01260v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01260",
      "comment": "28 Pages, Tables 12, Figure 09",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.01239",
      "title": "IO-RAE: Information-Obfuscation Reversible Adversarial Example for Audio Privacy Protection",
      "authors": [
        {
          "name": "Jiajie Zhu",
          "affiliation": null
        },
        {
          "name": "Xia Du",
          "affiliation": null
        },
        {
          "name": "Xiaoyuan Liu",
          "affiliation": null
        },
        {
          "name": "Jizhe Zhou",
          "affiliation": null
        },
        {
          "name": "Qizhen Xu",
          "affiliation": null
        },
        {
          "name": "Zheng Lin",
          "affiliation": null
        },
        {
          "name": "Chi-Man Pun",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancements in artificial intelligence have significantly accelerated the adoption of speech recognition technology, leading to its widespread integration across various applications. However, this surge in usage also highlights a critical issue: audio data is highly vulnerable to unauthorized exposure and analysis, posing significant privacy risks for businesses and individuals. This paper introduces an Information-Obfuscation Reversible Adversarial Example (IO-RAE) framework, the pioneering method designed to safeguard audio privacy using reversible adversarial examples. IO-RAE leverages large language models to generate misleading yet contextually coherent content, effectively preventing unauthorized eavesdropping by humans and Automatic Speech Recognition (ASR) systems. Additionally, we propose the Cumulative Signal Attack technique, which mitigates high-frequency noise and enhances attack efficacy by targeting low-frequency signals. Our approach ensures the protection of audio data without degrading its quality or our ability. Experimental evaluations demonstrate the superiority of our method, achieving a targeted misguidance rate of 96.5% and a remarkable 100% untargeted misguidance rate in obfuscating target keywords across multiple ASR models, including a commercial black-box system from Google. Furthermore, the quality of the recovered audio, measured by the Perceptual Evaluation of Speech Quality score, reached 4.45, comparable to high-quality original recordings. Notably, the recovered audio processed by ASR systems exhibited an error rate of 0%, indicating nearly lossless recovery. These results highlight the practical applicability and effectiveness of our IO-RAE framework in protecting sensitive audio privacy.",
      "publishedDate": "2026-01-03T17:08:35Z",
      "updatedDate": "2026-01-03T17:08:35Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.CR",
        "cs.MM",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01239v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01239",
      "comment": "10 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "tool-use",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01153",
      "title": "SongSage: A Large Musical Language Model with Lyric Generative Pre-training",
      "authors": [
        {
          "name": "Jiani Guo",
          "affiliation": null
        },
        {
          "name": "Jiajia Li",
          "affiliation": null
        },
        {
          "name": "Jie Wu",
          "affiliation": null
        },
        {
          "name": "Zuchao Li",
          "affiliation": null
        },
        {
          "name": "Yujiu Yang",
          "affiliation": null
        },
        {
          "name": "Ping Wang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.",
      "publishedDate": "2026-01-03T10:54:37Z",
      "updatedDate": "2026-01-03T10:54:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01153v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01153",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00995",
      "title": "Grain-Aware Data Transformations: Type-Level Formal Verification at Zero Computational Cost",
      "authors": [
        {
          "name": "Nikos Karayannidis",
          "affiliation": null
        }
      ],
      "abstract": "Data transformation correctness is a major challenge in data engineering: how to verify pipeline accuracy before deployment. Traditional methods involve costly iterative testing, data materialization, and manual error detection, due to the lack of formal approaches to reasoning about data granularity (grain), which can shift during transformations, causing issues like fan traps (metrics duplication) and chasm traps (data loss). We introduce the first formal, mathematical definition of grain, extending it from an informal concept in dimensional modeling to a universal, type-theoretic framework applicable to any data type. Encoding grain into the type system allows compile-time verification of transformation correctness, shifting validation from runtime. We define three core grain relations-equality, ordering, and incomparability-and prove a general grain inference theorem that computes the output grain of equi-joins from input grains using type-level operations. This covers all join scenarios, including comparable and incomparable keys. Together with inference rules for relational operations, this enables verification through schema analysis alone, at zero cost. Our approach allows engineers to verify that entire pipeline DAGs maintain correctness properties, detecting grain-related errors such as fan traps, chasm traps, and aggregation issues before data processing. It emphasizes the importance of grain, focusing on critical characteristics rather than all data details. We provide machine-checked formal proofs in Lean 4, reducing verification costs by 98-99%. Additionally, large language models can automatically generate correctness proofs, shifting human effort from proof writing to proof verification, thus democratizing formal methods in data engineering and supporting confident deployment of AI-generated pipelines with machine-checkable guarantees.",
      "publishedDate": "2026-01-02T22:26:31Z",
      "updatedDate": "2026-01-02T22:26:31Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00995v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00995",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00942",
      "title": "Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures",
      "authors": [
        {
          "name": "Kabir Grover",
          "affiliation": null
        }
      ],
      "abstract": "The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.",
      "publishedDate": "2026-01-02T18:10:10Z",
      "updatedDate": "2026-01-02T18:10:10Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00942v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00942",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00936",
      "title": "Emoji-Based Jailbreaking of Large Language Models",
      "authors": [
        {
          "name": "M P V S Gopinadh",
          "affiliation": null
        },
        {
          "name": "S Mahaboob Hussain",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are integral to modern AI applications, but their safety alignment mechanisms can be bypassed through adversarial prompt engineering. This study investigates emoji-based jailbreaking, where emoji sequences are embedded in textual prompts to trigger harmful and unethical outputs from LLMs. We evaluated 50 emoji-based prompts on four open-source LLMs: Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B. Metrics included jailbreak success rate, safety alignment adherence, and latency, with responses categorized as successful, partial and failed. Results revealed model-specific vulnerabilities: Gemma 2 9B and Mistral 7B exhibited 10 % success rates, while Qwen 2 7B achieved full alignment (0% success). A chi-square test (chi^2 = 32.94, p < 0.001) confirmed significant inter-model differences. While prior works focused on emoji attacks targeting safety judges or classifiers, our empirical analysis examines direct prompt-level vulnerabilities in LLMs. The results reveal limitations in safety mechanisms and highlight the necessity for systematic handling of emoji-based representations in prompt-level safety and alignment pipelines.",
      "publishedDate": "2026-01-02T10:49:06Z",
      "updatedDate": "2026-01-02T10:49:06Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00936v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00936",
      "comment": "7 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02314",
      "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
      "authors": [
        {
          "name": "Sourena Khanzadeh",
          "affiliation": null
        }
      ],
      "abstract": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
      "publishedDate": "2026-01-05T18:05:29Z",
      "updatedDate": "2026-01-05T18:05:29Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02314v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02314",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01885",
      "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents",
      "authors": [
        {
          "name": "Yi Yu",
          "affiliation": null
        },
        {
          "name": "Liuyi Yao",
          "affiliation": null
        },
        {
          "name": "Yuexiang Xie",
          "affiliation": null
        },
        {
          "name": "Qingquan Tan",
          "affiliation": null
        },
        {
          "name": "Jiaqi Feng",
          "affiliation": null
        },
        {
          "name": "Yaliang Li",
          "affiliation": null
        },
        {
          "name": "Libing Wu",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.",
      "publishedDate": "2026-01-05T08:24:16Z",
      "updatedDate": "2026-01-05T08:24:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01885v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01885",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01522",
      "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making",
      "authors": [
        {
          "name": "Danial Amin",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.",
      "publishedDate": "2026-01-04T13:19:27Z",
      "updatedDate": "2026-01-04T13:19:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.ET"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01522v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01522",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01498",
      "title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents",
      "authors": [
        {
          "name": "Bingguang Hao",
          "affiliation": null
        },
        {
          "name": "Zengzhuang Xu",
          "affiliation": null
        },
        {
          "name": "Yuntao Wen",
          "affiliation": null
        },
        {
          "name": "Xinyi Xu",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        },
        {
          "name": "Tong Zhao",
          "affiliation": null
        },
        {
          "name": "Maolin Wang",
          "affiliation": null
        },
        {
          "name": "Long Chen",
          "affiliation": null
        },
        {
          "name": "Dong Wang",
          "affiliation": null
        },
        {
          "name": "Yicheng Chen",
          "affiliation": null
        },
        {
          "name": "Cunyin Peng",
          "affiliation": null
        },
        {
          "name": "Xiangyu Zhao",
          "affiliation": null
        },
        {
          "name": "Chenyi Zhuang",
          "affiliation": null
        },
        {
          "name": "Ji Zhang",
          "affiliation": null
        }
      ],
      "abstract": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.",
      "publishedDate": "2026-01-04T11:56:33Z",
      "updatedDate": "2026-01-04T11:56:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01498v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01498",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "tool-use",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "tool-use",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01357",
      "title": "Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows",
      "authors": [
        {
          "name": "Ke Xiao",
          "affiliation": null
        },
        {
          "name": "Haoze Zhang",
          "affiliation": null
        },
        {
          "name": "Runze Mao",
          "affiliation": null
        },
        {
          "name": "Han Li",
          "affiliation": null
        },
        {
          "name": "Zhi X. Chen",
          "affiliation": null
        }
      ],
      "abstract": "The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.",
      "publishedDate": "2026-01-04T04:00:28Z",
      "updatedDate": "2026-01-04T04:00:28Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "physics.flu-dyn"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01357v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01357",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01241",
      "title": "MCP-SandboxScan: WASM-based Secure Execution and Runtime Analysis for MCP Tools",
      "authors": [
        {
          "name": "Zhuoran Tan",
          "affiliation": null
        },
        {
          "name": "Run Hao",
          "affiliation": null
        },
        {
          "name": "Jeremy Singer",
          "affiliation": null
        },
        {
          "name": "Yutian Tang",
          "affiliation": null
        },
        {
          "name": "Christos Anagnostopoulos",
          "affiliation": null
        }
      ],
      "abstract": "Tool-augmented LLM agents raise new security risks: tool executions can introduce runtime-only behaviors, including prompt injection and unintended exposure of external inputs (e.g., environment secrets or local files). While existing scanners often focus on static artifacts, analyzing runtime behavior is challenging because directly executing untrusted tools can itself be dangerous. We present MCP-SandboxScan, a lightweight framework motivated by the Model Context Protocol (MCP) that safely executes untrusted tools inside a WebAssembly/WASI sandbox and produces auditable reports of external-to-sink exposures. Our prototype (i) extracts LLM-relevant sinks from runtime outputs (prompt/messages and structured tool-return fields), (ii) instantiates external-input candidates from environment values, mounted file contents, and output-surfaced HTTP fetch intents, and (iii) links sources to sinks via snippet-based substring matching. Case studies on three representative tools show that MCP-SandboxScan can surface provenance evidence when external inputs appear in prompt/messages or tool-return payloads, and can expose filesystem capability violations as runtime evidence. We further compare against a lightweight static string-signature baseline and use a micro-benchmark to characterize false negatives under transformations and false positives from short-token collisions.",
      "publishedDate": "2026-01-03T17:25:38Z",
      "updatedDate": "2026-01-03T17:25:38Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01241v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01241",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01090",
      "title": "Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai",
      "authors": [
        {
          "name": "Erica Coppolillo",
          "affiliation": null
        },
        {
          "name": "Luca Luceri",
          "affiliation": null
        },
        {
          "name": "Emilio Ferrara",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms. We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content. These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.",
      "publishedDate": "2026-01-03T06:33:08Z",
      "updatedDate": "2026-01-03T06:33:08Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA",
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01090v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01090",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00994",
      "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems",
      "authors": [
        {
          "name": "Michael Bao",
          "affiliation": null
        }
      ],
      "abstract": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.",
      "publishedDate": "2026-01-02T22:10:09Z",
      "updatedDate": "2026-01-02T22:10:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00994v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00994",
      "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00930",
      "title": "AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation",
      "authors": [
        {
          "name": "Nicolas Bougie",
          "affiliation": null
        },
        {
          "name": "Gian Maria Marconi",
          "affiliation": null
        },
        {
          "name": "Tony Yip",
          "affiliation": null
        },
        {
          "name": "Narimasa Watanabe",
          "affiliation": null
        }
      ],
      "abstract": "Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.",
      "publishedDate": "2026-01-02T03:01:33Z",
      "updatedDate": "2026-01-02T03:01:33Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00930v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00930",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01891",
      "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems",
      "authors": [
        {
          "name": "Niloufar Alipour Talemi",
          "affiliation": null
        },
        {
          "name": "Julia Boone",
          "affiliation": null
        },
        {
          "name": "Fatemeh Afghah",
          "affiliation": null
        }
      ],
      "abstract": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.",
      "publishedDate": "2026-01-05T08:34:17Z",
      "updatedDate": "2026-01-05T08:34:17Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01891v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01891",
      "comment": "Accepted to the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026, GeoCV Workshop",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning",
          "planning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01857",
      "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios",
      "authors": [
        {
          "name": "Defei Xia",
          "affiliation": null
        },
        {
          "name": "Bingfeng Pi",
          "affiliation": null
        },
        {
          "name": "Shenbin Zhang",
          "affiliation": null
        },
        {
          "name": "Song Hua",
          "affiliation": null
        },
        {
          "name": "Yunfei Wei",
          "affiliation": null
        },
        {
          "name": "Lei Zuo",
          "affiliation": null
        }
      ],
      "abstract": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.",
      "publishedDate": "2026-01-05T07:35:12Z",
      "updatedDate": "2026-01-05T07:35:12Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01857v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01857",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01685",
      "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage",
      "authors": [
        {
          "name": "Jinwei Hu",
          "affiliation": null
        },
        {
          "name": "Xinmiao Huang",
          "affiliation": null
        },
        {
          "name": "Youcheng Sun",
          "affiliation": null
        },
        {
          "name": "Yi Dong",
          "affiliation": null
        },
        {
          "name": "Xiaowei Huang",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.",
      "publishedDate": "2026-01-04T22:50:23Z",
      "updatedDate": "2026-01-04T22:50:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01685v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01685",
      "comment": "Under Review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01366",
      "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models",
      "authors": [
        {
          "name": "Zixian Liu",
          "affiliation": null
        },
        {
          "name": "Sihao Liu",
          "affiliation": null
        },
        {
          "name": "Yuqi Zhao",
          "affiliation": null
        }
      ],
      "abstract": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.",
      "publishedDate": "2026-01-04T04:39:39Z",
      "updatedDate": "2026-01-04T04:39:39Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01366v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01366",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "code-generation",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02346",
      "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
      "authors": [
        {
          "name": "Falcon LLM Team",
          "affiliation": null
        },
        {
          "name": "Iheb Chaabane",
          "affiliation": null
        },
        {
          "name": "Puneesh Khanna",
          "affiliation": null
        },
        {
          "name": "Suhail Mohmad",
          "affiliation": null
        },
        {
          "name": "Slim Frikha",
          "affiliation": null
        },
        {
          "name": "Shi Hu",
          "affiliation": null
        },
        {
          "name": "Abdalgader Abubaker",
          "affiliation": null
        },
        {
          "name": "Reda Alami",
          "affiliation": null
        },
        {
          "name": "Mikhail Lubinets",
          "affiliation": null
        },
        {
          "name": "Mohamed El Amine Seddik",
          "affiliation": null
        },
        {
          "name": "Hakim Hacid",
          "affiliation": null
        }
      ],
      "abstract": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
      "publishedDate": "2026-01-05T18:44:27Z",
      "updatedDate": "2026-01-05T18:44:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02346v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02346",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02224",
      "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality",
      "authors": [
        {
          "name": "Fabian Lukassen",
          "affiliation": null
        },
        {
          "name": "Jan Herrmann",
          "affiliation": null
        },
        {
          "name": "Christoph Weisser",
          "affiliation": null
        },
        {
          "name": "Benjamin Saefken",
          "affiliation": null
        },
        {
          "name": "Thomas Kneib",
          "affiliation": null
        }
      ],
      "abstract": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.",
      "publishedDate": "2026-01-05T15:52:20Z",
      "updatedDate": "2026-01-05T15:52:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02224v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02224",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02170",
      "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Haolang Lu",
          "affiliation": null
        },
        {
          "name": "Minghui Pan",
          "affiliation": null
        },
        {
          "name": "Ripeng Li",
          "affiliation": null
        },
        {
          "name": "Guoshun Nan",
          "affiliation": null
        },
        {
          "name": "Jialin Zhuang",
          "affiliation": null
        },
        {
          "name": "Zijie Zhao",
          "affiliation": null
        },
        {
          "name": "Zhongxiang Sun",
          "affiliation": null
        },
        {
          "name": "Kun Wang",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.",
      "publishedDate": "2026-01-05T14:47:41Z",
      "updatedDate": "2026-01-05T14:47:41Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02170v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02170",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02163",
      "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning",
      "authors": [
        {
          "name": "Chuanrui Hu",
          "affiliation": null
        },
        {
          "name": "Xingze Gao",
          "affiliation": null
        },
        {
          "name": "Zuyi Zhou",
          "affiliation": null
        },
        {
          "name": "Dannong Xu",
          "affiliation": null
        },
        {
          "name": "Yi Bai",
          "affiliation": null
        },
        {
          "name": "Xintong Li",
          "affiliation": null
        },
        {
          "name": "Hui Zhang",
          "affiliation": null
        },
        {
          "name": "Tong Li",
          "affiliation": null
        },
        {
          "name": "Chong Zhang",
          "affiliation": null
        },
        {
          "name": "Lidong Bing",
          "affiliation": null
        },
        {
          "name": "Yafeng Deng",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.",
      "publishedDate": "2026-01-05T14:39:43Z",
      "updatedDate": "2026-01-05T14:39:43Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02163v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02163",
      "comment": "16 pages, 6 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02158",
      "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience",
      "authors": [
        {
          "name": "Almaz Ermilov",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.",
      "publishedDate": "2026-01-05T14:36:02Z",
      "updatedDate": "2026-01-05T14:36:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "physics.geo-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02158v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02158",
      "comment": "24 pages, 8 figures, 10 tables; benchmark and code at https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02147",
      "title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models",
      "authors": [
        {
          "name": "Sunny Gupta",
          "affiliation": null
        },
        {
          "name": "Shounak Das",
          "affiliation": null
        },
        {
          "name": "Amit Sethi",
          "affiliation": null
        }
      ],
      "abstract": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.",
      "publishedDate": "2026-01-05T14:22:20Z",
      "updatedDate": "2026-01-05T14:22:20Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02147v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02147",
      "comment": "Accepted at the AAAI 2026 Workshop AIR-FM, Assessing and Improving Reliability of Foundation Models in the Real World",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02076",
      "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
      "authors": [
        {
          "name": "Yingte Shu",
          "affiliation": null
        },
        {
          "name": "Yuchuan Tian",
          "affiliation": null
        },
        {
          "name": "Chao Xu",
          "affiliation": null
        },
        {
          "name": "Yunhe Wang",
          "affiliation": null
        },
        {
          "name": "Hanting Chen",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
      "publishedDate": "2026-01-05T12:57:33Z",
      "updatedDate": "2026-01-05T12:57:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02076v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02076",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02046",
      "title": "Agentic Retoucher for Text-To-Image Generation",
      "authors": [
        {
          "name": "Shaocheng Shen",
          "affiliation": null
        },
        {
          "name": "Jianfeng Liang. Chunlei Cai",
          "affiliation": null
        },
        {
          "name": "Cong Geng",
          "affiliation": null
        },
        {
          "name": "Huiyu Duan",
          "affiliation": null
        },
        {
          "name": "Xiaoyun Zhang",
          "affiliation": null
        },
        {
          "name": "Qiang Hu",
          "affiliation": null
        },
        {
          "name": "Guangtao Zhai",
          "affiliation": null
        }
      ],
      "abstract": "Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.",
      "publishedDate": "2026-01-05T12:06:43Z",
      "updatedDate": "2026-01-05T12:06:43Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02046v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02046",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02021",
      "title": "AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI",
      "authors": [
        {
          "name": "Runze Zheng",
          "affiliation": null
        },
        {
          "name": "Yuqing Zheng",
          "affiliation": null
        },
        {
          "name": "Zhengyi Cheng",
          "affiliation": null
        },
        {
          "name": "Long Luo",
          "affiliation": null
        },
        {
          "name": "Haoxiang Luo",
          "affiliation": null
        },
        {
          "name": "Gang Sun",
          "affiliation": null
        },
        {
          "name": "Hongfang Yu",
          "affiliation": null
        },
        {
          "name": "Dusit Niyato",
          "affiliation": null
        }
      ],
      "abstract": "The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI.",
      "publishedDate": "2026-01-05T11:30:04Z",
      "updatedDate": "2026-01-05T11:30:04Z",
      "primaryCategory": "cs.NI",
      "arxivCategories": [
        "cs.NI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02021v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02021",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "agents",
        "tool-use",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "agents",
          "tool-use",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01984",
      "title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation",
      "authors": [
        {
          "name": "Weijian Ma",
          "affiliation": null
        },
        {
          "name": "Shizhao Sun",
          "affiliation": null
        },
        {
          "name": "Tianyu Yu",
          "affiliation": null
        },
        {
          "name": "Ruiyu Wang",
          "affiliation": null
        },
        {
          "name": "Tat-Seng Chua",
          "affiliation": null
        },
        {
          "name": "Jiang Bian",
          "affiliation": null
        }
      ],
      "abstract": "Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.",
      "publishedDate": "2026-01-05T10:38:26Z",
      "updatedDate": "2026-01-05T10:38:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01984v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01984",
      "comment": "Preprint. Under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01982",
      "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems",
      "authors": [
        {
          "name": "Noel Thomas",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.",
      "publishedDate": "2026-01-05T10:36:40Z",
      "updatedDate": "2026-01-05T10:36:40Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01982v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01982",
      "comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01910",
      "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning",
      "authors": [
        {
          "name": "Minh Hieu Ha",
          "affiliation": null
        },
        {
          "name": "Khanh Ly Ta",
          "affiliation": null
        },
        {
          "name": "Hung Phan",
          "affiliation": null
        },
        {
          "name": "Tung Doan",
          "affiliation": null
        },
        {
          "name": "Tung Dao",
          "affiliation": null
        },
        {
          "name": "Dao Tran",
          "affiliation": null
        },
        {
          "name": "Huynh Thi Thanh Binh",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency. We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.",
      "publishedDate": "2026-01-05T08:55:27Z",
      "updatedDate": "2026-01-05T08:55:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01910v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01910",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01875",
      "title": "Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence",
      "authors": [
        {
          "name": "Kewen Cao",
          "affiliation": null
        },
        {
          "name": "Jianxu Chen",
          "affiliation": null
        },
        {
          "name": "Yongbing Zhang",
          "affiliation": null
        },
        {
          "name": "Ye Zhang",
          "affiliation": null
        },
        {
          "name": "Hongxiao Wang",
          "affiliation": null
        }
      ],
      "abstract": "Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.",
      "publishedDate": "2026-01-05T08:02:49Z",
      "updatedDate": "2026-01-05T08:02:49Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01875v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01875",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01874",
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "authors": [
        {
          "name": "Shuhang Chen",
          "affiliation": null
        },
        {
          "name": "Yunqiu Xu",
          "affiliation": null
        },
        {
          "name": "Junjie Xie",
          "affiliation": null
        },
        {
          "name": "Aojun Lu",
          "affiliation": null
        },
        {
          "name": "Tao Feng",
          "affiliation": null
        },
        {
          "name": "Zeying Huang",
          "affiliation": null
        },
        {
          "name": "Ning Zhang",
          "affiliation": null
        },
        {
          "name": "Yi Sun",
          "affiliation": null
        },
        {
          "name": "Yi Yang",
          "affiliation": null
        },
        {
          "name": "Hangjie Yuan",
          "affiliation": null
        }
      ],
      "abstract": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
      "publishedDate": "2026-01-05T08:02:18Z",
      "updatedDate": "2026-01-05T08:02:18Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01874v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01874",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01825",
      "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning",
      "authors": [
        {
          "name": "Yaxin Cui",
          "affiliation": null
        },
        {
          "name": "Yuanqiang Zeng",
          "affiliation": null
        },
        {
          "name": "Jiapeng Yan",
          "affiliation": null
        },
        {
          "name": "Keling Lin",
          "affiliation": null
        },
        {
          "name": "Kai Ji",
          "affiliation": null
        },
        {
          "name": "Jianhui Zeng",
          "affiliation": null
        },
        {
          "name": "Sheng Zhang",
          "affiliation": null
        },
        {
          "name": "Xin Luo",
          "affiliation": null
        },
        {
          "name": "Binzhu Su",
          "affiliation": null
        },
        {
          "name": "Chaolai Shen",
          "affiliation": null
        },
        {
          "name": "Jiahao Yu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.",
      "publishedDate": "2026-01-05T06:44:29Z",
      "updatedDate": "2026-01-05T06:44:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01825v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01825",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01804",
      "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs",
      "authors": [
        {
          "name": "Zhengjian Kang",
          "affiliation": null
        },
        {
          "name": "Qi Chen",
          "affiliation": null
        },
        {
          "name": "Rui Liu",
          "affiliation": null
        },
        {
          "name": "Kangtong Mo",
          "affiliation": null
        },
        {
          "name": "Xingyu Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Deng",
          "affiliation": null
        },
        {
          "name": "Ye Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.",
      "publishedDate": "2026-01-05T05:30:13Z",
      "updatedDate": "2026-01-05T05:30:13Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01804v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01804",
      "comment": "7 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01798",
      "title": "VerLM: Explaining Face Verification Using Natural Language",
      "authors": [
        {
          "name": "Syed Abdul Hannan",
          "affiliation": null
        },
        {
          "name": "Hazim Bukhari",
          "affiliation": null
        },
        {
          "name": "Thomas Cantalapiedra",
          "affiliation": null
        },
        {
          "name": "Eman Ansar",
          "affiliation": null
        },
        {
          "name": "Massa Baali",
          "affiliation": null
        },
        {
          "name": "Rita Singh",
          "affiliation": null
        },
        {
          "name": "Bhiksha Raj",
          "affiliation": null
        }
      ],
      "abstract": "Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.",
      "publishedDate": "2026-01-05T05:16:07Z",
      "updatedDate": "2026-01-05T05:16:07Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01798v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01798",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01739",
      "title": "K-EXAONE Technical Report",
      "authors": [
        {
          "name": "Eunbi Choi",
          "affiliation": null
        },
        {
          "name": "Kibong Choi",
          "affiliation": null
        },
        {
          "name": "Seokhee Hong",
          "affiliation": null
        },
        {
          "name": "Junwon Hwang",
          "affiliation": null
        },
        {
          "name": "Hyojin Jeon",
          "affiliation": null
        },
        {
          "name": "Hyunjik Jo",
          "affiliation": null
        },
        {
          "name": "Joonkee Kim",
          "affiliation": null
        },
        {
          "name": "Seonghwan Kim",
          "affiliation": null
        },
        {
          "name": "Soyeon Kim",
          "affiliation": null
        },
        {
          "name": "Sunkyoung Kim",
          "affiliation": null
        },
        {
          "name": "Yireun Kim",
          "affiliation": null
        },
        {
          "name": "Yongil Kim",
          "affiliation": null
        },
        {
          "name": "Haeju Lee",
          "affiliation": null
        },
        {
          "name": "Jinsik Lee",
          "affiliation": null
        },
        {
          "name": "Kyungmin Lee",
          "affiliation": null
        },
        {
          "name": "Sangha Park",
          "affiliation": null
        },
        {
          "name": "Heuiyeen Yeen",
          "affiliation": null
        },
        {
          "name": "Hwan Chang",
          "affiliation": null
        },
        {
          "name": "Stanley Jungkyu Choi",
          "affiliation": null
        },
        {
          "name": "Yejin Choi",
          "affiliation": null
        },
        {
          "name": "Jiwon Ham",
          "affiliation": null
        },
        {
          "name": "Kijeong Jeon",
          "affiliation": null
        },
        {
          "name": "Geunyeong Jeong",
          "affiliation": null
        },
        {
          "name": "Gerrard Jeongwon Jo",
          "affiliation": null
        },
        {
          "name": "Yonghwan Jo",
          "affiliation": null
        },
        {
          "name": "Jiyeon Jung",
          "affiliation": null
        },
        {
          "name": "Naeun Kang",
          "affiliation": null
        },
        {
          "name": "Dohoon Kim",
          "affiliation": null
        },
        {
          "name": "Euisoon Kim",
          "affiliation": null
        },
        {
          "name": "Hayeon Kim",
          "affiliation": null
        },
        {
          "name": "Hyosang Kim",
          "affiliation": null
        },
        {
          "name": "Hyunseo Kim",
          "affiliation": null
        },
        {
          "name": "Jieun Kim",
          "affiliation": null
        },
        {
          "name": "Minu Kim",
          "affiliation": null
        },
        {
          "name": "Myoungshin Kim",
          "affiliation": null
        },
        {
          "name": "Unsol Kim",
          "affiliation": null
        },
        {
          "name": "Youchul Kim",
          "affiliation": null
        },
        {
          "name": "YoungJin Kim",
          "affiliation": null
        },
        {
          "name": "Chaeeun Lee",
          "affiliation": null
        },
        {
          "name": "Chaeyoon Lee",
          "affiliation": null
        },
        {
          "name": "Changhun Lee",
          "affiliation": null
        },
        {
          "name": "Dahm Lee",
          "affiliation": null
        },
        {
          "name": "Edward Hwayoung Lee",
          "affiliation": null
        },
        {
          "name": "Honglak Lee",
          "affiliation": null
        },
        {
          "name": "Jinsang Lee",
          "affiliation": null
        },
        {
          "name": "Jiyoung Lee",
          "affiliation": null
        },
        {
          "name": "Sangeun Lee",
          "affiliation": null
        },
        {
          "name": "Seungwon Lim",
          "affiliation": null
        },
        {
          "name": "Solji Lim",
          "affiliation": null
        },
        {
          "name": "Woohyung Lim",
          "affiliation": null
        },
        {
          "name": "Chanwoo Moon",
          "affiliation": null
        },
        {
          "name": "Jaewoo Park",
          "affiliation": null
        },
        {
          "name": "Jinho Park",
          "affiliation": null
        },
        {
          "name": "Yongmin Park",
          "affiliation": null
        },
        {
          "name": "Hyerin Seo",
          "affiliation": null
        },
        {
          "name": "Wooseok Seo",
          "affiliation": null
        },
        {
          "name": "Yongwoo Song",
          "affiliation": null
        },
        {
          "name": "Sejong Yang",
          "affiliation": null
        },
        {
          "name": "Sihoon Yang",
          "affiliation": null
        },
        {
          "name": "Chang En Yea",
          "affiliation": null
        },
        {
          "name": "Sihyuk Yi",
          "affiliation": null
        },
        {
          "name": "Chansik Yoon",
          "affiliation": null
        },
        {
          "name": "Dongkeun Yoon",
          "affiliation": null
        },
        {
          "name": "Sangyeon Yoon",
          "affiliation": null
        },
        {
          "name": "Hyeongu Yun",
          "affiliation": null
        }
      ],
      "abstract": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
      "publishedDate": "2026-01-05T02:30:59Z",
      "updatedDate": "2026-01-05T02:30:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01739v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01739",
      "comment": "29 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01714",
      "title": "Entropy-Aligned Decoding of LMs for Better Writing and Reasoning",
      "authors": [
        {
          "name": "Kareem Ahmed",
          "affiliation": null
        },
        {
          "name": "Sameer Singh",
          "affiliation": null
        }
      ],
      "abstract": "Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.",
      "publishedDate": "2026-01-05T01:37:10Z",
      "updatedDate": "2026-01-05T01:37:10Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01714v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01714",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01708",
      "title": "A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription",
      "authors": [
        {
          "name": "Unggi Lee",
          "affiliation": null
        },
        {
          "name": "Joo Young Kim",
          "affiliation": null
        },
        {
          "name": "Ran Ju",
          "affiliation": null
        },
        {
          "name": "Minyoung Jung",
          "affiliation": null
        },
        {
          "name": "Jeyeon Eo",
          "affiliation": null
        }
      ],
      "abstract": "Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.",
      "publishedDate": "2026-01-05T01:02:21Z",
      "updatedDate": "2026-01-05T01:02:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01708v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01708",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01580",
      "title": "The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs",
      "authors": [
        {
          "name": "Zibo Zhao",
          "affiliation": null
        },
        {
          "name": "Yuanting Zha",
          "affiliation": null
        },
        {
          "name": "Haipeng Zhang",
          "affiliation": null
        },
        {
          "name": "Xingcheng Xu",
          "affiliation": null
        }
      ],
      "abstract": "Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.",
      "publishedDate": "2026-01-04T15:59:15Z",
      "updatedDate": "2026-01-04T15:59:15Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01580v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01580",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01552",
      "title": "HalluZig: Hallucination Detection using Zigzag Persistence",
      "authors": [
        {
          "name": "Shreyas N. Samaga",
          "affiliation": null
        },
        {
          "name": "Gilberto Gonzalez Arroyo",
          "affiliation": null
        },
        {
          "name": "Tamal K. Dey",
          "affiliation": null
        }
      ],
      "abstract": "The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.",
      "publishedDate": "2026-01-04T14:55:43Z",
      "updatedDate": "2026-01-04T14:55:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01552v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01552",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01546",
      "title": "Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation",
      "authors": [
        {
          "name": "Letian Kong",
          "affiliation": null
        },
        {
          "name": "Qianran",
          "affiliation": null
        },
        {
          "name": "Jin",
          "affiliation": null
        },
        {
          "name": "Renyu Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.",
      "publishedDate": "2026-01-04T14:42:00Z",
      "updatedDate": "2026-01-04T14:42:00Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01546v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01546",
      "comment": "39 pages, 2 figures, 3 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01513",
      "title": "FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Gen Li",
          "affiliation": null
        },
        {
          "name": "Peiyu Liu",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.",
      "publishedDate": "2026-01-04T12:46:35Z",
      "updatedDate": "2026-01-04T12:46:35Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01513v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01513",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01490",
      "title": "Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints",
      "authors": [
        {
          "name": "Junichiro Niimi",
          "affiliation": null
        }
      ],
      "abstract": "With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.",
      "publishedDate": "2026-01-04T11:35:39Z",
      "updatedDate": "2026-01-04T11:35:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01490v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01490",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01407",
      "title": "From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models",
      "authors": [
        {
          "name": "Arjhun Sreedar",
          "affiliation": null
        },
        {
          "name": "Rohan Pillay",
          "affiliation": null
        },
        {
          "name": "Laukik Patade",
          "affiliation": null
        }
      ],
      "abstract": "This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.",
      "publishedDate": "2026-01-04T07:08:37Z",
      "updatedDate": "2026-01-04T07:08:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01407v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01407",
      "comment": "10 pages, 1 figure",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01400",
      "title": "EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery",
      "authors": [
        {
          "name": "Jicheng Ma",
          "affiliation": null
        },
        {
          "name": "Guohua Wang",
          "affiliation": null
        },
        {
          "name": "Xinhua Feng",
          "affiliation": null
        },
        {
          "name": "Yiming Liu",
          "affiliation": null
        },
        {
          "name": "Zhichao Hu",
          "affiliation": null
        },
        {
          "name": "Yuhong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \\textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.",
      "publishedDate": "2026-01-04T06:40:25Z",
      "updatedDate": "2026-01-04T06:40:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01400v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01400",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01362",
      "title": "Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning",
      "authors": [
        {
          "name": "Jerry Huang",
          "affiliation": null
        },
        {
          "name": "Peng Lu",
          "affiliation": null
        },
        {
          "name": "Qiuhao Zeng",
          "affiliation": null
        },
        {
          "name": "Yusuke Iwasawa",
          "affiliation": null
        },
        {
          "name": "Yutaka Matsuo",
          "affiliation": null
        },
        {
          "name": "Sarath Chandar",
          "affiliation": null
        },
        {
          "name": "Edison Marrese-Taylor",
          "affiliation": null
        },
        {
          "name": "Irene Li",
          "affiliation": null
        }
      ],
      "abstract": "Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.",
      "publishedDate": "2026-01-04T04:29:12Z",
      "updatedDate": "2026-01-04T04:29:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01362v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01362",
      "comment": "Accepted to The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01341",
      "title": "Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems",
      "authors": [
        {
          "name": "Md Abdullah Al Kafi",
          "affiliation": null
        },
        {
          "name": "Raka Moni",
          "affiliation": null
        },
        {
          "name": "Sumit Kumar Banshal",
          "affiliation": null
        }
      ],
      "abstract": "The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.",
      "publishedDate": "2026-01-04T03:09:23Z",
      "updatedDate": "2026-01-04T03:09:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01341v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01341",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01322",
      "title": "LinMU: Multimodal Understanding Made Linear",
      "authors": [
        {
          "name": "Hongjie Wang",
          "affiliation": null
        },
        {
          "name": "Niraj K. Jha",
          "affiliation": null
        }
      ],
      "abstract": "Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.",
      "publishedDate": "2026-01-04T01:17:36Z",
      "updatedDate": "2026-01-04T01:17:36Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.IV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01322v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01322",
      "comment": "23 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01298",
      "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware",
      "authors": [
        {
          "name": "Jorge L. Ruiz Williams",
          "affiliation": null
        }
      ],
      "abstract": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.",
      "publishedDate": "2026-01-03T23:11:21Z",
      "updatedDate": "2026-01-03T23:11:21Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.DC",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01298v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01298",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01266",
      "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment",
      "authors": [
        {
          "name": "Rhitabrat Pokharel",
          "affiliation": null
        },
        {
          "name": "Hamid Hassanzadeh",
          "affiliation": null
        },
        {
          "name": "Ameeta Agrawal",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.",
      "publishedDate": "2026-01-03T19:24:51Z",
      "updatedDate": "2026-01-03T19:24:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01266v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01266",
      "comment": "Accepted at AIMedHealth @ AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01233",
      "title": "Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling",
      "authors": [
        {
          "name": "Kangchen Zhu",
          "affiliation": null
        },
        {
          "name": "Zhiliang Tian",
          "affiliation": null
        },
        {
          "name": "Shangwen Wang",
          "affiliation": null
        },
        {
          "name": "Mingyue Leng",
          "affiliation": null
        },
        {
          "name": "Xiaoguang Mao",
          "affiliation": null
        }
      ],
      "abstract": "Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.",
      "publishedDate": "2026-01-03T16:43:05Z",
      "updatedDate": "2026-01-03T16:43:05Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01233v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01233",
      "comment": "Accepted by ICSE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01195",
      "title": "Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering",
      "authors": [
        {
          "name": "Wuzhenghong Wen",
          "affiliation": null
        },
        {
          "name": "Chao Xue",
          "affiliation": null
        },
        {
          "name": "Su Pan",
          "affiliation": null
        },
        {
          "name": "Yuwei Sun",
          "affiliation": null
        },
        {
          "name": "Minlong Peng",
          "affiliation": null
        }
      ],
      "abstract": "Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.",
      "publishedDate": "2026-01-03T14:27:01Z",
      "updatedDate": "2026-01-03T14:27:01Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01195v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01195",
      "comment": "11 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01118",
      "title": "ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services",
      "authors": [
        {
          "name": "Qingqing Long",
          "affiliation": null
        },
        {
          "name": "Haotian Chen",
          "affiliation": null
        },
        {
          "name": "Chenyang Zhao",
          "affiliation": null
        },
        {
          "name": "Xiaolei Du",
          "affiliation": null
        },
        {
          "name": "Xuezhi Wang",
          "affiliation": null
        },
        {
          "name": "Pengyao Wang",
          "affiliation": null
        },
        {
          "name": "Chengzan Li",
          "affiliation": null
        },
        {
          "name": "Yuanchun Zhou",
          "affiliation": null
        },
        {
          "name": "Hengshu Zhu",
          "affiliation": null
        }
      ],
      "abstract": "The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.",
      "publishedDate": "2026-01-03T08:42:53Z",
      "updatedDate": "2026-01-03T08:42:53Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.DL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01118v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01118",
      "comment": "12 pages, 9 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "agents",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01095",
      "title": "NarrativeTrack: Evaluating Video Language Models Beyond the Frame",
      "authors": [
        {
          "name": "Hyeonjeong Ha",
          "affiliation": null
        },
        {
          "name": "Jinjin Ge",
          "affiliation": null
        },
        {
          "name": "Bo Feng",
          "affiliation": null
        },
        {
          "name": "Kaixin Ma",
          "affiliation": null
        },
        {
          "name": "Gargi Chakraborty",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.",
      "publishedDate": "2026-01-03T07:12:55Z",
      "updatedDate": "2026-01-03T07:12:55Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01095v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01095",
      "comment": "VideoLLM Fine-Grained Evaluation",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01011",
      "title": "Intention Collapse: Intention-Level Metrics for Reasoning in Language Models",
      "authors": [
        {
          "name": "Patricio Vera",
          "affiliation": null
        }
      ],
      "abstract": "Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies",
      "publishedDate": "2026-01-03T00:19:53Z",
      "updatedDate": "2026-01-03T00:19:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01011v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01011",
      "comment": "21 pages, 4 figures, 3 tables. Code: https://github.com/patriciomvera/intention-collapse-experiments",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00998",
      "title": "DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models",
      "authors": [
        {
          "name": "Yue Zhou",
          "affiliation": null
        },
        {
          "name": "Jue Chen",
          "affiliation": null
        },
        {
          "name": "Zilun Zhang",
          "affiliation": null
        },
        {
          "name": "Penghui Huang",
          "affiliation": null
        },
        {
          "name": "Ran Ding",
          "affiliation": null
        },
        {
          "name": "Zhentao Zou",
          "affiliation": null
        },
        {
          "name": "PengFei Gao",
          "affiliation": null
        },
        {
          "name": "Yuchen Wei",
          "affiliation": null
        },
        {
          "name": "Ke Li",
          "affiliation": null
        },
        {
          "name": "Xue Yang",
          "affiliation": null
        },
        {
          "name": "Xue Jiang",
          "affiliation": null
        },
        {
          "name": "Hongxin Yang",
          "affiliation": null
        },
        {
          "name": "Jonathan Li",
          "affiliation": null
        }
      ],
      "abstract": "Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench",
      "publishedDate": "2026-01-02T22:42:38Z",
      "updatedDate": "2026-01-02T22:42:38Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00998v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00998",
      "comment": "20 pages, 17 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation",
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation",
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00923",
      "title": "Context Collapse: In-Context Learning and Model Collapse",
      "authors": [
        {
          "name": "Josef Ott",
          "affiliation": null
        }
      ],
      "abstract": "This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.",
      "publishedDate": "2026-01-01T17:33:47Z",
      "updatedDate": "2026-01-01T17:33:47Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00923v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00923",
      "comment": "Master's thesis",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01196",
      "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners",
      "authors": [
        {
          "name": "Shenqi Lu",
          "affiliation": null
        },
        {
          "name": "Liangwei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.",
      "publishedDate": "2026-01-03T14:40:39Z",
      "updatedDate": "2026-01-03T14:40:39Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01196v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01196",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "robotics",
        "prompting",
        "agents",
        "tool-use",
        "planning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "robotics",
          "prompting",
          "agents",
          "tool-use",
          "planning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01673",
      "title": "Exposing Hidden Interfaces: LLM-Guided Type Inference for Reverse Engineering macOS Private Frameworks",
      "authors": [
        {
          "name": "Arina Kharlamova",
          "affiliation": null
        },
        {
          "name": "Youcheng Sun",
          "affiliation": null
        },
        {
          "name": "Ting Yu",
          "affiliation": null
        }
      ],
      "abstract": "Private macOS frameworks underpin critical services and daemons but remain undocumented and distributed only as stripped binaries, complicating security analysis. We present MOTIF, an agentic framework that integrates tool-augmented analysis with a finetuned large language model specialized for Objective-C type inference. The agent manages runtime metadata extraction, binary inspection, and constraint checking, while the model generates candidate method signatures that are validated and refined into compilable headers. On MOTIF-Bench, a benchmark built from public frameworks with groundtruth headers, MOTIF improves signature recovery from 15% to 86% compared to baseline static analysis tooling, with consistent gains in tool-use correctness and inference stability. Case studies on private frameworks show that reconstructed headers compile, link, and facilitate downstream security research and vulnerability studies. By transforming opaque binaries into analyzable interfaces, MOTIF establishes a scalable foundation for systematic auditing of macOS internals.",
      "publishedDate": "2026-01-04T21:44:55Z",
      "updatedDate": "2026-01-04T21:44:55Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01673v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01673",
      "comment": "IEEE S&P'26 under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01320",
      "title": "Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python",
      "authors": [
        {
          "name": "Muntasir Adnan",
          "affiliation": null
        },
        {
          "name": "Carlos C. N. Kuhn",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.",
      "publishedDate": "2026-01-04T01:13:37Z",
      "updatedDate": "2026-01-04T01:13:37Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01320v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01320",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02075",
      "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
      "authors": [
        {
          "name": "Zhuofan Shi",
          "affiliation": null
        },
        {
          "name": "Hubao A",
          "affiliation": null
        },
        {
          "name": "Yufei Shao",
          "affiliation": null
        },
        {
          "name": "Mengyan Dai",
          "affiliation": null
        },
        {
          "name": "Yadong Yu",
          "affiliation": null
        },
        {
          "name": "Pan Xiang",
          "affiliation": null
        },
        {
          "name": "Dongliang Huang",
          "affiliation": null
        },
        {
          "name": "Hongxu An",
          "affiliation": null
        },
        {
          "name": "Chunxiao Xin",
          "affiliation": null
        },
        {
          "name": "Haiyang Shen",
          "affiliation": null
        },
        {
          "name": "Zhenyu Wang",
          "affiliation": null
        },
        {
          "name": "Yunshan Na",
          "affiliation": null
        },
        {
          "name": "Gang Huang",
          "affiliation": null
        },
        {
          "name": "Xiang Jing",
          "affiliation": null
        }
      ],
      "abstract": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
      "publishedDate": "2026-01-05T12:56:51Z",
      "updatedDate": "2026-01-05T12:56:51Z",
      "primaryCategory": "cs.CE",
      "arxivCategories": [
        "cs.CE",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02075v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02075",
      "comment": "24 pages,4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01993",
      "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support",
      "authors": [
        {
          "name": "Dong Xue",
          "affiliation": null
        },
        {
          "name": "Jicheng Tu",
          "affiliation": null
        },
        {
          "name": "Ming Wang",
          "affiliation": null
        },
        {
          "name": "Xin Yan",
          "affiliation": null
        },
        {
          "name": "Fangzhou Liu",
          "affiliation": null
        },
        {
          "name": "Jie Hu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.",
      "publishedDate": "2026-01-05T10:54:18Z",
      "updatedDate": "2026-01-05T10:54:18Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01993v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01993",
      "comment": "33 pages, 16 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01844",
      "title": "Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Udiptaman Das",
          "affiliation": null
        },
        {
          "name": "Krishnasai B. Atmakuri",
          "affiliation": null
        },
        {
          "name": "Duy Ho",
          "affiliation": null
        },
        {
          "name": "Chi Lee",
          "affiliation": null
        },
        {
          "name": "Yugyung Lee",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.",
      "publishedDate": "2026-01-05T07:16:29Z",
      "updatedDate": "2026-01-05T07:16:29Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01844v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01844",
      "comment": "13 pages, 5 tables, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "agents",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "agents",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02345",
      "title": "Question Answering for Multi-Release Systems: A Case Study at Ciena",
      "authors": [
        {
          "name": "Parham Khamsepour",
          "affiliation": null
        },
        {
          "name": "Mark Cole",
          "affiliation": null
        },
        {
          "name": "Ish Ashraf",
          "affiliation": null
        },
        {
          "name": "Sandeep Puri",
          "affiliation": null
        },
        {
          "name": "Mehrdad Sabetzadeh",
          "affiliation": null
        },
        {
          "name": "Shiva Nejati",
          "affiliation": null
        }
      ],
      "abstract": "Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.",
      "publishedDate": "2026-01-05T18:44:26Z",
      "updatedDate": "2026-01-05T18:44:26Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02345v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02345",
      "comment": "Accepted for publication in SANER 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02065",
      "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory",
      "authors": [
        {
          "name": "Md. Asif Hossain",
          "affiliation": null
        },
        {
          "name": "Nabil Subhan",
          "affiliation": null
        },
        {
          "name": "Mantasha Rahman Mahi",
          "affiliation": null
        },
        {
          "name": "Jannatul Ferdous Nabila",
          "affiliation": null
        }
      ],
      "abstract": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings",
      "publishedDate": "2026-01-05T12:41:44Z",
      "updatedDate": "2026-01-05T12:41:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02065v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02065",
      "comment": "5 pages, 3 figures, 1 table",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02023",
      "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
      "authors": [
        {
          "name": "Amirali Ebrahimzadeh",
          "affiliation": null
        },
        {
          "name": "Seyyed M. Salili",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
      "publishedDate": "2026-01-05T11:30:56Z",
      "updatedDate": "2026-01-05T11:30:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02023v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02023",
      "comment": "25 pages, 8 figures, 3 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01950",
      "title": "Face Normal Estimation from Rags to Riches",
      "authors": [
        {
          "name": "Meng Wang",
          "affiliation": null
        },
        {
          "name": "Wenjing Dai",
          "affiliation": null
        },
        {
          "name": "Jiawan Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaojie Guo",
          "affiliation": null
        }
      ],
      "abstract": "Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.",
      "publishedDate": "2026-01-05T09:57:24Z",
      "updatedDate": "2026-01-05T09:57:24Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01950v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01950",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01896",
      "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG",
      "authors": [
        {
          "name": "Jingyu Liu",
          "affiliation": null
        },
        {
          "name": "Jiaen Lin",
          "affiliation": null
        },
        {
          "name": "Yong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.",
      "publishedDate": "2026-01-05T08:40:37Z",
      "updatedDate": "2026-01-05T08:40:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01896v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01896",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01872",
      "title": "CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios",
      "authors": [
        {
          "name": "Hongbo Duan",
          "affiliation": null
        },
        {
          "name": "Shangyi Luo",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Deng",
          "affiliation": null
        },
        {
          "name": "Yanbo Chen",
          "affiliation": null
        },
        {
          "name": "Yuanhao Chiang",
          "affiliation": null
        },
        {
          "name": "Yi Liu",
          "affiliation": null
        },
        {
          "name": "Fangming Liu",
          "affiliation": null
        },
        {
          "name": "Xueqian Wang",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.",
      "publishedDate": "2026-01-05T08:00:34Z",
      "updatedDate": "2026-01-05T08:00:34Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01872v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01872",
      "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "robotics",
        "agents",
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "rag",
          "robotics",
          "agents",
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01785",
      "title": "SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines",
      "authors": [
        {
          "name": "Rajiv Chaitanya Muttur",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.",
      "publishedDate": "2026-01-05T04:39:31Z",
      "updatedDate": "2026-01-05T04:39:31Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01785v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01785",
      "comment": "Presented at ICEdge 2025; nominated for Best Paper Award",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00926",
      "title": "MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers",
      "authors": [
        {
          "name": "Satya Swaroop Gudipudi",
          "affiliation": null
        },
        {
          "name": "Sahil Girhepuje",
          "affiliation": null
        },
        {
          "name": "Ponnurangam Kumaraguru",
          "affiliation": null
        },
        {
          "name": "Kristine Ma",
          "affiliation": null
        }
      ],
      "abstract": "Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.",
      "publishedDate": "2026-01-01T23:31:02Z",
      "updatedDate": "2026-01-01T23:31:02Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00926v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00926",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02002",
      "title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models",
      "authors": [
        {
          "name": "Antonio Colacicco",
          "affiliation": null
        },
        {
          "name": "Vito Guida",
          "affiliation": null
        },
        {
          "name": "Dario Di Palma",
          "affiliation": null
        },
        {
          "name": "Fedelucio Narducci",
          "affiliation": null
        },
        {
          "name": "Tommaso Di Noia",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.",
      "publishedDate": "2026-01-05T11:03:56Z",
      "updatedDate": "2026-01-05T11:03:56Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02002v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02002",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01954",
      "title": "Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations",
      "authors": [
        {
          "name": "Alexander Korn",
          "affiliation": null
        },
        {
          "name": "Lea Zaruchas",
          "affiliation": null
        },
        {
          "name": "Chetan Arora",
          "affiliation": null
        },
        {
          "name": "Andreas Metzger",
          "affiliation": null
        },
        {
          "name": "Sven Smolka",
          "affiliation": null
        },
        {
          "name": "Fanyu Wang",
          "affiliation": null
        },
        {
          "name": "Andreas Vogelsang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.",
      "publishedDate": "2026-01-05T10:01:20Z",
      "updatedDate": "2026-01-05T10:01:20Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01954v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01954",
      "comment": "To be published at The 3rd ACM International Conference on AI Foundation Models and Software Engineering FORGE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01213",
      "title": "Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation",
      "authors": [
        {
          "name": "Riccardo Gelato",
          "affiliation": null
        },
        {
          "name": "Carlo Sgaravatti",
          "affiliation": null
        },
        {
          "name": "Jakob Grahn",
          "affiliation": null
        },
        {
          "name": "Giacomo Boracchi",
          "affiliation": null
        },
        {
          "name": "Filippo Maria Bianchi",
          "affiliation": null
        }
      ],
      "abstract": "Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.",
      "publishedDate": "2026-01-03T15:41:12Z",
      "updatedDate": "2026-01-03T15:41:12Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01213v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01213",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01027",
      "title": "A Platform for Interactive AI Character Experiences",
      "authors": [
        {
          "name": "Rafael Wampfler",
          "affiliation": null
        },
        {
          "name": "Chen Yang",
          "affiliation": null
        },
        {
          "name": "Dillon Elste",
          "affiliation": null
        },
        {
          "name": "Nikola Kovacevic",
          "affiliation": null
        },
        {
          "name": "Philine Witzig",
          "affiliation": null
        },
        {
          "name": "Markus Gross",
          "affiliation": null
        }
      ],
      "abstract": "From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.",
      "publishedDate": "2026-01-03T01:27:19Z",
      "updatedDate": "2026-01-03T01:27:19Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01027v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01027",
      "comment": null,
      "journalRef": "SIGGRAPH Conference Papers '25, August 10-14, 2025, Vancouver, BC, Canada",
      "doi": "10.1145/3721238.3730762",
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02236",
      "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models",
      "authors": [
        {
          "name": "Yihao Liang",
          "affiliation": null
        },
        {
          "name": "Ze Wang",
          "affiliation": null
        },
        {
          "name": "Hao Chen",
          "affiliation": null
        },
        {
          "name": "Ximeng Sun",
          "affiliation": null
        },
        {
          "name": "Jialian Wu",
          "affiliation": null
        },
        {
          "name": "Xiaodong Yu",
          "affiliation": null
        },
        {
          "name": "Jiang Liu",
          "affiliation": null
        },
        {
          "name": "Emad Barsoum",
          "affiliation": null
        },
        {
          "name": "Zicheng Liu",
          "affiliation": null
        },
        {
          "name": "Niraj K. Jha",
          "affiliation": null
        }
      ],
      "abstract": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM",
      "publishedDate": "2026-01-05T16:09:22Z",
      "updatedDate": "2026-01-05T16:09:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02236v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02236",
      "comment": "33 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02201",
      "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents",
      "authors": [
        {
          "name": "Keyu Wang",
          "affiliation": null
        },
        {
          "name": "Bingchen Miao",
          "affiliation": null
        },
        {
          "name": "Wendong Bu",
          "affiliation": null
        },
        {
          "name": "Yu Wu",
          "affiliation": null
        },
        {
          "name": "Juncheng Li",
          "affiliation": null
        },
        {
          "name": "Shengyu Zhang",
          "affiliation": null
        },
        {
          "name": "Wenqiao Zhang",
          "affiliation": null
        },
        {
          "name": "Siliang Tang",
          "affiliation": null
        },
        {
          "name": "Jun Xiao",
          "affiliation": null
        },
        {
          "name": "Yueting Zhuang",
          "affiliation": null
        }
      ],
      "abstract": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.",
      "publishedDate": "2026-01-05T15:24:05Z",
      "updatedDate": "2026-01-05T15:24:05Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02201v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02201",
      "comment": "19 pages, 12 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02060",
      "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming",
      "authors": [
        {
          "name": "Nguyet-Anh H. Lang",
          "affiliation": null
        },
        {
          "name": "Eric Lang",
          "affiliation": null
        },
        {
          "name": "Thanh Le-Cong",
          "affiliation": null
        },
        {
          "name": "Bach Le",
          "affiliation": null
        },
        {
          "name": "Quyet-Thang Huynh",
          "affiliation": null
        }
      ],
      "abstract": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.",
      "publishedDate": "2026-01-05T12:33:37Z",
      "updatedDate": "2026-01-05T12:33:37Z",
      "primaryCategory": "cs.PL",
      "arxivCategories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02060v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02060",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01972",
      "title": "Hidden State Poisoning Attacks against Mamba-based Language Models",
      "authors": [
        {
          "name": "Alexandre Le Mercier",
          "affiliation": null
        },
        {
          "name": "Chris Develder",
          "affiliation": null
        },
        {
          "name": "Thomas Demeester",
          "affiliation": null
        }
      ],
      "abstract": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.",
      "publishedDate": "2026-01-05T10:27:19Z",
      "updatedDate": "2026-01-05T10:27:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01972v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01972",
      "comment": "17 pages, 4 figures. Submitted to ACL 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01957",
      "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing",
      "authors": [
        {
          "name": "Tianbo Wang",
          "affiliation": null
        },
        {
          "name": "Yuqing Ma",
          "affiliation": null
        },
        {
          "name": "Kewei Liao",
          "affiliation": null
        },
        {
          "name": "Zhange Zhang",
          "affiliation": null
        },
        {
          "name": "Simin Li",
          "affiliation": null
        },
        {
          "name": "Jinyang Guo",
          "affiliation": null
        },
        {
          "name": "Xianglong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.",
      "publishedDate": "2026-01-05T10:02:22Z",
      "updatedDate": "2026-01-05T10:02:22Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01957v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01957",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01870",
      "title": "Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion",
      "authors": [
        {
          "name": "Wenyu Shao",
          "affiliation": null
        },
        {
          "name": "Hongbo Liu",
          "affiliation": null
        },
        {
          "name": "Yunchuan Ma",
          "affiliation": null
        },
        {
          "name": "Ruili Wang",
          "affiliation": null
        }
      ],
      "abstract": "Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.",
      "publishedDate": "2026-01-05T08:00:03Z",
      "updatedDate": "2026-01-05T08:00:03Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01870v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01870",
      "comment": "Accepted by IEEE Transactions on Multimedia",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01827",
      "title": "Aspect Extraction from E-Commerce Product and Service Reviews",
      "authors": [
        {
          "name": "Valiant Lance D. Dionela",
          "affiliation": null
        },
        {
          "name": "Fatima Kriselle S. Dy",
          "affiliation": null
        },
        {
          "name": "Robin James M. Hombrebueno",
          "affiliation": null
        },
        {
          "name": "Aaron Rae M. Nicolas",
          "affiliation": null
        },
        {
          "name": "Charibeth K. Cheng",
          "affiliation": null
        },
        {
          "name": "Raphael W. Gonda",
          "affiliation": null
        }
      ],
      "abstract": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.",
      "publishedDate": "2026-01-05T06:45:51Z",
      "updatedDate": "2026-01-05T06:45:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01827v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01827",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01765",
      "title": "A New Benchmark for the Appropriate Evaluation of RTL Code Optimization",
      "authors": [
        {
          "name": "Yao Lu",
          "affiliation": null
        },
        {
          "name": "Shang Liu",
          "affiliation": null
        },
        {
          "name": "Hangan Zhou",
          "affiliation": null
        },
        {
          "name": "Wenji Fang",
          "affiliation": null
        },
        {
          "name": "Qijun Zhang",
          "affiliation": null
        },
        {
          "name": "Zhiyao Xie",
          "affiliation": null
        }
      ],
      "abstract": "The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.",
      "publishedDate": "2026-01-05T03:47:26Z",
      "updatedDate": "2026-01-05T03:47:26Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01765v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01765",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "tool-use",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01461",
      "title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR",
      "authors": [
        {
          "name": "Yuxiang Mei",
          "affiliation": null
        },
        {
          "name": "Dongxing Xu",
          "affiliation": null
        },
        {
          "name": "Jiaen Liang",
          "affiliation": null
        },
        {
          "name": "Yanhua Long",
          "affiliation": null
        }
      ],
      "abstract": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.",
      "publishedDate": "2026-01-04T10:08:53Z",
      "updatedDate": "2026-01-04T10:08:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01461v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01461",
      "comment": "5 pages, 1 figure",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01416",
      "title": "AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval",
      "authors": [
        {
          "name": "Yue Zhou",
          "affiliation": null
        },
        {
          "name": "Ran Ding",
          "affiliation": null
        },
        {
          "name": "Xue Yang",
          "affiliation": null
        },
        {
          "name": "Xue Jiang",
          "affiliation": null
        },
        {
          "name": "Xingzhao Liu",
          "affiliation": null
        }
      ],
      "abstract": "Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot",
      "publishedDate": "2026-01-04T07:38:51Z",
      "updatedDate": "2026-01-04T07:38:51Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01416v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01416",
      "comment": "12 pages, 9 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "planning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01332",
      "title": "FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness",
      "authors": [
        {
          "name": "Hossam Amer",
          "affiliation": null
        },
        {
          "name": "Maryam Dialameh",
          "affiliation": null
        },
        {
          "name": "Hossein Rajabzadeh",
          "affiliation": null
        },
        {
          "name": "Walid Ahmed",
          "affiliation": null
        },
        {
          "name": "Weiwei Zhang",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.",
      "publishedDate": "2026-01-04T02:33:30Z",
      "updatedDate": "2026-01-04T02:33:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01332v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01332",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01271",
      "title": "CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs",
      "authors": [
        {
          "name": "Qingxiao Tao",
          "affiliation": null
        },
        {
          "name": "Xiaodong Gu",
          "affiliation": null
        },
        {
          "name": "Hao Zhong",
          "affiliation": null
        },
        {
          "name": "Beijun Shen",
          "affiliation": null
        }
      ],
      "abstract": "Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.",
      "publishedDate": "2026-01-03T20:03:03Z",
      "updatedDate": "2026-01-03T20:03:03Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01271v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01271",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "tool-use",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "tool-use",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01244",
      "title": "Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure",
      "authors": [
        {
          "name": "Zsolt Csibi",
          "affiliation": null
        },
        {
          "name": "Bence György Gortka",
          "affiliation": null
        },
        {
          "name": "Natabara Gyöngyössy",
          "affiliation": null
        },
        {
          "name": "Kornél Nagy",
          "affiliation": null
        },
        {
          "name": "Dávid Márk Nemeskey",
          "affiliation": null
        },
        {
          "name": "Martin Sallai",
          "affiliation": null
        },
        {
          "name": "András Simonyi",
          "affiliation": null
        },
        {
          "name": "András Márk Szekeres",
          "affiliation": null
        },
        {
          "name": "Gábor Palkó",
          "affiliation": null
        }
      ],
      "abstract": "We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.",
      "publishedDate": "2026-01-03T17:32:48Z",
      "updatedDate": "2026-01-03T17:32:48Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01244v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01244",
      "comment": "18 pages, 1 figures. To appear in the XXII. Magyar Számítógépes Nyelvészeti Konferencia (MSZNY 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01215",
      "title": "Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code",
      "authors": [
        {
          "name": "Prateek Rajput",
          "affiliation": null
        },
        {
          "name": "Yewei Song",
          "affiliation": null
        },
        {
          "name": "Abdoul Aziz Bonkoungou",
          "affiliation": null
        },
        {
          "name": "Iyiola E. Olatunji",
          "affiliation": null
        },
        {
          "name": "Abdoul Kader Kabore",
          "affiliation": null
        },
        {
          "name": "Jacques Klein",
          "affiliation": null
        },
        {
          "name": "Tegawendé F. Bissyandé",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.",
      "publishedDate": "2026-01-03T15:42:21Z",
      "updatedDate": "2026-01-03T15:42:21Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01215v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01215",
      "comment": "11 Pages, 11 figures, Accepted at ICSE SEIP",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01184",
      "title": "SecureCodeRL: Security-Aware Reinforcement Learning for Code Generation with Partial-Credit Rewards",
      "authors": [
        {
          "name": "Suryansh Singh Sijwali",
          "affiliation": null
        },
        {
          "name": "Suman Saha",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can generate plausible code, but in settings that require exact stdin/stdout behavior they frequently produce programs that compile yet fail tests, and in some cases they introduce security-sensitive patterns. This paper presents SecureCodeRL, a reinforcement learning (RL) pipeline for security-aware code generation that optimizes a combined reward R = αRfunc + \\b{eta}Rsec. The key idea is a partial-credit functional reward that assigns intermediate scores for syntactic validity, successful execution, and producing output, reducing reward sparsity that otherwise stalls learning on competitive programming style tasks. I evaluate supervised fine-tuning (SFT) and PPO variants on a small held-out prompt set from APPS+ and observe that PPO with partial credit (using a continued-training variant) improves syntax validity from 45% (SFT) to 60% and achieves the only non-zero test success signal in this pilot evaluation (5% at-least-one-test-pass), while remaining 100% clean under Bandit static analysis. Although Bandit findings were absent in this small evaluation, the security term is integrated into training to discourage insecure shortcuts when they appear.",
      "publishedDate": "2026-01-03T13:36:36Z",
      "updatedDate": "2026-01-03T13:36:36Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01184v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01184",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01162",
      "title": "Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models",
      "authors": [
        {
          "name": "Zihua Yang",
          "affiliation": null
        },
        {
          "name": "Xin Liao",
          "affiliation": null
        },
        {
          "name": "Yiqun Zhang",
          "affiliation": null
        },
        {
          "name": "Yiu-ming Cheung",
          "affiliation": null
        }
      ],
      "abstract": "Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE",
      "publishedDate": "2026-01-03T11:37:46Z",
      "updatedDate": "2026-01-03T11:37:46Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01162v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01162",
      "comment": "Submitted to ICPR 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01129",
      "title": "RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian",
      "authors": [
        {
          "name": "Kla Tantithamthavorn",
          "affiliation": null
        },
        {
          "name": "Yaotian Zou",
          "affiliation": null
        },
        {
          "name": "Andy Wong",
          "affiliation": null
        },
        {
          "name": "Michael Gupta",
          "affiliation": null
        },
        {
          "name": "Zhe Wang",
          "affiliation": null
        },
        {
          "name": "Mike Buller",
          "affiliation": null
        },
        {
          "name": "Ryan Jiang",
          "affiliation": null
        },
        {
          "name": "Matthew Watson",
          "affiliation": null
        },
        {
          "name": "Minwoo Jeong",
          "affiliation": null
        },
        {
          "name": "Kun Chen",
          "affiliation": null
        },
        {
          "name": "Ming Wu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning? In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).",
      "publishedDate": "2026-01-03T09:27:56Z",
      "updatedDate": "2026-01-03T09:27:56Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01129v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01129",
      "comment": "Accepted at the 48th International Conference on Software Engineering (ICSE'26), SEIP Track. 12 Pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00993",
      "title": "WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift",
      "authors": [
        {
          "name": "Julian D. Santamaria",
          "affiliation": null
        },
        {
          "name": "Claudia Isaza",
          "affiliation": null
        },
        {
          "name": "Jhony H. Giraldo",
          "affiliation": null
        }
      ],
      "abstract": "Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.",
      "publishedDate": "2026-01-02T21:58:19Z",
      "updatedDate": "2026-01-02T21:58:19Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00993v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00993",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00927",
      "title": "Measuring Social Media Polarization Using Large Language Models and Heuristic Rules",
      "authors": [
        {
          "name": "Jawad Chowdhury",
          "affiliation": null
        },
        {
          "name": "Rezaur Rashid",
          "affiliation": null
        },
        {
          "name": "Gabriel Terejanu",
          "affiliation": null
        }
      ],
      "abstract": "Understanding affective polarization in online discourse is crucial for evaluating the societal impact of social media interactions. This study presents a novel framework that leverages large language models (LLMs) and domain-informed heuristics to systematically analyze and quantify affective polarization in discussions on divisive topics such as climate change and gun control. Unlike most prior approaches that relied on sentiment analysis or predefined classifiers, our method integrates LLMs to extract stance, affective tone, and agreement patterns from large-scale social media discussions. We then apply a rule-based scoring system capable of quantifying affective polarization even in small conversations consisting of single interactions, based on stance alignment, emotional content, and interaction dynamics. Our analysis reveals distinct polarization patterns that are event dependent: (i) anticipation-driven polarization, where extreme polarization escalates before well-publicized events, and (ii) reactive polarization, where intense affective polarization spikes immediately after sudden, high-impact events. By combining AI-driven content annotation with domain-informed scoring, our framework offers a scalable and interpretable approach to measuring affective polarization. The source code is publicly available at: https://github.com/hasanjawad001/llm-social-media-polarization.",
      "publishedDate": "2026-01-02T01:11:58Z",
      "updatedDate": "2026-01-02T01:11:58Z",
      "primaryCategory": "cs.SI",
      "arxivCategories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00927v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00927",
      "comment": "Foundations and Applications of Big Data Analytics (FAB), Niagara Falls, Canada, 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01948",
      "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation",
      "authors": [
        {
          "name": "Zhihao Gu",
          "affiliation": null
        },
        {
          "name": "Ming Yang",
          "affiliation": null
        },
        {
          "name": "Difan Zou",
          "affiliation": null
        },
        {
          "name": "Dong Xu",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.",
      "publishedDate": "2026-01-05T09:56:24Z",
      "updatedDate": "2026-01-05T09:56:24Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01948v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01948",
      "comment": "Accepted to AAAI2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "planning",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "prompting",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00978",
      "title": "From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly",
      "authors": [
        {
          "name": "Yanyi Chen",
          "affiliation": null
        },
        {
          "name": "Min Deng",
          "affiliation": null
        }
      ],
      "abstract": "Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.",
      "publishedDate": "2026-01-02T20:12:50Z",
      "updatedDate": "2026-01-02T20:12:50Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00978v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00978",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "planning",
        "multi-agent",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "multi-agent",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02316",
      "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations",
      "authors": [
        {
          "name": "Siddharth Joshi",
          "affiliation": null
        },
        {
          "name": "Haoli Yin",
          "affiliation": null
        },
        {
          "name": "Rishabh Adiga",
          "affiliation": null
        },
        {
          "name": "Ricardo Monti",
          "affiliation": null
        },
        {
          "name": "Aldo Carranza",
          "affiliation": null
        },
        {
          "name": "Alex Fang",
          "affiliation": null
        },
        {
          "name": "Alvin Deng",
          "affiliation": null
        },
        {
          "name": "Amro Abbas",
          "affiliation": null
        },
        {
          "name": "Brett Larsen",
          "affiliation": null
        },
        {
          "name": "Cody Blakeney",
          "affiliation": null
        },
        {
          "name": "Darren Teh",
          "affiliation": null
        },
        {
          "name": "David Schwab",
          "affiliation": null
        },
        {
          "name": "Fan Pan",
          "affiliation": null
        },
        {
          "name": "Haakon Mongstad",
          "affiliation": null
        },
        {
          "name": "Jack Urbanek",
          "affiliation": null
        },
        {
          "name": "Jason Lee",
          "affiliation": null
        },
        {
          "name": "Jason Telanoff",
          "affiliation": null
        },
        {
          "name": "Josh Wills",
          "affiliation": null
        },
        {
          "name": "Kaleigh Mentzer",
          "affiliation": null
        },
        {
          "name": "Luke Merrick",
          "affiliation": null
        },
        {
          "name": "Parth Doshi",
          "affiliation": null
        },
        {
          "name": "Paul Burstein",
          "affiliation": null
        },
        {
          "name": "Pratyush Maini",
          "affiliation": null
        },
        {
          "name": "Scott Loftin",
          "affiliation": null
        },
        {
          "name": "Spandan Das",
          "affiliation": null
        },
        {
          "name": "Tony Jiang",
          "affiliation": null
        },
        {
          "name": "Vineeth Dorna",
          "affiliation": null
        },
        {
          "name": "Zhengping Wang",
          "affiliation": null
        },
        {
          "name": "Bogdan Gaza",
          "affiliation": null
        },
        {
          "name": "Ari Morcos",
          "affiliation": null
        },
        {
          "name": "Matthew Leavitt",
          "affiliation": null
        }
      ],
      "abstract": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.",
      "publishedDate": "2026-01-05T18:07:51Z",
      "updatedDate": "2026-01-05T18:07:51Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02316v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02316",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    }
  ]
}