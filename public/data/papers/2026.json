{
  "year": "2026",
  "count": 876,
  "papers": [
    {
      "id": "2601.00791",
      "title": "Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning",
      "authors": [
        {
          "name": "Valentin Noël",
          "affiliation": null
        }
      ],
      "abstract": "We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\\text{MW}} = 1.16 \\times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.",
      "publishedDate": "2026-01-02T18:49:37Z",
      "updatedDate": "2026-01-02T18:49:37Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00791v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00791",
      "comment": "58 pages, 19 figures, Under Review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00756",
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "authors": [
        {
          "name": "Thomas Katraouras",
          "affiliation": null
        },
        {
          "name": "Dimitrios Rafailidis",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.",
      "publishedDate": "2026-01-02T17:22:34Z",
      "updatedDate": "2026-01-02T17:22:34Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00756v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00756",
      "comment": "Accepted to the 41st ACM/SIGAPP Symposium on Applied Computing (SAC '26)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00694",
      "title": "A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference",
      "authors": [
        {
          "name": "Qingwen Pu",
          "affiliation": null
        },
        {
          "name": "Kun Xie",
          "affiliation": null
        },
        {
          "name": "Hong Yang",
          "affiliation": null
        },
        {
          "name": "Guocong Zhai",
          "affiliation": null
        }
      ],
      "abstract": "Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.",
      "publishedDate": "2026-01-02T14:13:28Z",
      "updatedDate": "2026-01-02T14:13:28Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00694v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00694",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00635",
      "title": "SEMODS: A Validated Dataset of Open-Source Software Engineering Models",
      "authors": [
        {
          "name": "Alexandra González",
          "affiliation": null
        },
        {
          "name": "Xavier Franch",
          "affiliation": null
        },
        {
          "name": "Silverio Martínez-Fernández",
          "affiliation": null
        }
      ],
      "abstract": "Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.",
      "publishedDate": "2026-01-02T10:38:24Z",
      "updatedDate": "2026-01-02T10:38:24Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00635v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00635",
      "comment": "Accepted at the 3rd ACM international conference on AI Foundation Models and Software Engineering (FORGE 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00497",
      "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
      "authors": [
        {
          "name": "Lev Sorokin",
          "affiliation": null
        },
        {
          "name": "Ivan Vasilev",
          "affiliation": null
        },
        {
          "name": "Ken E. Friedl",
          "affiliation": null
        },
        {
          "name": "Andrea Stocco",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
      "publishedDate": "2026-01-01T22:30:15Z",
      "updatedDate": "2026-01-05T18:03:57Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00497v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00497",
      "comment": "Accepted for publication at the 33th International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "evaluation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00348",
      "title": "Robust Uncertainty Quantification for Factual Generation of Large Language Models",
      "authors": [
        {
          "name": "Yuhao Zhang",
          "affiliation": null
        },
        {
          "name": "Zhongliang Yang",
          "affiliation": null
        },
        {
          "name": "Linna Zhou",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.",
      "publishedDate": "2026-01-01T14:06:58Z",
      "updatedDate": "2026-01-01T14:06:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00348v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00348",
      "comment": "9 pages, 5 tables, 5 figures, accepted to IJCNN 2025",
      "journalRef": "2025 International Joint Conference on Neural Networks (IJCNN), Rome, Italy, 2025, pp. 1-9",
      "doi": "10.1109/IJCNN64981.2025.11227634",
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00282",
      "title": "Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations",
      "authors": [
        {
          "name": "Qianli Wang",
          "affiliation": null
        },
        {
          "name": "Nils Feldhus",
          "affiliation": null
        },
        {
          "name": "Pepa Atanasova",
          "affiliation": null
        },
        {
          "name": "Fedor Splitt",
          "affiliation": null
        },
        {
          "name": "Simon Ostermann",
          "affiliation": null
        },
        {
          "name": "Sebastian Möller",
          "affiliation": null
        },
        {
          "name": "Vera Schmitt",
          "affiliation": null
        }
      ],
      "abstract": "Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\\%) and faithfulness (up to 2.38\\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.",
      "publishedDate": "2026-01-01T09:50:01Z",
      "updatedDate": "2026-01-01T09:50:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00282v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00282",
      "comment": "In submission",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00274",
      "title": "Making Theft Useless: Adulteration-Based Protection of Proprietary Knowledge Graphs in GraphRAG Systems",
      "authors": [
        {
          "name": "Weijie Wang",
          "affiliation": null
        },
        {
          "name": "Peizhuo Lv",
          "affiliation": null
        },
        {
          "name": "Yan Wang",
          "affiliation": null
        },
        {
          "name": "Rujie Dai",
          "affiliation": null
        },
        {
          "name": "Guokun Xu",
          "affiliation": null
        },
        {
          "name": "Qiujian Lv",
          "affiliation": null
        },
        {
          "name": "Hangcheng Liu",
          "affiliation": null
        },
        {
          "name": "Weiqing Huang",
          "affiliation": null
        },
        {
          "name": "Wei Dong",
          "affiliation": null
        },
        {
          "name": "Jiaheng Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) has emerged as a key technique for enhancing Large Language Models (LLMs) with proprietary Knowledge Graphs (KGs) in knowledge-intensive applications. As these KGs often represent an organization's highly valuable intellectual property (IP), they face a significant risk of theft for private use. In this scenario, attackers operate in isolated environments. This private-use threat renders passive defenses like watermarking ineffective, as they require output access for detection. Simultaneously, the low-latency demands of GraphRAG make strong encryption which incurs prohibitive overhead impractical. To address these challenges, we propose AURA, a novel framework based on Data Adulteration designed to make any stolen KG unusable to an adversary. Our framework pre-emptively injects plausible but false adulterants into the KG. For an attacker, these adulterants deteriorate the retrieved context and lead to factually incorrect responses. Conversely, for authorized users, a secret key enables the efficient filtering of all adulterants via encrypted metadata tags before they are passed to the LLM, ensuring query results remain completely accurate. Our evaluation demonstrates the effectiveness of this approach: AURA degrades the performance of unauthorized systems to an accuracy of just 5.3%, while maintaining 100% fidelity for authorized users with negligible overhead. Furthermore, AURA proves robust against various sanitization attempts, retaining 80.2% of its adulterants.",
      "publishedDate": "2026-01-01T09:27:24Z",
      "updatedDate": "2026-01-01T09:27:24Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00274v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00274",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00254",
      "title": "An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems",
      "authors": [
        {
          "name": "Md Hasan Saju",
          "affiliation": null
        },
        {
          "name": "Maher Muhtadi",
          "affiliation": null
        },
        {
          "name": "Akramul Azim",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.",
      "publishedDate": "2026-01-01T08:05:51Z",
      "updatedDate": "2026-01-01T08:05:51Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00254v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00254",
      "comment": null,
      "journalRef": "https://conf.researchr.org/home/cascon-2025",
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "code-generation",
        "agents",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "agents",
          "tool-use",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00213",
      "title": "Overlooked Safety Vulnerability in LLMs: Malicious Intelligent Optimization Algorithm Request and its Jailbreak",
      "authors": [
        {
          "name": "Haoran Gu",
          "affiliation": null
        },
        {
          "name": "Handing Wang",
          "affiliation": null
        },
        {
          "name": "Yi Mei",
          "affiliation": null
        },
        {
          "name": "Mengjie Zhang",
          "affiliation": null
        },
        {
          "name": "Yaochu Jin",
          "affiliation": null
        }
      ],
      "abstract": "The widespread deployment of large language models (LLMs) has raised growing concerns about their misuse risks and associated safety issues. While prior studies have examined the safety of LLMs in general usage, code generation, and agent-based applications, their vulnerabilities in automated algorithm design remain underexplored. To fill this gap, this study investigates this overlooked safety vulnerability, with a particular focus on intelligent optimization algorithm design, given its prevalent use in complex decision-making scenarios. We introduce MalOptBench, a benchmark consisting of 60 malicious optimization algorithm requests, and propose MOBjailbreak, a jailbreak method tailored for this scenario. Through extensive evaluation of 13 mainstream LLMs including the latest GPT-5 and DeepSeek-V3.1, we reveal that most models remain highly susceptible to such attacks, with an average attack success rate of 83.59% and an average harmfulness score of 4.28 out of 5 on original harmful prompts, and near-complete failure under MOBjailbreak. Furthermore, we assess state-of-the-art plug-and-play defenses that can be applied to closed-source models, and find that they are only marginally effective against MOBjailbreak and prone to exaggerated safety behaviors. These findings highlight the urgent need for stronger alignment techniques to safeguard LLMs against misuse in algorithm design.",
      "publishedDate": "2026-01-01T05:14:32Z",
      "updatedDate": "2026-01-01T05:14:32Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00213v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00213",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00202",
      "title": "Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models",
      "authors": [
        {
          "name": "Wang Xing",
          "affiliation": null
        },
        {
          "name": "Wei Song",
          "affiliation": null
        },
        {
          "name": "Siyu Lin",
          "affiliation": null
        },
        {
          "name": "Chen Wu",
          "affiliation": null
        },
        {
          "name": "Zhesi Li",
          "affiliation": null
        },
        {
          "name": "Man Wang",
          "affiliation": null
        }
      ],
      "abstract": "Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.",
      "publishedDate": "2026-01-01T04:38:00Z",
      "updatedDate": "2026-01-01T04:38:00Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00202v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00202",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00770",
      "title": "LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization",
      "authors": [
        {
          "name": "Simon Paquette-Greenbaum",
          "affiliation": null
        },
        {
          "name": "Jiangbo Yu",
          "affiliation": null
        }
      ],
      "abstract": "Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.",
      "publishedDate": "2026-01-02T18:02:13Z",
      "updatedDate": "2026-01-02T18:02:13Z",
      "primaryCategory": "cs.CE",
      "arxivCategories": [
        "cs.CE",
        "cs.AI",
        "econ.GN"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00770v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00770",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00596",
      "title": "Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence",
      "authors": [
        {
          "name": "Sumanth Balaji",
          "affiliation": null
        },
        {
          "name": "Piyush Mishra",
          "affiliation": null
        },
        {
          "name": "Aashraya Sachdeva",
          "affiliation": null
        },
        {
          "name": "Suraj Agrawal",
          "affiliation": null
        }
      ],
      "abstract": "Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.",
      "publishedDate": "2026-01-02T07:21:23Z",
      "updatedDate": "2026-01-02T07:21:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00596v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00596",
      "comment": "17 pages, 3 figures, preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00516",
      "title": "Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI",
      "authors": [
        {
          "name": "Laksh Advani",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both \"wrong plan for this task\" and \"malformed plan structure.\" On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.",
      "publishedDate": "2026-01-02T00:27:11Z",
      "updatedDate": "2026-01-02T00:27:11Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00516v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00516",
      "comment": "Accepted to AAAI Trustagent 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00268",
      "title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity",
      "authors": [
        {
          "name": "Doyoung Kim",
          "affiliation": null
        },
        {
          "name": "Zhiwei Ren",
          "affiliation": null
        },
        {
          "name": "Jie Hao",
          "affiliation": null
        },
        {
          "name": "Zhongkai Sun",
          "affiliation": null
        },
        {
          "name": "Lichao Wang",
          "affiliation": null
        },
        {
          "name": "Xiyao Ma",
          "affiliation": null
        },
        {
          "name": "Zack Ye",
          "affiliation": null
        },
        {
          "name": "Xu Han",
          "affiliation": null
        },
        {
          "name": "Jun Yin",
          "affiliation": null
        },
        {
          "name": "Heng Ji",
          "affiliation": null
        },
        {
          "name": "Wei Shen",
          "affiliation": null
        },
        {
          "name": "Xing Fan",
          "affiliation": null
        },
        {
          "name": "Benjamin Yao",
          "affiliation": null
        },
        {
          "name": "Chenlei Guo",
          "affiliation": null
        }
      ],
      "abstract": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.",
      "publishedDate": "2026-01-01T09:19:20Z",
      "updatedDate": "2026-01-01T09:19:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00268v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00268",
      "comment": "26 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00227",
      "title": "FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems",
      "authors": [
        {
          "name": "Shanli Xing",
          "affiliation": null
        },
        {
          "name": "Yiyan Zhai",
          "affiliation": null
        },
        {
          "name": "Alexander Jiang",
          "affiliation": null
        },
        {
          "name": "Yixin Dong",
          "affiliation": null
        },
        {
          "name": "Yong Wu",
          "affiliation": null
        },
        {
          "name": "Zihao Ye",
          "affiliation": null
        },
        {
          "name": "Charlie Ruan",
          "affiliation": null
        },
        {
          "name": "Yingyi Huang",
          "affiliation": null
        },
        {
          "name": "Yineng Zhang",
          "affiliation": null
        },
        {
          "name": "Liangsheng Yin",
          "affiliation": null
        },
        {
          "name": "Aksara Bayyapu",
          "affiliation": null
        },
        {
          "name": "Luis Ceze",
          "affiliation": null
        },
        {
          "name": "Tianqi Chen",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.",
      "publishedDate": "2026-01-01T06:18:53Z",
      "updatedDate": "2026-01-01T06:18:53Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00227v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00227",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00513",
      "title": "When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents",
      "authors": [
        {
          "name": "Laksh Advani",
          "affiliation": null
        }
      ],
      "abstract": "Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.",
      "publishedDate": "2026-01-01T23:54:15Z",
      "updatedDate": "2026-01-01T23:54:15Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00513v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00513",
      "comment": "Accepted to Trustagent workshop AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00777",
      "title": "Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection",
      "authors": [
        {
          "name": "Akanksha Chuchra",
          "affiliation": null
        },
        {
          "name": "Shukesh Reddy",
          "affiliation": null
        },
        {
          "name": "Sudeepta Mishra",
          "affiliation": null
        },
        {
          "name": "Abhijit Das",
          "affiliation": null
        },
        {
          "name": "Abhinav Dhall",
          "affiliation": null
        }
      ],
      "abstract": "While Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have shown strong generalisation in detecting image and video deepfakes, their use for audio deepfake detection remains largely unexplored. In this work, we aim to explore the potential of MLLMs for audio deepfake detection. Combining audio inputs with a range of text prompts as queries to find out the viability of MLLMs to learn robust representations across modalities for audio deepfake detection. Therefore, we attempt to explore text-aware and context-rich, question-answer based prompts with binary decisions. We hypothesise that such a feature-guided reasoning will help in facilitating deeper multimodal understanding and enable robust feature learning for audio deepfake detection. We evaluate the performance of two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, in two evaluation modes: (a) zero-shot and (b) fine-tuned. Our experiments demonstrate that combining audio with a multi-prompt approach could be a viable way forward for audio deepfake detection. Our experiments show that the models perform poorly without task-specific training and struggle to generalise to out-of-domain data. However, they achieve good performance on in-domain data with minimal supervision, indicating promising potential for audio deepfake detection.",
      "publishedDate": "2026-01-02T18:17:22Z",
      "updatedDate": "2026-01-02T18:17:22Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00777v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00777",
      "comment": "Accepted at IJCB 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00747",
      "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
      "authors": [
        {
          "name": "Max Ruiz Luyten",
          "affiliation": null
        },
        {
          "name": "Mihaela van der Schaar",
          "affiliation": null
        }
      ],
      "abstract": "State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.",
      "publishedDate": "2026-01-02T17:10:31Z",
      "updatedDate": "2026-01-02T17:10:31Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00747v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00747",
      "comment": "56 pages, 9 figures, submitted to Twenty-Ninth Annual Conference on Artificial Intelligence and Statistics",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00736",
      "title": "Exploring the Performance of Large Language Models on Subjective Span Identification Tasks",
      "authors": [
        {
          "name": "Alphaeus Dmonte",
          "affiliation": null
        },
        {
          "name": "Roland Oruche",
          "affiliation": null
        },
        {
          "name": "Tharindu Ranasinghe",
          "affiliation": null
        },
        {
          "name": "Marcos Zampieri",
          "affiliation": null
        },
        {
          "name": "Prasad Calyam",
          "affiliation": null
        }
      ],
      "abstract": "Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.",
      "publishedDate": "2026-01-02T16:30:14Z",
      "updatedDate": "2026-01-02T16:30:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00736v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00736",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00730",
      "title": "Grading Handwritten Engineering Exams with Multimodal Large Language Models",
      "authors": [
        {
          "name": "Janez Perš",
          "affiliation": null
        },
        {
          "name": "Jon Muhovič",
          "affiliation": null
        },
        {
          "name": "Andrej Košir",
          "affiliation": null
        },
        {
          "name": "Boštjan Murovec",
          "affiliation": null
        }
      ],
      "abstract": "Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\\approx$17% at $D_{\\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.",
      "publishedDate": "2026-01-02T16:10:08Z",
      "updatedDate": "2026-01-02T16:10:08Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00730v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00730",
      "comment": "10 pages, 5 figures, 2 tables. Supplementary material available at https://lmi.fe.uni-lj.si/en/janez-pers-2/supplementary-material/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00680",
      "title": "Sigmoid Head for Quality Estimation under Language Ambiguity",
      "authors": [
        {
          "name": "Tu Anh Dinh",
          "affiliation": null
        },
        {
          "name": "Jan Niehues",
          "affiliation": null
        }
      ],
      "abstract": "Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.",
      "publishedDate": "2026-01-02T13:12:28Z",
      "updatedDate": "2026-01-02T13:12:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00680v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00680",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00675",
      "title": "RoboReward: General-Purpose Vision-Language Reward Models for Robotics",
      "authors": [
        {
          "name": "Tony Lee",
          "affiliation": null
        },
        {
          "name": "Andrew Wagenmaker",
          "affiliation": null
        },
        {
          "name": "Karl Pertsch",
          "affiliation": null
        },
        {
          "name": "Percy Liang",
          "affiliation": null
        },
        {
          "name": "Sergey Levine",
          "affiliation": null
        },
        {
          "name": "Chelsea Finn",
          "affiliation": null
        }
      ],
      "abstract": "A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotics, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) RoboReward, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a negative examples data augmentation pipeline that generates calibrated negative and near-misses via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we build a large training and evaluation dataset spanning diverse tasks and embodiments to test whether state-of-the-art VLMs can reliably provide rewards for robot learning. Our evaluation of open and proprietary VLMs finds that no model excels across tasks, highlighting substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B model in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5 while narrowing the gap to RL training with human-provided rewards. We release the full dataset, trained reward models, and evaluation suite on our website to advance the development of general-purpose reward models in robotics: https://crfm.stanford.edu/helm/robo-reward-bench (project website).",
      "publishedDate": "2026-01-02T12:47:34Z",
      "updatedDate": "2026-01-08T08:49:12Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00675v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00675",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "robotics",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00575",
      "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
      "authors": [
        {
          "name": "Ishir Garg",
          "affiliation": null
        },
        {
          "name": "Neel Kolhe",
          "affiliation": null
        },
        {
          "name": "Xuandong Zhao",
          "affiliation": null
        },
        {
          "name": "Dawn Song",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/",
      "publishedDate": "2026-01-02T05:26:27Z",
      "updatedDate": "2026-01-02T05:26:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00575v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00575",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00559",
      "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
      "authors": [
        {
          "name": "Jason Quantrill",
          "affiliation": null
        },
        {
          "name": "Noura Khajehnouri",
          "affiliation": null
        },
        {
          "name": "Zihan Guo",
          "affiliation": null
        },
        {
          "name": "Manar H. Alalfi",
          "affiliation": null
        }
      ],
      "abstract": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
      "publishedDate": "2026-01-02T04:17:36Z",
      "updatedDate": "2026-01-02T04:17:36Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00559v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00559",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00506",
      "title": "Rule-Based Approaches to Atomic Sentence Extraction",
      "authors": [
        {
          "name": "Lineesha Kamana",
          "affiliation": null
        },
        {
          "name": "Akshita Ananda Subramanian",
          "affiliation": null
        },
        {
          "name": "Mehuli Ghosh",
          "affiliation": null
        },
        {
          "name": "Suman Saha",
          "affiliation": null
        }
      ],
      "abstract": "Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the \"split-and-rephrase\" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.",
      "publishedDate": "2026-01-01T23:19:51Z",
      "updatedDate": "2026-01-01T23:19:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00506v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00506",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00501",
      "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
      "authors": [
        {
          "name": "Ahmad Rezaei",
          "affiliation": null
        },
        {
          "name": "Mohsen Gholami",
          "affiliation": null
        },
        {
          "name": "Saeed Ranjbar Alvar",
          "affiliation": null
        },
        {
          "name": "Kevin Cannons",
          "affiliation": null
        },
        {
          "name": "Mohammad Asiful Hossain",
          "affiliation": null
        },
        {
          "name": "Zhou Weimin",
          "affiliation": null
        },
        {
          "name": "Shunbo Zhou",
          "affiliation": null
        },
        {
          "name": "Yong Zhang",
          "affiliation": null
        },
        {
          "name": "Mohammad Akbari",
          "affiliation": null
        }
      ],
      "abstract": "We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.",
      "publishedDate": "2026-01-01T22:48:26Z",
      "updatedDate": "2026-01-01T22:48:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00501v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00501",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00482",
      "title": "Multi-Agent Coordinated Rename Refactoring",
      "authors": [
        {
          "name": "Abhiram Bellur",
          "affiliation": null
        },
        {
          "name": "Mohammed Raihan Ullah",
          "affiliation": null
        },
        {
          "name": "Fraol Batole",
          "affiliation": null
        },
        {
          "name": "Mohit Kansara",
          "affiliation": null
        },
        {
          "name": "Masaharu Morimoto",
          "affiliation": null
        },
        {
          "name": "Kai Ishikawa",
          "affiliation": null
        },
        {
          "name": "Haifeng Chen",
          "affiliation": null
        },
        {
          "name": "Yaroslav Zharov",
          "affiliation": null
        },
        {
          "name": "Timofey Bryksin",
          "affiliation": null
        },
        {
          "name": "Tien N. Nguyen",
          "affiliation": null
        },
        {
          "name": "Hridesh Rajan",
          "affiliation": null
        },
        {
          "name": "Danny Dig",
          "affiliation": null
        }
      ],
      "abstract": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat. We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...",
      "publishedDate": "2026-01-01T21:29:43Z",
      "updatedDate": "2026-01-01T21:29:43Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00482v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00482",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "multi-agent",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00448",
      "title": "Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games",
      "authors": [
        {
          "name": "Dimitris Vartziotis",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.",
      "publishedDate": "2026-01-01T19:15:17Z",
      "updatedDate": "2026-01-01T19:15:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00448v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00448",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00388",
      "title": "Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach",
      "authors": [
        {
          "name": "Biao Wu",
          "affiliation": null
        },
        {
          "name": "Meng Fang",
          "affiliation": null
        },
        {
          "name": "Ling Chen",
          "affiliation": null
        },
        {
          "name": "Ke Xu",
          "affiliation": null
        },
        {
          "name": "Tao Cheng",
          "affiliation": null
        },
        {
          "name": "Jun Wang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.",
      "publishedDate": "2026-01-01T16:51:41Z",
      "updatedDate": "2026-01-05T18:27:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00388v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00388",
      "comment": "Accepted to AAAI 2026. Project Page: https://github.com/aialt/geo-r",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00364",
      "title": "The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining",
      "authors": [
        {
          "name": "Jiandong Shao",
          "affiliation": null
        },
        {
          "name": "Raphael Tang",
          "affiliation": null
        },
        {
          "name": "Crystina Zhang",
          "affiliation": null
        },
        {
          "name": "Karin Sevegnani",
          "affiliation": null
        },
        {
          "name": "Pontus Stenetorp",
          "affiliation": null
        },
        {
          "name": "Jianfei Yang",
          "affiliation": null
        },
        {
          "name": "Yao Lu",
          "affiliation": null
        }
      ],
      "abstract": "Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.",
      "publishedDate": "2026-01-01T14:52:06Z",
      "updatedDate": "2026-01-01T14:52:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00364v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00364",
      "comment": "under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00339",
      "title": "Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems",
      "authors": [
        {
          "name": "Alaa Saleh",
          "affiliation": null
        },
        {
          "name": "Praveen Kumar Donta",
          "affiliation": null
        },
        {
          "name": "Roberto Morabito",
          "affiliation": null
        },
        {
          "name": "Sasu Tarkoma",
          "affiliation": null
        },
        {
          "name": "Anders Lindgren",
          "affiliation": null
        },
        {
          "name": "Qiyang Zhang",
          "affiliation": null
        },
        {
          "name": "Schahram Dustdar Susanna Pirttikangas",
          "affiliation": null
        },
        {
          "name": "Lauri Lovén",
          "affiliation": null
        }
      ],
      "abstract": "Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.",
      "publishedDate": "2026-01-01T13:30:38Z",
      "updatedDate": "2026-01-01T13:30:38Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.DC",
        "cs.ET",
        "cs.MA",
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00339v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00339",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00269",
      "title": "FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering",
      "authors": [
        {
          "name": "Chaodong Tong",
          "affiliation": null
        },
        {
          "name": "Qi Zhang",
          "affiliation": null
        },
        {
          "name": "Chen Li",
          "affiliation": null
        },
        {
          "name": "Lei Jiang",
          "affiliation": null
        },
        {
          "name": "Yanbing Liu",
          "affiliation": null
        }
      ],
      "abstract": "Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.",
      "publishedDate": "2026-01-01T09:19:39Z",
      "updatedDate": "2026-01-01T09:19:39Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00269v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00269",
      "comment": "14 pages, 9 figures, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00215",
      "title": "From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning",
      "authors": [
        {
          "name": "Omar Sharif",
          "affiliation": null
        },
        {
          "name": "Eftekhar Hossain",
          "affiliation": null
        },
        {
          "name": "Patrick Ng",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7. To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.",
      "publishedDate": "2026-01-01T05:19:28Z",
      "updatedDate": "2026-01-01T05:19:28Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00215v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00215",
      "comment": "23 pages, 15 Figures, 10 Tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00166",
      "title": "Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description",
      "authors": [
        {
          "name": "Yongmin Yoo",
          "affiliation": null
        },
        {
          "name": "Kris W Pan",
          "affiliation": null
        }
      ],
      "abstract": "Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.",
      "publishedDate": "2026-01-01T02:10:26Z",
      "updatedDate": "2026-01-01T02:10:26Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00166v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00166",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00150",
      "title": "FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications",
      "authors": [
        {
          "name": "Yehui Yang",
          "affiliation": null
        },
        {
          "name": "Dalu Yang",
          "affiliation": null
        },
        {
          "name": "Wenshuo Zhou",
          "affiliation": null
        },
        {
          "name": "Fangxin Shang",
          "affiliation": null
        },
        {
          "name": "Yifan Liu",
          "affiliation": null
        },
        {
          "name": "Jie Ren",
          "affiliation": null
        },
        {
          "name": "Haojun Fei",
          "affiliation": null
        },
        {
          "name": "Qing Yang",
          "affiliation": null
        },
        {
          "name": "Yanwu Xu",
          "affiliation": null
        },
        {
          "name": "Tao Chen",
          "affiliation": null
        }
      ],
      "abstract": "As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.",
      "publishedDate": "2026-01-01T00:42:54Z",
      "updatedDate": "2026-01-06T08:08:49Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.MM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00150v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00150",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00685",
      "title": "Human-like AI-based Auto-Field-in-Field Whole-Brain Radiotherapy Treatment Planning With Conversation Large Language Model Feedback",
      "authors": [
        {
          "name": "Adnan Jafar",
          "affiliation": null
        },
        {
          "name": "An Qin",
          "affiliation": null
        },
        {
          "name": "Gavin Atkins",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Hu",
          "affiliation": null
        },
        {
          "name": "Yin Gao",
          "affiliation": null
        },
        {
          "name": "Xun Jia",
          "affiliation": null
        }
      ],
      "abstract": "Whole-brain radiotherapy (WBRT) is a common treatment due to its simplicity and effectiveness. While automated Field-in-Field (Auto-FiF) functions assist WBRT planning in modern treatment planning systems, it still requires manual approaches for optimal plan generation including patient-specific hyperparameters definition and plan refinement based on quality feedback. This study introduces an automated WBRT planning pipeline that integrates a deep learning (DL) Hyperparameter Prediction model for patient-specific parameter generation and a large-language model (LLM)-based conversational interface for interactive plan refinement. The Hyperparameter Prediction module was trained on 55 WBRT cases using geometric features of clinical target volume (CTV) and organs at risk (OARs) to determine optimal Auto-FiF settings in RayStation treatment planning system. Plans were generated under predicted hyperparameters. For cases in which the generated plan was suboptimal, quality feedback via voice input was captured by a Conversation module, transcribed using Whisper, and interpreted by GPT-4o to adjust planning settings. Plan quality was evaluated in 15 independent cases using clinical metrics and expert review, and model explainability was supported through analysis of feature importance. Fourteen of 15 DL-generated plans were clinically acceptable. Normalized to identical CTV D95% as the clinical plans, the DL-generated and clinical plans showed no statistically significant differences in doses to the eyes, lenses, or CTV dose metrics D1% and D99%. The DL-based planning required under 1 minute of computation and achieved total workflow execution in approximately 7 minutes with a single mouse click, compared to 15 minutes for manual planning. In cases requiring adjustment, the Conversational module successfully improved dose conformity and hotspot reduction.",
      "publishedDate": "2026-01-02T13:23:12Z",
      "updatedDate": "2026-01-02T13:23:12Z",
      "primaryCategory": "physics.med-ph",
      "arxivCategories": [
        "physics.med-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00685v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00685",
      "comment": "22 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00742",
      "title": "Materials Informatics: Emergence To Autonomous Discovery In The Age Of AI",
      "authors": [
        {
          "name": "Turab Lookman",
          "affiliation": null
        },
        {
          "name": "YuJie Liu",
          "affiliation": null
        },
        {
          "name": "Zhibin Gao",
          "affiliation": null
        }
      ],
      "abstract": "This perspective explores the evolution of materials informatics, from its foundational roots in physics and information theory to its maturation through artificial intelligence (AI). We trace the field's trajectory from early milestones to the transformative impact of the Materials Genome Initiative and the recent advent of large language models (LLMs). Rather than a mere toolkit, we present materials informatics as an evolving ecosystem, reviewing key methodologies such as Bayesian Optimization, Reinforcement Learning, and Transformers that drive inverse design and autonomous self-driving laboratories. We specifically address the practical challenges of LLM integration, comparing specialist versus generalist models and discussing solutions for uncertainty quantification. Looking forward, we assess the transition of AI from a predictive tool to a collaborative research partner. By leveraging active learning and retrieval-augmented generation (RAG), the field is moving toward a new era of autonomous materials science, increasingly characterized by \"human-out-of-the-loop\" discovery processes.",
      "publishedDate": "2026-01-02T16:53:19Z",
      "updatedDate": "2026-01-02T16:53:19Z",
      "primaryCategory": "physics.comp-ph",
      "arxivCategories": [
        "physics.comp-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00742v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00742",
      "comment": "44 pages, 14 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "agents"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00536",
      "title": "Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends",
      "authors": [
        {
          "name": "Yuelyu Ji",
          "affiliation": null
        },
        {
          "name": "Zhuochun Li",
          "affiliation": null
        },
        {
          "name": "Rui Meng",
          "affiliation": null
        },
        {
          "name": "Daqing He",
          "affiliation": null
        }
      ],
      "abstract": "Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \\emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.",
      "publishedDate": "2026-01-02T02:38:01Z",
      "updatedDate": "2026-01-02T02:38:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00536v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00536",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00526",
      "title": "Federated Customization of Large Models: Approaches, Experiments, and Insights",
      "authors": [
        {
          "name": "Yuchuan Ye",
          "affiliation": null
        },
        {
          "name": "Ming Ding",
          "affiliation": null
        },
        {
          "name": "Youjia Chen",
          "affiliation": null
        },
        {
          "name": "Peng Cheng",
          "affiliation": null
        },
        {
          "name": "Dusit Niyato",
          "affiliation": null
        }
      ],
      "abstract": "In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.",
      "publishedDate": "2026-01-02T01:45:52Z",
      "updatedDate": "2026-01-02T01:45:52Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.DC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00526v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00526",
      "comment": "8 pages, 1 figure",
      "journalRef": null,
      "doi": "10.1109/MNET.2025.3648812",
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00509",
      "title": "Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback",
      "authors": [
        {
          "name": "Vidyut Sriram",
          "affiliation": null
        },
        {
          "name": "Sawan Pandita",
          "affiliation": null
        },
        {
          "name": "Achintya Lakshmanan",
          "affiliation": null
        },
        {
          "name": "Aneesh Shamraj",
          "affiliation": null
        },
        {
          "name": "Suman Saha",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on \"stubborn\" models.",
      "publishedDate": "2026-01-01T23:34:00Z",
      "updatedDate": "2026-01-01T23:34:00Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00509v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00509",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00376",
      "title": "In Line with Context: Repository-Level Code Generation via Context Inlining",
      "authors": [
        {
          "name": "Chao Hu",
          "affiliation": null
        },
        {
          "name": "Wenhao Zeng",
          "affiliation": null
        },
        {
          "name": "Yuling Shi",
          "affiliation": null
        },
        {
          "name": "Beijun Shen",
          "affiliation": null
        },
        {
          "name": "Xiaodong Gu",
          "affiliation": null
        }
      ],
      "abstract": "Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.",
      "publishedDate": "2026-01-01T15:56:24Z",
      "updatedDate": "2026-01-01T15:56:24Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00376v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00376",
      "comment": "Accepted to FSE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "rag",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00216",
      "title": "From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark",
      "authors": [
        {
          "name": "Jinning Zhang",
          "affiliation": null
        },
        {
          "name": "Jie Song",
          "affiliation": null
        },
        {
          "name": "Wenhui Tu",
          "affiliation": null
        },
        {
          "name": "Zecheng Li",
          "affiliation": null
        },
        {
          "name": "Jingxuan Li",
          "affiliation": null
        },
        {
          "name": "Jin Li",
          "affiliation": null
        },
        {
          "name": "Xuan Liu",
          "affiliation": null
        },
        {
          "name": "Taole Sha",
          "affiliation": null
        },
        {
          "name": "Zichen Wei",
          "affiliation": null
        },
        {
          "name": "Yan Li",
          "affiliation": null
        }
      ],
      "abstract": "In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.",
      "publishedDate": "2026-01-01T05:20:54Z",
      "updatedDate": "2026-01-01T05:20:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00216v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00216",
      "comment": "35 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00641",
      "title": "Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs",
      "authors": [
        {
          "name": "Nils Rautenberg",
          "affiliation": null
        },
        {
          "name": "Sven Schippkus",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting. We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer. Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.",
      "publishedDate": "2026-01-02T10:52:33Z",
      "updatedDate": "2026-01-02T10:52:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00641v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00641",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00624",
      "title": "Do Chatbot LLMs Talk Too Much? The YapBench Benchmark",
      "authors": [
        {
          "name": "Vadim Borisov",
          "affiliation": null
        },
        {
          "name": "Michael Gröger",
          "affiliation": null
        },
        {
          "name": "Mina Mikhael",
          "affiliation": null
        },
        {
          "name": "Richard H. Schreiber",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality. We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores. YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.",
      "publishedDate": "2026-01-02T09:43:52Z",
      "updatedDate": "2026-01-02T09:43:52Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00624v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00624",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "tool-use",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00504",
      "title": "MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation",
      "authors": [
        {
          "name": "Miaowei Wang",
          "affiliation": null
        },
        {
          "name": "Jakub Zadrożny",
          "affiliation": null
        },
        {
          "name": "Oisin Mac Aodha",
          "affiliation": null
        },
        {
          "name": "Amir Vaxman",
          "affiliation": null
        }
      ],
      "abstract": "Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.",
      "publishedDate": "2026-01-01T22:56:37Z",
      "updatedDate": "2026-01-01T22:56:37Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00504v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00504",
      "comment": "AAAI2026 Accepted",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00469",
      "title": "DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis",
      "authors": [
        {
          "name": "Negin Ayoughi",
          "affiliation": null
        },
        {
          "name": "David Dewar",
          "affiliation": null
        },
        {
          "name": "Shiva Nejati",
          "affiliation": null
        },
        {
          "name": "Mehrdad Sabetzadeh",
          "affiliation": null
        }
      ],
      "abstract": "Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.",
      "publishedDate": "2026-01-01T20:48:15Z",
      "updatedDate": "2026-01-05T17:09:37Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00469v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00469",
      "comment": "Accepted for publication in ICSE-SEIP 2026",
      "journalRef": null,
      "doi": "10.1145/3786583.3786879",
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00224",
      "title": "Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback",
      "authors": [
        {
          "name": "Yan Sun",
          "affiliation": null
        },
        {
          "name": "Ming Cai",
          "affiliation": null
        },
        {
          "name": "Stanley Kok",
          "affiliation": null
        }
      ],
      "abstract": "As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.",
      "publishedDate": "2026-01-01T06:10:06Z",
      "updatedDate": "2026-01-07T15:49:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00224v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00224",
      "comment": "WITS 2025 (Workshop on Information Technologies and Systems 2025)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-11T04:11:33.599Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02298",
      "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)",
      "authors": [
        {
          "name": "Mahmoud Elgenedy",
          "affiliation": null
        }
      ],
      "abstract": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.",
      "publishedDate": "2026-01-05T17:33:16Z",
      "updatedDate": "2026-01-05T17:33:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "eess.SP"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02298v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02298",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.02186",
      "title": "Toward Global Large Language Models in Medicine",
      "authors": [
        {
          "name": "Rui Yang",
          "affiliation": null
        },
        {
          "name": "Huitao Li",
          "affiliation": null
        },
        {
          "name": "Weihao Xuan",
          "affiliation": null
        },
        {
          "name": "Heli Qi",
          "affiliation": null
        },
        {
          "name": "Xin Li",
          "affiliation": null
        },
        {
          "name": "Kunyu Yu",
          "affiliation": null
        },
        {
          "name": "Yingjian Chen",
          "affiliation": null
        },
        {
          "name": "Rongrong Wang",
          "affiliation": null
        },
        {
          "name": "Jacques Behmoaras",
          "affiliation": null
        },
        {
          "name": "Tianxi Cai",
          "affiliation": null
        },
        {
          "name": "Bibhas Chakraborty",
          "affiliation": null
        },
        {
          "name": "Qingyu Chen",
          "affiliation": null
        },
        {
          "name": "Lionel Tim-Ee Cheng",
          "affiliation": null
        },
        {
          "name": "Marie-Louise Damwanza",
          "affiliation": null
        },
        {
          "name": "Chido Dzinotyiwei",
          "affiliation": null
        },
        {
          "name": "Aosong Feng",
          "affiliation": null
        },
        {
          "name": "Chuan Hong",
          "affiliation": null
        },
        {
          "name": "Yusuke Iwasawa",
          "affiliation": null
        },
        {
          "name": "Yuhe Ke",
          "affiliation": null
        },
        {
          "name": "Linah Kitala",
          "affiliation": null
        },
        {
          "name": "Taehoon Ko",
          "affiliation": null
        },
        {
          "name": "Jisan Lee",
          "affiliation": null
        },
        {
          "name": "Irene Li",
          "affiliation": null
        },
        {
          "name": "Jonathan Chong Kai Liew",
          "affiliation": null
        },
        {
          "name": "Hongfang Liu",
          "affiliation": null
        },
        {
          "name": "Lian Leng Low",
          "affiliation": null
        },
        {
          "name": "Edison Marrese-Taylor",
          "affiliation": null
        },
        {
          "name": "Yutaka Matsuo",
          "affiliation": null
        },
        {
          "name": "Isheanesu Misi",
          "affiliation": null
        },
        {
          "name": "Yilin Ning",
          "affiliation": null
        },
        {
          "name": "Jasmine Chiat Ling Ong",
          "affiliation": null
        },
        {
          "name": "Marcus Eng Hock Ong",
          "affiliation": null
        },
        {
          "name": "Enrico Petretto",
          "affiliation": null
        },
        {
          "name": "Hossein Rouhizadeh",
          "affiliation": null
        },
        {
          "name": "Abiram Sandralegar",
          "affiliation": null
        },
        {
          "name": "Oren Schreier",
          "affiliation": null
        },
        {
          "name": "Iain Bee Huat Tan",
          "affiliation": null
        },
        {
          "name": "Patrick Tan",
          "affiliation": null
        },
        {
          "name": "Daniel Shu Wei Ting",
          "affiliation": null
        },
        {
          "name": "Junjue Wang",
          "affiliation": null
        },
        {
          "name": "Chunhua Weng",
          "affiliation": null
        },
        {
          "name": "Matthew Yu Heng Wong",
          "affiliation": null
        },
        {
          "name": "Fang Wu",
          "affiliation": null
        },
        {
          "name": "Yunze Xiao",
          "affiliation": null
        },
        {
          "name": "Xuhai Xu",
          "affiliation": null
        },
        {
          "name": "Qingcheng Zeng",
          "affiliation": null
        },
        {
          "name": "Zhuo Zheng",
          "affiliation": null
        },
        {
          "name": "Yifan Peng",
          "affiliation": null
        },
        {
          "name": "Douglas Teodoro",
          "affiliation": null
        },
        {
          "name": "Nan Liu",
          "affiliation": null
        }
      ],
      "abstract": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.",
      "publishedDate": "2026-01-05T15:05:49Z",
      "updatedDate": "2026-01-05T15:05:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02186v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02186",
      "comment": "182 pages, 65 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02179",
      "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
      "authors": [
        {
          "name": "Caiqi Zhang",
          "affiliation": null
        },
        {
          "name": "Ruihan Yang",
          "affiliation": null
        },
        {
          "name": "Xiaochen Zhu",
          "affiliation": null
        },
        {
          "name": "Chengzu Li",
          "affiliation": null
        },
        {
          "name": "Tiancheng Hu",
          "affiliation": null
        },
        {
          "name": "Yijiang River Dong",
          "affiliation": null
        },
        {
          "name": "Deqing Yang",
          "affiliation": null
        },
        {
          "name": "Nigel Collier",
          "affiliation": null
        }
      ],
      "abstract": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
      "publishedDate": "2026-01-05T14:58:04Z",
      "updatedDate": "2026-01-05T14:58:04Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02179v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02179",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02078",
      "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot",
      "authors": [
        {
          "name": "Chenghao Yin",
          "affiliation": null
        },
        {
          "name": "Da Huang",
          "affiliation": null
        },
        {
          "name": "Di Yang",
          "affiliation": null
        },
        {
          "name": "Jichao Wang",
          "affiliation": null
        },
        {
          "name": "Nanshu Zhao",
          "affiliation": null
        },
        {
          "name": "Chen Xu",
          "affiliation": null
        },
        {
          "name": "Wenjun Sun",
          "affiliation": null
        },
        {
          "name": "Linjie Hou",
          "affiliation": null
        },
        {
          "name": "Zhijun Li",
          "affiliation": null
        },
        {
          "name": "Junhui Wu",
          "affiliation": null
        },
        {
          "name": "Zhaobo Liu",
          "affiliation": null
        },
        {
          "name": "Zhen Xiao",
          "affiliation": null
        },
        {
          "name": "Sheng Zhang",
          "affiliation": null
        },
        {
          "name": "Lei Bao",
          "affiliation": null
        },
        {
          "name": "Rui Feng",
          "affiliation": null
        },
        {
          "name": "Zhenquan Pang",
          "affiliation": null
        },
        {
          "name": "Jiayu Li",
          "affiliation": null
        },
        {
          "name": "Qian Wang",
          "affiliation": null
        },
        {
          "name": "Maoqing Yao",
          "affiliation": null
        }
      ],
      "abstract": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.",
      "publishedDate": "2026-01-05T12:59:39Z",
      "updatedDate": "2026-01-05T12:59:39Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02078v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02078",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "prompting",
        "tool-use",
        "rag",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "tool-use",
          "rag",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02071",
      "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations",
      "authors": [
        {
          "name": "Adeshola Okubena",
          "affiliation": null
        },
        {
          "name": "Yusuf Ali Mohammed",
          "affiliation": null
        },
        {
          "name": "Moe Elbadawi",
          "affiliation": null
        }
      ],
      "abstract": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.",
      "publishedDate": "2026-01-05T12:50:50Z",
      "updatedDate": "2026-01-07T10:02:54Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02071v2",
      "arxivUrl": "https://arxiv.org/abs/2601.02071",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01925",
      "title": "AR-MOT: Autoregressive Multi-object Tracking",
      "authors": [
        {
          "name": "Lianjie Jia",
          "affiliation": null
        },
        {
          "name": "Yuhan Wu",
          "affiliation": null
        },
        {
          "name": "Binghao Ran",
          "affiliation": null
        },
        {
          "name": "Yifan Wang",
          "affiliation": null
        },
        {
          "name": "Lijun Wang",
          "affiliation": null
        },
        {
          "name": "Huchuan Lu",
          "affiliation": null
        }
      ],
      "abstract": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
      "publishedDate": "2026-01-05T09:17:28Z",
      "updatedDate": "2026-01-05T09:17:28Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01925v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01925",
      "comment": "12 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01868",
      "title": "DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs",
      "authors": [
        {
          "name": "Jinghan Ru",
          "affiliation": null
        },
        {
          "name": "Siyuan Yan",
          "affiliation": null
        },
        {
          "name": "Yuguo Yin",
          "affiliation": null
        },
        {
          "name": "Yuexian Zou",
          "affiliation": null
        },
        {
          "name": "Zongyuan Ge",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.",
      "publishedDate": "2026-01-05T07:55:36Z",
      "updatedDate": "2026-01-05T07:55:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01868v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01868",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01836",
      "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "authors": [
        {
          "name": "Dasol Choi",
          "affiliation": null
        },
        {
          "name": "DongGeon Lee",
          "affiliation": null
        },
        {
          "name": "Brigitta Jesica Kartono",
          "affiliation": null
        },
        {
          "name": "Helena Berndt",
          "affiliation": null
        },
        {
          "name": "Taeyoun Kwon",
          "affiliation": null
        },
        {
          "name": "Joonwon Jang",
          "affiliation": null
        },
        {
          "name": "Haon Park",
          "affiliation": null
        },
        {
          "name": "Hwanjo Yu",
          "affiliation": null
        },
        {
          "name": "Minsuk Kahng",
          "affiliation": null
        }
      ],
      "abstract": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
      "publishedDate": "2026-01-05T06:57:45Z",
      "updatedDate": "2026-01-05T06:57:45Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01836v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01836",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01768",
      "title": "Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation",
      "authors": [
        {
          "name": "Meiman Xiao",
          "affiliation": null
        },
        {
          "name": "Ante Wang",
          "affiliation": null
        },
        {
          "name": "Qingguo Hu",
          "affiliation": null
        },
        {
          "name": "Zhongjian Miao",
          "affiliation": null
        },
        {
          "name": "Huangjun Shen",
          "affiliation": null
        },
        {
          "name": "Longyue Wang",
          "affiliation": null
        },
        {
          "name": "Weihua Luo",
          "affiliation": null
        },
        {
          "name": "Jinsong Su",
          "affiliation": null
        }
      ],
      "abstract": "Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure their response lengths, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.",
      "publishedDate": "2026-01-05T03:49:14Z",
      "updatedDate": "2026-01-07T11:47:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01768v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01768",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01718",
      "title": "Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications",
      "authors": [
        {
          "name": "YuanLab. ai",
          "affiliation": null
        },
        {
          "name": ":",
          "affiliation": null
        },
        {
          "name": "Shawn Wu",
          "affiliation": null
        },
        {
          "name": "Sean Wang",
          "affiliation": null
        },
        {
          "name": "Louie Li",
          "affiliation": null
        },
        {
          "name": "Darcy Chen",
          "affiliation": null
        },
        {
          "name": "Allen Wang",
          "affiliation": null
        },
        {
          "name": "Jiangang Luo",
          "affiliation": null
        },
        {
          "name": "Xudong Zhao",
          "affiliation": null
        },
        {
          "name": "Joseph Shen",
          "affiliation": null
        },
        {
          "name": "Gawain Ma",
          "affiliation": null
        },
        {
          "name": "Jasper Jia",
          "affiliation": null
        },
        {
          "name": "Marcus Mao",
          "affiliation": null
        },
        {
          "name": "Claire Wang",
          "affiliation": null
        },
        {
          "name": "Hunter He",
          "affiliation": null
        },
        {
          "name": "Carol Wang",
          "affiliation": null
        },
        {
          "name": "Zera Zhang",
          "affiliation": null
        },
        {
          "name": "Jason Wang",
          "affiliation": null
        },
        {
          "name": "Chonly Shen",
          "affiliation": null
        },
        {
          "name": "Leo Zhang",
          "affiliation": null
        },
        {
          "name": "Logan Chen",
          "affiliation": null
        },
        {
          "name": "Qasim Meng",
          "affiliation": null
        },
        {
          "name": "James Gong",
          "affiliation": null
        },
        {
          "name": "Danied Zhao",
          "affiliation": null
        },
        {
          "name": "Penn Zheng",
          "affiliation": null
        },
        {
          "name": "Owen Zhu",
          "affiliation": null
        },
        {
          "name": "Tong Yu",
          "affiliation": null
        }
      ],
      "abstract": "We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.",
      "publishedDate": "2026-01-05T01:44:09Z",
      "updatedDate": "2026-01-05T01:44:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01718v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01718",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01609",
      "title": "Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration",
      "authors": [
        {
          "name": "Albert Sadowski",
          "affiliation": null
        },
        {
          "name": "Jarosław A. Chudziak",
          "affiliation": null
        }
      ],
      "abstract": "Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.",
      "publishedDate": "2026-01-04T17:19:20Z",
      "updatedDate": "2026-01-04T17:19:20Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01609v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01609",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01592",
      "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
      "authors": [
        {
          "name": "Xin Wang",
          "affiliation": null
        },
        {
          "name": "Yunhao Chen",
          "affiliation": null
        },
        {
          "name": "Juncheng Li",
          "affiliation": null
        },
        {
          "name": "Yixu Wang",
          "affiliation": null
        },
        {
          "name": "Yang Yao",
          "affiliation": null
        },
        {
          "name": "Tianle Gu",
          "affiliation": null
        },
        {
          "name": "Jie Li",
          "affiliation": null
        },
        {
          "name": "Yan Teng",
          "affiliation": null
        },
        {
          "name": "Xingjun Ma",
          "affiliation": null
        },
        {
          "name": "Yingchun Wang",
          "affiliation": null
        },
        {
          "name": "Xia Hu",
          "affiliation": null
        }
      ],
      "abstract": "The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.",
      "publishedDate": "2026-01-04T16:41:33Z",
      "updatedDate": "2026-01-04T16:41:33Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01592v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01592",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "reasoning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01477",
      "title": "Can Legislation Be Made Machine-Readable in PROLEG?",
      "authors": [
        {
          "name": "May-Myo Zin",
          "affiliation": null
        },
        {
          "name": "Sabine Wehnert",
          "affiliation": null
        },
        {
          "name": "Yuntao Kong",
          "affiliation": null
        },
        {
          "name": "Ha-Thanh Nguyen",
          "affiliation": null
        },
        {
          "name": "Wachara Fungwacharakorn",
          "affiliation": null
        },
        {
          "name": "Jieying Xue",
          "affiliation": null
        },
        {
          "name": "Michał Araszkiewicz",
          "affiliation": null
        },
        {
          "name": "Randy Goebel",
          "affiliation": null
        },
        {
          "name": "Ken Satoh",
          "affiliation": null
        },
        {
          "name": "Le-Minh Nguyen",
          "affiliation": null
        }
      ],
      "abstract": "The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to \"compile\" natural language text to if-then rules, then to further \"compile\" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.",
      "publishedDate": "2026-01-04T10:53:16Z",
      "updatedDate": "2026-01-04T10:53:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01477v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01477",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01432",
      "title": "Personalizing black-box models for nonparametric regression with minimax optimality",
      "authors": [
        {
          "name": "Sai Li",
          "affiliation": null
        },
        {
          "name": "Linjun Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models.",
      "publishedDate": "2026-01-04T08:32:28Z",
      "updatedDate": "2026-01-04T08:32:28Z",
      "primaryCategory": "stat.ME",
      "arxivCategories": [
        "stat.ME",
        "stat.ML"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01432v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01432",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01378",
      "title": "Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification",
      "authors": [
        {
          "name": "Han Yuan",
          "affiliation": null
        },
        {
          "name": "Yilin Wu",
          "affiliation": null
        },
        {
          "name": "Li Zhang",
          "affiliation": null
        },
        {
          "name": "Zheng Ma",
          "affiliation": null
        }
      ],
      "abstract": "Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.",
      "publishedDate": "2026-01-04T05:09:11Z",
      "updatedDate": "2026-01-04T05:09:11Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01378v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01378",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01321",
      "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
      "authors": [
        {
          "name": "Rong Zhou",
          "affiliation": null
        },
        {
          "name": "Dongping Chen",
          "affiliation": null
        },
        {
          "name": "Zihan Jia",
          "affiliation": null
        },
        {
          "name": "Yao Su",
          "affiliation": null
        },
        {
          "name": "Yixin Liu",
          "affiliation": null
        },
        {
          "name": "Yiwen Lu",
          "affiliation": null
        },
        {
          "name": "Dongwei Shi",
          "affiliation": null
        },
        {
          "name": "Yue Huang",
          "affiliation": null
        },
        {
          "name": "Tianyang Xu",
          "affiliation": null
        },
        {
          "name": "Yi Pan",
          "affiliation": null
        },
        {
          "name": "Xinliang Li",
          "affiliation": null
        },
        {
          "name": "Yohannes Abate",
          "affiliation": null
        },
        {
          "name": "Qingyu Chen",
          "affiliation": null
        },
        {
          "name": "Zhengzhong Tu",
          "affiliation": null
        },
        {
          "name": "Yu Yang",
          "affiliation": null
        },
        {
          "name": "Yu Zhang",
          "affiliation": null
        },
        {
          "name": "Qingsong Wen",
          "affiliation": null
        },
        {
          "name": "Gengchen Mai",
          "affiliation": null
        },
        {
          "name": "Sunyang Fu",
          "affiliation": null
        },
        {
          "name": "Jiachen Li",
          "affiliation": null
        },
        {
          "name": "Xuyu Wang",
          "affiliation": null
        },
        {
          "name": "Ziran Wang",
          "affiliation": null
        },
        {
          "name": "Jing Huang",
          "affiliation": null
        },
        {
          "name": "Tianming Liu",
          "affiliation": null
        },
        {
          "name": "Yong Chen",
          "affiliation": null
        },
        {
          "name": "Lichao Sun",
          "affiliation": null
        },
        {
          "name": "Lifang He",
          "affiliation": null
        }
      ],
      "abstract": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.",
      "publishedDate": "2026-01-04T01:17:09Z",
      "updatedDate": "2026-01-04T01:17:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01321v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01321",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "robotics",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "robotics",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01260",
      "title": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance",
      "authors": [
        {
          "name": "Hamad Khan",
          "affiliation": null
        },
        {
          "name": "Saddam Hussain Khan",
          "affiliation": null
        }
      ],
      "abstract": "The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.",
      "publishedDate": "2026-01-03T19:01:33Z",
      "updatedDate": "2026-01-03T19:01:33Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01260v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01260",
      "comment": "28 Pages, Tables 12, Figure 09",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.01239",
      "title": "IO-RAE: Information-Obfuscation Reversible Adversarial Example for Audio Privacy Protection",
      "authors": [
        {
          "name": "Jiajie Zhu",
          "affiliation": null
        },
        {
          "name": "Xia Du",
          "affiliation": null
        },
        {
          "name": "Xiaoyuan Liu",
          "affiliation": null
        },
        {
          "name": "Jizhe Zhou",
          "affiliation": null
        },
        {
          "name": "Qizhen Xu",
          "affiliation": null
        },
        {
          "name": "Zheng Lin",
          "affiliation": null
        },
        {
          "name": "Chi-Man Pun",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancements in artificial intelligence have significantly accelerated the adoption of speech recognition technology, leading to its widespread integration across various applications. However, this surge in usage also highlights a critical issue: audio data is highly vulnerable to unauthorized exposure and analysis, posing significant privacy risks for businesses and individuals. This paper introduces an Information-Obfuscation Reversible Adversarial Example (IO-RAE) framework, the pioneering method designed to safeguard audio privacy using reversible adversarial examples. IO-RAE leverages large language models to generate misleading yet contextually coherent content, effectively preventing unauthorized eavesdropping by humans and Automatic Speech Recognition (ASR) systems. Additionally, we propose the Cumulative Signal Attack technique, which mitigates high-frequency noise and enhances attack efficacy by targeting low-frequency signals. Our approach ensures the protection of audio data without degrading its quality or our ability. Experimental evaluations demonstrate the superiority of our method, achieving a targeted misguidance rate of 96.5% and a remarkable 100% untargeted misguidance rate in obfuscating target keywords across multiple ASR models, including a commercial black-box system from Google. Furthermore, the quality of the recovered audio, measured by the Perceptual Evaluation of Speech Quality score, reached 4.45, comparable to high-quality original recordings. Notably, the recovered audio processed by ASR systems exhibited an error rate of 0%, indicating nearly lossless recovery. These results highlight the practical applicability and effectiveness of our IO-RAE framework in protecting sensitive audio privacy.",
      "publishedDate": "2026-01-03T17:08:35Z",
      "updatedDate": "2026-01-03T17:08:35Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.CR",
        "cs.MM",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01239v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01239",
      "comment": "10 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "tool-use",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01153",
      "title": "SongSage: A Large Musical Language Model with Lyric Generative Pre-training",
      "authors": [
        {
          "name": "Jiani Guo",
          "affiliation": null
        },
        {
          "name": "Jiajia Li",
          "affiliation": null
        },
        {
          "name": "Jie Wu",
          "affiliation": null
        },
        {
          "name": "Zuchao Li",
          "affiliation": null
        },
        {
          "name": "Yujiu Yang",
          "affiliation": null
        },
        {
          "name": "Ping Wang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.",
      "publishedDate": "2026-01-03T10:54:37Z",
      "updatedDate": "2026-01-03T10:54:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01153v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01153",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00995",
      "title": "Grain-Aware Data Transformations: Type-Level Formal Verification at Zero Computational Cost",
      "authors": [
        {
          "name": "Nikos Karayannidis",
          "affiliation": null
        }
      ],
      "abstract": "Data transformation correctness is a major challenge in data engineering: how to verify pipeline accuracy before deployment. Traditional methods involve costly iterative testing, data materialization, and manual error detection, due to the lack of formal approaches to reasoning about data granularity (grain), which can shift during transformations, causing issues like fan traps (metrics duplication) and chasm traps (data loss). We introduce the first formal, mathematical definition of grain, extending it from an informal concept in dimensional modeling to a universal, type-theoretic framework applicable to any data type. Encoding grain into the type system allows compile-time verification of transformation correctness, shifting validation from runtime. We define three core grain relations-equality, ordering, and incomparability-and prove a general grain inference theorem that computes the output grain of equi-joins from input grains using type-level operations. This covers all join scenarios, including comparable and incomparable keys. Together with inference rules for relational operations, this enables verification through schema analysis alone, at zero cost. Our approach allows engineers to verify that entire pipeline DAGs maintain correctness properties, detecting grain-related errors such as fan traps, chasm traps, and aggregation issues before data processing. It emphasizes the importance of grain, focusing on critical characteristics rather than all data details. We provide machine-checked formal proofs in Lean 4, reducing verification costs by 98-99%. Additionally, large language models can automatically generate correctness proofs, shifting human effort from proof writing to proof verification, thus democratizing formal methods in data engineering and supporting confident deployment of AI-generated pipelines with machine-checkable guarantees.",
      "publishedDate": "2026-01-02T22:26:31Z",
      "updatedDate": "2026-01-02T22:26:31Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00995v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00995",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00942",
      "title": "Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures",
      "authors": [
        {
          "name": "Kabir Grover",
          "affiliation": null
        }
      ],
      "abstract": "The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.",
      "publishedDate": "2026-01-02T18:10:10Z",
      "updatedDate": "2026-01-02T18:10:10Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00942v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00942",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00936",
      "title": "Emoji-Based Jailbreaking of Large Language Models",
      "authors": [
        {
          "name": "M P V S Gopinadh",
          "affiliation": null
        },
        {
          "name": "S Mahaboob Hussain",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are integral to modern AI applications, but their safety alignment mechanisms can be bypassed through adversarial prompt engineering. This study investigates emoji-based jailbreaking, where emoji sequences are embedded in textual prompts to trigger harmful and unethical outputs from LLMs. We evaluated 50 emoji-based prompts on four open-source LLMs: Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B. Metrics included jailbreak success rate, safety alignment adherence, and latency, with responses categorized as successful, partial and failed. Results revealed model-specific vulnerabilities: Gemma 2 9B and Mistral 7B exhibited 10 % success rates, while Qwen 2 7B achieved full alignment (0% success). A chi-square test (chi^2 = 32.94, p < 0.001) confirmed significant inter-model differences. While prior works focused on emoji attacks targeting safety judges or classifiers, our empirical analysis examines direct prompt-level vulnerabilities in LLMs. The results reveal limitations in safety mechanisms and highlight the necessity for systematic handling of emoji-based representations in prompt-level safety and alignment pipelines.",
      "publishedDate": "2026-01-02T10:49:06Z",
      "updatedDate": "2026-01-02T10:49:06Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00936v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00936",
      "comment": "7 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02314",
      "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
      "authors": [
        {
          "name": "Sourena Khanzadeh",
          "affiliation": null
        }
      ],
      "abstract": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
      "publishedDate": "2026-01-05T18:05:29Z",
      "updatedDate": "2026-01-05T18:05:29Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02314v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02314",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01885",
      "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents",
      "authors": [
        {
          "name": "Yi Yu",
          "affiliation": null
        },
        {
          "name": "Liuyi Yao",
          "affiliation": null
        },
        {
          "name": "Yuexiang Xie",
          "affiliation": null
        },
        {
          "name": "Qingquan Tan",
          "affiliation": null
        },
        {
          "name": "Jiaqi Feng",
          "affiliation": null
        },
        {
          "name": "Yaliang Li",
          "affiliation": null
        },
        {
          "name": "Libing Wu",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.",
      "publishedDate": "2026-01-05T08:24:16Z",
      "updatedDate": "2026-01-05T08:24:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01885v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01885",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01522",
      "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making",
      "authors": [
        {
          "name": "Danial Amin",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.",
      "publishedDate": "2026-01-04T13:19:27Z",
      "updatedDate": "2026-01-04T13:19:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.ET"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01522v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01522",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01498",
      "title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents",
      "authors": [
        {
          "name": "Bingguang Hao",
          "affiliation": null
        },
        {
          "name": "Zengzhuang Xu",
          "affiliation": null
        },
        {
          "name": "Yuntao Wen",
          "affiliation": null
        },
        {
          "name": "Xinyi Xu",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        },
        {
          "name": "Tong Zhao",
          "affiliation": null
        },
        {
          "name": "Maolin Wang",
          "affiliation": null
        },
        {
          "name": "Long Chen",
          "affiliation": null
        },
        {
          "name": "Dong Wang",
          "affiliation": null
        },
        {
          "name": "Yicheng Chen",
          "affiliation": null
        },
        {
          "name": "Cunyin Peng",
          "affiliation": null
        },
        {
          "name": "Xiangyu Zhao",
          "affiliation": null
        },
        {
          "name": "Chenyi Zhuang",
          "affiliation": null
        },
        {
          "name": "Ji Zhang",
          "affiliation": null
        }
      ],
      "abstract": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.",
      "publishedDate": "2026-01-04T11:56:33Z",
      "updatedDate": "2026-01-04T11:56:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01498v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01498",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "tool-use",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "tool-use",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01357",
      "title": "Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows",
      "authors": [
        {
          "name": "Ke Xiao",
          "affiliation": null
        },
        {
          "name": "Haoze Zhang",
          "affiliation": null
        },
        {
          "name": "Runze Mao",
          "affiliation": null
        },
        {
          "name": "Han Li",
          "affiliation": null
        },
        {
          "name": "Zhi X. Chen",
          "affiliation": null
        }
      ],
      "abstract": "The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.",
      "publishedDate": "2026-01-04T04:00:28Z",
      "updatedDate": "2026-01-04T04:00:28Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "physics.flu-dyn"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01357v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01357",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01241",
      "title": "MCP-SandboxScan: WASM-based Secure Execution and Runtime Analysis for MCP Tools",
      "authors": [
        {
          "name": "Zhuoran Tan",
          "affiliation": null
        },
        {
          "name": "Run Hao",
          "affiliation": null
        },
        {
          "name": "Jeremy Singer",
          "affiliation": null
        },
        {
          "name": "Yutian Tang",
          "affiliation": null
        },
        {
          "name": "Christos Anagnostopoulos",
          "affiliation": null
        }
      ],
      "abstract": "Tool-augmented LLM agents raise new security risks: tool executions can introduce runtime-only behaviors, including prompt injection and unintended exposure of external inputs (e.g., environment secrets or local files). While existing scanners often focus on static artifacts, analyzing runtime behavior is challenging because directly executing untrusted tools can itself be dangerous. We present MCP-SandboxScan, a lightweight framework motivated by the Model Context Protocol (MCP) that safely executes untrusted tools inside a WebAssembly/WASI sandbox and produces auditable reports of external-to-sink exposures. Our prototype (i) extracts LLM-relevant sinks from runtime outputs (prompt/messages and structured tool-return fields), (ii) instantiates external-input candidates from environment values, mounted file contents, and output-surfaced HTTP fetch intents, and (iii) links sources to sinks via snippet-based substring matching. Case studies on three representative tools show that MCP-SandboxScan can surface provenance evidence when external inputs appear in prompt/messages or tool-return payloads, and can expose filesystem capability violations as runtime evidence. We further compare against a lightweight static string-signature baseline and use a micro-benchmark to characterize false negatives under transformations and false positives from short-token collisions.",
      "publishedDate": "2026-01-03T17:25:38Z",
      "updatedDate": "2026-01-03T17:25:38Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01241v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01241",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01090",
      "title": "Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai",
      "authors": [
        {
          "name": "Erica Coppolillo",
          "affiliation": null
        },
        {
          "name": "Luca Luceri",
          "affiliation": null
        },
        {
          "name": "Emilio Ferrara",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms. We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content. These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.",
      "publishedDate": "2026-01-03T06:33:08Z",
      "updatedDate": "2026-01-03T06:33:08Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA",
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01090v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01090",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00994",
      "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems",
      "authors": [
        {
          "name": "Michael Bao",
          "affiliation": null
        }
      ],
      "abstract": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.",
      "publishedDate": "2026-01-02T22:10:09Z",
      "updatedDate": "2026-01-02T22:10:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00994v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00994",
      "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00930",
      "title": "AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation",
      "authors": [
        {
          "name": "Nicolas Bougie",
          "affiliation": null
        },
        {
          "name": "Gian Maria Marconi",
          "affiliation": null
        },
        {
          "name": "Tony Yip",
          "affiliation": null
        },
        {
          "name": "Narimasa Watanabe",
          "affiliation": null
        }
      ],
      "abstract": "Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.",
      "publishedDate": "2026-01-02T03:01:33Z",
      "updatedDate": "2026-01-02T03:01:33Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00930v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00930",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01891",
      "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems",
      "authors": [
        {
          "name": "Niloufar Alipour Talemi",
          "affiliation": null
        },
        {
          "name": "Julia Boone",
          "affiliation": null
        },
        {
          "name": "Fatemeh Afghah",
          "affiliation": null
        }
      ],
      "abstract": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.",
      "publishedDate": "2026-01-05T08:34:17Z",
      "updatedDate": "2026-01-05T08:34:17Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01891v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01891",
      "comment": "Accepted to the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026, GeoCV Workshop",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning",
          "planning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01857",
      "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios",
      "authors": [
        {
          "name": "Defei Xia",
          "affiliation": null
        },
        {
          "name": "Bingfeng Pi",
          "affiliation": null
        },
        {
          "name": "Shenbin Zhang",
          "affiliation": null
        },
        {
          "name": "Song Hua",
          "affiliation": null
        },
        {
          "name": "Yunfei Wei",
          "affiliation": null
        },
        {
          "name": "Lei Zuo",
          "affiliation": null
        }
      ],
      "abstract": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.",
      "publishedDate": "2026-01-05T07:35:12Z",
      "updatedDate": "2026-01-07T01:48:24Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01857v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01857",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01685",
      "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage",
      "authors": [
        {
          "name": "Jinwei Hu",
          "affiliation": null
        },
        {
          "name": "Xinmiao Huang",
          "affiliation": null
        },
        {
          "name": "Youcheng Sun",
          "affiliation": null
        },
        {
          "name": "Yi Dong",
          "affiliation": null
        },
        {
          "name": "Xiaowei Huang",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.",
      "publishedDate": "2026-01-04T22:50:23Z",
      "updatedDate": "2026-01-04T22:50:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01685v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01685",
      "comment": "Under Review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01366",
      "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models",
      "authors": [
        {
          "name": "Zixian Liu",
          "affiliation": null
        },
        {
          "name": "Sihao Liu",
          "affiliation": null
        },
        {
          "name": "Yuqi Zhao",
          "affiliation": null
        }
      ],
      "abstract": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.",
      "publishedDate": "2026-01-04T04:39:39Z",
      "updatedDate": "2026-01-04T04:39:39Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01366v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01366",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "code-generation",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02346",
      "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
      "authors": [
        {
          "name": "Falcon LLM Team",
          "affiliation": null
        },
        {
          "name": "Iheb Chaabane",
          "affiliation": null
        },
        {
          "name": "Puneesh Khanna",
          "affiliation": null
        },
        {
          "name": "Suhail Mohmad",
          "affiliation": null
        },
        {
          "name": "Slim Frikha",
          "affiliation": null
        },
        {
          "name": "Shi Hu",
          "affiliation": null
        },
        {
          "name": "Abdalgader Abubaker",
          "affiliation": null
        },
        {
          "name": "Reda Alami",
          "affiliation": null
        },
        {
          "name": "Mikhail Lubinets",
          "affiliation": null
        },
        {
          "name": "Mohamed El Amine Seddik",
          "affiliation": null
        },
        {
          "name": "Hakim Hacid",
          "affiliation": null
        }
      ],
      "abstract": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
      "publishedDate": "2026-01-05T18:44:27Z",
      "updatedDate": "2026-01-05T18:44:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02346v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02346",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02224",
      "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality",
      "authors": [
        {
          "name": "Fabian Lukassen",
          "affiliation": null
        },
        {
          "name": "Jan Herrmann",
          "affiliation": null
        },
        {
          "name": "Christoph Weisser",
          "affiliation": null
        },
        {
          "name": "Benjamin Saefken",
          "affiliation": null
        },
        {
          "name": "Thomas Kneib",
          "affiliation": null
        }
      ],
      "abstract": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.",
      "publishedDate": "2026-01-05T15:52:20Z",
      "updatedDate": "2026-01-05T15:52:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02224v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02224",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02170",
      "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Haolang Lu",
          "affiliation": null
        },
        {
          "name": "Minghui Pan",
          "affiliation": null
        },
        {
          "name": "Ripeng Li",
          "affiliation": null
        },
        {
          "name": "Guoshun Nan",
          "affiliation": null
        },
        {
          "name": "Jialin Zhuang",
          "affiliation": null
        },
        {
          "name": "Zijie Zhao",
          "affiliation": null
        },
        {
          "name": "Zhongxiang Sun",
          "affiliation": null
        },
        {
          "name": "Kun Wang",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.",
      "publishedDate": "2026-01-05T14:47:41Z",
      "updatedDate": "2026-01-05T14:47:41Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02170v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02170",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02163",
      "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning",
      "authors": [
        {
          "name": "Chuanrui Hu",
          "affiliation": null
        },
        {
          "name": "Xingze Gao",
          "affiliation": null
        },
        {
          "name": "Zuyi Zhou",
          "affiliation": null
        },
        {
          "name": "Dannong Xu",
          "affiliation": null
        },
        {
          "name": "Yi Bai",
          "affiliation": null
        },
        {
          "name": "Xintong Li",
          "affiliation": null
        },
        {
          "name": "Hui Zhang",
          "affiliation": null
        },
        {
          "name": "Tong Li",
          "affiliation": null
        },
        {
          "name": "Chong Zhang",
          "affiliation": null
        },
        {
          "name": "Lidong Bing",
          "affiliation": null
        },
        {
          "name": "Yafeng Deng",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.",
      "publishedDate": "2026-01-05T14:39:43Z",
      "updatedDate": "2026-01-09T02:23:07Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02163v2",
      "arxivUrl": "https://arxiv.org/abs/2601.02163",
      "comment": "16 pages, 7 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02158",
      "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience",
      "authors": [
        {
          "name": "Almaz Ermilov",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.",
      "publishedDate": "2026-01-05T14:36:02Z",
      "updatedDate": "2026-01-05T14:36:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "physics.geo-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02158v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02158",
      "comment": "24 pages, 8 figures, 10 tables; benchmark and code at https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02147",
      "title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models",
      "authors": [
        {
          "name": "Sunny Gupta",
          "affiliation": null
        },
        {
          "name": "Shounak Das",
          "affiliation": null
        },
        {
          "name": "Amit Sethi",
          "affiliation": null
        }
      ],
      "abstract": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.",
      "publishedDate": "2026-01-05T14:22:20Z",
      "updatedDate": "2026-01-05T14:22:20Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02147v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02147",
      "comment": "Accepted at the AAAI 2026 Workshop AIR-FM, Assessing and Improving Reliability of Foundation Models in the Real World",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02076",
      "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
      "authors": [
        {
          "name": "Yingte Shu",
          "affiliation": null
        },
        {
          "name": "Yuchuan Tian",
          "affiliation": null
        },
        {
          "name": "Chao Xu",
          "affiliation": null
        },
        {
          "name": "Yunhe Wang",
          "affiliation": null
        },
        {
          "name": "Hanting Chen",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
      "publishedDate": "2026-01-05T12:57:33Z",
      "updatedDate": "2026-01-05T12:57:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02076v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02076",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02046",
      "title": "Agentic Retoucher for Text-To-Image Generation",
      "authors": [
        {
          "name": "Shaocheng Shen",
          "affiliation": null
        },
        {
          "name": "Jianfeng Liang",
          "affiliation": null
        },
        {
          "name": "Chunlei Cai",
          "affiliation": null
        },
        {
          "name": "Cong Geng",
          "affiliation": null
        },
        {
          "name": "Huiyu Duan",
          "affiliation": null
        },
        {
          "name": "Xiaoyun Zhang",
          "affiliation": null
        },
        {
          "name": "Qiang Hu",
          "affiliation": null
        },
        {
          "name": "Guangtao Zhai",
          "affiliation": null
        }
      ],
      "abstract": "Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.",
      "publishedDate": "2026-01-05T12:06:43Z",
      "updatedDate": "2026-01-08T10:57:37Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02046v2",
      "arxivUrl": "https://arxiv.org/abs/2601.02046",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02021",
      "title": "AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI",
      "authors": [
        {
          "name": "Runze Zheng",
          "affiliation": null
        },
        {
          "name": "Yuqing Zheng",
          "affiliation": null
        },
        {
          "name": "Zhengyi Cheng",
          "affiliation": null
        },
        {
          "name": "Long Luo",
          "affiliation": null
        },
        {
          "name": "Haoxiang Luo",
          "affiliation": null
        },
        {
          "name": "Gang Sun",
          "affiliation": null
        },
        {
          "name": "Hongfang Yu",
          "affiliation": null
        },
        {
          "name": "Dusit Niyato",
          "affiliation": null
        }
      ],
      "abstract": "The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI.",
      "publishedDate": "2026-01-05T11:30:04Z",
      "updatedDate": "2026-01-05T11:30:04Z",
      "primaryCategory": "cs.NI",
      "arxivCategories": [
        "cs.NI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02021v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02021",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "agents",
        "tool-use",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "agents",
          "tool-use",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01984",
      "title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation",
      "authors": [
        {
          "name": "Weijian Ma",
          "affiliation": null
        },
        {
          "name": "Shizhao Sun",
          "affiliation": null
        },
        {
          "name": "Tianyu Yu",
          "affiliation": null
        },
        {
          "name": "Ruiyu Wang",
          "affiliation": null
        },
        {
          "name": "Tat-Seng Chua",
          "affiliation": null
        },
        {
          "name": "Jiang Bian",
          "affiliation": null
        }
      ],
      "abstract": "Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.",
      "publishedDate": "2026-01-05T10:38:26Z",
      "updatedDate": "2026-01-05T10:38:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01984v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01984",
      "comment": "Preprint. Under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01982",
      "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems",
      "authors": [
        {
          "name": "Noel Thomas",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.",
      "publishedDate": "2026-01-05T10:36:40Z",
      "updatedDate": "2026-01-05T10:36:40Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01982v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01982",
      "comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01910",
      "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning",
      "authors": [
        {
          "name": "Minh Hieu Ha",
          "affiliation": null
        },
        {
          "name": "Khanh Ly Ta",
          "affiliation": null
        },
        {
          "name": "Hung Phan",
          "affiliation": null
        },
        {
          "name": "Tung Doan",
          "affiliation": null
        },
        {
          "name": "Tung Dao",
          "affiliation": null
        },
        {
          "name": "Dao Tran",
          "affiliation": null
        },
        {
          "name": "Huynh Thi Thanh Binh",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency. We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.",
      "publishedDate": "2026-01-05T08:55:27Z",
      "updatedDate": "2026-01-05T08:55:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01910v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01910",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01875",
      "title": "Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence",
      "authors": [
        {
          "name": "Kewen Cao",
          "affiliation": null
        },
        {
          "name": "Jianxu Chen",
          "affiliation": null
        },
        {
          "name": "Yongbing Zhang",
          "affiliation": null
        },
        {
          "name": "Ye Zhang",
          "affiliation": null
        },
        {
          "name": "Hongxiao Wang",
          "affiliation": null
        }
      ],
      "abstract": "Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.",
      "publishedDate": "2026-01-05T08:02:49Z",
      "updatedDate": "2026-01-05T08:02:49Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01875v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01875",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01874",
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "authors": [
        {
          "name": "Shuhang Chen",
          "affiliation": null
        },
        {
          "name": "Yunqiu Xu",
          "affiliation": null
        },
        {
          "name": "Junjie Xie",
          "affiliation": null
        },
        {
          "name": "Aojun Lu",
          "affiliation": null
        },
        {
          "name": "Tao Feng",
          "affiliation": null
        },
        {
          "name": "Zeying Huang",
          "affiliation": null
        },
        {
          "name": "Ning Zhang",
          "affiliation": null
        },
        {
          "name": "Yi Sun",
          "affiliation": null
        },
        {
          "name": "Yi Yang",
          "affiliation": null
        },
        {
          "name": "Hangjie Yuan",
          "affiliation": null
        }
      ],
      "abstract": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
      "publishedDate": "2026-01-05T08:02:18Z",
      "updatedDate": "2026-01-05T08:02:18Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01874v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01874",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01825",
      "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning",
      "authors": [
        {
          "name": "Yaxin Cui",
          "affiliation": null
        },
        {
          "name": "Yuanqiang Zeng",
          "affiliation": null
        },
        {
          "name": "Jiapeng Yan",
          "affiliation": null
        },
        {
          "name": "Keling Lin",
          "affiliation": null
        },
        {
          "name": "Kai Ji",
          "affiliation": null
        },
        {
          "name": "Jianhui Zeng",
          "affiliation": null
        },
        {
          "name": "Sheng Zhang",
          "affiliation": null
        },
        {
          "name": "Xin Luo",
          "affiliation": null
        },
        {
          "name": "Binzhu Su",
          "affiliation": null
        },
        {
          "name": "Chaolai Shen",
          "affiliation": null
        },
        {
          "name": "Jiahao Yu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.",
      "publishedDate": "2026-01-05T06:44:29Z",
      "updatedDate": "2026-01-05T06:44:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01825v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01825",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01804",
      "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs",
      "authors": [
        {
          "name": "Zhengjian Kang",
          "affiliation": null
        },
        {
          "name": "Qi Chen",
          "affiliation": null
        },
        {
          "name": "Rui Liu",
          "affiliation": null
        },
        {
          "name": "Kangtong Mo",
          "affiliation": null
        },
        {
          "name": "Xingyu Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Deng",
          "affiliation": null
        },
        {
          "name": "Ye Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.",
      "publishedDate": "2026-01-05T05:30:13Z",
      "updatedDate": "2026-01-09T03:41:24Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01804v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01804",
      "comment": "7 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01798",
      "title": "VerLM: Explaining Face Verification Using Natural Language",
      "authors": [
        {
          "name": "Syed Abdul Hannan",
          "affiliation": null
        },
        {
          "name": "Hazim Bukhari",
          "affiliation": null
        },
        {
          "name": "Thomas Cantalapiedra",
          "affiliation": null
        },
        {
          "name": "Eman Ansar",
          "affiliation": null
        },
        {
          "name": "Massa Baali",
          "affiliation": null
        },
        {
          "name": "Rita Singh",
          "affiliation": null
        },
        {
          "name": "Bhiksha Raj",
          "affiliation": null
        }
      ],
      "abstract": "Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.",
      "publishedDate": "2026-01-05T05:16:07Z",
      "updatedDate": "2026-01-05T05:16:07Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01798v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01798",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01739",
      "title": "K-EXAONE Technical Report",
      "authors": [
        {
          "name": "Eunbi Choi",
          "affiliation": null
        },
        {
          "name": "Kibong Choi",
          "affiliation": null
        },
        {
          "name": "Seokhee Hong",
          "affiliation": null
        },
        {
          "name": "Junwon Hwang",
          "affiliation": null
        },
        {
          "name": "Hyojin Jeon",
          "affiliation": null
        },
        {
          "name": "Hyunjik Jo",
          "affiliation": null
        },
        {
          "name": "Joonkee Kim",
          "affiliation": null
        },
        {
          "name": "Seonghwan Kim",
          "affiliation": null
        },
        {
          "name": "Soyeon Kim",
          "affiliation": null
        },
        {
          "name": "Sunkyoung Kim",
          "affiliation": null
        },
        {
          "name": "Yireun Kim",
          "affiliation": null
        },
        {
          "name": "Yongil Kim",
          "affiliation": null
        },
        {
          "name": "Haeju Lee",
          "affiliation": null
        },
        {
          "name": "Jinsik Lee",
          "affiliation": null
        },
        {
          "name": "Kyungmin Lee",
          "affiliation": null
        },
        {
          "name": "Sangha Park",
          "affiliation": null
        },
        {
          "name": "Heuiyeen Yeen",
          "affiliation": null
        },
        {
          "name": "Hwan Chang",
          "affiliation": null
        },
        {
          "name": "Stanley Jungkyu Choi",
          "affiliation": null
        },
        {
          "name": "Yejin Choi",
          "affiliation": null
        },
        {
          "name": "Jiwon Ham",
          "affiliation": null
        },
        {
          "name": "Kijeong Jeon",
          "affiliation": null
        },
        {
          "name": "Geunyeong Jeong",
          "affiliation": null
        },
        {
          "name": "Gerrard Jeongwon Jo",
          "affiliation": null
        },
        {
          "name": "Yonghwan Jo",
          "affiliation": null
        },
        {
          "name": "Jiyeon Jung",
          "affiliation": null
        },
        {
          "name": "Naeun Kang",
          "affiliation": null
        },
        {
          "name": "Dohoon Kim",
          "affiliation": null
        },
        {
          "name": "Euisoon Kim",
          "affiliation": null
        },
        {
          "name": "Hayeon Kim",
          "affiliation": null
        },
        {
          "name": "Hyosang Kim",
          "affiliation": null
        },
        {
          "name": "Hyunseo Kim",
          "affiliation": null
        },
        {
          "name": "Jieun Kim",
          "affiliation": null
        },
        {
          "name": "Minu Kim",
          "affiliation": null
        },
        {
          "name": "Myoungshin Kim",
          "affiliation": null
        },
        {
          "name": "Unsol Kim",
          "affiliation": null
        },
        {
          "name": "Youchul Kim",
          "affiliation": null
        },
        {
          "name": "YoungJin Kim",
          "affiliation": null
        },
        {
          "name": "Chaeeun Lee",
          "affiliation": null
        },
        {
          "name": "Chaeyoon Lee",
          "affiliation": null
        },
        {
          "name": "Changhun Lee",
          "affiliation": null
        },
        {
          "name": "Dahm Lee",
          "affiliation": null
        },
        {
          "name": "Edward Hwayoung Lee",
          "affiliation": null
        },
        {
          "name": "Honglak Lee",
          "affiliation": null
        },
        {
          "name": "Jinsang Lee",
          "affiliation": null
        },
        {
          "name": "Jiyoung Lee",
          "affiliation": null
        },
        {
          "name": "Sangeun Lee",
          "affiliation": null
        },
        {
          "name": "Seungwon Lim",
          "affiliation": null
        },
        {
          "name": "Solji Lim",
          "affiliation": null
        },
        {
          "name": "Woohyung Lim",
          "affiliation": null
        },
        {
          "name": "Chanwoo Moon",
          "affiliation": null
        },
        {
          "name": "Jaewoo Park",
          "affiliation": null
        },
        {
          "name": "Jinho Park",
          "affiliation": null
        },
        {
          "name": "Yongmin Park",
          "affiliation": null
        },
        {
          "name": "Hyerin Seo",
          "affiliation": null
        },
        {
          "name": "Wooseok Seo",
          "affiliation": null
        },
        {
          "name": "Yongwoo Song",
          "affiliation": null
        },
        {
          "name": "Sejong Yang",
          "affiliation": null
        },
        {
          "name": "Sihoon Yang",
          "affiliation": null
        },
        {
          "name": "Chang En Yea",
          "affiliation": null
        },
        {
          "name": "Sihyuk Yi",
          "affiliation": null
        },
        {
          "name": "Chansik Yoon",
          "affiliation": null
        },
        {
          "name": "Dongkeun Yoon",
          "affiliation": null
        },
        {
          "name": "Sangyeon Yoon",
          "affiliation": null
        },
        {
          "name": "Hyeongu Yun",
          "affiliation": null
        }
      ],
      "abstract": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
      "publishedDate": "2026-01-05T02:30:59Z",
      "updatedDate": "2026-01-09T01:37:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01739v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01739",
      "comment": "29 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01714",
      "title": "Entropy-Aligned Decoding of LMs for Better Writing and Reasoning",
      "authors": [
        {
          "name": "Kareem Ahmed",
          "affiliation": null
        },
        {
          "name": "Sameer Singh",
          "affiliation": null
        }
      ],
      "abstract": "Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.",
      "publishedDate": "2026-01-05T01:37:10Z",
      "updatedDate": "2026-01-05T01:37:10Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01714v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01714",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01708",
      "title": "A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription",
      "authors": [
        {
          "name": "Unggi Lee",
          "affiliation": null
        },
        {
          "name": "Joo Young Kim",
          "affiliation": null
        },
        {
          "name": "Ran Ju",
          "affiliation": null
        },
        {
          "name": "Minyoung Jung",
          "affiliation": null
        },
        {
          "name": "Jeyeon Eo",
          "affiliation": null
        }
      ],
      "abstract": "Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.",
      "publishedDate": "2026-01-05T01:02:21Z",
      "updatedDate": "2026-01-05T01:02:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01708v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01708",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01580",
      "title": "The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs",
      "authors": [
        {
          "name": "Zibo Zhao",
          "affiliation": null
        },
        {
          "name": "Yuanting Zha",
          "affiliation": null
        },
        {
          "name": "Haipeng Zhang",
          "affiliation": null
        },
        {
          "name": "Xingcheng Xu",
          "affiliation": null
        }
      ],
      "abstract": "Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.",
      "publishedDate": "2026-01-04T15:59:15Z",
      "updatedDate": "2026-01-04T15:59:15Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01580v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01580",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01552",
      "title": "HalluZig: Hallucination Detection using Zigzag Persistence",
      "authors": [
        {
          "name": "Shreyas N. Samaga",
          "affiliation": null
        },
        {
          "name": "Gilberto Gonzalez Arroyo",
          "affiliation": null
        },
        {
          "name": "Tamal K. Dey",
          "affiliation": null
        }
      ],
      "abstract": "The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.",
      "publishedDate": "2026-01-04T14:55:43Z",
      "updatedDate": "2026-01-04T14:55:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01552v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01552",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01546",
      "title": "Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation",
      "authors": [
        {
          "name": "Letian Kong",
          "affiliation": null
        },
        {
          "name": "Qianran",
          "affiliation": null
        },
        {
          "name": "Jin",
          "affiliation": null
        },
        {
          "name": "Renyu Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.",
      "publishedDate": "2026-01-04T14:42:00Z",
      "updatedDate": "2026-01-04T14:42:00Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01546v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01546",
      "comment": "39 pages, 2 figures, 3 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01513",
      "title": "FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Gen Li",
          "affiliation": null
        },
        {
          "name": "Peiyu Liu",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.",
      "publishedDate": "2026-01-04T12:46:35Z",
      "updatedDate": "2026-01-07T15:36:31Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01513v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01513",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01490",
      "title": "Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints",
      "authors": [
        {
          "name": "Junichiro Niimi",
          "affiliation": null
        }
      ],
      "abstract": "With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.",
      "publishedDate": "2026-01-04T11:35:39Z",
      "updatedDate": "2026-01-04T11:35:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01490v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01490",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01407",
      "title": "From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models",
      "authors": [
        {
          "name": "Arjhun Sreedar",
          "affiliation": null
        },
        {
          "name": "Rohan Pillay",
          "affiliation": null
        },
        {
          "name": "Laukik Patade",
          "affiliation": null
        }
      ],
      "abstract": "This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.",
      "publishedDate": "2026-01-04T07:08:37Z",
      "updatedDate": "2026-01-04T07:08:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01407v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01407",
      "comment": "10 pages, 1 figure",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01400",
      "title": "EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery",
      "authors": [
        {
          "name": "Jicheng Ma",
          "affiliation": null
        },
        {
          "name": "Guohua Wang",
          "affiliation": null
        },
        {
          "name": "Xinhua Feng",
          "affiliation": null
        },
        {
          "name": "Yiming Liu",
          "affiliation": null
        },
        {
          "name": "Zhichao Hu",
          "affiliation": null
        },
        {
          "name": "Yuhong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \\textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.",
      "publishedDate": "2026-01-04T06:40:25Z",
      "updatedDate": "2026-01-04T06:40:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01400v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01400",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01362",
      "title": "Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning",
      "authors": [
        {
          "name": "Jerry Huang",
          "affiliation": null
        },
        {
          "name": "Peng Lu",
          "affiliation": null
        },
        {
          "name": "Qiuhao Zeng",
          "affiliation": null
        },
        {
          "name": "Yusuke Iwasawa",
          "affiliation": null
        },
        {
          "name": "Yutaka Matsuo",
          "affiliation": null
        },
        {
          "name": "Sarath Chandar",
          "affiliation": null
        },
        {
          "name": "Edison Marrese-Taylor",
          "affiliation": null
        },
        {
          "name": "Irene Li",
          "affiliation": null
        }
      ],
      "abstract": "Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.",
      "publishedDate": "2026-01-04T04:29:12Z",
      "updatedDate": "2026-01-04T04:29:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01362v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01362",
      "comment": "Accepted to The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01341",
      "title": "Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems",
      "authors": [
        {
          "name": "Md Abdullah Al Kafi",
          "affiliation": null
        },
        {
          "name": "Raka Moni",
          "affiliation": null
        },
        {
          "name": "Sumit Kumar Banshal",
          "affiliation": null
        }
      ],
      "abstract": "The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.",
      "publishedDate": "2026-01-04T03:09:23Z",
      "updatedDate": "2026-01-04T03:09:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01341v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01341",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01322",
      "title": "LinMU: Multimodal Understanding Made Linear",
      "authors": [
        {
          "name": "Hongjie Wang",
          "affiliation": null
        },
        {
          "name": "Niraj K. Jha",
          "affiliation": null
        }
      ],
      "abstract": "Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.",
      "publishedDate": "2026-01-04T01:17:36Z",
      "updatedDate": "2026-01-04T01:17:36Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.IV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01322v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01322",
      "comment": "23 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01298",
      "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware",
      "authors": [
        {
          "name": "Jorge L. Ruiz Williams",
          "affiliation": null
        }
      ],
      "abstract": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.",
      "publishedDate": "2026-01-03T23:11:21Z",
      "updatedDate": "2026-01-03T23:11:21Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.DC",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01298v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01298",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01266",
      "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment",
      "authors": [
        {
          "name": "Rhitabrat Pokharel",
          "affiliation": null
        },
        {
          "name": "Hamid Reza Hassanzadeh",
          "affiliation": null
        },
        {
          "name": "Ameeta Agrawal",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.",
      "publishedDate": "2026-01-03T19:24:51Z",
      "updatedDate": "2026-01-08T18:28:40Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01266v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01266",
      "comment": "Accepted at AIMedHealth @ AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01233",
      "title": "Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling",
      "authors": [
        {
          "name": "Kangchen Zhu",
          "affiliation": null
        },
        {
          "name": "Zhiliang Tian",
          "affiliation": null
        },
        {
          "name": "Shangwen Wang",
          "affiliation": null
        },
        {
          "name": "Mingyue Leng",
          "affiliation": null
        },
        {
          "name": "Xiaoguang Mao",
          "affiliation": null
        }
      ],
      "abstract": "Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.",
      "publishedDate": "2026-01-03T16:43:05Z",
      "updatedDate": "2026-01-03T16:43:05Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01233v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01233",
      "comment": "Accepted by ICSE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01195",
      "title": "Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering",
      "authors": [
        {
          "name": "Wuzhenghong Wen",
          "affiliation": null
        },
        {
          "name": "Chao Xue",
          "affiliation": null
        },
        {
          "name": "Su Pan",
          "affiliation": null
        },
        {
          "name": "Yuwei Sun",
          "affiliation": null
        },
        {
          "name": "Minlong Peng",
          "affiliation": null
        }
      ],
      "abstract": "Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.",
      "publishedDate": "2026-01-03T14:27:01Z",
      "updatedDate": "2026-01-03T14:27:01Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01195v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01195",
      "comment": "11 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01118",
      "title": "ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services",
      "authors": [
        {
          "name": "Qingqing Long",
          "affiliation": null
        },
        {
          "name": "Haotian Chen",
          "affiliation": null
        },
        {
          "name": "Chenyang Zhao",
          "affiliation": null
        },
        {
          "name": "Xiaolei Du",
          "affiliation": null
        },
        {
          "name": "Xuezhi Wang",
          "affiliation": null
        },
        {
          "name": "Pengyao Wang",
          "affiliation": null
        },
        {
          "name": "Chengzan Li",
          "affiliation": null
        },
        {
          "name": "Yuanchun Zhou",
          "affiliation": null
        },
        {
          "name": "Hengshu Zhu",
          "affiliation": null
        }
      ],
      "abstract": "The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.",
      "publishedDate": "2026-01-03T08:42:53Z",
      "updatedDate": "2026-01-03T08:42:53Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.DL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01118v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01118",
      "comment": "12 pages, 9 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "agents",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01095",
      "title": "NarrativeTrack: Evaluating Video Language Models Beyond the Frame",
      "authors": [
        {
          "name": "Hyeonjeong Ha",
          "affiliation": null
        },
        {
          "name": "Jinjin Ge",
          "affiliation": null
        },
        {
          "name": "Bo Feng",
          "affiliation": null
        },
        {
          "name": "Kaixin Ma",
          "affiliation": null
        },
        {
          "name": "Gargi Chakraborty",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.",
      "publishedDate": "2026-01-03T07:12:55Z",
      "updatedDate": "2026-01-03T07:12:55Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01095v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01095",
      "comment": "VideoLLM Fine-Grained Evaluation",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01011",
      "title": "Intention Collapse: Intention-Level Metrics for Reasoning in Language Models",
      "authors": [
        {
          "name": "Patricio Vera",
          "affiliation": null
        }
      ],
      "abstract": "Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies",
      "publishedDate": "2026-01-03T00:19:53Z",
      "updatedDate": "2026-01-03T00:19:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01011v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01011",
      "comment": "21 pages, 4 figures, 3 tables. Code: https://github.com/patriciomvera/intention-collapse-experiments",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00998",
      "title": "DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models",
      "authors": [
        {
          "name": "Yue Zhou",
          "affiliation": null
        },
        {
          "name": "Jue Chen",
          "affiliation": null
        },
        {
          "name": "Zilun Zhang",
          "affiliation": null
        },
        {
          "name": "Penghui Huang",
          "affiliation": null
        },
        {
          "name": "Ran Ding",
          "affiliation": null
        },
        {
          "name": "Zhentao Zou",
          "affiliation": null
        },
        {
          "name": "PengFei Gao",
          "affiliation": null
        },
        {
          "name": "Yuchen Wei",
          "affiliation": null
        },
        {
          "name": "Ke Li",
          "affiliation": null
        },
        {
          "name": "Xue Yang",
          "affiliation": null
        },
        {
          "name": "Xue Jiang",
          "affiliation": null
        },
        {
          "name": "Hongxin Yang",
          "affiliation": null
        },
        {
          "name": "Jonathan Li",
          "affiliation": null
        }
      ],
      "abstract": "Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench",
      "publishedDate": "2026-01-02T22:42:38Z",
      "updatedDate": "2026-01-02T22:42:38Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00998v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00998",
      "comment": "20 pages, 17 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation",
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation",
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00923",
      "title": "Context Collapse: In-Context Learning and Model Collapse",
      "authors": [
        {
          "name": "Josef Ott",
          "affiliation": null
        }
      ],
      "abstract": "This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.",
      "publishedDate": "2026-01-01T17:33:47Z",
      "updatedDate": "2026-01-01T17:33:47Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00923v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00923",
      "comment": "Master's thesis",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01196",
      "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners",
      "authors": [
        {
          "name": "Shenqi Lu",
          "affiliation": null
        },
        {
          "name": "Liangwei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.",
      "publishedDate": "2026-01-03T14:40:39Z",
      "updatedDate": "2026-01-03T14:40:39Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01196v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01196",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "robotics",
        "prompting",
        "agents",
        "tool-use",
        "planning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "robotics",
          "prompting",
          "agents",
          "tool-use",
          "planning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01673",
      "title": "Exposing Hidden Interfaces: LLM-Guided Type Inference for Reverse Engineering macOS Private Frameworks",
      "authors": [
        {
          "name": "Arina Kharlamova",
          "affiliation": null
        },
        {
          "name": "Youcheng Sun",
          "affiliation": null
        },
        {
          "name": "Ting Yu",
          "affiliation": null
        }
      ],
      "abstract": "Private macOS frameworks underpin critical services and daemons but remain undocumented and distributed only as stripped binaries, complicating security analysis. We present MOTIF, an agentic framework that integrates tool-augmented analysis with a finetuned large language model specialized for Objective-C type inference. The agent manages runtime metadata extraction, binary inspection, and constraint checking, while the model generates candidate method signatures that are validated and refined into compilable headers. On MOTIF-Bench, a benchmark built from public frameworks with groundtruth headers, MOTIF improves signature recovery from 15% to 86% compared to baseline static analysis tooling, with consistent gains in tool-use correctness and inference stability. Case studies on private frameworks show that reconstructed headers compile, link, and facilitate downstream security research and vulnerability studies. By transforming opaque binaries into analyzable interfaces, MOTIF establishes a scalable foundation for systematic auditing of macOS internals.",
      "publishedDate": "2026-01-04T21:44:55Z",
      "updatedDate": "2026-01-04T21:44:55Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01673v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01673",
      "comment": "IEEE S&P'26 under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01320",
      "title": "Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python",
      "authors": [
        {
          "name": "Muntasir Adnan",
          "affiliation": null
        },
        {
          "name": "Carlos C. N. Kuhn",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.",
      "publishedDate": "2026-01-04T01:13:37Z",
      "updatedDate": "2026-01-04T01:13:37Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01320v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01320",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02075",
      "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
      "authors": [
        {
          "name": "Zhuofan Shi",
          "affiliation": null
        },
        {
          "name": "Hubao A",
          "affiliation": null
        },
        {
          "name": "Yufei Shao",
          "affiliation": null
        },
        {
          "name": "Dongliang Huang",
          "affiliation": null
        },
        {
          "name": "Hongxu An",
          "affiliation": null
        },
        {
          "name": "Chunxiao Xin",
          "affiliation": null
        },
        {
          "name": "Haiyang Shen",
          "affiliation": null
        },
        {
          "name": "Zhenyu Wang",
          "affiliation": null
        },
        {
          "name": "Yunshan Na",
          "affiliation": null
        },
        {
          "name": "Gang Huang",
          "affiliation": null
        },
        {
          "name": "Xiang Jing",
          "affiliation": null
        }
      ],
      "abstract": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
      "publishedDate": "2026-01-05T12:56:51Z",
      "updatedDate": "2026-01-07T10:06:36Z",
      "primaryCategory": "cs.CE",
      "arxivCategories": [
        "cs.CE",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02075v3",
      "arxivUrl": "https://arxiv.org/abs/2601.02075",
      "comment": "24 pages,4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01993",
      "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support",
      "authors": [
        {
          "name": "Dong Xue",
          "affiliation": null
        },
        {
          "name": "Jicheng Tu",
          "affiliation": null
        },
        {
          "name": "Ming Wang",
          "affiliation": null
        },
        {
          "name": "Xin Yan",
          "affiliation": null
        },
        {
          "name": "Fangzhou Liu",
          "affiliation": null
        },
        {
          "name": "Jie Hu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.",
      "publishedDate": "2026-01-05T10:54:18Z",
      "updatedDate": "2026-01-05T10:54:18Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01993v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01993",
      "comment": "33 pages, 16 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01844",
      "title": "Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Udiptaman Das",
          "affiliation": null
        },
        {
          "name": "Krishnasai B. Atmakuri",
          "affiliation": null
        },
        {
          "name": "Duy Ho",
          "affiliation": null
        },
        {
          "name": "Chi Lee",
          "affiliation": null
        },
        {
          "name": "Yugyung Lee",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.",
      "publishedDate": "2026-01-05T07:16:29Z",
      "updatedDate": "2026-01-05T07:16:29Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01844v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01844",
      "comment": "13 pages, 5 tables, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "agents",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "agents",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02345",
      "title": "Question Answering for Multi-Release Systems: A Case Study at Ciena",
      "authors": [
        {
          "name": "Parham Khamsepour",
          "affiliation": null
        },
        {
          "name": "Mark Cole",
          "affiliation": null
        },
        {
          "name": "Ish Ashraf",
          "affiliation": null
        },
        {
          "name": "Sandeep Puri",
          "affiliation": null
        },
        {
          "name": "Mehrdad Sabetzadeh",
          "affiliation": null
        },
        {
          "name": "Shiva Nejati",
          "affiliation": null
        }
      ],
      "abstract": "Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.",
      "publishedDate": "2026-01-05T18:44:26Z",
      "updatedDate": "2026-01-05T18:44:26Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02345v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02345",
      "comment": "Accepted for publication in SANER 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02065",
      "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory",
      "authors": [
        {
          "name": "Md. Asif Hossain",
          "affiliation": null
        },
        {
          "name": "Nabil Subhan",
          "affiliation": null
        },
        {
          "name": "Mantasha Rahman Mahi",
          "affiliation": null
        },
        {
          "name": "Jannatul Ferdous Nabila",
          "affiliation": null
        }
      ],
      "abstract": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings",
      "publishedDate": "2026-01-05T12:41:44Z",
      "updatedDate": "2026-01-05T12:41:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02065v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02065",
      "comment": "5 pages, 3 figures, 1 table",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02023",
      "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
      "authors": [
        {
          "name": "Amirali Ebrahimzadeh",
          "affiliation": null
        },
        {
          "name": "Seyyed M. Salili",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
      "publishedDate": "2026-01-05T11:30:56Z",
      "updatedDate": "2026-01-05T11:30:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02023v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02023",
      "comment": "25 pages, 8 figures, 3 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01950",
      "title": "Face Normal Estimation from Rags to Riches",
      "authors": [
        {
          "name": "Meng Wang",
          "affiliation": null
        },
        {
          "name": "Wenjing Dai",
          "affiliation": null
        },
        {
          "name": "Jiawan Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaojie Guo",
          "affiliation": null
        }
      ],
      "abstract": "Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.",
      "publishedDate": "2026-01-05T09:57:24Z",
      "updatedDate": "2026-01-05T09:57:24Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01950v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01950",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01896",
      "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG",
      "authors": [
        {
          "name": "Jingyu Liu",
          "affiliation": null
        },
        {
          "name": "Jiaen Lin",
          "affiliation": null
        },
        {
          "name": "Yong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.",
      "publishedDate": "2026-01-05T08:40:37Z",
      "updatedDate": "2026-01-06T15:41:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01896v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01896",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01872",
      "title": "CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios",
      "authors": [
        {
          "name": "Hongbo Duan",
          "affiliation": null
        },
        {
          "name": "Shangyi Luo",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Deng",
          "affiliation": null
        },
        {
          "name": "Yanbo Chen",
          "affiliation": null
        },
        {
          "name": "Yuanhao Chiang",
          "affiliation": null
        },
        {
          "name": "Yi Liu",
          "affiliation": null
        },
        {
          "name": "Fangming Liu",
          "affiliation": null
        },
        {
          "name": "Xueqian Wang",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.",
      "publishedDate": "2026-01-05T08:00:34Z",
      "updatedDate": "2026-01-05T08:00:34Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01872v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01872",
      "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "robotics",
        "agents",
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "rag",
          "robotics",
          "agents",
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01785",
      "title": "SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines",
      "authors": [
        {
          "name": "Rajiv Chaitanya Muttur",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.",
      "publishedDate": "2026-01-05T04:39:31Z",
      "updatedDate": "2026-01-05T04:39:31Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01785v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01785",
      "comment": "Presented at ICEdge 2025; nominated for Best Paper Award",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00926",
      "title": "MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers",
      "authors": [
        {
          "name": "Satya Swaroop Gudipudi",
          "affiliation": null
        },
        {
          "name": "Sahil Girhepuje",
          "affiliation": null
        },
        {
          "name": "Ponnurangam Kumaraguru",
          "affiliation": null
        },
        {
          "name": "Kristine Ma",
          "affiliation": null
        }
      ],
      "abstract": "Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.",
      "publishedDate": "2026-01-01T23:31:02Z",
      "updatedDate": "2026-01-01T23:31:02Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00926v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00926",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02002",
      "title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models",
      "authors": [
        {
          "name": "Antonio Colacicco",
          "affiliation": null
        },
        {
          "name": "Vito Guida",
          "affiliation": null
        },
        {
          "name": "Dario Di Palma",
          "affiliation": null
        },
        {
          "name": "Fedelucio Narducci",
          "affiliation": null
        },
        {
          "name": "Tommaso Di Noia",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.",
      "publishedDate": "2026-01-05T11:03:56Z",
      "updatedDate": "2026-01-05T11:03:56Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02002v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02002",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01954",
      "title": "Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations",
      "authors": [
        {
          "name": "Alexander Korn",
          "affiliation": null
        },
        {
          "name": "Lea Zaruchas",
          "affiliation": null
        },
        {
          "name": "Chetan Arora",
          "affiliation": null
        },
        {
          "name": "Andreas Metzger",
          "affiliation": null
        },
        {
          "name": "Sven Smolka",
          "affiliation": null
        },
        {
          "name": "Fanyu Wang",
          "affiliation": null
        },
        {
          "name": "Andreas Vogelsang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.",
      "publishedDate": "2026-01-05T10:01:20Z",
      "updatedDate": "2026-01-05T10:01:20Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01954v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01954",
      "comment": "To be published at The 3rd ACM International Conference on AI Foundation Models and Software Engineering FORGE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01213",
      "title": "Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation",
      "authors": [
        {
          "name": "Riccardo Gelato",
          "affiliation": null
        },
        {
          "name": "Carlo Sgaravatti",
          "affiliation": null
        },
        {
          "name": "Jakob Grahn",
          "affiliation": null
        },
        {
          "name": "Giacomo Boracchi",
          "affiliation": null
        },
        {
          "name": "Filippo Maria Bianchi",
          "affiliation": null
        }
      ],
      "abstract": "Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.",
      "publishedDate": "2026-01-03T15:41:12Z",
      "updatedDate": "2026-01-03T15:41:12Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01213v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01213",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01027",
      "title": "A Platform for Interactive AI Character Experiences",
      "authors": [
        {
          "name": "Rafael Wampfler",
          "affiliation": null
        },
        {
          "name": "Chen Yang",
          "affiliation": null
        },
        {
          "name": "Dillon Elste",
          "affiliation": null
        },
        {
          "name": "Nikola Kovacevic",
          "affiliation": null
        },
        {
          "name": "Philine Witzig",
          "affiliation": null
        },
        {
          "name": "Markus Gross",
          "affiliation": null
        }
      ],
      "abstract": "From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.",
      "publishedDate": "2026-01-03T01:27:19Z",
      "updatedDate": "2026-01-03T01:27:19Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01027v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01027",
      "comment": null,
      "journalRef": "SIGGRAPH Conference Papers '25, August 10-14, 2025, Vancouver, BC, Canada",
      "doi": "10.1145/3721238.3730762",
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02236",
      "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models",
      "authors": [
        {
          "name": "Yihao Liang",
          "affiliation": null
        },
        {
          "name": "Ze Wang",
          "affiliation": null
        },
        {
          "name": "Hao Chen",
          "affiliation": null
        },
        {
          "name": "Ximeng Sun",
          "affiliation": null
        },
        {
          "name": "Jialian Wu",
          "affiliation": null
        },
        {
          "name": "Xiaodong Yu",
          "affiliation": null
        },
        {
          "name": "Jiang Liu",
          "affiliation": null
        },
        {
          "name": "Emad Barsoum",
          "affiliation": null
        },
        {
          "name": "Zicheng Liu",
          "affiliation": null
        },
        {
          "name": "Niraj K. Jha",
          "affiliation": null
        }
      ],
      "abstract": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM",
      "publishedDate": "2026-01-05T16:09:22Z",
      "updatedDate": "2026-01-05T16:09:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02236v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02236",
      "comment": "33 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02201",
      "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents",
      "authors": [
        {
          "name": "Keyu Wang",
          "affiliation": null
        },
        {
          "name": "Bingchen Miao",
          "affiliation": null
        },
        {
          "name": "Wendong Bu",
          "affiliation": null
        },
        {
          "name": "Yu Wu",
          "affiliation": null
        },
        {
          "name": "Juncheng Li",
          "affiliation": null
        },
        {
          "name": "Shengyu Zhang",
          "affiliation": null
        },
        {
          "name": "Wenqiao Zhang",
          "affiliation": null
        },
        {
          "name": "Siliang Tang",
          "affiliation": null
        },
        {
          "name": "Jun Xiao",
          "affiliation": null
        },
        {
          "name": "Yueting Zhuang",
          "affiliation": null
        }
      ],
      "abstract": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.",
      "publishedDate": "2026-01-05T15:24:05Z",
      "updatedDate": "2026-01-05T15:24:05Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02201v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02201",
      "comment": "19 pages, 12 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02060",
      "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming",
      "authors": [
        {
          "name": "Nguyet-Anh H. Lang",
          "affiliation": null
        },
        {
          "name": "Eric Lang",
          "affiliation": null
        },
        {
          "name": "Thanh Le-Cong",
          "affiliation": null
        },
        {
          "name": "Bach Le",
          "affiliation": null
        },
        {
          "name": "Quyet-Thang Huynh",
          "affiliation": null
        }
      ],
      "abstract": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.",
      "publishedDate": "2026-01-05T12:33:37Z",
      "updatedDate": "2026-01-05T12:33:37Z",
      "primaryCategory": "cs.PL",
      "arxivCategories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02060v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02060",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01972",
      "title": "Hidden State Poisoning Attacks against Mamba-based Language Models",
      "authors": [
        {
          "name": "Alexandre Le Mercier",
          "affiliation": null
        },
        {
          "name": "Chris Develder",
          "affiliation": null
        },
        {
          "name": "Thomas Demeester",
          "affiliation": null
        }
      ],
      "abstract": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.",
      "publishedDate": "2026-01-05T10:27:19Z",
      "updatedDate": "2026-01-06T11:54:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01972v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01972",
      "comment": "17 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01957",
      "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing",
      "authors": [
        {
          "name": "Tianbo Wang",
          "affiliation": null
        },
        {
          "name": "Yuqing Ma",
          "affiliation": null
        },
        {
          "name": "Kewei Liao",
          "affiliation": null
        },
        {
          "name": "Zhange Zhang",
          "affiliation": null
        },
        {
          "name": "Simin Li",
          "affiliation": null
        },
        {
          "name": "Jinyang Guo",
          "affiliation": null
        },
        {
          "name": "Xianglong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.",
      "publishedDate": "2026-01-05T10:02:22Z",
      "updatedDate": "2026-01-05T10:02:22Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01957v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01957",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01870",
      "title": "Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion",
      "authors": [
        {
          "name": "Wenyu Shao",
          "affiliation": null
        },
        {
          "name": "Hongbo Liu",
          "affiliation": null
        },
        {
          "name": "Yunchuan Ma",
          "affiliation": null
        },
        {
          "name": "Ruili Wang",
          "affiliation": null
        }
      ],
      "abstract": "Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.",
      "publishedDate": "2026-01-05T08:00:03Z",
      "updatedDate": "2026-01-05T08:00:03Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01870v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01870",
      "comment": "Accepted by IEEE Transactions on Multimedia",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01827",
      "title": "Aspect Extraction from E-Commerce Product and Service Reviews",
      "authors": [
        {
          "name": "Valiant Lance D. Dionela",
          "affiliation": null
        },
        {
          "name": "Fatima Kriselle S. Dy",
          "affiliation": null
        },
        {
          "name": "Robin James M. Hombrebueno",
          "affiliation": null
        },
        {
          "name": "Aaron Rae M. Nicolas",
          "affiliation": null
        },
        {
          "name": "Charibeth K. Cheng",
          "affiliation": null
        },
        {
          "name": "Raphael W. Gonda",
          "affiliation": null
        }
      ],
      "abstract": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.",
      "publishedDate": "2026-01-05T06:45:51Z",
      "updatedDate": "2026-01-05T06:45:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01827v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01827",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01765",
      "title": "A New Benchmark for the Appropriate Evaluation of RTL Code Optimization",
      "authors": [
        {
          "name": "Yao Lu",
          "affiliation": null
        },
        {
          "name": "Shang Liu",
          "affiliation": null
        },
        {
          "name": "Hangan Zhou",
          "affiliation": null
        },
        {
          "name": "Wenji Fang",
          "affiliation": null
        },
        {
          "name": "Qijun Zhang",
          "affiliation": null
        },
        {
          "name": "Zhiyao Xie",
          "affiliation": null
        }
      ],
      "abstract": "The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.",
      "publishedDate": "2026-01-05T03:47:26Z",
      "updatedDate": "2026-01-05T03:47:26Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01765v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01765",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "tool-use",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01461",
      "title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR",
      "authors": [
        {
          "name": "Yuxiang Mei",
          "affiliation": null
        },
        {
          "name": "Dongxing Xu",
          "affiliation": null
        },
        {
          "name": "Jiaen Liang",
          "affiliation": null
        },
        {
          "name": "Yanhua Long",
          "affiliation": null
        }
      ],
      "abstract": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.",
      "publishedDate": "2026-01-04T10:08:53Z",
      "updatedDate": "2026-01-04T10:08:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01461v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01461",
      "comment": "5 pages, 1 figure",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01416",
      "title": "AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval",
      "authors": [
        {
          "name": "Yue Zhou",
          "affiliation": null
        },
        {
          "name": "Ran Ding",
          "affiliation": null
        },
        {
          "name": "Xue Yang",
          "affiliation": null
        },
        {
          "name": "Xue Jiang",
          "affiliation": null
        },
        {
          "name": "Xingzhao Liu",
          "affiliation": null
        }
      ],
      "abstract": "Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot",
      "publishedDate": "2026-01-04T07:38:51Z",
      "updatedDate": "2026-01-04T07:38:51Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01416v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01416",
      "comment": "12 pages, 9 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "planning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01332",
      "title": "FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness",
      "authors": [
        {
          "name": "Hossam Amer",
          "affiliation": null
        },
        {
          "name": "Maryam Dialameh",
          "affiliation": null
        },
        {
          "name": "Hossein Rajabzadeh",
          "affiliation": null
        },
        {
          "name": "Walid Ahmed",
          "affiliation": null
        },
        {
          "name": "Weiwei Zhang",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.",
      "publishedDate": "2026-01-04T02:33:30Z",
      "updatedDate": "2026-01-04T02:33:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01332v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01332",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01271",
      "title": "CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs",
      "authors": [
        {
          "name": "Qingxiao Tao",
          "affiliation": null
        },
        {
          "name": "Xiaodong Gu",
          "affiliation": null
        },
        {
          "name": "Hao Zhong",
          "affiliation": null
        },
        {
          "name": "Beijun Shen",
          "affiliation": null
        }
      ],
      "abstract": "Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.",
      "publishedDate": "2026-01-03T20:03:03Z",
      "updatedDate": "2026-01-03T20:03:03Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01271v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01271",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "tool-use",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "tool-use",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01244",
      "title": "Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure",
      "authors": [
        {
          "name": "Zsolt Csibi",
          "affiliation": null
        },
        {
          "name": "Bence György Gortka",
          "affiliation": null
        },
        {
          "name": "Natabara Gyöngyössy",
          "affiliation": null
        },
        {
          "name": "Kornél Nagy",
          "affiliation": null
        },
        {
          "name": "Dávid Márk Nemeskey",
          "affiliation": null
        },
        {
          "name": "Martin Sallai",
          "affiliation": null
        },
        {
          "name": "András Simonyi",
          "affiliation": null
        },
        {
          "name": "András Márk Szekeres",
          "affiliation": null
        },
        {
          "name": "Gábor Palkó",
          "affiliation": null
        }
      ],
      "abstract": "We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.",
      "publishedDate": "2026-01-03T17:32:48Z",
      "updatedDate": "2026-01-03T17:32:48Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01244v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01244",
      "comment": "18 pages, 1 figures. To appear in the XXII. Magyar Számítógépes Nyelvészeti Konferencia (MSZNY 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01215",
      "title": "Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code",
      "authors": [
        {
          "name": "Prateek Rajput",
          "affiliation": null
        },
        {
          "name": "Yewei Song",
          "affiliation": null
        },
        {
          "name": "Abdoul Aziz Bonkoungou",
          "affiliation": null
        },
        {
          "name": "Iyiola E. Olatunji",
          "affiliation": null
        },
        {
          "name": "Abdoul Kader Kabore",
          "affiliation": null
        },
        {
          "name": "Jacques Klein",
          "affiliation": null
        },
        {
          "name": "Tegawendé F. Bissyandé",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.",
      "publishedDate": "2026-01-03T15:42:21Z",
      "updatedDate": "2026-01-03T15:42:21Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01215v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01215",
      "comment": "11 Pages, 11 figures, Accepted at ICSE SEIP",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01184",
      "title": "SecureCodeRL: Security-Aware Reinforcement Learning for Code Generation with Partial-Credit Rewards",
      "authors": [
        {
          "name": "Suryansh Singh Sijwali",
          "affiliation": null
        },
        {
          "name": "Suman Saha",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can generate plausible code, but in settings that require exact stdin/stdout behavior they frequently produce programs that compile yet fail tests, and in some cases they introduce security-sensitive patterns. This paper presents SecureCodeRL, a reinforcement learning (RL) pipeline for security-aware code generation that optimizes a combined reward R = αRfunc + \\b{eta}Rsec. The key idea is a partial-credit functional reward that assigns intermediate scores for syntactic validity, successful execution, and producing output, reducing reward sparsity that otherwise stalls learning on competitive programming style tasks. I evaluate supervised fine-tuning (SFT) and PPO variants on a small held-out prompt set from APPS+ and observe that PPO with partial credit (using a continued-training variant) improves syntax validity from 45% (SFT) to 60% and achieves the only non-zero test success signal in this pilot evaluation (5% at-least-one-test-pass), while remaining 100% clean under Bandit static analysis. Although Bandit findings were absent in this small evaluation, the security term is integrated into training to discourage insecure shortcuts when they appear.",
      "publishedDate": "2026-01-03T13:36:36Z",
      "updatedDate": "2026-01-03T13:36:36Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01184v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01184",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01162",
      "title": "Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models",
      "authors": [
        {
          "name": "Zihua Yang",
          "affiliation": null
        },
        {
          "name": "Xin Liao",
          "affiliation": null
        },
        {
          "name": "Yiqun Zhang",
          "affiliation": null
        },
        {
          "name": "Yiu-ming Cheung",
          "affiliation": null
        }
      ],
      "abstract": "Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE",
      "publishedDate": "2026-01-03T11:37:46Z",
      "updatedDate": "2026-01-03T11:37:46Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01162v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01162",
      "comment": "Submitted to ICPR 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01129",
      "title": "RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian",
      "authors": [
        {
          "name": "Kla Tantithamthavorn",
          "affiliation": null
        },
        {
          "name": "Yaotian Zou",
          "affiliation": null
        },
        {
          "name": "Andy Wong",
          "affiliation": null
        },
        {
          "name": "Michael Gupta",
          "affiliation": null
        },
        {
          "name": "Zhe Wang",
          "affiliation": null
        },
        {
          "name": "Mike Buller",
          "affiliation": null
        },
        {
          "name": "Ryan Jiang",
          "affiliation": null
        },
        {
          "name": "Matthew Watson",
          "affiliation": null
        },
        {
          "name": "Minwoo Jeong",
          "affiliation": null
        },
        {
          "name": "Kun Chen",
          "affiliation": null
        },
        {
          "name": "Ming Wu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning? In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).",
      "publishedDate": "2026-01-03T09:27:56Z",
      "updatedDate": "2026-01-03T09:27:56Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01129v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01129",
      "comment": "Accepted at the 48th International Conference on Software Engineering (ICSE'26), SEIP Track. 12 Pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00993",
      "title": "WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift",
      "authors": [
        {
          "name": "Julian D. Santamaria",
          "affiliation": null
        },
        {
          "name": "Claudia Isaza",
          "affiliation": null
        },
        {
          "name": "Jhony H. Giraldo",
          "affiliation": null
        }
      ],
      "abstract": "Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.",
      "publishedDate": "2026-01-02T21:58:19Z",
      "updatedDate": "2026-01-02T21:58:19Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00993v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00993",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00927",
      "title": "Measuring Social Media Polarization Using Large Language Models and Heuristic Rules",
      "authors": [
        {
          "name": "Jawad Chowdhury",
          "affiliation": null
        },
        {
          "name": "Rezaur Rashid",
          "affiliation": null
        },
        {
          "name": "Gabriel Terejanu",
          "affiliation": null
        }
      ],
      "abstract": "Understanding affective polarization in online discourse is crucial for evaluating the societal impact of social media interactions. This study presents a novel framework that leverages large language models (LLMs) and domain-informed heuristics to systematically analyze and quantify affective polarization in discussions on divisive topics such as climate change and gun control. Unlike most prior approaches that relied on sentiment analysis or predefined classifiers, our method integrates LLMs to extract stance, affective tone, and agreement patterns from large-scale social media discussions. We then apply a rule-based scoring system capable of quantifying affective polarization even in small conversations consisting of single interactions, based on stance alignment, emotional content, and interaction dynamics. Our analysis reveals distinct polarization patterns that are event dependent: (i) anticipation-driven polarization, where extreme polarization escalates before well-publicized events, and (ii) reactive polarization, where intense affective polarization spikes immediately after sudden, high-impact events. By combining AI-driven content annotation with domain-informed scoring, our framework offers a scalable and interpretable approach to measuring affective polarization. The source code is publicly available at: https://github.com/hasanjawad001/llm-social-media-polarization.",
      "publishedDate": "2026-01-02T01:11:58Z",
      "updatedDate": "2026-01-02T01:11:58Z",
      "primaryCategory": "cs.SI",
      "arxivCategories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00927v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00927",
      "comment": "Foundations and Applications of Big Data Analytics (FAB), Niagara Falls, Canada, 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01948",
      "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation",
      "authors": [
        {
          "name": "Zhihao Gu",
          "affiliation": null
        },
        {
          "name": "Ming Yang",
          "affiliation": null
        },
        {
          "name": "Difan Zou",
          "affiliation": null
        },
        {
          "name": "Dong Xu",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.",
      "publishedDate": "2026-01-05T09:56:24Z",
      "updatedDate": "2026-01-05T09:56:24Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01948v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01948",
      "comment": "Accepted to AAAI2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "planning",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "prompting",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00978",
      "title": "From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly",
      "authors": [
        {
          "name": "Yanyi Chen",
          "affiliation": null
        },
        {
          "name": "Min Deng",
          "affiliation": null
        }
      ],
      "abstract": "Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.",
      "publishedDate": "2026-01-02T20:12:50Z",
      "updatedDate": "2026-01-02T20:12:50Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00978v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00978",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "planning",
        "multi-agent",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "multi-agent",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02316",
      "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations",
      "authors": [
        {
          "name": "Siddharth Joshi",
          "affiliation": null
        },
        {
          "name": "Haoli Yin",
          "affiliation": null
        },
        {
          "name": "Rishabh Adiga",
          "affiliation": null
        },
        {
          "name": "Ricardo Monti",
          "affiliation": null
        },
        {
          "name": "Aldo Carranza",
          "affiliation": null
        },
        {
          "name": "Alex Fang",
          "affiliation": null
        },
        {
          "name": "Alvin Deng",
          "affiliation": null
        },
        {
          "name": "Amro Abbas",
          "affiliation": null
        },
        {
          "name": "Brett Larsen",
          "affiliation": null
        },
        {
          "name": "Cody Blakeney",
          "affiliation": null
        },
        {
          "name": "Darren Teh",
          "affiliation": null
        },
        {
          "name": "David Schwab",
          "affiliation": null
        },
        {
          "name": "Fan Pan",
          "affiliation": null
        },
        {
          "name": "Haakon Mongstad",
          "affiliation": null
        },
        {
          "name": "Jack Urbanek",
          "affiliation": null
        },
        {
          "name": "Jason Lee",
          "affiliation": null
        },
        {
          "name": "Jason Telanoff",
          "affiliation": null
        },
        {
          "name": "Josh Wills",
          "affiliation": null
        },
        {
          "name": "Kaleigh Mentzer",
          "affiliation": null
        },
        {
          "name": "Luke Merrick",
          "affiliation": null
        },
        {
          "name": "Parth Doshi",
          "affiliation": null
        },
        {
          "name": "Paul Burstein",
          "affiliation": null
        },
        {
          "name": "Pratyush Maini",
          "affiliation": null
        },
        {
          "name": "Scott Loftin",
          "affiliation": null
        },
        {
          "name": "Spandan Das",
          "affiliation": null
        },
        {
          "name": "Tony Jiang",
          "affiliation": null
        },
        {
          "name": "Vineeth Dorna",
          "affiliation": null
        },
        {
          "name": "Zhengping Wang",
          "affiliation": null
        },
        {
          "name": "Bogdan Gaza",
          "affiliation": null
        },
        {
          "name": "Ari Morcos",
          "affiliation": null
        },
        {
          "name": "Matthew Leavitt",
          "affiliation": null
        }
      ],
      "abstract": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.",
      "publishedDate": "2026-01-05T18:07:51Z",
      "updatedDate": "2026-01-05T18:07:51Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02316v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02316",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03251",
      "title": "NavAI: A Generalizable LLM Framework for Navigation Tasks in Virtual Reality Environments",
      "authors": [
        {
          "name": "Xue Qin",
          "affiliation": null
        },
        {
          "name": "Matthew DiGiovanni",
          "affiliation": null
        }
      ],
      "abstract": "Navigation is one of the fundamental tasks for automated exploration in Virtual Reality (VR). Existing technologies primarily focus on path optimization in 360-degree image datasets and 3D simulators, which cannot be directly applied to immersive VR environments. To address this gap, we present NavAI, a generalizable large language model (LLM)-based navigation framework that supports both basic actions and complex goal-directed tasks across diverse VR applications. We evaluate NavAI in three distinct VR environments through goal-oriented and exploratory tasks. Results show that it achieves high accuracy, with an 89% success rate in goal-oriented tasks. Our analysis also highlights current limitations of relying entirely on LLMs, particularly in scenarios that require dynamic goal assessment. Finally, we discuss the limitations observed during the experiments and offer insights for future research directions.",
      "publishedDate": "2026-01-06T18:54:54Z",
      "updatedDate": "2026-01-06T18:54:54Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03251v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03251",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03211",
      "title": "Fine-tuning Small Language Models as Efficient Enterprise Search Relevance Labelers",
      "authors": [
        {
          "name": "Yue Kang",
          "affiliation": null
        },
        {
          "name": "Zhuoyi Huang",
          "affiliation": null
        },
        {
          "name": "Benji Schussheim",
          "affiliation": null
        },
        {
          "name": "Diana Licon",
          "affiliation": null
        },
        {
          "name": "Dina Atia",
          "affiliation": null
        },
        {
          "name": "Shixing Cao",
          "affiliation": null
        },
        {
          "name": "Jacob Danovitch",
          "affiliation": null
        },
        {
          "name": "Kunho Kim",
          "affiliation": null
        },
        {
          "name": "Billy Norcilien",
          "affiliation": null
        },
        {
          "name": "Jonah Karpman",
          "affiliation": null
        },
        {
          "name": "Mahmound Sayed",
          "affiliation": null
        },
        {
          "name": "Mike Taylor",
          "affiliation": null
        },
        {
          "name": "Tao Sun",
          "affiliation": null
        },
        {
          "name": "Pavel Metrikov",
          "affiliation": null
        },
        {
          "name": "Vipul Agarwal",
          "affiliation": null
        },
        {
          "name": "Chris Quirk",
          "affiliation": null
        },
        {
          "name": "Ye-Yi Wang",
          "affiliation": null
        },
        {
          "name": "Nick Craswell",
          "affiliation": null
        },
        {
          "name": "Irene Shaffer",
          "affiliation": null
        },
        {
          "name": "Tianwei Chen",
          "affiliation": null
        },
        {
          "name": "Sulaiman Vesal",
          "affiliation": null
        },
        {
          "name": "Soundar Srinivasan",
          "affiliation": null
        }
      ],
      "abstract": "In enterprise search, building high-quality datasets at scale remains a central challenge due to the difficulty of acquiring labeled data. To resolve this challenge, we propose an efficient approach to fine-tune small language models (SLMs) for accurate relevance labeling, enabling high-throughput, domain-specific labeling comparable or even better in quality to that of state-of-the-art large language models (LLMs). To overcome the lack of high-quality and accessible datasets in the enterprise domain, our method leverages on synthetic data generation. Specifically, we employ an LLM to synthesize realistic enterprise queries from a seed document, apply BM25 to retrieve hard negatives, and use a teacher LLM to assign relevance scores. The resulting dataset is then distilled into an SLM, producing a compact relevance labeler. We evaluate our approach on a high-quality benchmark consisting of 923 enterprise query-document pairs annotated by trained human annotators, and show that the distilled SLM achieves agreement with human judgments on par with or better than the teacher LLM. Furthermore, our fine-tuned labeler substantially improves throughput, achieving 17 times increase while also being 19 times more cost-effective. This approach enables scalable and cost-effective relevance labeling for enterprise-scale retrieval applications, supporting rapid offline evaluation and iteration in real-world settings.",
      "publishedDate": "2026-01-06T17:48:40Z",
      "updatedDate": "2026-01-06T17:48:40Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03211v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03211",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03087",
      "title": "Audit Me If You Can: Query-Efficient Active Fairness Auditing of Black-Box LLMs",
      "authors": [
        {
          "name": "David Hartmann",
          "affiliation": null
        },
        {
          "name": "Lena Pohlmann",
          "affiliation": null
        },
        {
          "name": "Lelia Hanslik",
          "affiliation": null
        },
        {
          "name": "Noah Gießing",
          "affiliation": null
        },
        {
          "name": "Bettina Berendt",
          "affiliation": null
        },
        {
          "name": "Pieter Delobelle",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) exhibit systematic biases across demographic groups. Auditing is proposed as an accountability tool for black-box LLM applications, but suffers from resource-intensive query access. We conceptualise auditing as uncertainty estimation over a target fairness metric and introduce BAFA, the Bounded Active Fairness Auditor for query-efficient auditing of black-box LLMs. BAFA maintains a version space of surrogate models consistent with queried scores and computes uncertainty intervals for fairness metrics (e.g., $Δ$ AUC) via constrained empirical risk minimisation. Active query selection narrows these intervals to reduce estimation error. We evaluate BAFA on two standard fairness dataset case studies: \\textsc{CivilComments} and \\textsc{Bias-in-Bios}, comparing against stratified sampling, power sampling, and ablations. BAFA achieves target error thresholds with up to 40$\\times$ fewer queries than stratified sampling (e.g., 144 vs 5,956 queries at $\\varepsilon=0.02$ for \\textsc{CivilComments}) for tight thresholds, demonstrates substantially better performance over time, and shows lower variance across runs. These results suggest that active sampling can reduce resources needed for independent fairness auditing with LLMs, supporting continuous model evaluations.",
      "publishedDate": "2026-01-06T15:22:23Z",
      "updatedDate": "2026-01-06T15:22:23Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03087v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03087",
      "comment": "Submitted to ACL ARR 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03047",
      "title": "When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability",
      "authors": [
        {
          "name": "Raphael Ronge",
          "affiliation": null
        },
        {
          "name": "Markus Maier",
          "affiliation": null
        },
        {
          "name": "Frederick Eberhardt",
          "affiliation": null
        }
      ],
      "abstract": "Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature extraction and steering capabilities, our investigation suggests that major caution is warranted regarding the generalizability of these claims. We find that feature steering exhibits substantial fragility, with sensitivity to layer selection, steering magnitude, and context. We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another. While SAE-based interpretability produces compelling demonstrations in selected cases, current methods often fall short of the systematic reliability required for safety-critical applications. This suggests a necessary shift in focus from prioritizing interpretability of internal representations toward reliable prediction and control of model output. Our work contributes to a more nuanced understanding of what mechanistic interpretability has achieved and highlights fundamental challenges for AI safety that remain unresolved.",
      "publishedDate": "2026-01-06T14:29:51Z",
      "updatedDate": "2026-01-06T14:29:51Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03047v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03047",
      "comment": "33 pages (65 with appendix), 1 figure",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03034",
      "title": "NorwAI's Large Language Models: Technical Report",
      "authors": [
        {
          "name": "Jon Atle Gulla",
          "affiliation": null
        },
        {
          "name": "Peng Liu",
          "affiliation": null
        },
        {
          "name": "Lemei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.",
      "publishedDate": "2026-01-06T14:06:55Z",
      "updatedDate": "2026-01-08T10:58:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03034v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03034",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02991",
      "title": "Towards Faithful Reasoning in Comics for Small MLLMs",
      "authors": [
        {
          "name": "Chengcheng Feng",
          "affiliation": null
        },
        {
          "name": "Haojie Yin",
          "affiliation": null
        },
        {
          "name": "Yucheng Jin",
          "affiliation": null
        },
        {
          "name": "Kaizhu Huang",
          "affiliation": null
        }
      ],
      "abstract": "Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\\mathbf{12.1\\%}$ across different MLLMs.",
      "publishedDate": "2026-01-06T13:00:21Z",
      "updatedDate": "2026-01-06T13:00:21Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02991v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02991",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02927",
      "title": "PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding",
      "authors": [
        {
          "name": "Iñaki Erregue",
          "affiliation": null
        },
        {
          "name": "Kamal Nasrollahi",
          "affiliation": null
        },
        {
          "name": "Sergio Escalera",
          "affiliation": null
        }
      ],
      "abstract": "Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.",
      "publishedDate": "2026-01-06T11:11:06Z",
      "updatedDate": "2026-01-07T08:16:35Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02927v2",
      "arxivUrl": "https://arxiv.org/abs/2601.02927",
      "comment": "This paper has been accepted to the 6th Workshop on Real-World Surveillance: Applications and Challenges (WACV 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02915",
      "title": "ChemBART: A Pre-trained BART Model Assisting Organic Chemistry Analysis",
      "authors": [
        {
          "name": "Kenan Li",
          "affiliation": null
        },
        {
          "name": "Yijian Zhang",
          "affiliation": null
        },
        {
          "name": "Jin Wang",
          "affiliation": null
        },
        {
          "name": "Haipeng Gan",
          "affiliation": null
        },
        {
          "name": "Zeying Sun",
          "affiliation": null
        },
        {
          "name": "Xiaoguang Lei",
          "affiliation": null
        },
        {
          "name": "Hao Dong",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated transformative potential across diverse fields. While LLMs have been applied to molecular simplified molecular input line entry system (SMILES) in computer-aided synthesis planning (CASP), existing methodologies typically address single tasks, such as precursor prediction. We introduce ChemBART, a SMILES-based LLM pre-trained on chemical reactions, which enables a unified model for multiple downstream chemical tasks--achieving the paradigm of \"one model, one pre-training, multiple tasks.\" By leveraging outputs from a mask-filling pre-training task on reaction expressions, ChemBART effectively solves a variety of chemical problems, including precursor/reagent generation, temperature-yield regression, molecular property classification, and optimizing the policy and value functions within a reinforcement learning framework, integrated with Monte Carlo tree search for multi-step synthesis route design. Unlike single-molecule pre-trained LLMs constrained to specific applications, ChemBART addresses broader chemical challenges and integrates them for comprehensive synthesis planning. Crucially, ChemBART-designed multi-step synthesis routes and reaction conditions directly inspired wet-lab validation, which confirmed shorter pathways with ~30% yield improvement over literature benchmarks. Our work validates the power of reaction-focused pre-training and showcases the broad utility of ChemBART in advancing the complete synthesis planning cycle.",
      "publishedDate": "2026-01-06T10:55:38Z",
      "updatedDate": "2026-01-06T10:55:38Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02915v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02915",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "planning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02783",
      "title": "EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework",
      "authors": [
        {
          "name": "Junjue Wang",
          "affiliation": null
        },
        {
          "name": "Yanfei Zhong",
          "affiliation": null
        },
        {
          "name": "Zihang Chen",
          "affiliation": null
        },
        {
          "name": "Zhuo Zheng",
          "affiliation": null
        },
        {
          "name": "Ailong Ma",
          "affiliation": null
        },
        {
          "name": "Liangpei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.",
      "publishedDate": "2026-01-06T07:41:44Z",
      "updatedDate": "2026-01-06T07:41:44Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02783v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02783",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "planning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02764",
      "title": "Netflix Artwork Personalization via LLM Post-training",
      "authors": [
        {
          "name": "Hyunji Nam",
          "affiliation": null
        },
        {
          "name": "Sejoon Oh",
          "affiliation": null
        },
        {
          "name": "Emma Kong",
          "affiliation": null
        },
        {
          "name": "Yesu Feng",
          "affiliation": null
        },
        {
          "name": "Moumita Bhattacharya",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated success in various applications of user recommendation and personalization across e-commerce and entertainment. On many entertainment platforms such as Netflix, users typically interact with a wide range of titles, each represented by an artwork. Since users have diverse preferences, an artwork that appeals to one type of user may not resonate with another with different preferences. Given this user heterogeneity, our work explores the novel problem of personalized artwork recommendations according to diverse user preferences. Similar to the multi-dimensional nature of users' tastes, titles contain different themes and tones that may appeal to different viewers. For example, the same title might feature both heartfelt family drama and intense action scenes. Users who prefer romantic content may like the artwork emphasizing emotional warmth between the characters, while those who prefer action thrillers may find high-intensity action scenes more intriguing. Rather than a one-size-fits-all approach, we conduct post-training of pre-trained LLMs to make personalized artwork recommendations, selecting the most preferred visual representation of a title for each user and thereby improving user satisfaction and engagement. Our experimental results with Llama 3.1 8B models (trained on a dataset of 110K data points and evaluated on 5K held-out user-title pairs) show that the post-trained LLMs achieve 3-5\\% improvements over the Netflix production model, suggesting a promising direction for granular personalized recommendations using LLMs.",
      "publishedDate": "2026-01-06T06:56:53Z",
      "updatedDate": "2026-01-06T06:56:53Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02764v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02764",
      "comment": "6 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.02757",
      "title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery",
      "authors": [
        {
          "name": "Zixuan Xiao",
          "affiliation": null
        },
        {
          "name": "Jun Ma",
          "affiliation": null
        }
      ],
      "abstract": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.",
      "publishedDate": "2026-01-06T06:49:51Z",
      "updatedDate": "2026-01-06T06:49:51Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02757v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02757",
      "comment": null,
      "journalRef": "Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341",
      "doi": "10.1016/j.autcon.2025.106341",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02752",
      "title": "EComStage: Stage-wise and Orientation-specific Benchmarking for Large Language Models in E-commerce",
      "authors": [
        {
          "name": "Kaiyan Zhao",
          "affiliation": null
        },
        {
          "name": "Zijie Meng",
          "affiliation": null
        },
        {
          "name": "Zheyong Xie",
          "affiliation": null
        },
        {
          "name": "Jin Duan",
          "affiliation": null
        },
        {
          "name": "Yao Hu",
          "affiliation": null
        },
        {
          "name": "Zuozhu Liu",
          "affiliation": null
        },
        {
          "name": "Shaosheng Cao",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM)-based agents are increasingly deployed in e-commerce applications to assist customer services in tasks such as product inquiries, recommendations, and order management. Existing benchmarks primarily evaluate whether these agents successfully complete the final task, overlooking the intermediate reasoning stages that are crucial for effective decision-making. To address this gap, we propose EComStage, a unified benchmark for evaluating agent-capable LLMs across the comprehensive stage-wise reasoning process: Perception (understanding user intent), Planning (formulating an action plan), and Action (executing the decision). EComStage evaluates LLMs through seven separate representative tasks spanning diverse e-commerce scenarios, with all samples human-annotated and quality-checked. Unlike prior benchmarks that focus only on customer-oriented interactions, EComStage also evaluates merchant-oriented scenarios, including promotion management, content review, and operational support relevant to real-world applications. We evaluate a wide range of over 30 LLMs, spanning from 1B to over 200B parameters, including open-source models and closed-source APIs, revealing stage/orientation- specific strengths and weaknesses. Our results provide fine-grained, actionable insights for designing and optimizing LLM-based agents in real-world e-commerce settings.",
      "publishedDate": "2026-01-06T06:39:16Z",
      "updatedDate": "2026-01-06T06:39:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02752v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02752",
      "comment": "preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02749",
      "title": "The Path Ahead for Agentic AI: Challenges and Opportunities",
      "authors": [
        {
          "name": "Nadia Sibai",
          "affiliation": null
        },
        {
          "name": "Yara Ahmed",
          "affiliation": null
        },
        {
          "name": "Serry Sibaee",
          "affiliation": null
        },
        {
          "name": "Sawsan AlHalawani",
          "affiliation": null
        },
        {
          "name": "Adel Ammar",
          "affiliation": null
        },
        {
          "name": "Wadii Boulila",
          "affiliation": null
        }
      ],
      "abstract": "The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.",
      "publishedDate": "2026-01-06T06:31:42Z",
      "updatedDate": "2026-01-06T06:31:42Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02749v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02749",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "multi-agent",
        "tool-use",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent",
          "tool-use",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02736",
      "title": "Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism",
      "authors": [
        {
          "name": "Lingzhe Zhang",
          "affiliation": null
        },
        {
          "name": "Tong Jia",
          "affiliation": null
        },
        {
          "name": "Yunpeng Zhai",
          "affiliation": null
        },
        {
          "name": "Leyi Pan",
          "affiliation": null
        },
        {
          "name": "Chiming Duan",
          "affiliation": null
        },
        {
          "name": "Minghua He",
          "affiliation": null
        },
        {
          "name": "Pei Xiao",
          "affiliation": null
        },
        {
          "name": "Ying Li",
          "affiliation": null
        }
      ],
      "abstract": "Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \\textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.",
      "publishedDate": "2026-01-06T05:58:25Z",
      "updatedDate": "2026-01-06T05:58:25Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02736v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02736",
      "comment": "accepted by ICSE-NIER'26",
      "journalRef": null,
      "doi": "10.1145/3786582.3786803",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02669",
      "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
      "authors": [
        {
          "name": "Hongzhan Lin",
          "affiliation": null
        },
        {
          "name": "Zixin Chen",
          "affiliation": null
        },
        {
          "name": "Zhiqi Shen",
          "affiliation": null
        },
        {
          "name": "Ziyang Luo",
          "affiliation": null
        },
        {
          "name": "Zhen Ye",
          "affiliation": null
        },
        {
          "name": "Jing Ma",
          "affiliation": null
        },
        {
          "name": "Tat-Seng Chua",
          "affiliation": null
        },
        {
          "name": "Guandong Xu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.",
      "publishedDate": "2026-01-06T02:51:56Z",
      "updatedDate": "2026-01-06T02:51:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02669v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02669",
      "comment": "17 pages, 21 figures, 7 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02632",
      "title": "TAAF: A Trace Abstraction and Analysis Framework Synergizing Knowledge Graphs and LLMs",
      "authors": [
        {
          "name": "Alireza Ezaz",
          "affiliation": null
        },
        {
          "name": "Ghazal Khodabandeh",
          "affiliation": null
        },
        {
          "name": "Majid Babaei",
          "affiliation": null
        },
        {
          "name": "Naser Ezzati-Jivan",
          "affiliation": null
        }
      ],
      "abstract": "Execution traces are a critical source of information for understanding, debugging, and optimizing complex software systems. However, traces from OS kernels or large-scale applications like Chrome or MySQL are massive and difficult to analyze. Existing tools rely on predefined analyses, and custom insights often require writing domain-specific scripts, which is an error-prone and time-consuming task. This paper introduces TAAF (Trace Abstraction and Analysis Framework), a novel approach that combines time-indexing, knowledge graphs (KGs), and large language models (LLMs) to transform raw trace data into actionable insights. TAAF constructs a time-indexed KG from trace events to capture relationships among entities such as threads, CPUs, and system resources. An LLM then interprets query-specific subgraphs to answer natural-language questions, reducing the need for manual inspection and deep system expertise. To evaluate TAAF, we introduce TraceQA-100, a benchmark of 100 questions grounded in real kernel traces. Experiments across three LLMs and multiple temporal settings show that TAAF improves answer accuracy by up to 31.2%, particularly in multi-hop and causal reasoning tasks. We further analyze where graph-grounded reasoning helps and where limitations remain, offering a foundation for next-generation trace analysis tools.",
      "publishedDate": "2026-01-06T01:04:05Z",
      "updatedDate": "2026-01-06T01:04:05Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02632v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02632",
      "comment": "Accepted to ICSE 2026. DOI 10.1145/3744916.3787832",
      "journalRef": null,
      "doi": "10.1145/3744916.3787832",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02574",
      "title": "Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency",
      "authors": [
        {
          "name": "Haoran Wang",
          "affiliation": null
        },
        {
          "name": "Maryam Khalid",
          "affiliation": null
        },
        {
          "name": "Qiong Wu",
          "affiliation": null
        },
        {
          "name": "Jian Gao",
          "affiliation": null
        },
        {
          "name": "Cheng Cao",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly used in applications requiring factual accuracy, yet their outputs often contain hallucinated responses. While fact-checking can mitigate these errors, existing methods typically retrieve external evidence indiscriminately, overlooking the model's internal knowledge and potentially introducing irrelevant noise. Moreover, current systems lack targeted mechanisms to resolve specific uncertainties in the model's reasoning. Inspired by how humans fact-check, we argue that LLMs should adaptively decide whether to rely on internal knowledge or initiate retrieval based on their confidence in a given claim. We introduce Probabilistic Certainty and Consistency (PCC), a framework that estimates factual confidence by jointly modeling an LLM's probabilistic certainty and reasoning consistency. These confidence signals enable an adaptive verification strategy: the model answers directly when confident, triggers targeted retrieval when uncertain or inconsistent, and escalates to deep search when ambiguity is high. Our confidence-guided routing mechanism ensures that retrieval is invoked only when necessary, improving both efficiency and reliability. Extensive experiments across three challenging benchmarks show that PCC achieves better uncertainty quantification than verbalized confidence and consistently outperforms strong LLM-based fact-checking baselines. Furthermore, we demonstrate that PCC generalizes well across various LLMs.",
      "publishedDate": "2026-01-05T21:57:41Z",
      "updatedDate": "2026-01-05T21:57:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02574v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02574",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02535",
      "title": "ModeX: Evaluator-Free Best-of-N Selection for Open-Ended Generation",
      "authors": [
        {
          "name": "Hyeong Kyu Choi",
          "affiliation": null
        },
        {
          "name": "Sharon Li",
          "affiliation": null
        }
      ],
      "abstract": "Selecting a single high-quality output from multiple stochastic generations remains a fundamental challenge for large language models (LLMs), particularly in open-ended tasks where no canonical answer exists. While Best-of-N and self-consistency methods show that aggregating multiple generations can improve performance, existing approaches typically rely on external evaluators, reward models, or exact string-match voting, limiting their applicability and efficiency. We propose Mode Extraction (ModeX), an evaluator-free Best-of-N selection framework that generalizes majority voting to open-ended text generation by identifying the modal output representing the dominant semantic consensus among generated texts. ModeX constructs a similarity graph over candidate generations and recursively applies spectral clustering to select a representative centroid, without requiring additional inference or auxiliary models. We further instantiate this selection principle as ModeX--Lite, an improved version of ModeX with early pruning for efficiency. Across open-ended tasks--including text summarization, code generation, and mathematical reasoning--our approaches consistently outperform standard single- and multi-path baselines, providing a computationally efficient solution for robust open-ended text generation. Code is released in https://github.com/deeplearning-wisc/ModeX.",
      "publishedDate": "2026-01-05T20:16:32Z",
      "updatedDate": "2026-01-05T20:16:32Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02535v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02535",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02514",
      "title": "Textual Explanations and Their Evaluations for Reinforcement Learning Policy",
      "authors": [
        {
          "name": "Ahmad Terra",
          "affiliation": null
        },
        {
          "name": "Mohit Ahmed",
          "affiliation": null
        },
        {
          "name": "Rafia Inam",
          "affiliation": null
        },
        {
          "name": "Elena Fersman",
          "affiliation": null
        },
        {
          "name": "Martin Törngren",
          "affiliation": null
        }
      ],
      "abstract": "Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.",
      "publishedDate": "2026-01-05T19:38:07Z",
      "updatedDate": "2026-01-05T19:38:07Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02514v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02514",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02512",
      "title": "Green LLM Techniques in Action: How Effective Are Existing Techniques for Improving the Energy Efficiency of LLM-Based Applications in Industry?",
      "authors": [
        {
          "name": "Pelin Rabia Kuran",
          "affiliation": null
        },
        {
          "name": "Rumbidzai Chitakunye",
          "affiliation": null
        },
        {
          "name": "Vincenzo Stoico",
          "affiliation": null
        },
        {
          "name": "Ilja Heitlager",
          "affiliation": null
        },
        {
          "name": "Justus Bogner",
          "affiliation": null
        }
      ],
      "abstract": "The rapid adoption of large language models (LLMs) has raised concerns about their substantial energy consumption, especially when deployed at industry scale. While several techniques have been proposed to address this, limited empirical evidence exists regarding the effectiveness of applying them to LLM-based industry applications. To fill this gap, we analyzed a chatbot application in an industrial context at Schuberg Philis, a Dutch IT services company. We then selected four techniques, namely Small and Large Model Collaboration, Prompt Optimization, Quantization, and Batching, applied them to the application in eight variations, and then conducted experiments to study their impact on energy consumption, accuracy, and response time compared to the unoptimized baseline. Our results show that several techniques, such as Prompt Optimization and 2-bit Quantization, managed to reduce energy use significantly, sometimes by up to 90%. However, these techniques especially impacted accuracy negatively, to a degree that is not acceptable in practice. The only technique that achieved significant and strong energy reductions without harming the other qualities substantially was Small and Large Model Collaboration via Nvidia's Prompt Task and Complexity Classifier (NPCC) with prompt complexity thresholds. This highlights that reducing the energy consumption of LLM-based applications is not difficult in practice. However, improving their energy efficiency, i.e., reducing energy use without harming other qualities, remains challenging. Our study provides practical insights to move towards this goal.",
      "publishedDate": "2026-01-05T19:35:29Z",
      "updatedDate": "2026-01-05T19:35:29Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02512v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02512",
      "comment": "Accepted for publication at the 2026 International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP'26)",
      "journalRef": null,
      "doi": "10.1145/3786583.3786896",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "tool-use",
        "multi-agent",
        "prompting"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "multi-agent",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02511",
      "title": "LLM-Enhanced Reinforcement Learning for Time Series Anomaly Detection",
      "authors": [
        {
          "name": "Bahareh Golchin",
          "affiliation": null
        },
        {
          "name": "Banafsheh Rekabdar",
          "affiliation": null
        },
        {
          "name": "Danielle Justo",
          "affiliation": null
        }
      ],
      "abstract": "Detecting anomalies in time series data is crucial for finance, healthcare, sensor networks, and industrial monitoring applications. However, time series anomaly detection often suffers from sparse labels, complex temporal patterns, and costly expert annotation. We propose a unified framework that integrates Large Language Model (LLM)-based potential functions for reward shaping with Reinforcement Learning (RL), Variational Autoencoder (VAE)-enhanced dynamic reward scaling, and active learning with label propagation. An LSTM-based RL agent leverages LLM-derived semantic rewards to guide exploration, while VAE reconstruction errors add unsupervised anomaly signals. Active learning selects the most uncertain samples, and label propagation efficiently expands labeled data. Evaluations on Yahoo-A1 and SMD benchmarks demonstrate that our method achieves state-of-the-art detection accuracy under limited labeling budgets and operates effectively in data-constrained settings. This study highlights the promise of combining LLMs with RL and advanced unsupervised techniques for robust, scalable anomaly detection in real-world applications.",
      "publishedDate": "2026-01-05T19:33:30Z",
      "updatedDate": "2026-01-05T19:33:30Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02511v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02511",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02443",
      "title": "Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative",
      "authors": [
        {
          "name": "Li Wang",
          "affiliation": null
        },
        {
          "name": "Xi Chen",
          "affiliation": null
        },
        {
          "name": "XiangWen Deng",
          "affiliation": null
        },
        {
          "name": "HuaHui Yi",
          "affiliation": null
        },
        {
          "name": "ZeKun Jiang",
          "affiliation": null
        },
        {
          "name": "Kang Li",
          "affiliation": null
        },
        {
          "name": "Jian Li",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.",
      "publishedDate": "2026-01-05T13:31:44Z",
      "updatedDate": "2026-01-05T13:31:44Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02443v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02443",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02430",
      "title": "WebCoderBench: Benchmarking Web Application Generation with Comprehensive and Interpretable Evaluation Metrics",
      "authors": [
        {
          "name": "Chenxu Liu",
          "affiliation": null
        },
        {
          "name": "Yingjie Fu",
          "affiliation": null
        },
        {
          "name": "Wei Yang",
          "affiliation": null
        },
        {
          "name": "Ying Zhang",
          "affiliation": null
        },
        {
          "name": "Tao Xie",
          "affiliation": null
        }
      ],
      "abstract": "Web applications (web apps) have become a key arena for large language models (LLMs) to demonstrate their code generation capabilities and commercial potential. However, building a benchmark for LLM-generated web apps remains challenging due to the need for real-world user requirements, generalizable evaluation metrics without relying on ground-truth implementations or test cases, and interpretable evaluation results. To address these challenges, we introduce WebCoderBench, the first real-world-collected, generalizable, and interpretable benchmark for web app generation. WebCoderBench comprises 1,572 real user requirements, covering diverse modalities and expression styles that reflect realistic user intentions. WebCoderBench provides 24 fine-grained evaluation metrics across 9 perspectives, combining rule-based and LLM-as-a-judge paradigm for fully automated, objective, and general evaluation. Moreover, WebCoderBench adopts human-preference-aligned weights over metrics to yield interpretable overall scores. Experiments across 12 representative LLMs and 2 LLM-based agents show that there exists no dominant model across all evaluation metrics, offering an opportunity for LLM developers to optimize their models in a targeted manner for a more powerful version.",
      "publishedDate": "2026-01-05T05:23:07Z",
      "updatedDate": "2026-01-05T05:23:07Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02430v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02430",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "code-generation",
        "agents"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03204",
      "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents",
      "authors": [
        {
          "name": "Chenglin Yu",
          "affiliation": null
        },
        {
          "name": "Yuchen Wang",
          "affiliation": null
        },
        {
          "name": "Songmiao Wang",
          "affiliation": null
        },
        {
          "name": "Hongxia Yang",
          "affiliation": null
        },
        {
          "name": "Ming Li",
          "affiliation": null
        }
      ],
      "abstract": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent",
      "publishedDate": "2026-01-06T17:35:57Z",
      "updatedDate": "2026-01-06T17:35:57Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03204v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03204",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03137",
      "title": "Accurate Table Question Answering with Accessible LLMs",
      "authors": [
        {
          "name": "Yangfan Jiang",
          "affiliation": null
        },
        {
          "name": "Fei Wei",
          "affiliation": null
        },
        {
          "name": "Ergute Bao",
          "affiliation": null
        },
        {
          "name": "Yaliang Li",
          "affiliation": null
        },
        {
          "name": "Bolin Ding",
          "affiliation": null
        },
        {
          "name": "Yin Yang",
          "affiliation": null
        },
        {
          "name": "Xiaokui Xiao",
          "affiliation": null
        }
      ],
      "abstract": "Given a table T in a database and a question Q in natural language, the table question answering (TQA) task aims to return an accurate answer to Q based on the content of T. Recent state-of-the-art solutions leverage large language models (LLMs) to obtain high-quality answers. However, most rely on proprietary, large-scale LLMs with costly API access, posing a significant financial barrier. This paper instead focuses on TQA with smaller, open-weight LLMs that can run on a desktop or laptop. This setting is challenging, as such LLMs typically have weaker capabilities than large proprietary models, leading to substantial performance degradation with existing methods. We observe that a key reason for this degradation is that prior approaches often require the LLM to solve a highly sophisticated task using long, complex prompts, which exceed the capabilities of small open-weight LLMs. Motivated by this observation, we present Orchestra, a multi-agent approach that unlocks the potential of accessible LLMs for high-quality, cost-effective TQA. Orchestra coordinates a group of LLM agents, each responsible for a relatively simple task, through a structured, layered workflow to solve complex TQA problems -- akin to an orchestra. By reducing the prompt complexity faced by each agent, Orchestra significantly improves output reliability. We implement Orchestra on top of AgentScope, an open-source multi-agent framework, and evaluate it on multiple TQA benchmarks using a wide range of open-weight LLMs. Experimental results show that Orchestra achieves strong performance even with small- to medium-sized models. For example, with Qwen2.5-14B, Orchestra reaches 72.1% accuracy on WikiTQ, approaching the best prior result of 75.3% achieved with GPT-4; with larger Qwen, Llama, or DeepSeek models, Orchestra outperforms all prior methods and establishes new state-of-the-art results across all benchmarks.",
      "publishedDate": "2026-01-06T16:07:25Z",
      "updatedDate": "2026-01-06T16:07:25Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03137v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03137",
      "comment": "accepted for publication in the Proceedings of the IEEE International Conference on Data Engineering (ICDE) 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02744",
      "title": "SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation",
      "authors": [
        {
          "name": "Hanqi Jiang",
          "affiliation": null
        },
        {
          "name": "Junhao Chen",
          "affiliation": null
        },
        {
          "name": "Yi Pan",
          "affiliation": null
        },
        {
          "name": "Ling Chen",
          "affiliation": null
        },
        {
          "name": "Weihang You",
          "affiliation": null
        },
        {
          "name": "Yifan Zhou",
          "affiliation": null
        },
        {
          "name": "Ruidong Zhang",
          "affiliation": null
        },
        {
          "name": "Yohannes Abate",
          "affiliation": null
        },
        {
          "name": "Tianming Liu",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the \"Contextual Tunneling\" problem. Our code and data will be made publicly available upon acceptance.",
      "publishedDate": "2026-01-06T06:19:58Z",
      "updatedDate": "2026-01-06T06:19:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02744v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02744",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "code-generation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02695",
      "title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems",
      "authors": [
        {
          "name": "Guibin Zhang",
          "affiliation": null
        },
        {
          "name": "Haiyang Yu",
          "affiliation": null
        },
        {
          "name": "Kaiming Yang",
          "affiliation": null
        },
        {
          "name": "Bingli Wu",
          "affiliation": null
        },
        {
          "name": "Fei Huang",
          "affiliation": null
        },
        {
          "name": "Yongbin Li",
          "affiliation": null
        },
        {
          "name": "Shuicheng Yan",
          "affiliation": null
        }
      ],
      "abstract": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.",
      "publishedDate": "2026-01-06T04:06:46Z",
      "updatedDate": "2026-01-06T04:06:46Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02695v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02695",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "rag",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02598",
      "title": "LongDA: Benchmarking LLM Agents for Long-Document Data Analysis",
      "authors": [
        {
          "name": "Yiyang Li",
          "affiliation": null
        },
        {
          "name": "Zheyuan Zhang",
          "affiliation": null
        },
        {
          "name": "Tianyi Ma",
          "affiliation": null
        },
        {
          "name": "Zehong Wang",
          "affiliation": null
        },
        {
          "name": "Keerthiram Murugesan",
          "affiliation": null
        },
        {
          "name": "Chuxu Zhang",
          "affiliation": null
        },
        {
          "name": "Yanfang Ye",
          "affiliation": null
        }
      ],
      "abstract": "We introduce LongDA, a data analysis benchmark for evaluating LLM-based agents under documentation-intensive analytical workflows. In contrast to existing benchmarks that assume well-specified schemas and inputs, LongDA targets real-world settings in which navigating long documentation and complex data is the primary bottleneck. To this end, we manually curate raw data files, long and heterogeneous documentation, and expert-written publications from 17 publicly available U.S. national surveys, from which we extract 505 analytical queries grounded in real analytical practice. Solving these queries requires agents to first retrieve and integrate key information from multiple unstructured documents, before performing multi-step computations and writing executable code, which remains challenging for existing data analysis agents. To support the systematic evaluation under this setting, we develop LongTA, a tool-augmented agent framework that enables document access, retrieval, and code execution, and evaluate a range of proprietary and open-source models. Our experiments reveal substantial performance gaps even among state-of-the-art models, highlighting the challenges researchers should consider before applying LLM agents for decision support in real-world, high-stakes analytical settings.",
      "publishedDate": "2026-01-05T23:23:16Z",
      "updatedDate": "2026-01-05T23:23:16Z",
      "primaryCategory": "cs.DL",
      "arxivCategories": [
        "cs.DL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02598v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02598",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02577",
      "title": "Orchestral AI: A Framework for Agent Orchestration",
      "authors": [
        {
          "name": "Alexander Roman",
          "affiliation": null
        },
        {
          "name": "Jacob Roman",
          "affiliation": null
        }
      ],
      "abstract": "The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.",
      "publishedDate": "2026-01-05T22:02:11Z",
      "updatedDate": "2026-01-05T22:02:11Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "astro-ph.IM",
        "hep-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02577v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02577",
      "comment": "17 pages, 3 figures. For more information visit https://orchestral-ai.com",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02553",
      "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
      "authors": [
        {
          "name": "Jiaqi Liu",
          "affiliation": null
        },
        {
          "name": "Yaofeng Su",
          "affiliation": null
        },
        {
          "name": "Peng Xia",
          "affiliation": null
        },
        {
          "name": "Siwei Han",
          "affiliation": null
        },
        {
          "name": "Zeyu Zheng",
          "affiliation": null
        },
        {
          "name": "Cihang Xie",
          "affiliation": null
        },
        {
          "name": "Mingyu Ding",
          "affiliation": null
        },
        {
          "name": "Huaxiu Yao",
          "affiliation": null
        }
      ],
      "abstract": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.",
      "publishedDate": "2026-01-05T21:02:49Z",
      "updatedDate": "2026-01-05T21:02:49Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02553v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02553",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03236",
      "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
      "authors": [
        {
          "name": "Dongming Jiang",
          "affiliation": null
        },
        {
          "name": "Yi Li",
          "affiliation": null
        },
        {
          "name": "Guanpeng Li",
          "affiliation": null
        },
        {
          "name": "Bingzhe Li",
          "affiliation": null
        }
      ],
      "abstract": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.",
      "publishedDate": "2026-01-06T18:29:43Z",
      "updatedDate": "2026-01-06T18:29:43Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03236v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03236",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03227",
      "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
      "authors": [
        {
          "name": "Ruixing Zhang",
          "affiliation": null
        },
        {
          "name": "Zihan Liu",
          "affiliation": null
        },
        {
          "name": "Leilei Sun",
          "affiliation": null
        },
        {
          "name": "Tongyu Zhu",
          "affiliation": null
        },
        {
          "name": "Weifeng Lv",
          "affiliation": null
        }
      ],
      "abstract": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.",
      "publishedDate": "2026-01-06T18:13:24Z",
      "updatedDate": "2026-01-06T18:13:24Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03227v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03227",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03217",
      "title": "MalruleLib: Large-Scale Executable Misconception Reasoning with Step Traces for Modeling Student Thinking in Mathematics",
      "authors": [
        {
          "name": "Xinghe Chen",
          "affiliation": null
        },
        {
          "name": "Naiming Liu",
          "affiliation": null
        },
        {
          "name": "Shashank Sonkar",
          "affiliation": null
        }
      ],
      "abstract": "Student mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts. We introduce MalruleLib, a learning-science-grounded framework that translates documented misconceptions into executable procedures, drawing on 67 learning-science and mathematics education sources, and generates step-by-step traces of malrule-consistent student work. We formalize a core student-modeling problem as Malrule Reasoning Accuracy (MRA): infer a misconception from one worked mistake and predict the student's next answer under cross-template rephrasing. Across nine language models (4B-120B), accuracy drops from 66% on direct problem solving to 40% on cross-template misconception prediction. MalruleLib encodes 101 malrules over 498 parameterized problem templates and produces paired dual-path traces for both correct reasoning and malrule-consistent student reasoning. Because malrules are executable and templates are parameterizable, MalruleLib can generate over one million instances, enabling scalable supervision and controlled evaluation. Using MalruleLib, we observe cross-template degradations of 10-21%, while providing student step traces improves prediction by 3-15%. We release MalruleLib as infrastructure for educational AI that models student procedures across contexts, enabling diagnosis and feedback that targets the underlying misconception.",
      "publishedDate": "2026-01-06T17:59:37Z",
      "updatedDate": "2026-01-06T17:59:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03217v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03217",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03205",
      "title": "UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward",
      "authors": [
        {
          "name": "Yile Liu",
          "affiliation": null
        },
        {
          "name": "Yixian Liu",
          "affiliation": null
        },
        {
          "name": "Zongwei Li",
          "affiliation": null
        },
        {
          "name": "Yufei Huang",
          "affiliation": null
        },
        {
          "name": "Xinhua Feng",
          "affiliation": null
        },
        {
          "name": "Zhichao Hu",
          "affiliation": null
        },
        {
          "name": "Jinglu Hu",
          "affiliation": null
        },
        {
          "name": "Jianfeng Yan",
          "affiliation": null
        },
        {
          "name": "Fengzong Lian",
          "affiliation": null
        },
        {
          "name": "Yuhong Liu",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.",
      "publishedDate": "2026-01-06T17:41:32Z",
      "updatedDate": "2026-01-06T17:41:32Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03205v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03205",
      "comment": "19 pages, 6 figures, 7 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "planning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03194",
      "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
      "authors": [
        {
          "name": "Mohammad Zia Ur Rehman",
          "affiliation": null
        },
        {
          "name": "Sai Kartheek Reddy Kasu",
          "affiliation": null
        },
        {
          "name": "Shashivardhan Reddy Koppula",
          "affiliation": null
        },
        {
          "name": "Sai Rithwik Reddy Chirra",
          "affiliation": null
        },
        {
          "name": "Shwetank Shekhar Singh",
          "affiliation": null
        },
        {
          "name": "Nagendra Kumar",
          "affiliation": null
        }
      ],
      "abstract": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST",
      "publishedDate": "2026-01-06T17:16:45Z",
      "updatedDate": "2026-01-06T17:16:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03194v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03194",
      "comment": "Accepted in the proceedings of AAAI 2026",
      "journalRef": "AAA 2026 (AISI)",
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03192",
      "title": "MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory",
      "authors": [
        {
          "name": "Shengtao Zhang",
          "affiliation": null
        },
        {
          "name": "Jiaqian Wang",
          "affiliation": null
        },
        {
          "name": "Ruiwen Zhou",
          "affiliation": null
        },
        {
          "name": "Junwei Liao",
          "affiliation": null
        },
        {
          "name": "Yuchen Feng",
          "affiliation": null
        },
        {
          "name": "Weinan Zhang",
          "affiliation": null
        },
        {
          "name": "Ying Wen",
          "affiliation": null
        },
        {
          "name": "Zhiyu Li",
          "affiliation": null
        },
        {
          "name": "Feiyu Xiong",
          "affiliation": null
        },
        {
          "name": "Yutao Qi",
          "affiliation": null
        },
        {
          "name": "Bo Tang",
          "affiliation": null
        },
        {
          "name": "Muning Wen",
          "affiliation": null
        }
      ],
      "abstract": "The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.",
      "publishedDate": "2026-01-06T17:14:50Z",
      "updatedDate": "2026-01-06T17:14:50Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03192v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03192",
      "comment": "23 pages, 11 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03191",
      "title": "AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation",
      "authors": [
        {
          "name": "Anees Ur Rehman Hashmi",
          "affiliation": null
        },
        {
          "name": "Numan Saeed",
          "affiliation": null
        },
        {
          "name": "Christoph Lippert",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix",
      "publishedDate": "2026-01-06T17:13:23Z",
      "updatedDate": "2026-01-06T17:13:23Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03191v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03191",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03164",
      "title": "WebAnchor: Anchoring Agent Planning to Stabilize Long-Horizon Web Reasoning",
      "authors": [
        {
          "name": "Xinmiao Yu",
          "affiliation": null
        },
        {
          "name": "Liwen Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaocheng Feng",
          "affiliation": null
        },
        {
          "name": "Yong Jiang",
          "affiliation": null
        },
        {
          "name": "Bing Qin",
          "affiliation": null
        },
        {
          "name": "Pengjun Xie",
          "affiliation": null
        },
        {
          "name": "Jingren Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model(LLM)-based agents have shown strong capabilities in web information seeking, with reinforcement learning (RL) becoming a key optimization paradigm. However, planning remains a bottleneck, as existing methods struggle with long-horizon strategies. Our analysis reveals a critical phenomenon, plan anchor, where the first reasoning step disproportionately impacts downstream behavior in long-horizon web reasoning tasks. Current RL algorithms, fail to account for this by uniformly distributing rewards across the trajectory. To address this, we propose Anchor-GRPO, a two-stage RL framework that decouples planning and execution. In Stage 1, the agent optimizes its first-step planning using fine-grained rubrics derived from self-play experiences and human calibration. In Stage 2, execution is aligned with the initial plan through sparse rewards, ensuring stable and efficient tool usage. We evaluate Anchor-GRPO on four benchmarks: BrowseComp, BrowseComp-Zh, GAIA, and XBench-DeepSearch. Across models from 3B to 30B, Anchor-GRPO outperforms baseline GRPO and First-step GRPO, improving task success and tool efficiency. Notably, WebAnchor-30B achieves 46.0% pass@1 on BrowseComp and 76.4% on GAIA. Anchor-GRPO also demonstrates strong scalability, getting higher accuracy as model size and context length increase.",
      "publishedDate": "2026-01-06T16:36:40Z",
      "updatedDate": "2026-01-07T02:00:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03164v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03164",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03144",
      "title": "Self-Verification is All You Need To Pass The Japanese Bar Examination",
      "authors": [
        {
          "name": "Andrew Shin",
          "affiliation": null
        }
      ],
      "abstract": "Despite rapid advances in large language models (LLMs), achieving reliable performance on highly professional and structured examinations remains a significant challenge. The Japanese bar examination is a particularly demanding benchmark, requiring not only advanced legal reasoning but also strict adherence to complex answer formats that involve joint evaluation of multiple propositions. While recent studies have reported improvements by decomposing such questions into simpler true--false judgments, these approaches have not been systematically evaluated under the original exam format and scoring scheme, leaving open the question of whether they truly capture exam-level competence. In this paper, we present a self-verification model trained on a newly constructed dataset that faithfully replicates the authentic format and evaluation scale of the exam. Our model is able to exceed the official passing score when evaluated on the actual exam scale, marking the first demonstration, to our knowledge, of an LLM passing the Japanese bar examination without altering its original question structure or scoring rules. We further conduct extensive comparisons with alternative strategies, including multi-agent inference and decomposition-based supervision, and find that these methods fail to achieve comparable performance. Our results highlight the importance of format-faithful supervision and consistency verification, and suggest that carefully designed single-model approaches can outperform more complex systems in high-stakes professional reasoning tasks. Our dataset and codes are publicly available.",
      "publishedDate": "2026-01-06T16:13:47Z",
      "updatedDate": "2026-01-06T16:13:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03144v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03144",
      "comment": "https://github.com/shinandrew/self_verification",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "reasoning",
          "multi-agent",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03111",
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "authors": [
        {
          "name": "Yiyuan Li",
          "affiliation": null
        },
        {
          "name": "Zhen Huang",
          "affiliation": null
        },
        {
          "name": "Yanan Wu",
          "affiliation": null
        },
        {
          "name": "Weixun Wang",
          "affiliation": null
        },
        {
          "name": "Xuefeng Li",
          "affiliation": null
        },
        {
          "name": "Yijia Luo",
          "affiliation": null
        },
        {
          "name": "Wenbo Su",
          "affiliation": null
        },
        {
          "name": "Bo Zheng",
          "affiliation": null
        },
        {
          "name": "Pengfei Liu",
          "affiliation": null
        }
      ],
      "abstract": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
      "publishedDate": "2026-01-06T15:41:35Z",
      "updatedDate": "2026-01-06T15:41:35Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03111v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03111",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03093",
      "title": "ATLAS: Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning",
      "authors": [
        {
          "name": "Tuc Nguyen",
          "affiliation": null
        },
        {
          "name": "Thai Le",
          "affiliation": null
        }
      ],
      "abstract": "Recent work on activation and latent steering has demonstrated that modifying internal representations can effectively guide large language models (LLMs) toward improved reasoning and efficiency without additional training. However, most existing approaches rely on fixed steering policies and static intervention strengths, which limit their robustness across problem instances and often result in over- or under-steering. We propose Adaptive Test-time Latent Steering, called (ATLAS), a task- specific framework that dynamically controls steering decisions at inference time using an external, lightweight latent verifier. Given intermediate hidden states, the verifier predicts the quality of ongoing reasoning and adaptively selects whether and how strongly to apply steering, enabling per-example and per-step adjustment with minimal overhead. To our knowledge, ATLAS is the first method to integrate learned latent verification into test-time steering for enhancing LLMs reasoning. Experiments on multiple mathematical reasoning benchmarks show that ATLAS consistently outperforms both vanilla decoding and fixed steering baselines, achieving higher accuracy while substantially reducing test-time token usage. These results demonstrate that verifier-guided latent adaptation provides an effective and scalable mechanism for controlling reasoning efficiency without sacrificing solution quality. All source code will be publicly available.",
      "publishedDate": "2026-01-06T15:27:24Z",
      "updatedDate": "2026-01-06T15:27:24Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03093v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03093",
      "comment": "12 pages, 3 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03073",
      "title": "Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA",
      "authors": [
        {
          "name": "Tong Wu",
          "affiliation": null
        },
        {
          "name": "Thanet Markchom",
          "affiliation": null
        }
      ],
      "abstract": "Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.",
      "publishedDate": "2026-01-06T14:58:33Z",
      "updatedDate": "2026-01-06T14:58:33Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03073v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03073",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03066",
      "title": "Do LLMs Encode Functional Importance of Reasoning Tokens?",
      "authors": [
        {
          "name": "Janvijay Singh",
          "affiliation": null
        },
        {
          "name": "Dilek Hakkani-Tür",
          "affiliation": null
        }
      ],
      "abstract": "Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased computational cost and reduced ability to isolate functionally relevant reasoning. Prior work on compact reasoning shortens such chains through probabilistic sampling, heuristics, or supervision from frontier models, but offers limited insight into whether models internally encode token-level functional importance for answer generation. We address this gap diagnostically and propose greedy pruning, a likelihood-preserving deletion procedure that iteratively removes reasoning tokens whose removal minimally degrades model likelihood under a specified objective, yielding length-controlled reasoning chains. We evaluate pruned reasoning in a distillation framework and show that students trained on pruned chains outperform a frontier-model-supervised compression baseline at matched reasoning lengths. Finally, our analysis reveals systematic pruning patterns and shows that attention scores can predict greedy pruning ranks, further suggesting that models encode a nontrivial functional importance structure over reasoning tokens.",
      "publishedDate": "2026-01-06T14:50:02Z",
      "updatedDate": "2026-01-06T14:50:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03066v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03066",
      "comment": "20 pages, 8 figures, 2 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03052",
      "title": "Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph",
      "authors": [
        {
          "name": "Jianpeng Hu",
          "affiliation": null
        },
        {
          "name": "Yanzeng Li",
          "affiliation": null
        },
        {
          "name": "Jialun Zhong",
          "affiliation": null
        },
        {
          "name": "Wenfa Qi",
          "affiliation": null
        },
        {
          "name": "Lei Zou",
          "affiliation": null
        }
      ],
      "abstract": "The Retrieval-augmented generation (RAG) system based on Large language model (LLM) has made significant progress. It can effectively reduce factuality hallucinations, but faithfulness hallucinations still exist. Previous methods for detecting faithfulness hallucinations either neglect to capture the models' internal reasoning processes or handle those features coarsely, making it difficult for discriminators to learn. This paper proposes a semantic-level internal reasoning graph-based method for detecting faithfulness hallucination. Specifically, we first extend the layer-wise relevance propagation algorithm from the token level to the semantic level, constructing an internal reasoning graph based on attribution vectors. This provides a more faithful semantic-level representation of dependency. Furthermore, we design a general framework based on a small pre-trained language model to utilize the dependencies in LLM's reasoning for training and hallucination detection, which can dynamically adjust the pass rate of correct samples through a threshold. Experimental results demonstrate that our method achieves better overall performance compared to state-of-the-art baselines on RAGTruth and Dolly-15k.",
      "publishedDate": "2026-01-06T14:35:20Z",
      "updatedDate": "2026-01-06T14:35:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03052v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03052",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03043",
      "title": "Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage",
      "authors": [
        {
          "name": "Junhao Hu",
          "affiliation": null
        },
        {
          "name": "Fangze Li",
          "affiliation": null
        },
        {
          "name": "Mingtao Xu",
          "affiliation": null
        },
        {
          "name": "Feifan Meng",
          "affiliation": null
        },
        {
          "name": "Shiju Zhao",
          "affiliation": null
        },
        {
          "name": "Tiancheng Hu",
          "affiliation": null
        },
        {
          "name": "Ting Peng",
          "affiliation": null
        },
        {
          "name": "Anmin Liu",
          "affiliation": null
        },
        {
          "name": "Wenrui Huang",
          "affiliation": null
        },
        {
          "name": "Chenxu Liu",
          "affiliation": null
        },
        {
          "name": "Ziyue Hua",
          "affiliation": null
        },
        {
          "name": "Tao Xie",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less'' (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.",
      "publishedDate": "2026-01-06T14:23:58Z",
      "updatedDate": "2026-01-06T14:23:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03043v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03043",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03023",
      "title": "MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models",
      "authors": [
        {
          "name": "Lecheng Gong",
          "affiliation": null
        },
        {
          "name": "Weimin Fang",
          "affiliation": null
        },
        {
          "name": "Ting Yang",
          "affiliation": null
        },
        {
          "name": "Dongjie Tao",
          "affiliation": null
        },
        {
          "name": "Chunxiao Guo",
          "affiliation": null
        },
        {
          "name": "Peng Wei",
          "affiliation": null
        },
        {
          "name": "Bo Xie",
          "affiliation": null
        },
        {
          "name": "Jinqun Guan",
          "affiliation": null
        },
        {
          "name": "Zixiao Chen",
          "affiliation": null
        },
        {
          "name": "Fang Shi",
          "affiliation": null
        },
        {
          "name": "Jinjie Gu",
          "affiliation": null
        },
        {
          "name": "Junwei Liu",
          "affiliation": null
        }
      ],
      "abstract": "Medical conversational AI (AI) plays a pivotal role in the development of safer and more effective medical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing the information-gathering and diagnostic reasoning abilities of medical large language models (LLMs) have not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel benchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained evaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically designed to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-agent system to synthesize realistic patient records and chief complaints from underlying disease knowledge without accessing real-world electronic health records, thereby mitigating privacy and data-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical facts and augmented with a dynamic guidance mechanism that continuously detects and corrects hallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the simulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-generation pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject sampling to derive a prioritized set of rubric items (\"must-ask\" items) for each case. We perform a comprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment dimensions, current models face substantial challenges. Our results indicate that improving medical dialogue will require advances in dialogue management architectures, not just incremental tuning of the base-model.",
      "publishedDate": "2026-01-06T13:56:33Z",
      "updatedDate": "2026-01-07T02:10:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03023v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03023",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03018",
      "title": "Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis",
      "authors": [
        {
          "name": "Choonghan Kim",
          "affiliation": null
        },
        {
          "name": "Hyunmin Hwang",
          "affiliation": null
        },
        {
          "name": "Hangeol Chang",
          "affiliation": null
        },
        {
          "name": "Jaemin Kim",
          "affiliation": null
        },
        {
          "name": "Jinse Park",
          "affiliation": null
        },
        {
          "name": "Jae-Sung Lim",
          "affiliation": null
        },
        {
          "name": "Jong Chul Ye",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5",
      "publishedDate": "2026-01-06T13:44:04Z",
      "updatedDate": "2026-01-06T13:44:04Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03018v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03018",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03014",
      "title": "SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering",
      "authors": [
        {
          "name": "Junli Liang",
          "affiliation": null
        },
        {
          "name": "Pengfei Zhou",
          "affiliation": null
        },
        {
          "name": "Wangqiu Zhou",
          "affiliation": null
        },
        {
          "name": "Wenjie Qing",
          "affiliation": null
        },
        {
          "name": "Qi Zhao",
          "affiliation": null
        },
        {
          "name": "Ziwen Wang",
          "affiliation": null
        },
        {
          "name": "Qi Song",
          "affiliation": null
        },
        {
          "name": "Xiangyang Li",
          "affiliation": null
        }
      ],
      "abstract": "Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.",
      "publishedDate": "2026-01-06T13:39:51Z",
      "updatedDate": "2026-01-06T13:39:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03014v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03014",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03007",
      "title": "From inconsistency to decision: explainable operation and maintenance of battery energy storage systems",
      "authors": [
        {
          "name": "Jingbo Qu",
          "affiliation": null
        },
        {
          "name": "Yijie Wang",
          "affiliation": null
        },
        {
          "name": "Yujie Fu",
          "affiliation": null
        },
        {
          "name": "Putai Zhang",
          "affiliation": null
        },
        {
          "name": "Weihan Li",
          "affiliation": null
        },
        {
          "name": "Mian Li",
          "affiliation": null
        }
      ],
      "abstract": "Battery Energy Storage Systems (BESSs) are increasingly critical to power-system stability, yet their operation and maintenance remain dominated by reactive, expert-dependent diagnostics. While cell-level inconsistencies provide early warning signals of degradation and safety risks, the lack of scalable and interpretable decision-support frameworks prevents these signals from being effectively translated into operational actions. Here we introduce an inconsistency-driven operation and maintenance paradigm for large-scale BESSs that systematically transforms routine monitoring data into explainable, decision-oriented guidance. The proposed framework integrates multi-dimensional inconsistency evaluation with large language model-based semantic reasoning to bridge the gap between quantitative diagnostics and practical maintenance decisions. Using eight months of field data from an in-service battery system comprising 3,564 cells, we demonstrate how electrical, thermal, and aging-related inconsistencies can be distilled into structured operational records and converted into actionable maintenance insights through a multi-agent framework. The proposed approach enables accurate and explainable responses to real-world operation and maintenance queries, reducing response time and operational cost by over 80% compared with conventional expert-driven practices. These results establish a scalable pathway for intelligent operation and maintenance of battery energy storage systems, with direct implications for reliability, safety, and cost-effective integration of energy storage into modern power systems.",
      "publishedDate": "2026-01-06T13:32:04Z",
      "updatedDate": "2026-01-07T02:29:31Z",
      "primaryCategory": "eess.SY",
      "arxivCategories": [
        "eess.SY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03007v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03007",
      "comment": "13 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02993",
      "title": "Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Qianchi Zhang",
          "affiliation": null
        },
        {
          "name": "Hainan Zhang",
          "affiliation": null
        },
        {
          "name": "Liang Pang",
          "affiliation": null
        },
        {
          "name": "Hongwei Zheng",
          "affiliation": null
        },
        {
          "name": "Zhiming Zheng",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.",
      "publishedDate": "2026-01-06T13:07:38Z",
      "updatedDate": "2026-01-09T14:55:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02993v3",
      "arxivUrl": "https://arxiv.org/abs/2601.02993",
      "comment": "18 pages, 13 figures, 8 tables, under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02989",
      "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
      "authors": [
        {
          "name": "Hosein Hasani",
          "affiliation": null
        },
        {
          "name": "Mohammadali Banayeeanzade",
          "affiliation": null
        },
        {
          "name": "Ali Nafisi",
          "affiliation": null
        },
        {
          "name": "Sadegh Mohammadian",
          "affiliation": null
        },
        {
          "name": "Fatemeh Askari",
          "affiliation": null
        },
        {
          "name": "Mobin Bagherian",
          "affiliation": null
        },
        {
          "name": "Amirmohammad Izadi",
          "affiliation": null
        },
        {
          "name": "Mahdieh Soleymani Baghshah",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.",
      "publishedDate": "2026-01-06T12:58:27Z",
      "updatedDate": "2026-01-06T12:58:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02989v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02989",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02983",
      "title": "Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning",
      "authors": [
        {
          "name": "Yuankun Xie",
          "affiliation": null
        },
        {
          "name": "Xiaoxuan Guo",
          "affiliation": null
        },
        {
          "name": "Jiayi Zhou",
          "affiliation": null
        },
        {
          "name": "Tao Wang",
          "affiliation": null
        },
        {
          "name": "Jian Liu",
          "affiliation": null
        },
        {
          "name": "Ruibo Fu",
          "affiliation": null
        },
        {
          "name": "Xiaopeng Wang",
          "affiliation": null
        },
        {
          "name": "Haonan Cheng",
          "affiliation": null
        },
        {
          "name": "Long Ye",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.",
      "publishedDate": "2026-01-06T12:50:02Z",
      "updatedDate": "2026-01-06T12:50:02Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02983v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02983",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02972",
      "title": "Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning",
      "authors": [
        {
          "name": "Nathanaël Carraz Rakotonirina",
          "affiliation": null
        },
        {
          "name": "Ren Pang",
          "affiliation": null
        },
        {
          "name": "Neha Anna John",
          "affiliation": null
        },
        {
          "name": "Michael Bohlke-Schneider",
          "affiliation": null
        },
        {
          "name": "Momchil Hardalov",
          "affiliation": null
        }
      ],
      "abstract": "The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy--response length trade-off. Our approach reduces response length by an average of 28\\% for 8B models and 40\\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\\text{AUC}_{\\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.",
      "publishedDate": "2026-01-06T12:31:51Z",
      "updatedDate": "2026-01-06T12:31:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02972v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02972",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02968",
      "title": "Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models",
      "authors": [
        {
          "name": "Qingxiang Liu",
          "affiliation": null
        },
        {
          "name": "Zhiqing Cui",
          "affiliation": null
        },
        {
          "name": "Xiaoliang Luo",
          "affiliation": null
        },
        {
          "name": "Yuqian Wu",
          "affiliation": null
        },
        {
          "name": "Zhuoyang Jiang",
          "affiliation": null
        },
        {
          "name": "Huaiyu Wan",
          "affiliation": null
        },
        {
          "name": "Sheng Sun",
          "affiliation": null
        },
        {
          "name": "Lvchun Wang",
          "affiliation": null
        },
        {
          "name": "Wei Yu",
          "affiliation": null
        },
        {
          "name": "Yuxuan Liang",
          "affiliation": null
        }
      ],
      "abstract": "The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.",
      "publishedDate": "2026-01-06T12:27:04Z",
      "updatedDate": "2026-01-06T12:27:04Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02968v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02968",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02954",
      "title": "The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models",
      "authors": [
        {
          "name": "Yuhuan You",
          "affiliation": null
        },
        {
          "name": "Lai Wei",
          "affiliation": null
        },
        {
          "name": "Xihong Wu",
          "affiliation": null
        },
        {
          "name": "Tianshu Qu",
          "affiliation": null
        }
      ],
      "abstract": "Existing large audio-language models perceive the world as \"mono\" -- a single stream of audio that ignores the critical spatial dimension (\"where\") required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model's capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from \"mono\" semantic recognition to spatial intelligence.",
      "publishedDate": "2026-01-06T11:54:47Z",
      "updatedDate": "2026-01-06T11:54:47Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02954v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02954",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02950",
      "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning",
      "authors": [
        {
          "name": "Xuan Yang",
          "affiliation": null
        },
        {
          "name": "Furong Jia",
          "affiliation": null
        },
        {
          "name": "Roy Xie",
          "affiliation": null
        },
        {
          "name": "Xiong Xi",
          "affiliation": null
        },
        {
          "name": "Hengwei Bian",
          "affiliation": null
        },
        {
          "name": "Jian Li",
          "affiliation": null
        },
        {
          "name": "Monica Agrawal",
          "affiliation": null
        }
      ],
      "abstract": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.",
      "publishedDate": "2026-01-06T11:47:45Z",
      "updatedDate": "2026-01-06T11:47:45Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02950v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02950",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02918",
      "title": "Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning",
      "authors": [
        {
          "name": "Guoqiang Liang",
          "affiliation": null
        },
        {
          "name": "Jianyi Wang",
          "affiliation": null
        },
        {
          "name": "Zhonghua Wu",
          "affiliation": null
        },
        {
          "name": "Shangchen Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.",
      "publishedDate": "2026-01-06T11:00:17Z",
      "updatedDate": "2026-01-06T11:00:17Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02918v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02918",
      "comment": "Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02902",
      "title": "Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning",
      "authors": [
        {
          "name": "Xinglang Zhang",
          "affiliation": null
        },
        {
          "name": "Yunyao Zhang",
          "affiliation": null
        },
        {
          "name": "ZeLiang Chen",
          "affiliation": null
        },
        {
          "name": "Junqing Yu",
          "affiliation": null
        },
        {
          "name": "Wei Yang",
          "affiliation": null
        },
        {
          "name": "Zikai Song",
          "affiliation": null
        }
      ],
      "abstract": "Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.",
      "publishedDate": "2026-01-06T10:38:25Z",
      "updatedDate": "2026-01-06T10:38:25Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02902v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02902",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02888",
      "title": "RPIQ: Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization for Visually Impaired Assistance",
      "authors": [
        {
          "name": "Xuanyu Wang",
          "affiliation": null
        },
        {
          "name": "Haisen Su",
          "affiliation": null
        },
        {
          "name": "Jingtao Zhang",
          "affiliation": null
        },
        {
          "name": "Xiangxiang Wang",
          "affiliation": null
        },
        {
          "name": "Yongbin Yu",
          "affiliation": null
        },
        {
          "name": "Manping Fan",
          "affiliation": null
        },
        {
          "name": "Bo Gong",
          "affiliation": null
        },
        {
          "name": "Siqi Chen",
          "affiliation": null
        },
        {
          "name": "Mingsheng Cao",
          "affiliation": null
        },
        {
          "name": "Liyong Ren",
          "affiliation": null
        }
      ],
      "abstract": "Visually impaired users face significant challenges in daily information access and real-time environmental perception, and there is an urgent need for intelligent assistive systems with accurate recognition capabilities. Although large-scale models provide effective solutions for perception and reasoning, their practical deployment on assistive devices is severely constrained by excessive memory consumption and high inference costs. Moreover, existing quantization strategies often ignore inter-block error accumulation, leading to degraded model stability. To address these challenges, this study proposes a novel quantization framework -- Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization(RPIQ), whose quantization process adopts a multi-collaborative closed-loop compensation scheme based on Single Instance Calibration and Gauss-Seidel Iterative Quantization. Experiments on various types of large-scale models, including language models such as OPT, Qwen, and LLaMA, as well as vision-language models such as CogVLM2, demonstrate that RPIQ can compress models to 4-bit representation while significantly reducing peak memory consumption (approximately 60%-75% reduction compared to original full-precision models). The method maintains performance highly close to full-precision models across multiple language and visual tasks, and exhibits excellent recognition and reasoning capabilities in key applications such as text understanding and visual question answering in complex scenarios. While verifying the effectiveness of RPIQ for deployment in real assistive systems, this study also advances the computational efficiency and reliability of large models, enabling them to provide visually impaired users with the required information accurately and rapidly.",
      "publishedDate": "2026-01-06T10:22:34Z",
      "updatedDate": "2026-01-06T10:22:34Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02888v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02888",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "tool-use",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02880",
      "title": "ReTreVal: Reasoning Tree with Validation - A Hybrid Framework for Enhanced LLM Multi-Step Reasoning",
      "authors": [
        {
          "name": "Abhishek HS",
          "affiliation": null
        },
        {
          "name": "Pavan C Shekar",
          "affiliation": null
        },
        {
          "name": "Arpit Jain",
          "affiliation": null
        },
        {
          "name": "Ashwanth Krishnan",
          "affiliation": null
        }
      ],
      "abstract": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.",
      "publishedDate": "2026-01-06T10:05:30Z",
      "updatedDate": "2026-01-06T10:05:30Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02880v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02880",
      "comment": "14 pages, 1 figure, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02872",
      "title": "LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark",
      "authors": [
        {
          "name": "Ziyang Chen",
          "affiliation": null
        },
        {
          "name": "Xing Wu",
          "affiliation": null
        },
        {
          "name": "Junlong Jia",
          "affiliation": null
        },
        {
          "name": "Chaochen Gao",
          "affiliation": null
        },
        {
          "name": "Qi Fu",
          "affiliation": null
        },
        {
          "name": "Debing Zhang",
          "affiliation": null
        },
        {
          "name": "Songlin Hu",
          "affiliation": null
        }
      ],
      "abstract": "The rapid expansion of context length in large language models (LLMs) has outpaced existing evaluation benchmarks. Current long-context benchmarks often trade off scalability and realism: synthetic tasks underrepresent real-world complexity, while fully manual annotation is costly to scale to extreme lengths and diverse scenarios. We present LongBench Pro, a more realistic and comprehensive bilingual benchmark of 1,500 naturally occurring long-context samples in English and Chinese spanning 11 primary tasks and 25 secondary tasks, with input lengths from 8k to 256k tokens. LongBench Pro supports fine-grained analysis with task-specific metrics and a multi-dimensional taxonomy of context requirement (full vs. partial dependency), length (six levels), and difficulty (four levels calibrated by model performance). To balance quality with scalability, we propose a Human-Model Collaborative Construction pipeline: frontier LLMs draft challenging questions and reference answers, along with design rationales and solution processes, to reduce the cost of expert verification. Experts then rigorously validate correctness and refine problematic cases. Evaluating 46 widely used long-context LLMs on LongBench Pro yields three findings: (1) long-context optimization contributes more to long-context comprehension than parameter scaling; (2) effective context length is typically shorter than the claimed context length, with pronounced cross-lingual misalignment; and (3) the \"thinking\" paradigm helps primarily models trained with native reasoning, while mixed-thinking designs offer a promising Pareto trade-off. In summary, LongBench Pro provides a robust testbed for advancing long-context understanding.",
      "publishedDate": "2026-01-06T10:01:59Z",
      "updatedDate": "2026-01-06T10:01:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02872v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02872",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02739",
      "title": "Mitigating Prompt-Induced Hallucinations in Large Language Models via Structured Reasoning",
      "authors": [
        {
          "name": "Jinbo Hao",
          "affiliation": null
        },
        {
          "name": "Kai Yang",
          "affiliation": null
        },
        {
          "name": "Qingzhen Su",
          "affiliation": null
        },
        {
          "name": "Yang Chen",
          "affiliation": null
        },
        {
          "name": "Yifan Li",
          "affiliation": null
        },
        {
          "name": "Chao Jiang",
          "affiliation": null
        }
      ],
      "abstract": "To address hallucination issues in large language models (LLMs), this paper proposes a method for mitigating prompt-induced hallucinations. Building on a knowledge distillation chain-style model, we introduce a code module to guide knowledge-graph exploration and incorporate code as part of the chain-of-thought prompt, forming an external knowledge input that provides more accurate and structured information to the model. Based on this design, we develop an improved knowledge distillation chain-style model and leverage it to analyze and constrain the reasoning process of LLMs, thereby improving inference accuracy. We empirically evaluate the proposed approach using GPT-4 and LLaMA-3.3 on multiple public datasets. Experimental results demonstrate that incorporating code modules significantly enhances the model's ability to capture contextual information and effectively mitigates prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 improve by 15.64%, 13.38%, and 13.28%, respectively. Moreover, the proposed method achieves HIT@1, HIT@3, and HIT@5 scores exceeding 95% across several evaluation settings. These results indicate that the proposed approach substantially reduces hallucination behavior while improving the accuracy and verifiability of large language models.",
      "publishedDate": "2026-01-06T06:02:45Z",
      "updatedDate": "2026-01-06T06:02:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02739v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02739",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02737",
      "title": "Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench",
      "authors": [
        {
          "name": "Zanting Ye",
          "affiliation": null
        },
        {
          "name": "Xiaolong Niu",
          "affiliation": null
        },
        {
          "name": "Xuanbin Wu",
          "affiliation": null
        },
        {
          "name": "Xu Han",
          "affiliation": null
        },
        {
          "name": "Shengyuan Liu",
          "affiliation": null
        },
        {
          "name": "Jing Hao",
          "affiliation": null
        },
        {
          "name": "Zhihao Peng",
          "affiliation": null
        },
        {
          "name": "Hao Sun",
          "affiliation": null
        },
        {
          "name": "Jieqin Lv",
          "affiliation": null
        },
        {
          "name": "Fanghu Wang",
          "affiliation": null
        },
        {
          "name": "Yanchao Huang",
          "affiliation": null
        },
        {
          "name": "Hubing Wu",
          "affiliation": null
        },
        {
          "name": "Yixuan Yuan",
          "affiliation": null
        },
        {
          "name": "Habib Zaidi",
          "affiliation": null
        },
        {
          "name": "Arman Rahmim",
          "affiliation": null
        },
        {
          "name": "Yefeng Zheng",
          "affiliation": null
        },
        {
          "name": "Lijun Lu",
          "affiliation": null
        }
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.",
      "publishedDate": "2026-01-06T05:58:50Z",
      "updatedDate": "2026-01-06T05:58:50Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02737v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02737",
      "comment": "9 pages, 6 figures, 6 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02714",
      "title": "Time-Scaling Is What Agents Need Now",
      "authors": [
        {
          "name": "Zhi Liu",
          "affiliation": null
        },
        {
          "name": "Guangzhi Wang",
          "affiliation": null
        }
      ],
      "abstract": "Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on \"perception-representation,\" Reinforcement Learning on \"decision-making-behavior,\" and Symbolic AI on \"knowledge-reasoning.\" With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop \"perception-decision-action\" capabilities. Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency. This highlights the need for \"Time-Scaling\"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.",
      "publishedDate": "2026-01-06T05:01:17Z",
      "updatedDate": "2026-01-06T05:01:17Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02714v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02714",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "prompting",
        "agents"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02663",
      "title": "When Do Tools and Planning Help LLMs Think? A Cost- and Latency-Aware Benchmark",
      "authors": [
        {
          "name": "Subha Ghoshal",
          "affiliation": null
        },
        {
          "name": "Ali Al-Bustami",
          "affiliation": null
        }
      ],
      "abstract": "Modern large language models (LLMs) increasingly rely on inference-time planning and external tools to improve reasoning. We benchmark this behavior on two real-world settings: event-centric question answering over graph-structured knowledge (Event-QA) and persuasive response generation in Reddit ChangeMyView (CMV). Using LangChain and LangGraph, we compare a one-shot baseline against a plan--execute--replan agent equipped with task-specific tools (DBpedia SPARQL/lookup/schema exploration, Wikipedia-focused retrieval, and topical web search). We evaluate on 60 examples each from Event-QA and CMV (3 splits of 20), and report both mean end-to-end latency and per-example token cost estimates. We evaluate GPT-4o and GPT-4o-mini under identical workflows and report accuracy and end-to-end latency. On Event-QA, the best tool-augmented configuration improves accuracy (e.g., 47.5\\% $\\rightarrow$ 67.5\\% for GPT-4o) while increasing latency by orders of magnitude ($\\sim$8s $\\rightarrow$ $\\sim$317s per example). On CMV, one-shot prompting is strongest (e.g., GPT-4o-mini achieves 75\\% at $\\sim$6s), and planning+search increases latency substantially without consistent gains. However, complex multi-tool orchestration exposes failure modes where the smaller model degrades. Overall, the findings highlight the need for task-specific, cost-aware choices of both model size and agent/tooling complexity.",
      "publishedDate": "2026-01-06T02:24:29Z",
      "updatedDate": "2026-01-06T02:24:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02663v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02663",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02604",
      "title": "Scalable Construction of a Lung Cancer Knowledge Base: Profiling Semantic Reasoning in LLMs",
      "authors": [
        {
          "name": "Cesar Felipe Martínez Cisneros",
          "affiliation": null
        },
        {
          "name": "Jesús Ulises Quiroz Bautista",
          "affiliation": null
        },
        {
          "name": "Claudia Anahí Guzmán Solano",
          "affiliation": null
        },
        {
          "name": "Bogdan Kaleb García Rivera",
          "affiliation": null
        },
        {
          "name": "Iván García Pacheco",
          "affiliation": null
        },
        {
          "name": "Yalbi Itzel Balderas Martínez",
          "affiliation": null
        },
        {
          "name": "Kolawole John Adebayoc",
          "affiliation": null
        },
        {
          "name": "Ignacio Arroyo Fernández",
          "affiliation": null
        }
      ],
      "abstract": "The integration of Large Language Models (LLMs) into biomedical research offers new opportunities for domainspecific reasoning and knowledge representation. However, their performance depends heavily on the semantic quality of training data. In oncology, where precision and interpretability are vital, scalable methods for constructing structured knowledge bases are essential for effective fine-tuning. This study presents a pipeline for developing a lung cancer knowledge base using Open Information Extraction (OpenIE). The process includes: (1) identifying medical concepts with the MeSH thesaurus; (2) filtering open-access PubMed literature with permissive licenses (CC0); (3) extracting (subject, relation, object) triplets using OpenIE method; and (4) enriching triplet sets with Named Entity Recognition (NER) to ensure biomedical relevance. The resulting triplet sets provide a domain-specific, large-scale, and noise-aware resource for fine-tuning LLMs. We evaluated T5 models finetuned on this dataset through Supervised Semantic Fine-Tuning. Comparative assessments with ROUGE and BERTScore show significantly improved performance and semantic coherence, demonstrating the potential of OpenIE-derived resources as scalable, low-cost solutions for enhancing biomedical NLP.",
      "publishedDate": "2026-01-05T23:40:00Z",
      "updatedDate": "2026-01-05T23:40:00Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02604v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02604",
      "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
      "journalRef": null,
      "doi": "10.1109/ENC68268.2025.11311861",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02569",
      "title": "LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference",
      "authors": [
        {
          "name": "Hossein Rajabzadeh",
          "affiliation": null
        },
        {
          "name": "Maryam Dialameh",
          "affiliation": null
        },
        {
          "name": "Chul B. Park",
          "affiliation": null
        },
        {
          "name": "Il-Min Kim",
          "affiliation": null
        },
        {
          "name": "Hyock Ju Kwon",
          "affiliation": null
        }
      ],
      "abstract": "Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \\textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \\emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \\emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \\textbf{LLaMA2-7B}, \\textbf{LLaMA3-8B}, \\textbf{Qwen2.5-7B}, and \\textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \\textbf{2.6$\\times$ faster decoding} and \\textbf{45--55\\% KV-cache reduction} while staying within \\textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \\emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.",
      "publishedDate": "2026-01-05T21:47:47Z",
      "updatedDate": "2026-01-05T21:47:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02569v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02569",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02559",
      "title": "PerspectiveCoach: Exploring LLMs for Developer Reflection",
      "authors": [
        {
          "name": "Lauren Olson",
          "affiliation": null
        },
        {
          "name": "Emitzá Guzmán",
          "affiliation": null
        },
        {
          "name": "Florian Kunneman",
          "affiliation": null
        }
      ],
      "abstract": "Despite growing awareness of ethical challenges in software development, practitioners still lack structured tools that help them critically engage with the lived experiences of marginalized users. This paper presents PerspectiveCoach, a large language model (LLM)-powered conversational tool designed to guide developers through structured perspective-taking exercises and deepen critical reflection on how software design decisions affect marginalized communities. Through a controlled study with 18 front-end developers (balanced by sex), who interacted with the tool using a real case of online gender-based harassment, we examine how PerspectiveCoach supports ethical reasoning and engagement with user perspectives. Qualitative analysis revealed increased self-awareness, broadened perspectives, and more nuanced ethical articulation, while a complementary human-human study contextualized these findings. Text similarity analyses demonstrated that participants in the human-PerspectiveCoach study improved the fidelity of their restatements over multiple attempts, capturing both surface-level and semantic aspects of user concerns. However, human-PerspectiveCoach's restatements had a lower baseline than the human-human conversations, highlighting contextual differences in impersonal and interpersonal perspective-taking. Across the study, participants rated the tool highly for usability and relevance. This work contributes an exploratory design for LLM-powered end-user perspective-taking that supports critical, ethical self-reflection and offers empirical insights (i.e., enhancing adaptivity, centering plurality) into how such tools can help practitioners build more inclusive and socially responsive technologies.",
      "publishedDate": "2026-01-05T21:21:55Z",
      "updatedDate": "2026-01-05T21:21:55Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02559v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02559",
      "comment": "48th International Conference of Software Engineering",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02457",
      "title": "PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding",
      "authors": [
        {
          "name": "Souhail Hadgi",
          "affiliation": null
        },
        {
          "name": "Bingchen Gong",
          "affiliation": null
        },
        {
          "name": "Ramana Sundararaman",
          "affiliation": null
        },
        {
          "name": "Emery Pierson",
          "affiliation": null
        },
        {
          "name": "Lei Li",
          "affiliation": null
        },
        {
          "name": "Peter Wonka",
          "affiliation": null
        },
        {
          "name": "Maks Ovsjanikov",
          "affiliation": null
        }
      ],
      "abstract": "Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/",
      "publishedDate": "2026-01-05T18:55:45Z",
      "updatedDate": "2026-01-05T18:55:45Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02457v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02457",
      "comment": "Project website: https://souhail-hadgi.github.io/patchalign3dsite/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "rag",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02433",
      "title": "Physical Transformer",
      "authors": [
        {
          "name": "Tao Xu",
          "affiliation": null
        },
        {
          "name": "Zhixin Hu",
          "affiliation": null
        },
        {
          "name": "Li Luo",
          "affiliation": null
        },
        {
          "name": "Momiao Xiong",
          "affiliation": null
        }
      ],
      "abstract": "Digital AI systems spanning large language models, vision models, and generative architectures that operate primarily in symbolic, linguistic, or pixel domains. They have achieved striking progress, but almost all of this progress lives in virtual spaces. These systems transform embeddings and tokens, yet do not themselves touch the world and rarely admit a physical interpretation. In this work we propose a physical transformer that couples modern transformer style computation with geometric representation and physical dynamics. At the micro level, attention heads, and feed-forward blocks are modeled as interacting spins governed by effective Hamiltonians plus non-Hamiltonian bath terms. At the meso level, their aggregated state evolves on a learned Neural Differential Manifold (NDM) under Hamiltonian flows and Hamilton, Jacobi, Bellman (HJB) optimal control, discretized by symplectic layers that approximately preserve geometric and energetic invariants. At the macro level, the model maintains a generative semantic workspace and a two-dimensional information-phase portrait that tracks uncertainty and information gain over a reasoning trajectory. Within this hierarchy, reasoning tasks are formulated as controlled information flows on the manifold, with solutions corresponding to low cost trajectories that satisfy geometric, energetic, and workspace-consistency constraints. On simple toy problems involving numerical integration and dynamical systems, the physical transformer outperforms naive baselines in stability and long-horizon accuracy, highlighting the benefits of respecting underlying geometric and Hamiltonian structure. More broadly, the framework suggests a path toward physical AI that unify digital reasoning with physically grounded manifolds, opening a route to more interpretable and potentially unified models of reasoning, control, and interaction with the real world.",
      "publishedDate": "2026-01-05T06:29:39Z",
      "updatedDate": "2026-01-05T06:29:39Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02433v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02433",
      "comment": "38 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03178",
      "title": "DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation",
      "authors": [
        {
          "name": "Jiajun jiao",
          "affiliation": null
        },
        {
          "name": "Haowei Zhu",
          "affiliation": null
        },
        {
          "name": "Puyuan Yang",
          "affiliation": null
        },
        {
          "name": "Jianghui Wang",
          "affiliation": null
        },
        {
          "name": "Ji Liu",
          "affiliation": null
        },
        {
          "name": "Ziqiong Liu",
          "affiliation": null
        },
        {
          "name": "Dong Li",
          "affiliation": null
        },
        {
          "name": "Yuejian Fang",
          "affiliation": null
        },
        {
          "name": "Junhai Yong",
          "affiliation": null
        },
        {
          "name": "Bin Wang",
          "affiliation": null
        },
        {
          "name": "Emad Barsoum",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.",
      "publishedDate": "2026-01-06T16:55:55Z",
      "updatedDate": "2026-01-06T16:55:55Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03178v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03178",
      "comment": "Accepted to AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "planning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03061",
      "title": "Vertical tacit collusion in AI-mediated markets",
      "authors": [
        {
          "name": "Felipe M. Affonso",
          "affiliation": null
        }
      ],
      "abstract": "AI shopping agents are being deployed to hundreds of millions of consumers, creating a new intermediary between platforms, sellers, and buyers. We identify a novel market failure: vertical tacit collusion, where platforms controlling rankings and sellers controlling product descriptions independently learn to exploit documented AI cognitive biases. Using multi-agent simulation calibrated to empirical measurements of large language model biases, we show that joint exploitation produces consumer harm more than double what would occur if strategies were independent. This super-additive harm arises because platform ranking determines which products occupy bias-triggering positions while seller manipulation determines conversion rates. Unlike horizontal algorithmic collusion, vertical tacit collusion requires no coordination and evades antitrust detection because harm emerges from aligned incentives rather than agreement. Our findings identify an urgent regulatory gap as AI shopping agents reach mainstream adoption.",
      "publishedDate": "2026-01-06T14:43:14Z",
      "updatedDate": "2026-01-06T14:43:14Z",
      "primaryCategory": "cs.CY",
      "arxivCategories": [
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03061v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03061",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "multi-agent",
        "agents"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03011",
      "title": "ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios",
      "authors": [
        {
          "name": "Yihan Wei",
          "affiliation": null
        },
        {
          "name": "Shenghai Yuan",
          "affiliation": null
        },
        {
          "name": "Tianchen Deng",
          "affiliation": null
        },
        {
          "name": "Boyang Lou",
          "affiliation": null
        },
        {
          "name": "Enwen Hu",
          "affiliation": null
        }
      ],
      "abstract": "Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-scale data acquisition and filtering expands a domain vocabulary with a vision-language model (VLM), crawls the web, and enforces tri-modal (image, description, keyword) consistency with light human spot checks to yield refined candidates. Next, mixture-of-experts knowledge distillation uses complementary encoders (e.g., CLIP, DINOv2, BEiT) for kNN voting with dual-confidence activation and uncertainty sampling, converging to a high-precision set. Finally, region-evidence VLM adversarial labeling pairs a proposer (multi-granularity regions and semantic cues) with a validator (global and local chained consistency) to produce explainable labels and close the loop. On realistic corner-case scenarios (e.g., flooded-car inspection), ReCCur runs on consumer-grade GPUs, steadily improves purity and separability, and requires minimal human supervision, providing a practical substrate for downstream training and evaluation under resource constraints. Code and dataset will be released.",
      "publishedDate": "2026-01-06T13:36:43Z",
      "updatedDate": "2026-01-06T13:36:43Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03011v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03011",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02407",
      "title": "Evolving Personalities in Chaos: An LLM-Augmented Framework for Character Discovery in the Iterated Prisoners Dilemma under Environmental Stress",
      "authors": [
        {
          "name": "Oguzhan Yildirim",
          "affiliation": null
        }
      ],
      "abstract": "Standard simulations of the Iterated Prisoners Dilemma (IPD) operate in deterministic, noise-free environments, producing strategies that may be theoretically optimal but fragile when confronted with real-world uncertainty. This paper addresses two critical gaps in evolutionary game theory research: (1) the absence of realistic environmental stressors during strategy evolution, and (2) the Interpretability Gap, where evolved genetic strategies remain opaque binary sequences devoid of semantic meaning. We introduce a novel framework combining stochastic environmental perturbations (God Mode) with Large Language Model (LLM)-based behavioral profiling to transform evolved genotypes into interpretable character archetypes. Our experiments demonstrate that strategies evolved under chaotic conditions exhibit superior resilience and present distinct behavioral phenotypes, ranging from Ruthless Capitalists to Diplomatic Enforcers. These phenotypes are readily classified by LLMs but remain nearly impossible to interpret through manual genome inspection alone. This work bridges evolutionary computation with explainable AI and provides a template for automated agent characterization in multi-agent systems.",
      "publishedDate": "2026-01-01T18:34:05Z",
      "updatedDate": "2026-01-01T18:34:05Z",
      "primaryCategory": "cs.NE",
      "arxivCategories": [
        "cs.NE",
        "cs.GT"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02407v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02407",
      "comment": "10 pages, 5 figures. Project assignment; exploratory study on LLM-based adaptive agents",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02957",
      "title": "LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation",
      "authors": [
        {
          "name": "Fabian Lukassen",
          "affiliation": null
        },
        {
          "name": "Christoph Weisser",
          "affiliation": null
        },
        {
          "name": "Michael Schlee",
          "affiliation": null
        },
        {
          "name": "Manish Kumar",
          "affiliation": null
        },
        {
          "name": "Anton Thielmann",
          "affiliation": null
        },
        {
          "name": "Benjamin Saefken",
          "affiliation": null
        },
        {
          "name": "Thomas Kneib",
          "affiliation": null
        }
      ],
      "abstract": "This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.",
      "publishedDate": "2026-01-06T12:04:38Z",
      "updatedDate": "2026-01-06T12:04:38Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02957v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02957",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02956",
      "title": "Enhancing Multilingual RAG Systems with Debiased Language Preference-Guided Query Fusion",
      "authors": [
        {
          "name": "Jeonghyun Park",
          "affiliation": null
        },
        {
          "name": "Byeongjeong Kim",
          "affiliation": null
        },
        {
          "name": "Seojin Hwang",
          "affiliation": null
        },
        {
          "name": "Hwanhee Lee",
          "affiliation": null
        }
      ],
      "abstract": "Multilingual Retrieval-Augmented Generation (mRAG) systems often exhibit a perceived preference for high-resource languages, particularly English, resulting in the widespread adoption of English pivoting. While prior studies attribute this advantage to the superior English-centric capabilities of Large Language Models (LLMs), we find that such measurements are significantly distorted by structural priors inherent in evaluation benchmarks. Specifically, we identify exposure bias and a gold availability prior-both driven by the disproportionate concentration of resources in English-as well as cultural priors rooted in topic locality, as factors that hinder accurate assessment of genuine language preference. To address these biases, we propose DeLP (Debiased Language Preference), a calibrated metric designed to explicitly factor out these structural confounds. Our analysis using DeLP reveals that the previously reported English preference is largely a byproduct of evidence distribution rather than an inherent model bias. Instead, we find that retrievers fundamentally favor monolingual alignment between the query and the document language. Building on this insight, we introduce DELTA (DEbiased Language preference-guided Text Augmentation), a lightweight and efficient mRAG framework that strategically leverages monolingual alignment to optimize cross-lingual retrieval and generation. Experimental results demonstrate that DELTA consistently outperforms English pivoting and mRAG baselines across diverse languages.",
      "publishedDate": "2026-01-06T12:01:56Z",
      "updatedDate": "2026-01-06T12:01:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02956v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02956",
      "comment": "20 pages, 5 figures, 15 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02814",
      "title": "Causal-Enhanced AI Agents for Medical Research Screening",
      "authors": [
        {
          "name": "Duc Ngo",
          "affiliation": null
        },
        {
          "name": "Arya Rahgoza",
          "affiliation": null
        }
      ],
      "abstract": "Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care. We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways. Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.",
      "publishedDate": "2026-01-06T08:41:16Z",
      "updatedDate": "2026-01-06T08:41:16Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02814v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02814",
      "comment": "for submission to The 39th Canadian Conference on Artificial Intelligence",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02522",
      "title": "On the Effectiveness of Proposed Techniques to Reduce Energy Consumption in RAG Systems: A Controlled Experiment",
      "authors": [
        {
          "name": "Zhinuan",
          "affiliation": null
        },
        {
          "name": "Guo",
          "affiliation": null
        },
        {
          "name": "Chushu Gao",
          "affiliation": null
        },
        {
          "name": "Justus Bogner",
          "affiliation": null
        }
      ],
      "abstract": "The rising energy demands of machine learning (ML), e.g., implemented in popular variants like retrieval-augmented generation (RAG) systems, have raised significant concerns about their environmental sustainability. While previous research has proposed green tactics for ML-enabled systems, their empirical evaluation within RAG systems remains largely unexplored. This study presents a controlled experiment investigating five practical techniques aimed at reducing energy consumption in RAG systems. Using a production-like RAG system developed at our collaboration partner, the Software Improvement Group, we evaluated the impact of these techniques on energy consumption, latency, and accuracy. Through a total of 9 configurations spanning over 200 hours of trials using the CRAG dataset, we reveal that techniques such as increasing similarity retrieval thresholds, reducing embedding sizes, applying vector indexing, and using a BM25S reranker can significantly reduce energy usage, up to 60% in some cases. However, several techniques also led to unacceptable accuracy decreases, e.g., by up to 30% for the indexing strategies. Notably, finding an optimal retrieval threshold and reducing embedding size substantially reduced energy consumption and latency with no loss in accuracy, making these two techniques truly energy-efficient. We present the first comprehensive, empirical study on energy-efficient design techniques for RAG systems, providing guidance for developers and researchers aiming to build sustainable RAG applications.",
      "publishedDate": "2026-01-05T19:50:49Z",
      "updatedDate": "2026-01-05T19:50:49Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02522v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02522",
      "comment": "Accepted for publication at the 2026 International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS'26)",
      "journalRef": null,
      "doi": "10.1145/3786581.3786932",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02504",
      "title": "Enhancing Debugging Skills with AI-Powered Assistance: A Real-Time Tool for Debugging Support",
      "authors": [
        {
          "name": "Elizaveta Artser",
          "affiliation": null
        },
        {
          "name": "Daniil Karol",
          "affiliation": null
        },
        {
          "name": "Anna Potriasaeva",
          "affiliation": null
        },
        {
          "name": "Aleksei Rostovskii",
          "affiliation": null
        },
        {
          "name": "Katsiaryna Dzialets",
          "affiliation": null
        },
        {
          "name": "Ekaterina Koshchenko",
          "affiliation": null
        },
        {
          "name": "Xiaotian Su",
          "affiliation": null
        },
        {
          "name": "April Yi Wang",
          "affiliation": null
        },
        {
          "name": "Anastasiia Birillo",
          "affiliation": null
        }
      ],
      "abstract": "Debugging is a crucial skill in programming education and software development, yet it is often overlooked in CS curricula. To address this, we introduce an AI-powered debugging assistant integrated into an IDE. It offers real-time support by analyzing code, suggesting breakpoints, and providing contextual hints. Using RAG with LLMs, program slicing, and custom heuristics, it enhances efficiency by minimizing LLM calls and improving accuracy. A three-level evaluation - technical analysis, UX study, and classroom tests - highlights its potential for teaching debugging.",
      "publishedDate": "2026-01-05T19:20:59Z",
      "updatedDate": "2026-01-05T19:20:59Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02504v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02504",
      "comment": "Accepted at ICSE SEET 2026, 6 pages, 2 figures",
      "journalRef": null,
      "doi": "10.1145/3786580.3786976",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02428",
      "title": "A Dynamic Retrieval-Augmented Generation System with Selective Memory and Remembrance",
      "authors": [
        {
          "name": "Okan Bursa",
          "affiliation": null
        }
      ],
      "abstract": "We introduce \\emph{Adaptive RAG Memory} (ARM), a retrieval-augmented generation (RAG) framework that replaces a static vector index with a \\emph{dynamic} memory substrate governed by selective remembrance and decay. Frequently retrieved items are consolidated and protected from forgetting, while rarely used items gradually decay, inspired by cognitive consolidation and forgetting principles. On a lightweight retrieval benchmark, ARM reaches near state-of-the-art performance (e.g., NDCG@5 $\\approx$ 0.940, Recall@5 $=1.000$) with only $\\sim$22M parameters in the embedding layer, achieving the best efficiency among ultra-efficient models ($<$25M parameters). In addition, we compare static vs. dynamic RAG combinations across Llama 3.1 and GPT-4o. Llama 3.1 with static RAG achieves the highest key-term coverage (67.2\\%) at moderate latency, while GPT-4o with a dynamic selective retrieval policy attains the fastest responses (8.2s on average) with competitive coverage (58.7\\%). We further present an engineering optimization of the DynamicRAG implementation, making embedding weights configurable, adjustable at runtime, and robust to invalid settings. ARM yields competitive accuracy, self-regularizing memory growth, and interpretable retention dynamics without retraining the generator\\color{black} and provides practical trade-off between quality, latency and memory efficiency for production and research RAG system.",
      "publishedDate": "2026-01-04T21:51:41Z",
      "updatedDate": "2026-01-04T21:51:41Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02428v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02428",
      "comment": "6 Pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03170",
      "title": "Segment-Aware Conditioning for Training-Free Intra-Utterance Emotion and Duration Control in Text-to-Speech",
      "authors": [
        {
          "name": "Qifan Liang",
          "affiliation": null
        },
        {
          "name": "Yuansen Liu",
          "affiliation": null
        },
        {
          "name": "Ruixin Wei",
          "affiliation": null
        },
        {
          "name": "Nan Lu",
          "affiliation": null
        },
        {
          "name": "Junchuan Zhao",
          "affiliation": null
        },
        {
          "name": "Ye Wang",
          "affiliation": null
        }
      ],
      "abstract": "While controllable Text-to-Speech (TTS) has achieved notable progress, most existing methods remain limited to inter-utterance-level control, making fine-grained intra-utterance expression challenging due to their reliance on non-public datasets or complex multi-stage training. In this paper, we propose a training-free controllable framework for pretrained zero-shot TTS to enable intra-utterance emotion and duration expression. Specifically, we propose a segment-aware emotion conditioning strategy that combines causal masking with monotonic stream alignment filtering to isolate emotion conditioning and schedule mask transitions, enabling smooth intra-utterance emotion shifts while preserving global semantic coherence. Based on this, we further propose a segment-aware duration steering strategy to combine local duration embedding steering with global EOS logit modulation, allowing local duration adjustment while ensuring globally consistent termination. To eliminate the need for segment-level manual prompt engineering, we construct a 30,000-sample multi-emotion and duration-annotated text dataset to enable LLM-based automatic prompt construction. Extensive experiments demonstrate that our training-free method not only achieves state-of-the-art intra-utterance consistency in multi-emotion and duration control, but also maintains baseline-level speech quality of the underlying TTS model. Audio samples are available at https://aclanonymous111.github.io/TED-TTS-DemoPage/.",
      "publishedDate": "2026-01-06T16:51:04Z",
      "updatedDate": "2026-01-06T16:51:04Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03170v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03170",
      "comment": "24 pages, 8 figures, 7 tables, 3 lists",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03156",
      "title": "Prompt-Counterfactual Explanations for Generative AI System Behavior",
      "authors": [
        {
          "name": "Sofie Goethals",
          "affiliation": null
        },
        {
          "name": "Foster Provost",
          "affiliation": null
        },
        {
          "name": "João Sedoc",
          "affiliation": null
        }
      ],
      "abstract": "As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -the prompt- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.",
      "publishedDate": "2026-01-06T16:33:19Z",
      "updatedDate": "2026-01-06T16:33:19Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03156v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03156",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03130",
      "title": "Automatic Prompt Engineering with No Task Cues and No Tuning",
      "authors": [
        {
          "name": "Faisal Chowdhury",
          "affiliation": null
        },
        {
          "name": "Nandana Mihindukulasooriya",
          "affiliation": null
        },
        {
          "name": "Niharika S D'Souza",
          "affiliation": null
        },
        {
          "name": "Horst Samulowitz",
          "affiliation": null
        },
        {
          "name": "Neeru Gupta",
          "affiliation": null
        },
        {
          "name": "Tomasz Hanusiak",
          "affiliation": null
        },
        {
          "name": "Michal Kapitonow",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.",
      "publishedDate": "2026-01-06T16:04:45Z",
      "updatedDate": "2026-01-06T16:04:45Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03130v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03130",
      "comment": null,
      "journalRef": "The IEEE International Conference on Data Mining (ICDM) 2025 : Demo Track",
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02896",
      "title": "Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control",
      "authors": [
        {
          "name": "Harshvardhan Saini",
          "affiliation": null
        },
        {
          "name": "Yiming Tang",
          "affiliation": null
        },
        {
          "name": "Dianbo Liu",
          "affiliation": null
        }
      ],
      "abstract": "Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as \"black boxes\" with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt discovery. In specific, we propose two methods, RESGA and SAEGA, that both optimize randomly initialized prompts to achieve better aligned representation with an identified persona direction. We introduce fluent gradient ascent to control the fluency of discovered persona steering prompts. We demonstrate RESGA and SAEGA's effectiveness across Llama 3.1, Qwen 2.5, and Gemma 3 for steering three different personas,sycophancy, hallucination, and myopic reward. Crucially, on sycophancy, our automatically discovered prompts achieve significant improvement (49.90% compared with 79.24%). By grounding prompt discovery in mechanistically meaningful features, our method offers a new paradigm for controllable and interpretable behavior modification.",
      "publishedDate": "2026-01-06T10:34:14Z",
      "updatedDate": "2026-01-06T10:34:14Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02896v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02896",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02683",
      "title": "Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization",
      "authors": [
        {
          "name": "Dongyu Chen",
          "affiliation": null
        },
        {
          "name": "Jian Ma",
          "affiliation": null
        },
        {
          "name": "Xianpeng Zhang",
          "affiliation": null
        },
        {
          "name": "Lei Zhang",
          "affiliation": null
        },
        {
          "name": "Haonan Lu",
          "affiliation": null
        },
        {
          "name": "Chen Chen",
          "affiliation": null
        },
        {
          "name": "Chuangchuang Wang",
          "affiliation": null
        },
        {
          "name": "Kai Tang",
          "affiliation": null
        }
      ],
      "abstract": "Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.",
      "publishedDate": "2026-01-06T03:34:17Z",
      "updatedDate": "2026-01-06T03:34:17Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02683v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02683",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02662",
      "title": "When Prompting Meets Spiking: Graph Sparse Prompting via Spiking Graph Prompt Learning",
      "authors": [
        {
          "name": "Bo Jiang",
          "affiliation": null
        },
        {
          "name": "Weijun Zhao",
          "affiliation": null
        },
        {
          "name": "Beibei Wang",
          "affiliation": null
        },
        {
          "name": "Jin Tang",
          "affiliation": null
        }
      ],
      "abstract": "Graph Prompt Feature (GPF) learning has been widely used in adapting pre-trained GNN model on the downstream task. GPFs first introduce some prompt atoms and then learns the optimal prompt vector for each graph node using the linear combination of prompt atoms. However, existing GPFs generally conduct prompting over node's all feature dimensions which is obviously redundant and also be sensitive to node feature noise. To overcome this issue, for the first time, this paper proposes learning sparse graph prompts by leveraging the spiking neuron mechanism, termed Spiking Graph Prompt Feature (SpikingGPF). Our approach is motivated by the observation that spiking neuron can perform inexpensive information processing and produce sparse outputs which naturally fits the task of our graph sparse prompting. Specifically, SpikingGPF has two main aspects. First, it learns a sparse prompt vector for each node by exploiting a spiking neuron architecture, enabling prompting on selective node features. This yields a more compact and lightweight prompting design while also improving robustness against node noise. Second, SpikingGPF introduces a novel prompt representation learning model based on sparse representation theory, i.e., it represents each node prompt as a sparse combination of prompt atoms. This encourages a more compact representation and also facilitates efficient computation. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of SpikingGPF.",
      "publishedDate": "2026-01-06T02:22:04Z",
      "updatedDate": "2026-01-06T02:22:04Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02662v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02662",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03149",
      "title": "PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback",
      "authors": [
        {
          "name": "Dehao Yuan",
          "affiliation": null
        },
        {
          "name": "Tyler Farnan",
          "affiliation": null
        },
        {
          "name": "Stefan Tesliuc",
          "affiliation": null
        },
        {
          "name": "Doron L Bergman",
          "affiliation": null
        },
        {
          "name": "Yulun Wu",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Liu",
          "affiliation": null
        },
        {
          "name": "Minghui Liu",
          "affiliation": null
        },
        {
          "name": "James Montgomery",
          "affiliation": null
        },
        {
          "name": "Nam H Nguyen",
          "affiliation": null
        },
        {
          "name": "C. Bayan Bruss",
          "affiliation": null
        },
        {
          "name": "Furong Huang",
          "affiliation": null
        }
      ],
      "abstract": "Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware \"nextprompt\" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.",
      "publishedDate": "2026-01-06T16:18:59Z",
      "updatedDate": "2026-01-06T16:18:59Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03149v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03149",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03067",
      "title": "Joint Encoding of KV-Cache Blocks for Scalable LLM Serving",
      "authors": [
        {
          "name": "Joseph Kampeas",
          "affiliation": null
        },
        {
          "name": "Emir Haleva",
          "affiliation": null
        }
      ],
      "abstract": "Modern large language models (LLMs) drive interactive AI systems but are bottlenecked by the memory-heavy growth of key-value (KV) caches, which limits real-time throughput under concurrent loads. Existing KV-cache compression methods rely on rigid heuristics, disrupt tensor layouts, or require specialized compute, hindering scalability and deployment. We propose joint encoding of KV-cache blocks, which fuses similar blocks across requests and input chunks into shared representations while preserving standard cache structure. This alleviates the KV-cache memory bottleneck, supporting high-concurrency serving without specialized hardware. Theoretically, we analyze the rate-distortion tradeoff of fused cache blocks under a Poisson process model. Empirically, our method achieves up to 4.38 $\\times$ KV-cache compression with negligible accuracy loss across diverse LLMs and benchmarks, outperforming recent structured and adaptive compression baselines. In real LLM serving, joint encoding improves the token throughput by $\\sim$40\\% on a single-machine vLLM benchmark, demonstrating substantial gains in inference throughput. Code is available at https://github.com/sef1/kv_fast_fusion kv_joint_encoding.",
      "publishedDate": "2026-01-06T14:50:58Z",
      "updatedDate": "2026-01-06T14:50:58Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03067v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03067",
      "comment": "12 pages, 16 figures, 2 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02997",
      "title": "From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures",
      "authors": [
        {
          "name": "Waleed Khalid",
          "affiliation": null
        },
        {
          "name": "Dmitry Ignatov",
          "affiliation": null
        },
        {
          "name": "Radu Timofte",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.",
      "publishedDate": "2026-01-06T13:20:28Z",
      "updatedDate": "2026-01-06T13:20:28Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02997v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02997",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "agents",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02967",
      "title": "MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free",
      "authors": [
        {
          "name": "Yishu Lei",
          "affiliation": null
        },
        {
          "name": "Shuwei He",
          "affiliation": null
        },
        {
          "name": "Jing Hu",
          "affiliation": null
        },
        {
          "name": "Dan Zhang",
          "affiliation": null
        },
        {
          "name": "Xianlong Luo",
          "affiliation": null
        },
        {
          "name": "Danxiang Zhu",
          "affiliation": null
        },
        {
          "name": "Shikun Feng",
          "affiliation": null
        },
        {
          "name": "Rui Liu",
          "affiliation": null
        },
        {
          "name": "Jingzhou He",
          "affiliation": null
        },
        {
          "name": "Yu Sun",
          "affiliation": null
        },
        {
          "name": "Hua Wu",
          "affiliation": null
        },
        {
          "name": "Haifeng Wang",
          "affiliation": null
        }
      ],
      "abstract": "Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \\textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \\textit{gradient conflict} during optimization, as parameter updates required for distinct attributes contradict each other. To address this limitation, we introduce the \\textit{\\textbf{MoE-Adapter}}, a sparse Mixture-of-Experts~(MoE) architecture designed to decouple acoustic information. Specifically, it employs a dynamic gating mechanism that routes audio tokens to specialized experts capturing complementary feature subspaces while retaining shared experts for global context, thereby mitigating gradient conflicts and enabling fine-grained feature learning. Comprehensive experiments show that the MoE-Adapter achieves superior performance on both audio semantic and paralinguistic tasks, consistently outperforming dense linear baselines with comparable computational costs. Furthermore, we will release the related code and models to facilitate future research.",
      "publishedDate": "2026-01-06T12:24:38Z",
      "updatedDate": "2026-01-08T06:17:18Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02967v2",
      "arxivUrl": "https://arxiv.org/abs/2601.02967",
      "comment": "13 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02875",
      "title": "Revisiting Data Compression with Language Modeling",
      "authors": [
        {
          "name": "Chen-Han Tsai",
          "affiliation": null
        }
      ],
      "abstract": "In this report, we investigate the potential use of large language models (LLM's) in the task of data compression. Previous works have demonstrated promising results in applying LLM's towards compressing not only text, but also a wide range of multi-modal data. Despite the favorable performance achieved, there still remains several practical questions that pose a challenge towards replacing existing data compression algorithms with LLM's. In this work, we explore different methods to achieve a lower adjusted compression rate using LLM's as data compressors. In comparison to previous works, we were able to achieve a new state-of-the-art (SOTA) adjusted compression rate of around $18\\%$ on the enwik9 dataset without additional model training. Furthermore, we explore the use of LLM's in compressing non-English data, code data, byte stream sequences. We show that while LLM's excel in compressing data in text-dominant domains, their ability in compressing non-natural text sequences still remain competitive if configured in the right way.",
      "publishedDate": "2026-01-06T10:03:33Z",
      "updatedDate": "2026-01-06T10:03:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02875v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02875",
      "comment": "Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02868",
      "title": "CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation",
      "authors": [
        {
          "name": "Peiding Wang",
          "affiliation": null
        },
        {
          "name": "Li Zhang",
          "affiliation": null
        },
        {
          "name": "Fang Liu",
          "affiliation": null
        },
        {
          "name": "Chongyang Tao",
          "affiliation": null
        },
        {
          "name": "Yinghao Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.",
      "publishedDate": "2026-01-06T09:57:19Z",
      "updatedDate": "2026-01-06T09:57:19Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02868v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02868",
      "comment": "preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02563",
      "title": "Compressed code: the hidden effects of quantization and distillation on programming tokens",
      "authors": [
        {
          "name": "Viacheslav Siniaev",
          "affiliation": null
        },
        {
          "name": "Iaroslav Chelombitko",
          "affiliation": null
        },
        {
          "name": "Aleksey Komissarov",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional code generation capabilities, yet their token-level mechanisms remain underexplored, particularly in compressed models. Through systematic analysis of programming language token representations, we characterize how programming languages are encoded in LLM tokenizers by analyzing their vocabulary distribution and keyword coverage patterns. We introduce a novel cold-start probability analysis method that provides insights into model behavior without requiring explicit prompts. Additionally, we present a comprehensive evaluation of how different model optimization techniques - including quantization, distillation, model scaling, and task-specific fine-tuning - affect token-level representations and code generation quality. Our experiments, supported by comprehensive probability distribution analysis and evaluation metrics, reveal critical insights into token-level behavior and provide empirically-validated guidelines for maintaining code generation quality under various optimization constraints. These findings advance both theoretical understanding of LLM code generation and practical implementation of optimized models in production environments.",
      "publishedDate": "2026-01-05T21:32:47Z",
      "updatedDate": "2026-01-05T21:32:47Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.CL",
        "cs.LG",
        "cs.PL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02563v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02563",
      "comment": "18 pages, 1 figure and 6 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02438",
      "title": "Focus on What Matters: Fisher-Guided Adaptive Multimodal Fusion for Vulnerability Detection",
      "authors": [
        {
          "name": "Yun Bian",
          "affiliation": null
        },
        {
          "name": "Yi Chen",
          "affiliation": null
        },
        {
          "name": "HaiQuan Wang",
          "affiliation": null
        },
        {
          "name": "ShiHao Li",
          "affiliation": null
        },
        {
          "name": "Zhe Cui",
          "affiliation": null
        }
      ],
      "abstract": "Software vulnerability detection is a critical task for securing software systems and can be formulated as a binary classification problem: given a code snippet, determine whether it contains a vulnerability. Existing multimodal approaches typically fuse Natural Code Sequence (NCS) representations from pretrained language models with Code Property Graph (CPG) representations from graph neural networks, often under the implicit assumption that adding a modality necessarily yields extra information. In practice, sequence and graph representations can be redundant, and fluctuations in the quality of the graph modality can dilute the discriminative signal of the dominant modality. To address this, we propose TaCCS-DFA, a framework that introduces Fisher information as a geometric measure of how sensitive feature directions are to the classification decision, enabling task-oriented complementary fusion. TaCCS-DFA online estimates a low-rank principal Fisher subspace and restricts cross-modal attention to task-sensitive directions, thereby retrieving structural features from CPG that complement the sequence modality; meanwhile, an adaptive gating mechanism dynamically adjusts the contribution of the graph modality for each sample to suppress noise propagation. Our analysis shows that, under an isotropic perturbation assumption, the proposed mechanism admits a tighter risk bound than conventional full-spectrum attention. Experiments on BigVul, Devign, and ReVeal show that TaCCS-DFA achieves strong performance across multiple backbones. With CodeT5 as the backbone, TaCCS-DFA reaches an F1 score of 87.80\\% on the highly imbalanced BigVul dataset, improving over a strong baseline Vul-LMGNNs by 6.3 percentage points while maintaining low calibration error and computational overhead.",
      "publishedDate": "2026-01-05T09:31:21Z",
      "updatedDate": "2026-01-05T09:31:21Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02438v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02438",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02410",
      "title": "The Vibe-Check Protocol: Quantifying Cognitive Offloading in AI Programming",
      "authors": [
        {
          "name": "Aizierjiang Aiersilan",
          "affiliation": null
        }
      ],
      "abstract": "The integration of Large Language Models (LLMs) into software engineering education has driven the emergence of ``Vibe Coding,'' a paradigm where developers articulate high-level intent through natural language and delegate implementation to AI agents. While proponents argue this approach modernizes pedagogy by emphasizing conceptual design over syntactic memorization, accumulating empirical evidence raises concerns regarding skill retention and deep conceptual understanding. This paper proposes a theoretical framework to investigate the research question: \\textit{Is Vibe Coding a better way to learn software engineering?} We posit a divergence in student outcomes between those leveraging AI for acceleration versus those using it for cognitive offloading. To evaluate these educational trade-offs, we propose the \\textbf{Vibe-Check Protocol (VCP)}, a systematic benchmarking framework incorporating three quantitative metrics: the \\textit{Cold Start Refactor} ($M_{CSR}$) for modeling skill decay; \\textit{Hallucination Trap Detection} ($M_{HT}$) based on signal detection theory to evaluate error identification; and the \\textit{Explainability Gap} ($E_{gap}$) for quantifying the divergence between code complexity and conceptual comprehension. Through controlled comparisons, VCP aims to provide a quantitative basis for educators to determine the optimal pedagogical boundary: identifying contexts where Vibe Coding fosters genuine mastery and contexts where it introduces hidden technical debt and superficial competence.",
      "publishedDate": "2026-01-02T06:13:41Z",
      "updatedDate": "2026-01-02T06:13:41Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CY",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02410v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02410",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02456",
      "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation",
      "authors": [
        {
          "name": "Junhao Cai",
          "affiliation": null
        },
        {
          "name": "Zetao Cai",
          "affiliation": null
        },
        {
          "name": "Jiafei Cao",
          "affiliation": null
        },
        {
          "name": "Yilun Chen",
          "affiliation": null
        },
        {
          "name": "Zeyu He",
          "affiliation": null
        },
        {
          "name": "Lei Jiang",
          "affiliation": null
        },
        {
          "name": "Hang Li",
          "affiliation": null
        },
        {
          "name": "Hengjie Li",
          "affiliation": null
        },
        {
          "name": "Yang Li",
          "affiliation": null
        },
        {
          "name": "Yufei Liu",
          "affiliation": null
        },
        {
          "name": "Yanan Lu",
          "affiliation": null
        },
        {
          "name": "Qi Lv",
          "affiliation": null
        },
        {
          "name": "Haoxiang Ma",
          "affiliation": null
        },
        {
          "name": "Jiangmiao Pang",
          "affiliation": null
        },
        {
          "name": "Yu Qiao",
          "affiliation": null
        },
        {
          "name": "Zherui Qiu",
          "affiliation": null
        },
        {
          "name": "Yanqing Shen",
          "affiliation": null
        },
        {
          "name": "Xu Shi",
          "affiliation": null
        },
        {
          "name": "Yang Tian",
          "affiliation": null
        },
        {
          "name": "Bolun Wang",
          "affiliation": null
        },
        {
          "name": "Hanqing Wang",
          "affiliation": null
        },
        {
          "name": "Jiaheng Wang",
          "affiliation": null
        },
        {
          "name": "Tai Wang",
          "affiliation": null
        },
        {
          "name": "Xueyuan Wei",
          "affiliation": null
        },
        {
          "name": "Chao Wu",
          "affiliation": null
        },
        {
          "name": "Yiman Xie",
          "affiliation": null
        },
        {
          "name": "Boyang Xing",
          "affiliation": null
        },
        {
          "name": "Yuqiang Yang",
          "affiliation": null
        },
        {
          "name": "Yuyin Yang",
          "affiliation": null
        },
        {
          "name": "Qiaojun Yu",
          "affiliation": null
        },
        {
          "name": "Feng Yuan",
          "affiliation": null
        },
        {
          "name": "Jia Zeng",
          "affiliation": null
        },
        {
          "name": "Jingjing Zhang",
          "affiliation": null
        },
        {
          "name": "Shenghan Zhang",
          "affiliation": null
        },
        {
          "name": "Shi Zhang",
          "affiliation": null
        },
        {
          "name": "Zhuoma Zhaxi",
          "affiliation": null
        },
        {
          "name": "Bowen Zhou",
          "affiliation": null
        },
        {
          "name": "Yuanzhen Zhou",
          "affiliation": null
        },
        {
          "name": "Yunsong Zhou",
          "affiliation": null
        },
        {
          "name": "Hongrui Zhu",
          "affiliation": null
        },
        {
          "name": "Yangkun Zhu",
          "affiliation": null
        },
        {
          "name": "Yuchen Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting.",
      "publishedDate": "2026-01-05T18:54:29Z",
      "updatedDate": "2026-01-05T18:54:29Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02456v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02456",
      "comment": "Homepage: https://internrobotics.github.io/internvla-a1.github.io/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02845",
      "title": "TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents",
      "authors": [
        {
          "name": "Kai Li",
          "affiliation": null
        },
        {
          "name": "Xuanqing Yu",
          "affiliation": null
        },
        {
          "name": "Ziyi Ni",
          "affiliation": null
        },
        {
          "name": "Yi Zeng",
          "affiliation": null
        },
        {
          "name": "Yao Xu",
          "affiliation": null
        },
        {
          "name": "Zheqing Zhang",
          "affiliation": null
        },
        {
          "name": "Xin Li",
          "affiliation": null
        },
        {
          "name": "Jitao Sang",
          "affiliation": null
        },
        {
          "name": "Xiaogang Duan",
          "affiliation": null
        },
        {
          "name": "Xuelei Wang",
          "affiliation": null
        },
        {
          "name": "Chengbao Liu",
          "affiliation": null
        },
        {
          "name": "Jie Tan",
          "affiliation": null
        }
      ],
      "abstract": "Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.",
      "publishedDate": "2026-01-06T09:24:19Z",
      "updatedDate": "2026-01-06T09:24:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02845v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02845",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02578",
      "title": "DataParasite Enables Scalable and Repurposable Online Data Curation",
      "authors": [
        {
          "name": "Mengyi Sun",
          "affiliation": null
        }
      ],
      "abstract": "Many questions in computational social science rely on datasets assembled from heterogeneous online sources, a process that is often labor-intensive, costly, and difficult to reproduce. Recent advances in large language models enable agentic search and structured extraction from the web, but existing systems are frequently opaque, inflexible, or poorly suited to scientific data curation. Here we introduce DataParasite, an open-source, modular pipeline for scalable online data collection. DataParasite decomposes tabular curation tasks into independent, entity-level searches defined through lightweight configuration files and executed through a shared, task-agnostic python script. Crucially, the same pipeline can be repurposed to new tasks, including those without predefined entity lists, using only natural-language instructions. We evaluate the pipeline on multiple canonical tasks in computational social science, including faculty hiring histories, elite death events, and political career trajectories. Across tasks, DataParasite achieves high accuracy while reducing data-collection costs by an order of magnitude relative to manual curation. By lowering the technical and labor barriers to online data assembly, DataParasite provides a practical foundation for scalable, transparent, and reusable data curation in computational social science and beyond.",
      "publishedDate": "2026-01-05T22:04:16Z",
      "updatedDate": "2026-01-05T22:04:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02578v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02578",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02573",
      "title": "LendNova: Towards Automated Credit Risk Assessment with Language Models",
      "authors": [
        {
          "name": "Kiarash Shamsi",
          "affiliation": null
        },
        {
          "name": "Danijel Novokmet",
          "affiliation": null
        },
        {
          "name": "Joshua Peters",
          "affiliation": null
        },
        {
          "name": "Mao Lin Liu",
          "affiliation": null
        },
        {
          "name": "Paul K Edwards",
          "affiliation": null
        },
        {
          "name": "Vahab Khoshdel",
          "affiliation": null
        }
      ],
      "abstract": "Credit risk assessment is essential in the financial sector, but has traditionally depended on costly feature-based models that often fail to utilize all available information in raw credit records. This paper introduces LendNova, the first practical automated end-to-end pipeline for credit risk assessment, designed to utilize all available information in raw credit records by leveraging advanced NLP techniques and language models. LendNova transforms risk modeling by operating directly on raw, jargon-heavy credit bureau text using a language model that learns task-relevant representations without manual feature engineering. By automatically capturing patterns and risk signals embedded in the text, it replaces manual preprocessing steps, reducing costs and improving scalability. Evaluation on real-world data further demonstrates its strong potential in accurate and efficient risk assessment. LendNova establishes a baseline for intelligent credit risk agents, demonstrating the feasibility of language models in this domain. It lays the groundwork for future research toward foundation systems that enable more accurate, adaptable, and automated financial decision-making.",
      "publishedDate": "2026-01-05T21:53:36Z",
      "updatedDate": "2026-01-05T21:53:36Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02573v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02573",
      "comment": null,
      "journalRef": "AAAI 2026, Workshop on Agentic AI in Financial Services",
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02439",
      "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
      "authors": [
        {
          "name": "Hao Bai",
          "affiliation": null
        },
        {
          "name": "Alexey Taymanov",
          "affiliation": null
        },
        {
          "name": "Tong Zhang",
          "affiliation": null
        },
        {
          "name": "Aviral Kumar",
          "affiliation": null
        },
        {
          "name": "Spencer Whitehead",
          "affiliation": null
        }
      ],
      "abstract": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.",
      "publishedDate": "2026-01-05T09:35:11Z",
      "updatedDate": "2026-01-07T11:21:44Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02439v2",
      "arxivUrl": "https://arxiv.org/abs/2601.02439",
      "comment": "Slightly modified format; added Table 3 for better illustration of the scaling results",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03731",
      "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level",
      "authors": [
        {
          "name": "Jia Li",
          "affiliation": null
        },
        {
          "name": "Yuxin Su",
          "affiliation": null
        },
        {
          "name": "Michael R. Lyu",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.",
      "publishedDate": "2026-01-07T09:22:28Z",
      "updatedDate": "2026-01-09T16:30:25Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03731v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03731",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "agents",
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03702",
      "title": "A Chromatographic Process Design and Optimization Platform Powered by Large Language Models: A Case Application on Extract of Ginkgo Biloba Leaf",
      "authors": [
        {
          "name": "Zhilong Tang",
          "affiliation": null
        },
        {
          "name": "Shaohua Wu",
          "affiliation": null
        },
        {
          "name": "Xinyan Zhao",
          "affiliation": null
        },
        {
          "name": "Yu Wang",
          "affiliation": null
        },
        {
          "name": "Xingchu Gong",
          "affiliation": null
        }
      ],
      "abstract": "Chromatographic separation technology has been widely applied in pharmaceutical, chemical, and food industries due to its high efficiency. However, traditional human-dependent chromatographic process development faces challenges such as reliance on expert experience, long development cycles, and labor intensity. ChromR, a large language model (LLM)-driven platform for chromatographic process design and optimization, is presented in this work. The platform integrates ChromLLM, a domain-specific LLM trained for chromatography, along with a multi-agent system and an automated chromatographic experimental device. The multi-agent system comprises four agents: domain knowledge answering, experimental design, experimental execution, and data analysis. ChromR enables automatic completion of the entire workflow-including initial process parameter recommendation, experimental design, automated execution, data analysis, and multi-objective optimization. By utilizing ChromR, dependency on expert knowledge is effectively reduced, while labor input and development time are significantly decreased. Chromatographic purification of the extract of Ginkgo biloba leaf (EGBL) was selected as a case study. ChromR successfully developed a chromatographic process within one week that meets multiple objectives, including fraction quality and production efficiency, reducing development time to approximately one-seventh of that required by the conventional paradigm. An intelligent, automated, and universally applicable new paradigm was established for chromatographic process development.",
      "publishedDate": "2026-01-07T08:40:20Z",
      "updatedDate": "2026-01-07T08:40:20Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03702v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03702",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03699",
      "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
      "authors": [
        {
          "name": "Quy-Anh Dang",
          "affiliation": null
        },
        {
          "name": "Chris Ngo",
          "affiliation": null
        },
        {
          "name": "Truong-Son Hy",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval",
      "publishedDate": "2026-01-07T08:34:17Z",
      "updatedDate": "2026-01-07T08:34:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03699v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03699",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03624",
      "title": "Architecting Agentic Communities using Design Patterns",
      "authors": [
        {
          "name": "Zoran Milosevic",
          "affiliation": null
        },
        {
          "name": "Fethi Rabhi",
          "affiliation": null
        }
      ],
      "abstract": "The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.",
      "publishedDate": "2026-01-07T06:10:07Z",
      "updatedDate": "2026-01-08T20:30:07Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03624v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03624",
      "comment": "supplementary material accompanying this paper is also attached .. its title is \"Complete Agentic AI Design Patterns Catalogue\"",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03605",
      "title": "DiVA: Fine-grained Factuality Verification with Agentic-Discriminative Verifier",
      "authors": [
        {
          "name": "Hui Huang",
          "affiliation": null
        },
        {
          "name": "Muyun Yang",
          "affiliation": null
        },
        {
          "name": "Yuki Arase",
          "affiliation": null
        }
      ],
      "abstract": "Despite the significant advancements of Large Language Models (LLMs), their factuality remains a critical challenge, fueling growing interest in factuality verification. Existing research on factuality verification primarily conducts binary judgments (e.g., correct or incorrect), which fails to distinguish varying degrees of error severity. This limits its utility for applications such as fine-grained evaluation and preference optimization. To bridge this gap, we propose the Agentic Discriminative Verifier (DiVA), a hybrid framework that synergizes the agentic search capabilities of generative models with the precise scoring aptitude of discriminative models. We also construct a new benchmark, FGVeriBench, as a robust testbed for fine-grained factuality verification. Experimental results on FGVeriBench demonstrate that our DiVA significantly outperforms existing methods on factuality verification for both general and multi-hop questions.",
      "publishedDate": "2026-01-07T05:35:01Z",
      "updatedDate": "2026-01-07T05:35:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03605v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03605",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03578",
      "title": "PsychEthicsBench: Evaluating Large Language Models Against Australian Mental Health Ethics",
      "authors": [
        {
          "name": "Yaling Shen",
          "affiliation": null
        },
        {
          "name": "Stephanie Fong",
          "affiliation": null
        },
        {
          "name": "Yiwen Jiang",
          "affiliation": null
        },
        {
          "name": "Zimu Wang",
          "affiliation": null
        },
        {
          "name": "Feilong Tang",
          "affiliation": null
        },
        {
          "name": "Qingyang Xu",
          "affiliation": null
        },
        {
          "name": "Xiangyu Zhao",
          "affiliation": null
        },
        {
          "name": "Zhongxing Xu",
          "affiliation": null
        },
        {
          "name": "Jiahe Liu",
          "affiliation": null
        },
        {
          "name": "Jinpeng Hu",
          "affiliation": null
        },
        {
          "name": "Dominic Dwyer",
          "affiliation": null
        },
        {
          "name": "Zongyuan Ge",
          "affiliation": null
        }
      ],
      "abstract": "The increasing integration of large language models (LLMs) into mental health applications necessitates robust frameworks for evaluating professional safety alignment. Current evaluative approaches primarily rely on refusal-based safety signals, which offer limited insight into the nuanced behaviors required in clinical practice. In mental health, clinically inadequate refusals can be perceived as unempathetic and discourage help-seeking. To address this gap, we move beyond refusal-centric metrics and introduce \\texttt{PsychEthicsBench}, the first principle-grounded benchmark based on Australian psychology and psychiatry guidelines, designed to evaluate LLMs' ethical knowledge and behavioral responses through multiple-choice and open-ended tasks with fine-grained ethicality annotations. Empirical results across 14 models reveal that refusal rates are poor indicators of ethical behavior, revealing a significant divergence between safety triggers and clinical appropriateness. Notably, we find that domain-specific fine-tuning can degrade ethical robustness, as several specialized models underperform their base backbones in ethical alignment. PsychEthicsBench provides a foundation for systematic, jurisdiction-aware evaluation of LLMs in mental health, encouraging more responsible development in this domain.",
      "publishedDate": "2026-01-07T04:49:02Z",
      "updatedDate": "2026-01-07T04:49:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03578v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03578",
      "comment": "17 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03479",
      "title": "Efficient Sequential Recommendation for Long Term User Interest Via Personalization",
      "authors": [
        {
          "name": "Qiang Zhang",
          "affiliation": null
        },
        {
          "name": "Hanchao Yu",
          "affiliation": null
        },
        {
          "name": "Ivan Ji",
          "affiliation": null
        },
        {
          "name": "Chen Yuan",
          "affiliation": null
        },
        {
          "name": "Yi Zhang",
          "affiliation": null
        },
        {
          "name": "Chihuang Liu",
          "affiliation": null
        },
        {
          "name": "Xiaolong Wang",
          "affiliation": null
        },
        {
          "name": "Christopher E. Lambert",
          "affiliation": null
        },
        {
          "name": "Ren Chen",
          "affiliation": null
        },
        {
          "name": "Chen Kovacs",
          "affiliation": null
        },
        {
          "name": "Xinzhu Bei",
          "affiliation": null
        },
        {
          "name": "Renqin Cai",
          "affiliation": null
        },
        {
          "name": "Rui Li",
          "affiliation": null
        },
        {
          "name": "Lizhu Zhang",
          "affiliation": null
        },
        {
          "name": "Xiangjun Fan",
          "affiliation": null
        },
        {
          "name": "Qunshu Zhang",
          "affiliation": null
        },
        {
          "name": "Benyu Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent years have witnessed success of sequential modeling, generative recommender, and large language model for recommendation. Though the scaling law has been validated for sequential models, it showed inefficiency in computational capacity when considering real-world applications like recommendation, due to the non-linear(quadratic) increasing nature of the transformer model. To improve the efficiency of the sequential model, we introduced a novel approach to sequential recommendation that leverages personalization techniques to enhance efficiency and performance. Our method compresses long user interaction histories into learnable tokens, which are then combined with recent interactions to generate recommendations. This approach significantly reduces computational costs while maintaining high recommendation accuracy. Our method could be applied to existing transformer based recommendation models, e.g., HSTU and HLLM. Extensive experiments on multiple sequential models demonstrate its versatility and effectiveness. Source code is available at \\href{https://github.com/facebookresearch/PerSRec}{https://github.com/facebookresearch/PerSRec}.",
      "publishedDate": "2026-01-07T00:15:44Z",
      "updatedDate": "2026-01-07T00:15:44Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03479v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03479",
      "comment": "ICDM 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03475",
      "title": "CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support",
      "authors": [
        {
          "name": "Ruiqi Deng",
          "affiliation": null
        },
        {
          "name": "Geoffrey Martin",
          "affiliation": null
        },
        {
          "name": "Tony Wang",
          "affiliation": null
        },
        {
          "name": "Gongbo Zhang",
          "affiliation": null
        },
        {
          "name": "Yi Liu",
          "affiliation": null
        },
        {
          "name": "Chunhua Weng",
          "affiliation": null
        },
        {
          "name": "Yanshan Wang",
          "affiliation": null
        },
        {
          "name": "Justin F Rousseau",
          "affiliation": null
        },
        {
          "name": "Yifan Peng",
          "affiliation": null
        }
      ],
      "abstract": "Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs). Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks. The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.",
      "publishedDate": "2026-01-07T00:05:42Z",
      "updatedDate": "2026-01-07T00:05:42Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03475v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03475",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03417",
      "title": "Implicit Graph, Explicit Retrieval: Towards Efficient and Interpretable Long-horizon Memory for Large Language Models",
      "authors": [
        {
          "name": "Xin Zhang",
          "affiliation": null
        },
        {
          "name": "Kailai Yang",
          "affiliation": null
        },
        {
          "name": "Hao Li",
          "affiliation": null
        },
        {
          "name": "Chenyue Li",
          "affiliation": null
        },
        {
          "name": "Qiyu Wei",
          "affiliation": null
        },
        {
          "name": "Sophia Ananiadou",
          "affiliation": null
        }
      ],
      "abstract": "Long-horizon applications increasingly require large language models (LLMs) to answer queries when relevant evidence is sparse and dispersed across very long contexts. Existing memory systems largely follow two paradigms: explicit structured memories offer interpretability but often become brittle under long-context overload, while latent memory mechanisms are efficient and stable yet difficult to inspect. We propose LatentGraphMem, a memory framework that combines implicit graph memory with explicit subgraph retrieval. LatentGraphMem stores a graph-structured memory in latent space for stability and efficiency, and exposes a task-specific subgraph retrieval interface that returns a compact symbolic subgraph under a fixed budget for downstream reasoning and human inspection. During training, an explicit graph view is materialized to interface with a frozen reasoner for question-answering supervision. At inference time, retrieval is performed in latent space and only the retrieved subgraph is externalized. Experiments on long-horizon benchmarks across multiple model scales show that LatentGraphMem consistently outperforms representative explicit-graph and latent-memory baselines, while enabling parameter-efficient adaptation and flexible scaling to larger reasoners without introducing large symbolic artifacts.",
      "publishedDate": "2026-01-06T21:10:10Z",
      "updatedDate": "2026-01-06T21:10:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03417v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03417",
      "comment": "11 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03403",
      "title": "Tigrinya Number Verbalization: Rules, Algorithm, and Implementation",
      "authors": [
        {
          "name": "Fitsum Gaim",
          "affiliation": null
        },
        {
          "name": "Issayas Tesfamariam",
          "affiliation": null
        }
      ],
      "abstract": "We present a systematic formalization of Tigrinya cardinal and ordinal number verbalization, addressing a gap in computational resources for the language. This work documents the canonical rules governing the expression of numerical values in spoken Tigrinya, including the conjunction system, scale words, and special cases for dates, times, and currency. We provide a formal algorithm for number-to-word conversion and release an open-source implementation. Evaluation of frontier large language models (LLMs) reveals significant gaps in their ability to accurately verbalize Tigrinya numbers, underscoring the need for explicit rule documentation. This work serves language modeling, speech synthesis, and accessibility applications targeting Tigrinya-speaking communities.",
      "publishedDate": "2026-01-06T20:45:54Z",
      "updatedDate": "2026-01-06T20:45:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03403v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03403",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03385",
      "title": "SIGMA: Scalable Spectral Insights for LLM Collapse",
      "authors": [
        {
          "name": "Yi Gu",
          "affiliation": null
        },
        {
          "name": "Lingyou Pang",
          "affiliation": null
        },
        {
          "name": "Xiangkun Ye",
          "affiliation": null
        },
        {
          "name": "Tianyu Wang",
          "affiliation": null
        },
        {
          "name": "Jianyu Lin",
          "affiliation": null
        },
        {
          "name": "Carey E. Priebe",
          "affiliation": null
        },
        {
          "name": "Alexander Aue",
          "affiliation": null
        }
      ],
      "abstract": "The rapid adoption of synthetic data for training Large Language Models (LLMs) has introduced the technical challenge of \"model collapse\"-a degenerative process where recursive training on model-generated content leads to a contraction of distributional variance and representational quality. While the phenomenology of collapse is increasingly evident, rigorous methods to quantify and predict its onset in high-dimensional spaces remain elusive. In this paper, we introduce SIGMA (Spectral Inequalities for Gram Matrix Analysis), a unified framework that benchmarks model collapse through the spectral lens of the embedding Gram matrix. By deriving and utilizing deterministic and stochastic bounds on the matrix's spectrum, SIGMA provides a mathematically grounded metric to track the contraction of the representation space. Crucially, our stochastic formulation enables scalable estimation of these bounds, making the framework applicable to large-scale foundation models where full eigendecomposition is intractable. We demonstrate that SIGMA effectively captures the transition towards degenerate states, offering both theoretical insights into the mechanics of collapse and a practical, scalable tool for monitoring the health of recursive training pipelines.",
      "publishedDate": "2026-01-06T19:47:11Z",
      "updatedDate": "2026-01-06T19:47:11Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "math.PR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03385v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03385",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03785",
      "title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents",
      "authors": [
        {
          "name": "Dehao Tao",
          "affiliation": null
        },
        {
          "name": "Guoliang Ma",
          "affiliation": null
        },
        {
          "name": "Yongfeng Huang",
          "affiliation": null
        },
        {
          "name": "Minghu Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.",
      "publishedDate": "2026-01-07T10:36:29Z",
      "updatedDate": "2026-01-07T10:36:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03785v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03785",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03768",
      "title": "Agentic Proof Automation: A Case Study",
      "authors": [
        {
          "name": "Yichen Xu",
          "affiliation": null
        },
        {
          "name": "Martin Odersky",
          "affiliation": null
        }
      ],
      "abstract": "Proof engineering is notoriously labor-intensive: proofs that are straightforward on paper often require lengthy scripts in theorem provers. Recent advances in large language models (LLMs) create new opportunities for proof automation: modern LLMs not only generate proof scripts, but also support agentic behavior, exploring codebases and iteratively refining their outputs against prover feedback. These advances enable an emerging scheme where LLM-based agents undertake most proof engineering under human guidance. Humans provide mathematical insight (definitions, theorems, proof strategies); agents handle the mechanical work of proof development. We call this scheme agentic proof automation. We present this scheme through a case study: mechanizing the semantic type soundness of a sophisticated formal system, System Capless, in Lean 4, comprising over 14,000 lines of code. Using off-the-shelf LLM agents with a single lightweight proof-checking tool, the agents completed 189 proof engineering tasks with an 87% success rate, only 16% requiring human intervention. The case study demonstrates that agents are capable proof engineers that substantially boost productivity, though they fall short in creative reasoning and still require human guidance in certain cases. We release an interactive explorer where readers can examine all agent interactions; the mechanization is open-sourced for experiments and extensions.",
      "publishedDate": "2026-01-07T10:02:17Z",
      "updatedDate": "2026-01-07T10:02:17Z",
      "primaryCategory": "cs.PL",
      "arxivCategories": [
        "cs.PL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03768v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03768",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03315",
      "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
      "authors": [
        {
          "name": "Dhruv Trehan",
          "affiliation": null
        },
        {
          "name": "Paras Chopra",
          "affiliation": null
        }
      ],
      "abstract": "We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1",
      "publishedDate": "2026-01-06T13:20:54Z",
      "updatedDate": "2026-01-06T13:20:54Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03315v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03315",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03540",
      "title": "DeepSynth-Eval: Objectively Evaluating Information Consolidation in Deep Survey Writing",
      "authors": [
        {
          "name": "Hongzhi Zhang",
          "affiliation": null
        },
        {
          "name": "Yuanze Hu",
          "affiliation": null
        },
        {
          "name": "Tinghai Zhang",
          "affiliation": null
        },
        {
          "name": "Jia Fu",
          "affiliation": null
        },
        {
          "name": "Tao Wang",
          "affiliation": null
        },
        {
          "name": "Junwei Jing",
          "affiliation": null
        },
        {
          "name": "Zhaoxin Fan",
          "affiliation": null
        },
        {
          "name": "Qi Wang",
          "affiliation": null
        },
        {
          "name": "Ruiming Tang",
          "affiliation": null
        },
        {
          "name": "Han Li",
          "affiliation": null
        },
        {
          "name": "Guorui Zhou",
          "affiliation": null
        },
        {
          "name": "Kun Gai",
          "affiliation": null
        }
      ],
      "abstract": "The evolution of Large Language Models (LLMs) towards autonomous agents has catalyzed progress in Deep Research. While retrieval capabilities are well-benchmarked, the post-retrieval synthesis stage--where agents must digest massive amounts of context and consolidate fragmented evidence into coherent, long-form reports--remains under-evaluated due to the subjectivity of open-ended writing. To bridge this gap, we introduce DeepSynth-Eval, a benchmark designed to objectively evaluate information consolidation capabilities. We leverage high-quality survey papers as gold standards, reverse-engineering research requests and constructing \"Oracle Contexts\" from their bibliographies to isolate synthesis from retrieval noise. We propose a fine-grained evaluation protocol using General Checklists (for factual coverage) and Constraint Checklists (for structural organization), transforming subjective judgment into verifiable metrics. Experiments across 96 tasks reveal that synthesizing information from hundreds of references remains a significant challenge. Our results demonstrate that agentic plan-and-write workflows significantly outperform single-turn generation, effectively reducing hallucinations and improving adherence to complex structural constraints.",
      "publishedDate": "2026-01-07T03:07:52Z",
      "updatedDate": "2026-01-07T03:07:52Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03540v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03540",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04170",
      "title": "Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions",
      "authors": [
        {
          "name": "Abhishek Rath",
          "affiliation": null
        }
      ],
      "abstract": "Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies). We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements. We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.",
      "publishedDate": "2026-01-07T18:37:26Z",
      "updatedDate": "2026-01-07T18:37:26Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04170v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04170",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04160",
      "title": "All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection",
      "authors": [
        {
          "name": "Yuechen Jiang",
          "affiliation": null
        },
        {
          "name": "Zhiwei Liu",
          "affiliation": null
        },
        {
          "name": "Yupeng Cao",
          "affiliation": null
        },
        {
          "name": "Yueru He",
          "affiliation": null
        },
        {
          "name": "Ziyang Xu",
          "affiliation": null
        },
        {
          "name": "Chen Xu",
          "affiliation": null
        },
        {
          "name": "Zhiyang Deng",
          "affiliation": null
        },
        {
          "name": "Prayag Tiwari",
          "affiliation": null
        },
        {
          "name": "Xi Chen",
          "affiliation": null
        },
        {
          "name": "Alejandro Lopez-Lira",
          "affiliation": null
        },
        {
          "name": "Jimin Huang",
          "affiliation": null
        },
        {
          "name": "Junichi Tsujii",
          "affiliation": null
        },
        {
          "name": "Sophia Ananiadou",
          "affiliation": null
        }
      ],
      "abstract": "We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.",
      "publishedDate": "2026-01-07T18:18:28Z",
      "updatedDate": "2026-01-09T05:56:48Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CE",
        "q-fin.CP"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04160v3",
      "arxivUrl": "https://arxiv.org/abs/2601.04160",
      "comment": "48 pages; 24 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04157",
      "title": "FLEx: Language Modeling with Few-shot Language Explanations",
      "authors": [
        {
          "name": "Adar Avsian",
          "affiliation": null
        },
        {
          "name": "Christopher Richardson",
          "affiliation": null
        },
        {
          "name": "Anirudh Sundar",
          "affiliation": null
        },
        {
          "name": "Larry Heck",
          "affiliation": null
        }
      ],
      "abstract": "Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\\textbf{F}$ew-shot $\\textbf{L}$anguage $\\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\\% of CoT's remaining errors.",
      "publishedDate": "2026-01-07T18:12:05Z",
      "updatedDate": "2026-01-07T18:12:05Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04157v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04157",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04118",
      "title": "GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning",
      "authors": [
        {
          "name": "Wenshuai Li",
          "affiliation": null
        },
        {
          "name": "Xiantai Xiang",
          "affiliation": null
        },
        {
          "name": "Zixiao Wen",
          "affiliation": null
        },
        {
          "name": "Guangyao Zhou",
          "affiliation": null
        },
        {
          "name": "Ben Niu",
          "affiliation": null
        },
        {
          "name": "Feng Wang",
          "affiliation": null
        },
        {
          "name": "Lijia Huang",
          "affiliation": null
        },
        {
          "name": "Qiantong Wang",
          "affiliation": null
        },
        {
          "name": "Yuxin Hu",
          "affiliation": null
        }
      ],
      "abstract": "The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.",
      "publishedDate": "2026-01-07T17:26:41Z",
      "updatedDate": "2026-01-08T06:19:12Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04118v2",
      "arxivUrl": "https://arxiv.org/abs/2601.04118",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04086",
      "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures",
      "authors": [
        {
          "name": "Jinbo Hao",
          "affiliation": null
        },
        {
          "name": "Kai Yang",
          "affiliation": null
        },
        {
          "name": "Qingzhen Su",
          "affiliation": null
        },
        {
          "name": "Yifan Li",
          "affiliation": null
        },
        {
          "name": "Chao Jiang",
          "affiliation": null
        }
      ],
      "abstract": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.",
      "publishedDate": "2026-01-07T16:54:20Z",
      "updatedDate": "2026-01-07T16:54:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04086v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04086",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04055",
      "title": "Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients",
      "authors": [
        {
          "name": "Prith Sharma",
          "affiliation": null
        },
        {
          "name": "Austin Z. Henley",
          "affiliation": null
        }
      ],
      "abstract": "Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.",
      "publishedDate": "2026-01-07T16:20:08Z",
      "updatedDate": "2026-01-07T16:20:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04055v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04055",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04043",
      "title": "When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life",
      "authors": [
        {
          "name": "Xinyue Lou",
          "affiliation": null
        },
        {
          "name": "Jinan Xu",
          "affiliation": null
        },
        {
          "name": "Jingyi Yin",
          "affiliation": null
        },
        {
          "name": "Xiaolong Wang",
          "affiliation": null
        },
        {
          "name": "Zhaolu Kang",
          "affiliation": null
        },
        {
          "name": "Youwei Liao",
          "affiliation": null
        },
        {
          "name": "Yixuan Wang",
          "affiliation": null
        },
        {
          "name": "Xiangyu Shi",
          "affiliation": null
        },
        {
          "name": "Fengran Mo",
          "affiliation": null
        },
        {
          "name": "Su Yao",
          "affiliation": null
        },
        {
          "name": "Kaiyu Huang",
          "affiliation": null
        }
      ],
      "abstract": "As Multimodal Large Language Models (MLLMs) become an indispensable assistant in human life, the unsafe content generated by MLLMs poses a danger to human behavior, perpetually overhanging human society like a sword of Damocles. To investigate and evaluate the safety impact of MLLMs responses on human behavior in daily life, we introduce SaLAD, a multimodal safety benchmark which contains 2,013 real-world image-text samples across 10 common categories, with a balanced design covering both unsafe scenarios and cases of oversensitivity. It emphasizes realistic risk exposure, authentic visual inputs, and fine-grained cross-modal reasoning, ensuring that safety risks cannot be inferred from text alone. We further propose a safety-warning-based evaluation framework that encourages models to provide clear and informative safety warnings, rather than generic refusals. Results on 18 MLLMs demonstrate that the top-performing models achieve a safe response rate of only 57.2% on unsafe queries. Moreover, even popular safety alignment methods limit effectiveness of the models in our scenario, revealing the vulnerabilities of current MLLMs in identifying dangerous behaviors in daily life. Our dataset is available at https://github.com/xinyuelou/SaLAD.",
      "publishedDate": "2026-01-07T15:59:07Z",
      "updatedDate": "2026-01-07T15:59:07Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04043v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04043",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03986",
      "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
      "authors": [
        {
          "name": "Qi Qian",
          "affiliation": null
        },
        {
          "name": "Chengsong Huang",
          "affiliation": null
        },
        {
          "name": "Jingwen Xu",
          "affiliation": null
        },
        {
          "name": "Changze Lv",
          "affiliation": null
        },
        {
          "name": "Muling Wu",
          "affiliation": null
        },
        {
          "name": "Wenhao Liu",
          "affiliation": null
        },
        {
          "name": "Xiaohua Wang",
          "affiliation": null
        },
        {
          "name": "Zhenghua Wang",
          "affiliation": null
        },
        {
          "name": "Zisu Huang",
          "affiliation": null
        },
        {
          "name": "Muzhao Tian",
          "affiliation": null
        },
        {
          "name": "Jianhan Xu",
          "affiliation": null
        },
        {
          "name": "Kun Hu",
          "affiliation": null
        },
        {
          "name": "He-Da Wang",
          "affiliation": null
        },
        {
          "name": "Yao Hu",
          "affiliation": null
        },
        {
          "name": "Xuanjing Huang",
          "affiliation": null
        },
        {
          "name": "Xiaoqing Zheng",
          "affiliation": null
        }
      ],
      "abstract": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
      "publishedDate": "2026-01-07T14:59:03Z",
      "updatedDate": "2026-01-07T14:59:03Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03986v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03986",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03956",
      "title": "CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM",
      "authors": [
        {
          "name": "Kangjie Zhou",
          "affiliation": null
        },
        {
          "name": "Zhejia Wen",
          "affiliation": null
        },
        {
          "name": "Zhiyong Zhuo",
          "affiliation": null
        },
        {
          "name": "Zike Yan",
          "affiliation": null
        },
        {
          "name": "Pengying Wu",
          "affiliation": null
        },
        {
          "name": "Ieng Hou U",
          "affiliation": null
        },
        {
          "name": "Shuaiyang Li",
          "affiliation": null
        },
        {
          "name": "Han Gao",
          "affiliation": null
        },
        {
          "name": "Kang Ding",
          "affiliation": null
        },
        {
          "name": "Wenhan Cao",
          "affiliation": null
        },
        {
          "name": "Wei Pan",
          "affiliation": null
        },
        {
          "name": "Chang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigation, where robots must actively modify cluttered environments to create traversable paths. Existing VLM-based navigators are predominantly confined to passive obstacle avoidance, failing to reason about when and how to interact with objects to clear blocked paths. To bridge this gap, we propose Counterfactual Interactive Navigation via Skill-aware VLM (CoINS), a hierarchical framework that integrates skill-aware reasoning and robust low-level execution. Specifically, we fine-tune a VLM, named InterNav-VLM, which incorporates skill affordance and concrete constraint parameters into the input context and grounds them into a metric-scale environmental representation. By internalizing the logic of counterfactual reasoning through fine-tuning on the proposed InterNav dataset, the model learns to implicitly evaluate the causal effects of object removal on navigation connectivity, thereby determining interaction necessity and target selection. To execute the generated high-level plans, we develop a comprehensive skill library through reinforcement learning, specifically introducing traversability-oriented strategies to manipulate diverse objects for path clearance. A systematic benchmark in Isaac Sim is proposed to evaluate both the reasoning and execution aspects of interactive navigation. Extensive simulations and real-world experiments demonstrate that CoINS significantly outperforms representative baselines, achieving a 17\\% higher overall success rate and over 80\\% improvement in complex long-horizon scenarios compared to the best-performing baseline",
      "publishedDate": "2026-01-07T14:10:46Z",
      "updatedDate": "2026-01-07T14:10:46Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03956v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03956",
      "comment": "17 pages, 13 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "planning",
        "robotics"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "planning",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03948",
      "title": "Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification",
      "authors": [
        {
          "name": "Rui Sun",
          "affiliation": null
        },
        {
          "name": "Yifan Sun",
          "affiliation": null
        },
        {
          "name": "Sheng Xu",
          "affiliation": null
        },
        {
          "name": "Li Zhao",
          "affiliation": null
        },
        {
          "name": "Jing Li",
          "affiliation": null
        },
        {
          "name": "Daxin Jiang",
          "affiliation": null
        },
        {
          "name": "Cheng Hua",
          "affiliation": null
        },
        {
          "name": "Zuo Bai",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.",
      "publishedDate": "2026-01-07T14:03:22Z",
      "updatedDate": "2026-01-08T02:48:58Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03948v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03948",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03926",
      "title": "Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models",
      "authors": [
        {
          "name": "Haeun Jang",
          "affiliation": null
        },
        {
          "name": "Hwan Chang",
          "affiliation": null
        },
        {
          "name": "Hwanhee Lee",
          "affiliation": null
        }
      ],
      "abstract": "The deployment of Large Vision-Language Models (LVLMs) for real-world document question answering is often constrained by dynamic, user-defined policies that dictate information disclosure based on context. While ensuring adherence to these explicit constraints is critical, existing safety research primarily focuses on implicit social norms or text-only settings, overlooking the complexities of multimodal documents. In this paper, we introduce Doc-PP (Document Policy Preservation Benchmark), a novel benchmark constructed from real-world reports requiring reasoning across heterogeneous visual and textual elements under strict non-disclosure policies. Our evaluation highlights a systemic Reasoning-Induced Safety Gap: models frequently leak sensitive information when answers must be inferred through complex synthesis or aggregated across modalities, effectively circumventing existing safety constraints. Furthermore, we identify that providing extracted text improves perception but inadvertently facilitates leakage. To address these vulnerabilities, we propose DVA (Decompose-Verify-Aggregation), a structural inference framework that decouples reasoning from policy verification. Experimental results demonstrate that DVA significantly outperforms standard prompting defenses, offering a robust baseline for policy-compliant document understanding",
      "publishedDate": "2026-01-07T13:45:39Z",
      "updatedDate": "2026-01-07T13:45:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03926v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03926",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03914",
      "title": "When Models Decide and When They Bind: A Two-Stage Computation for Multiple-Choice Question-Answering",
      "authors": [
        {
          "name": "Hugh Mee Wong",
          "affiliation": null
        },
        {
          "name": "Rick Nouwen",
          "affiliation": null
        },
        {
          "name": "Albert Gatt",
          "affiliation": null
        }
      ],
      "abstract": "Multiple-choice question answering (MCQA) is easy to evaluate but adds a meta-task: models must both solve the problem and output the symbol that *represents* the answer, conflating reasoning errors with symbol-binding failures. We study how language models implement MCQA internally using representational analyses (PCA, linear probes) as well as causal interventions. We find that option-boundary (newline) residual states often contain strong linearly decodable signals related to per-option correctness. Winner-identity probing reveals a two-stage progression: the winning *content position* becomes decodable immediately after the final option is processed, while the *output symbol* is represented closer to the answer emission position. Tests under symbol and content permutations support a two-stage mechanism in which models first select a winner in content space and then bind or route that winner to the appropriate symbol to emit.",
      "publishedDate": "2026-01-07T13:27:48Z",
      "updatedDate": "2026-01-07T13:27:48Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03914v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03914",
      "comment": "Under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03905",
      "title": "Current Agents Fail to Leverage World Model as Tool for Foresight",
      "authors": [
        {
          "name": "Cheng Qian",
          "affiliation": null
        },
        {
          "name": "Emre Can Acikgoz",
          "affiliation": null
        },
        {
          "name": "Bingxuan Li",
          "affiliation": null
        },
        {
          "name": "Xiusi Chen",
          "affiliation": null
        },
        {
          "name": "Yuji Zhang",
          "affiliation": null
        },
        {
          "name": "Bingxiang He",
          "affiliation": null
        },
        {
          "name": "Qinyu Luo",
          "affiliation": null
        },
        {
          "name": "Dilek Hakkani-Tür",
          "affiliation": null
        },
        {
          "name": "Gokhan Tur",
          "affiliation": null
        },
        {
          "name": "Yunzhu Li",
          "affiliation": null
        },
        {
          "name": "Heng Ji",
          "affiliation": null
        }
      ],
      "abstract": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.",
      "publishedDate": "2026-01-07T13:15:23Z",
      "updatedDate": "2026-01-08T02:36:21Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03905v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03905",
      "comment": "36 Pages, 13 Figures, 17 Tables (Meta data updated)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03895",
      "title": "Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training",
      "authors": [
        {
          "name": "Chi Liu",
          "affiliation": null
        },
        {
          "name": "Xin Chen",
          "affiliation": null
        }
      ],
      "abstract": "Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.",
      "publishedDate": "2026-01-07T13:04:52Z",
      "updatedDate": "2026-01-07T13:04:52Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03895v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03895",
      "comment": "10 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03878",
      "title": "Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design",
      "authors": [
        {
          "name": "Giovanni Rosa",
          "affiliation": null
        },
        {
          "name": "David Moreno-Lumbreras",
          "affiliation": null
        },
        {
          "name": "Gregorio Robles",
          "affiliation": null
        },
        {
          "name": "Jesús M. González-Barahona",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.",
      "publishedDate": "2026-01-07T12:46:57Z",
      "updatedDate": "2026-01-07T12:46:57Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03878v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03878",
      "comment": "This paper is a Stage 1 Registered Report. The study protocol and analysis plan were peer reviewed and accepted at SANER 2026 with a Continuity Acceptance (CA) score for Stage 2",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03872",
      "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
      "authors": [
        {
          "name": "Jinyang Wu",
          "affiliation": null
        },
        {
          "name": "Guocheng Zhai",
          "affiliation": null
        },
        {
          "name": "Ruihan Jin",
          "affiliation": null
        },
        {
          "name": "Jiahao Yuan",
          "affiliation": null
        },
        {
          "name": "Yuhao Shen",
          "affiliation": null
        },
        {
          "name": "Shuai Zhang",
          "affiliation": null
        },
        {
          "name": "Zhengqi Wen",
          "affiliation": null
        },
        {
          "name": "Jianhua Tao",
          "affiliation": null
        }
      ],
      "abstract": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) \\textbf{training-free cluster-based routing} that exploits empirical priors for domain-specific alignment, and (2) \\textbf{RL-based multi-step routing} that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.",
      "publishedDate": "2026-01-07T12:38:33Z",
      "updatedDate": "2026-01-07T12:38:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03872v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03872",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03823",
      "title": "Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning",
      "authors": [
        {
          "name": "Fei Wu",
          "affiliation": null
        },
        {
          "name": "Zhenrong Zhang",
          "affiliation": null
        },
        {
          "name": "Qikai Chang",
          "affiliation": null
        },
        {
          "name": "Jianshu Zhang",
          "affiliation": null
        },
        {
          "name": "Quan Liu",
          "affiliation": null
        },
        {
          "name": "Jun Du",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) elicits long chain-of-thought reasoning in large language models (LLMs), but outcome-based rewards lead to coarse-grained advantage estimation. While existing approaches improve RLVR via token-level entropy or sequence-level length control, they lack a semantically grounded, step-level measure of reasoning progress. As a result, LLMs fail to distinguish necessary deduction from redundant verification: they may continue checking after reaching a correct solution and, in extreme cases, overturn a correct trajectory into an incorrect final answer. To remedy the lack of process supervision, we introduce a training-free probing mechanism that extracts intermediate confidence and correctness and combines them into a Step Potential signal that explicitly estimates the reasoning state at each step. Building on this signal, we propose Step Potential Advantage Estimation (SPAE), a fine-grained credit assignment method that amplifies potential gains, penalizes potential drops, and applies penalty after potential saturates to encourage timely termination. Experiments across multiple benchmarks show SPAE consistently improves accuracy while substantially reducing response length, outperforming strong RL baselines and recent efficient reasoning and token-level advantage estimation methods. The code is available at https://github.com/cii030/SPAE-RL.",
      "publishedDate": "2026-01-07T11:36:01Z",
      "updatedDate": "2026-01-07T11:36:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03823v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03823",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03822",
      "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
      "authors": [
        {
          "name": "Muyang Zhao",
          "affiliation": null
        },
        {
          "name": "Qi Qi",
          "affiliation": null
        },
        {
          "name": "Hao Sun",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.",
      "publishedDate": "2026-01-07T11:30:55Z",
      "updatedDate": "2026-01-07T11:30:55Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03822v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03822",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03812",
      "title": "AI Generated Text Detection",
      "authors": [
        {
          "name": "Adilkhan Alikhanov",
          "affiliation": null
        },
        {
          "name": "Aidar Amangeldi",
          "affiliation": null
        },
        {
          "name": "Diar Demeubay",
          "affiliation": null
        },
        {
          "name": "Dilnaz Akhmetzhan",
          "affiliation": null
        },
        {
          "name": "Nurbek Moldakhmetov",
          "affiliation": null
        },
        {
          "name": "Omar Polat",
          "affiliation": null
        },
        {
          "name": "Galymzhan Zharas",
          "affiliation": null
        }
      ],
      "abstract": "The rapid development of large language models has led to an increase in AI-generated text, with students increasingly using LLM-generated content as their own work, which violates academic integrity. This paper presents an evaluation of AI text detection methods, including both traditional machine learning models and transformer-based architectures. We utilize two datasets, HC3 and DAIGT v2, to build a unified benchmark and apply a topic-based data split to prevent information leakage. This approach ensures robust generalization across unseen domains. Our experiments show that TF-IDF logistic regression achieves a reasonable baseline accuracy of 82.87%. However, deep learning models outperform it. The BiLSTM classifier achieves an accuracy of 88.86%, while DistilBERT achieves a similar accuracy of 88.11% with the highest ROC-AUC score of 0.96, demonstrating the strongest overall performance. The results indicate that contextual semantic modeling is significantly superior to lexical features and highlight the importance of mitigating topic memorization through appropriate evaluation protocols. The limitations of this work are primarily related to dataset diversity and computational constraints. In future work, we plan to expand dataset diversity and utilize parameter-efficient fine-tuning methods such as LoRA. We also plan to explore smaller or distilled models and employ more efficient batching strategies and hardware-aware optimization.",
      "publishedDate": "2026-01-07T11:18:10Z",
      "updatedDate": "2026-01-07T11:18:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03812v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03812",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03808",
      "title": "From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs",
      "authors": [
        {
          "name": "Usha Shrestha",
          "affiliation": null
        },
        {
          "name": "Dmitry Ignatov",
          "affiliation": null
        },
        {
          "name": "Radu Timofte",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.",
      "publishedDate": "2026-01-07T11:13:02Z",
      "updatedDate": "2026-01-07T11:13:02Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03808v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03808",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "code-generation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03792",
      "title": "VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation",
      "authors": [
        {
          "name": "Huynh Trung Kiet",
          "affiliation": null
        },
        {
          "name": "Dao Sy Duy Minh",
          "affiliation": null
        },
        {
          "name": "Nguyen Dinh Ha Duong",
          "affiliation": null
        },
        {
          "name": "Le Hoang Minh Huy",
          "affiliation": null
        },
        {
          "name": "Long Nguyen",
          "affiliation": null
        },
        {
          "name": "Dien Dinh",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in general medical domains. However, their performance significantly degrades in specialized, culturally specific domains such as Vietnamese Traditional Medicine (VTM), primarily due to the scarcity of high-quality, structured benchmarks. In this paper, we introduce VietMed-MCQ, a novel multiple-choice question dataset generated via a Retrieval-Augmented Generation (RAG) pipeline with an automated consistency check mechanism. Unlike previous synthetic datasets, our framework incorporates a dual-model validation approach to ensure reasoning consistency through independent answer verification, though the substring-based evidence checking has known limitations. The complete dataset of 3,190 questions spans three difficulty levels and underwent validation by one medical expert and four students, achieving 94.2 percent approval with substantial inter-rater agreement (Fleiss' kappa = 0.82). We benchmark seven open-source models on VietMed-MCQ. Results reveal that general-purpose models with strong Chinese priors outperform Vietnamese-centric models, highlighting cross-lingual conceptual transfer, while all models still struggle with complex diagnostic reasoning. Our code and dataset are publicly available to foster research in low-resource medical domains.",
      "publishedDate": "2026-01-07T10:49:56Z",
      "updatedDate": "2026-01-07T10:49:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03792v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03792",
      "comment": "11 pages, 4 figures. Dataset and code released",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03783",
      "title": "HearSay Benchmark: Do Audio LLMs Leak What They Hear?",
      "authors": [
        {
          "name": "Jin Wang",
          "affiliation": null
        },
        {
          "name": "Liang Lin",
          "affiliation": null
        },
        {
          "name": "Kaiwen Luo",
          "affiliation": null
        },
        {
          "name": "Weiliu Wang",
          "affiliation": null
        },
        {
          "name": "Yitian Chen",
          "affiliation": null
        },
        {
          "name": "Moayad Aloqaily",
          "affiliation": null
        },
        {
          "name": "Xuehai Tang",
          "affiliation": null
        },
        {
          "name": "Zhenhong Zhou",
          "affiliation": null
        },
        {
          "name": "Kun Wang",
          "affiliation": null
        },
        {
          "name": "Li Sun",
          "affiliation": null
        },
        {
          "name": "Qingsong Wen",
          "affiliation": null
        }
      ],
      "abstract": "While Audio Large Language Models (ALLMs) have achieved remarkable progress in understanding and generation, their potential privacy implications remain largely unexplored. This paper takes the first step to investigate whether ALLMs inadvertently leak user privacy solely through acoustic voiceprints and introduces $\\textit{HearSay}$, a comprehensive benchmark constructed from over 22,000 real-world audio clips. To ensure data quality, the benchmark is meticulously curated through a rigorous pipeline involving automated profiling and human verification, guaranteeing that all privacy labels are grounded in factual records. Extensive experiments on $\\textit{HearSay}$ yield three critical findings: $\\textbf{Significant Privacy Leakage}$: ALLMs inherently extract private attributes from voiceprints, reaching 92.89% accuracy on gender and effectively profiling social attributes. $\\textbf{Insufficient Safety Mechanisms}$: Alarmingly, existing safeguards are severely inadequate; most models fail to refuse privacy-intruding requests, exhibiting near-zero refusal rates for physiological traits. $\\textbf{Reasoning Amplifies Risk}$: Chain-of-Thought (CoT) reasoning exacerbates privacy risks in capable models by uncovering deeper acoustic correlations. These findings expose critical vulnerabilities in ALLMs, underscoring the urgent need for targeted privacy alignment. The codes and dataset are available at https://github.com/JinWang79/HearSay_Benchmark",
      "publishedDate": "2026-01-07T10:33:44Z",
      "updatedDate": "2026-01-07T10:33:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03783v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03783",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03781",
      "title": "MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction",
      "authors": [
        {
          "name": "Xiaokun Sun",
          "affiliation": null
        },
        {
          "name": "Zezhong Wu",
          "affiliation": null
        },
        {
          "name": "Zewen Ding",
          "affiliation": null
        },
        {
          "name": "Linli Xu",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.",
      "publishedDate": "2026-01-07T10:25:48Z",
      "updatedDate": "2026-01-07T10:25:48Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03781v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03781",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03775",
      "title": "Do LLM Self-Explanations Help Users Predict Model Behavior? Evaluating Counterfactual Simulatability with Pragmatic Perturbations",
      "authors": [
        {
          "name": "Pingjun Hong",
          "affiliation": null
        },
        {
          "name": "Benjamin Roth",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can produce verbalized self-explanations, yet prior studies suggest that such rationales may not reliably reflect the model's true decision process. We ask whether these explanations nevertheless help users predict model behavior, operationalized as counterfactual simulatability. Using StrategyQA, we evaluate how well humans and LLM judges can predict a model's answers to counterfactual follow-up questions, with and without access to the model's chain-of-thought or post-hoc explanations. We compare LLM-generated counterfactuals with pragmatics-based perturbations as alternative ways to construct test cases for assessing the potential usefulness of explanations. Our results show that self-explanations consistently improve simulation accuracy for both LLM judges and humans, but the degree and stability of gains depend strongly on the perturbation strategy and judge strength. We also conduct a qualitative analysis of free-text justifications written by human users when predicting the model's behavior, which provides evidence that access to explanations helps humans form more accurate predictions on the perturbed questions.",
      "publishedDate": "2026-01-07T10:13:26Z",
      "updatedDate": "2026-01-07T10:13:26Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03775v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03775",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03769",
      "title": "EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation",
      "authors": [
        {
          "name": "Zihang Li",
          "affiliation": null
        },
        {
          "name": "Yuhang Wang",
          "affiliation": null
        },
        {
          "name": "Yikun Zong",
          "affiliation": null
        },
        {
          "name": "Wenhan Yu",
          "affiliation": null
        },
        {
          "name": "Xiaokun Yuan",
          "affiliation": null
        },
        {
          "name": "Runhan Jiang",
          "affiliation": null
        },
        {
          "name": "Zirui Liu",
          "affiliation": null
        },
        {
          "name": "Tong Yang",
          "affiliation": null
        },
        {
          "name": "Arthur Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the \"answer right but reasoning wrong\" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.",
      "publishedDate": "2026-01-07T10:02:27Z",
      "updatedDate": "2026-01-12T09:46:16Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03769v3",
      "arxivUrl": "https://arxiv.org/abs/2601.03769",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03743",
      "title": "O-Researcher: An Open Ended Deep Research Model via Multi-Agent Distillation and Agentic RL",
      "authors": [
        {
          "name": "Yi Yao",
          "affiliation": null
        },
        {
          "name": "He Zhu",
          "affiliation": null
        },
        {
          "name": "Piaohong Wang",
          "affiliation": null
        },
        {
          "name": "Jincheng Ren",
          "affiliation": null
        },
        {
          "name": "Xinlong Yang",
          "affiliation": null
        },
        {
          "name": "Qianben Chen",
          "affiliation": null
        },
        {
          "name": "Xiaowan Li",
          "affiliation": null
        },
        {
          "name": "Dingfeng Shi",
          "affiliation": null
        },
        {
          "name": "Jiaxian Li",
          "affiliation": null
        },
        {
          "name": "Qiexiang Wang",
          "affiliation": null
        },
        {
          "name": "Sinuo Wang",
          "affiliation": null
        },
        {
          "name": "Xinpeng Liu",
          "affiliation": null
        },
        {
          "name": "Jiaqi Wu",
          "affiliation": null
        },
        {
          "name": "Minghao Liu",
          "affiliation": null
        },
        {
          "name": "Wangchunshu Zhou",
          "affiliation": null
        }
      ],
      "abstract": "The performance gap between closed-source and open-source large language models (LLMs) is largely attributed to disparities in access to high-quality training data. To bridge this gap, we introduce a novel framework for the automated synthesis of sophisticated, research-grade instructional data. Our approach centers on a multi-agent workflow where collaborative AI agents simulate complex tool-integrated reasoning to generate diverse and high-fidelity data end-to-end. Leveraging this synthesized data, we develop a two-stage training strategy that integrates supervised fine-tuning with a novel reinforcement learning method, designed to maximize model alignment and capability. Extensive experiments demonstrate that our framework empowers open-source models across multiple scales, enabling them to achieve new state-of-the-art performance on the major deep research benchmark. This work provides a scalable and effective pathway for advancing open-source LLMs without relying on proprietary data or models.",
      "publishedDate": "2026-01-07T09:31:10Z",
      "updatedDate": "2026-01-07T09:31:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03743v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03743",
      "comment": "22 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03733",
      "title": "RadDiff: Describing Differences in Radiology Image Sets with Natural Language",
      "authors": [
        {
          "name": "Xiaoxian Shen",
          "affiliation": null
        },
        {
          "name": "Yuhui Zhang",
          "affiliation": null
        },
        {
          "name": "Sahithi Ankireddy",
          "affiliation": null
        },
        {
          "name": "Xiaohan Wang",
          "affiliation": null
        },
        {
          "name": "Maya Varma",
          "affiliation": null
        },
        {
          "name": "Henry Guo",
          "affiliation": null
        },
        {
          "name": "Curtis Langlotz",
          "affiliation": null
        },
        {
          "name": "Serena Yeung-Levy",
          "affiliation": null
        }
      ],
      "abstract": "Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.",
      "publishedDate": "2026-01-07T09:25:04Z",
      "updatedDate": "2026-01-07T09:25:04Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03733v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03733",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03728",
      "title": "CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval",
      "authors": [
        {
          "name": "Zhipeng Qian",
          "affiliation": null
        },
        {
          "name": "Zihan Liang",
          "affiliation": null
        },
        {
          "name": "Yufei Ma",
          "affiliation": null
        },
        {
          "name": "Ben Chen",
          "affiliation": null
        },
        {
          "name": "Huangyu Dai",
          "affiliation": null
        },
        {
          "name": "Yiwei Ma",
          "affiliation": null
        },
        {
          "name": "Jiayi Ji",
          "affiliation": null
        },
        {
          "name": "Chenyi Lei",
          "affiliation": null
        },
        {
          "name": "Han Li",
          "affiliation": null
        },
        {
          "name": "Xiaoshuai Sun",
          "affiliation": null
        }
      ],
      "abstract": "Composed Image Retrieval (CIR) enables users to search for target images using both a reference image and manipulation text, offering substantial advantages over single-modality retrieval systems. However, existing CIR methods suffer from representation space fragmentation: queries and targets comprise heterogeneous modalities and are processed by distinct encoders, forcing models to bridge misaligned representation spaces only through post-hoc alignment, which fundamentally limits retrieval performance. This architectural asymmetry manifests as three distinct, well-separated clusters in the feature space, directly demonstrating how heterogeneous modalities create fundamentally misaligned representation spaces from initialization. In this work, we propose CSMCIR, a unified representation framework that achieves efficient query-target alignment through three synergistic components. First, we introduce a Multi-level Chain-of-Thought (MCoT) prompting strategy that guides Multimodal Large Language Models to generate discriminative, semantically compatible captions for target images, establishing modal symmetry. Building upon this, we design a symmetric dual-tower architecture where both query and target sides utilize the identical shared-parameter Q-Former for cross-modal encoding, ensuring consistent feature representations and further reducing the alignment gap. Finally, this architectural symmetry enables an entropy-based, temporally dynamic Memory Bank strategy that provides high-quality negative samples while maintaining consistency with the evolving model state. Extensive experiments on four benchmark datasets demonstrate that our CSMCIR achieves state-of-the-art performance with superior training efficiency. Comprehensive ablation studies further validate the effectiveness of each proposed component.",
      "publishedDate": "2026-01-07T09:21:38Z",
      "updatedDate": "2026-01-07T09:21:38Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03728v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03728",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03725",
      "title": "EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning",
      "authors": [
        {
          "name": "Jing-Cheng Pang",
          "affiliation": null
        },
        {
          "name": "Liu Sun",
          "affiliation": null
        },
        {
          "name": "Chang Zhou",
          "affiliation": null
        },
        {
          "name": "Xian Tang",
          "affiliation": null
        },
        {
          "name": "Haichuan Ma",
          "affiliation": null
        },
        {
          "name": "Kun Jiang",
          "affiliation": null
        },
        {
          "name": "Jianlong Wang",
          "affiliation": null
        },
        {
          "name": "Kai Zhang",
          "affiliation": null
        },
        {
          "name": "Sijie Wu",
          "affiliation": null
        },
        {
          "name": "Haoran Cai",
          "affiliation": null
        },
        {
          "name": "Chenwei Wu",
          "affiliation": null
        },
        {
          "name": "Xubin Li",
          "affiliation": null
        },
        {
          "name": "Xin Chen",
          "affiliation": null
        }
      ],
      "abstract": "Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model's evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.",
      "publishedDate": "2026-01-07T09:20:05Z",
      "updatedDate": "2026-01-07T09:20:05Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03725v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03725",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03723",
      "title": "ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization",
      "authors": [
        {
          "name": "Shijie Zhang",
          "affiliation": null
        },
        {
          "name": "Kevin Zhang",
          "affiliation": null
        },
        {
          "name": "Zheyuan Gu",
          "affiliation": null
        },
        {
          "name": "Xiang Guo",
          "affiliation": null
        },
        {
          "name": "Rujun Guo",
          "affiliation": null
        },
        {
          "name": "Shaoyu Liu",
          "affiliation": null
        },
        {
          "name": "Guanjun Jiang",
          "affiliation": null
        },
        {
          "name": "Xiaozhao Wang",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffers from a structural limitation: it imposes a uniform, static trust region constraint across all samples. This design implicitly assumes signal homogeneity, a premise misaligned with the heterogeneous nature of outcome-driven learning, where advantage magnitudes and variances fluctuate significantly. Consequently, static constraints fail to fully exploit high-quality signals while insufficiently suppressing noise, often precipitating rapid entropy collapse. To address this, we propose \\textbf{E}lastic \\textbf{T}rust \\textbf{R}egions (\\textbf{ETR}), a dynamic mechanism that aligns optimization constraints with signal quality. ETR constructs a signal-aware landscape through dual-level elasticity: at the micro level, it scales clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths; at the macro level, it leverages group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Extensive experiments on AIME and MATH benchmarks demonstrate that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.",
      "publishedDate": "2026-01-07T09:19:53Z",
      "updatedDate": "2026-01-07T09:19:53Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03723v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03723",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "tool-use",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03717",
      "title": "MIND: From Passive Mimicry to Active Reasoning through Capability-Aware Multi-Perspective CoT Distillation",
      "authors": [
        {
          "name": "Jin Cui",
          "affiliation": null
        },
        {
          "name": "Jiaqi Guo",
          "affiliation": null
        },
        {
          "name": "Jiepeng Zhou",
          "affiliation": null
        },
        {
          "name": "Ruixuan Yang",
          "affiliation": null
        },
        {
          "name": "Jiayi Lu",
          "affiliation": null
        },
        {
          "name": "Jiajun Xu",
          "affiliation": null
        },
        {
          "name": "Jiangcheng Song",
          "affiliation": null
        },
        {
          "name": "Boran Zhao",
          "affiliation": null
        },
        {
          "name": "Pengju Ren",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) have emerged with remarkable capabilities in complex tasks through Chain-of-Thought reasoning, practical resource constraints have sparked interest in transferring these abilities to smaller models. However, achieving both domain performance and cross-domain generalization remains challenging. Existing approaches typically restrict students to following a single golden rationale and treat different reasoning paths independently. Due to distinct inductive biases and intrinsic preferences, alongside the student's evolving capacity and reasoning preferences during training, a teacher's \"optimal\" rationale could act as out-of-distribution noise. This misalignment leads to a degeneration of the student's latent reasoning distribution, causing suboptimal performance. To bridge this gap, we propose MIND, a capability-adaptive framework that transitions distillation from passive mimicry to active cognitive construction. We synthesize diverse teacher perspectives through a novel \"Teaching Assistant\" network. By employing a Feedback-Driven Inertia Calibration mechanism, this network utilizes inertia-filtered training loss to align supervision with the student's current adaptability, effectively enhancing performance while mitigating catastrophic forgetting. Extensive experiments demonstrate that MIND achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks, and our sophisticated latent space analysis further confirms the mechanism of reasoning ability internalization.",
      "publishedDate": "2026-01-07T09:08:59Z",
      "updatedDate": "2026-01-07T09:08:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03717v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03717",
      "comment": "13 pages, 8 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03713",
      "title": "BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion",
      "authors": [
        {
          "name": "Qingyao Tian",
          "affiliation": null
        },
        {
          "name": "Bingyu Yang",
          "affiliation": null
        },
        {
          "name": "Huai Liao",
          "affiliation": null
        },
        {
          "name": "Xinyan Huang",
          "affiliation": null
        },
        {
          "name": "Junyong Li",
          "affiliation": null
        },
        {
          "name": "Dong Yi",
          "affiliation": null
        },
        {
          "name": "Hongbin Liu",
          "affiliation": null
        }
      ],
      "abstract": "Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM's ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.",
      "publishedDate": "2026-01-07T09:00:52Z",
      "updatedDate": "2026-01-07T09:00:52Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03713v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03713",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03703",
      "title": "TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL",
      "authors": [
        {
          "name": "Lang Cao",
          "affiliation": null
        },
        {
          "name": "Hui Ruan",
          "affiliation": null
        },
        {
          "name": "Yongqian Li",
          "affiliation": null
        },
        {
          "name": "Peng Chao",
          "affiliation": null
        },
        {
          "name": "Wu Ning",
          "affiliation": null
        },
        {
          "name": "Haonan Song",
          "affiliation": null
        },
        {
          "name": "Renhong Chen",
          "affiliation": null
        },
        {
          "name": "Yitong Li",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks. However, standard GRPO treats each rollout trajectory as an independent flat sequence and assigns a single sequence-level advantage to all tokens, which leads to sample inefficiency and a length bias toward verbose, redundant chains of thought without improving logical depth. We introduce TreeAdv (Tree-Structured Advantage Redistribution for Group-Based RL), which makes the tree structure of group rollouts explicit for both exploration and advantage assignment. Specifically, TreeAdv builds a group of trees (a forest) based on an entropy-driven sampling method where each tree branches at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts. Then, TreeAdv aggregates token-level advantages for internal tree segments by redistributing the advantages of complete rollouts (all leaf nodes), and TreeAdv can easily apply to group-based objectives such as GRPO or GSPO. Across 10 math reasoning benchmarks, TreeAdv consistently outperforms GRPO and GSPO, while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.",
      "publishedDate": "2026-01-07T08:42:14Z",
      "updatedDate": "2026-01-07T08:42:14Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03703v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03703",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03682",
      "title": "From Implicit to Explicit: Token-Efficient Logical Supervision for Mathematical Reasoning in LLMs",
      "authors": [
        {
          "name": "Shaojie Wang",
          "affiliation": null
        },
        {
          "name": "Liang Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent studies reveal that large language models (LLMs) exhibit limited logical reasoning abilities in mathematical problem-solving, instead often relying on pattern-matching and memorization. We systematically analyze this limitation, focusing on logical relationship understanding, which is a core capability underlying genuine logical reasoning, and reveal that errors related to this capability account for over 90\\% of incorrect predictions, with Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) failing to substantially reduce these errors. To address this bottleneck, we propose First-Step Logical Reasoning (FSLR), a lightweight training framework targeting logical relationship understanding. Our key insight is that the first planning step-identifying which variables to use and which operation to apply-encourages the model to derive logical relationships directly from the problem statement. By training models on this isolated step, FSLR provides explicit supervision for logical relationship understanding, unlike CoT-SFT which implicitly embeds such relationships within complete solution trajectories. Extensive experiments across multiple models and datasets demonstrate that FSLR consistently outperforms CoT-SFT under both in-distribution and out-of-distribution settings, with average improvements of 3.2\\% and 4.6\\%, respectively. Moreover, FSLR achieves 4-6x faster training and reduces training token consumption by over 80\\%.",
      "publishedDate": "2026-01-07T08:15:01Z",
      "updatedDate": "2026-01-07T08:15:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03682v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03682",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03661",
      "title": "AMIR-GRPO: Inducing Implicit Preference Signals into GRPO",
      "authors": [
        {
          "name": "Amir Hossein Yari",
          "affiliation": null
        },
        {
          "name": "Fajri Koto",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning has become the primary paradigm for aligning large language models (LLMs) on complex reasoning tasks, with group relative policy optimization (GRPO) widely used in large-scale post-training. However, GRPO faces structural limitations in reasoning-heavy settings: sequence-level advantage normalization introduces systematic length bias, penalties for low-quality trajectories are diluted, and the scalar objective discards rich pairwise preference information embedded in within-group reward rankings. As a result, valuable supervision from costly rollouts remains underutilized. We propose AMIR-GRPO, which augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no additional annotations. This mechanism amplifies suppression of low-reward trajectories, attenuates response-level length bias, and transforms each rollout group into a denser set of supervision constraints. Across multiple mathematical reasoning benchmarks, AMIR-GRPO consistently outperforms strong GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and delivers broader coverage gains beyond the subset of instances solved by standard GRPO.",
      "publishedDate": "2026-01-07T07:22:58Z",
      "updatedDate": "2026-01-07T07:22:58Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03661v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03661",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03615",
      "title": "Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation",
      "authors": [
        {
          "name": "Binh Nguyen",
          "affiliation": null
        },
        {
          "name": "Thai Le",
          "affiliation": null
        }
      ],
      "abstract": "Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \\textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \\textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \\textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \\textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.",
      "publishedDate": "2026-01-07T05:46:45Z",
      "updatedDate": "2026-01-07T05:46:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03615v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03615",
      "comment": "Preprint for ACL 2026 submission",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03604",
      "title": "Interleaved Tool-Call Reasoning for Protein Function Understanding",
      "authors": [
        {
          "name": "Chuanliu Fan",
          "affiliation": null
        },
        {
          "name": "Zicheng Ma",
          "affiliation": null
        },
        {
          "name": "Huanran Meng",
          "affiliation": null
        },
        {
          "name": "Aijia Zhang",
          "affiliation": null
        },
        {
          "name": "Wenjie Du",
          "affiliation": null
        },
        {
          "name": "Jun Zhang",
          "affiliation": null
        },
        {
          "name": "Yi Qin Gao",
          "affiliation": null
        },
        {
          "name": "Ziqiang Cao",
          "affiliation": null
        },
        {
          "name": "Guohong Fu",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.",
      "publishedDate": "2026-01-07T05:34:38Z",
      "updatedDate": "2026-01-07T05:34:38Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03604v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03604",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03603",
      "title": "A Comparative Study of Traditional Machine Learning, Deep Learning, and Large Language Models for Mental Health Forecasting using Smartphone Sensing Data",
      "authors": [
        {
          "name": "Kaidong Feng",
          "affiliation": null
        },
        {
          "name": "Zhu Sun",
          "affiliation": null
        },
        {
          "name": "Roy Ka-Wei Lee",
          "affiliation": null
        },
        {
          "name": "Xun Jiang",
          "affiliation": null
        },
        {
          "name": "Yin-Leng Theng",
          "affiliation": null
        },
        {
          "name": "Yi Ding",
          "affiliation": null
        }
      ],
      "abstract": "Smartphone sensing offers an unobtrusive and scalable way to track daily behaviors linked to mental health, capturing changes in sleep, mobility, and phone use that often precede symptoms of stress, anxiety, or depression. While most prior studies focus on detection that responds to existing conditions, forecasting mental health enables proactive support through Just-in-Time Adaptive Interventions. In this paper, we present the first comprehensive benchmarking study comparing traditional machine learning (ML), deep learning (DL), and large language model (LLM) approaches for mental health forecasting using the College Experience Sensing (CES) dataset, the most extensive longitudinal dataset of college student mental health to date. We systematically evaluate models across temporal windows, feature granularities, personalization strategies, and class imbalance handling. Our results show that DL models, particularly Transformer (Macro-F1 = 0.58), achieve the best overall performance, while LLMs show strength in contextual reasoning but weaker temporal modeling. Personalization substantially improves forecasts of severe mental health states. By revealing how different modeling approaches interpret phone sensing behavioral data over time, this work lays the groundwork for next-generation, adaptive, and human-centered mental health technologies that can advance both research and real-world well-being.",
      "publishedDate": "2026-01-07T05:33:00Z",
      "updatedDate": "2026-01-13T05:06:22Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03603v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03603",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03597",
      "title": "From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs",
      "authors": [
        {
          "name": "Yingjian Chen",
          "affiliation": null
        },
        {
          "name": "Haoran Liu",
          "affiliation": null
        },
        {
          "name": "Yinhong Liu",
          "affiliation": null
        },
        {
          "name": "Sherry T. Tong",
          "affiliation": null
        },
        {
          "name": "Aosong Feng",
          "affiliation": null
        },
        {
          "name": "Jinghui Lu",
          "affiliation": null
        },
        {
          "name": "Juntao Zhang",
          "affiliation": null
        },
        {
          "name": "Yusuke Iwasawa",
          "affiliation": null
        },
        {
          "name": "Yutaka Matsuo",
          "affiliation": null
        },
        {
          "name": "Irene Li",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) show strong reasoning ability in open-domain question answering, yet their reasoning processes are typically linear and often logically inconsistent. In contrast, real-world reasoning requires integrating multiple premises and solving subproblems in parallel. Existing methods, such as Chain-of-Thought (CoT), express reasoning in a linear textual form, which may appear coherent but frequently leads to inconsistent conclusions. Recent approaches rely on externally provided graphs and do not explore how LLMs can construct and use their own graph-structured reasoning, particularly in open-domain QA. To fill this gap, we novelly explore graph-structured reasoning of LLMs in general-domain question answering. We propose Self-Graph Reasoning (SGR), a framework that enables LLMs to explicitly represent their reasoning process as a structured graph before producing the final answer. We further construct a graph-structured reasoning dataset that merges multiple candidate reasoning graphs into refined graph structures for model training. Experiments on five QA benchmarks across both general and specialized domains show that SGR consistently improves reasoning consistency and yields a 17.74% gain over the base model. The LLaMA-3.3-70B model fine-tuned with SGR performs comparably to GPT-4o and surpasses Claude-3.5-Haiku, demonstrating the effectiveness of graph-structured reasoning.",
      "publishedDate": "2026-01-07T05:27:41Z",
      "updatedDate": "2026-01-07T05:27:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03597v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03597",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03590",
      "title": "Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions",
      "authors": [
        {
          "name": "Zhongbin Guo",
          "affiliation": null
        },
        {
          "name": "Zhen Yang",
          "affiliation": null
        },
        {
          "name": "Yushan Li",
          "affiliation": null
        },
        {
          "name": "Xinyue Zhang",
          "affiliation": null
        },
        {
          "name": "Wenyu Gao",
          "affiliation": null
        },
        {
          "name": "Jiacheng Wang",
          "affiliation": null
        },
        {
          "name": "Chengzhi Li",
          "affiliation": null
        },
        {
          "name": "Xiangrui Liu",
          "affiliation": null
        },
        {
          "name": "Ping Jian",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant \"spatial gap\" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .",
      "publishedDate": "2026-01-07T05:13:52Z",
      "updatedDate": "2026-01-07T05:13:52Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03590v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03590",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "robotics",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "robotics",
          "agents",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03589",
      "title": "OLA: Output Language Alignment in Code-Switched LLM Interactions",
      "authors": [
        {
          "name": "Juhyun Oh",
          "affiliation": null
        },
        {
          "name": "Haneul Yoo",
          "affiliation": null
        },
        {
          "name": "Faiz Ghifari Haznitrama",
          "affiliation": null
        },
        {
          "name": "Alice Oh",
          "affiliation": null
        }
      ],
      "abstract": "Code-switching, alternating between languages within a conversation, is natural for multilingual users, yet poses fundamental challenges for large language models (LLMs). When a user code-switches in their prompt to an LLM, they typically do not specify the expected language of the LLM response, and thus LLMs must infer the output language from contextual and pragmatic cues. We find that current LLMs systematically fail to align with this expectation, responding in undesired languages even when cues are clear to humans. We introduce OLA, a benchmark to evaluate LLMs' Output Language Alignment in code-switched interactions. OLA focuses on Korean--English code-switching and spans simple intra-sentential mixing to instruction-content mismatches. Even frontier models frequently misinterpret implicit language expectation, exhibiting a bias toward non-English responses. We further show this bias generalizes beyond Korean to Chinese and Indonesian pairs. Models also show instability through mid-response switching and language intrusions. Chain-of-Thought prompting fails to resolve these errors, indicating weak pragmatic reasoning about output language. However, Code-Switching Aware DPO with minimal data (about 1K examples) substantially reduces misalignment, suggesting these failures stem from insufficient alignment rather than fundamental limitations. Our results highlight the need to align multilingual LLMs with users' implicit expectations in real-world code-switched interactions.",
      "publishedDate": "2026-01-07T05:07:22Z",
      "updatedDate": "2026-01-07T05:07:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03589v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03589",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03570",
      "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
      "authors": [
        {
          "name": "Barry Menglong Yao",
          "affiliation": null
        },
        {
          "name": "Sha Li",
          "affiliation": null
        },
        {
          "name": "Yunzhi Yao",
          "affiliation": null
        },
        {
          "name": "Minqian Liu",
          "affiliation": null
        },
        {
          "name": "Zaishuo Xia",
          "affiliation": null
        },
        {
          "name": "Qifan Wang",
          "affiliation": null
        },
        {
          "name": "Lifu Huang",
          "affiliation": null
        }
      ],
      "abstract": "Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.",
      "publishedDate": "2026-01-07T04:29:15Z",
      "updatedDate": "2026-01-07T04:29:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03570v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03570",
      "comment": "12 pages, 19 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03559",
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "authors": [
        {
          "name": "Shidong Cao",
          "affiliation": null
        },
        {
          "name": "Hongzhan Lin",
          "affiliation": null
        },
        {
          "name": "Yuxuan Gu",
          "affiliation": null
        },
        {
          "name": "Ziyang Luo",
          "affiliation": null
        },
        {
          "name": "Jing Ma",
          "affiliation": null
        }
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.",
      "publishedDate": "2026-01-07T03:58:42Z",
      "updatedDate": "2026-01-07T03:58:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03559v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03559",
      "comment": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03555",
      "title": "SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models",
      "authors": [
        {
          "name": "Yuxuan Jiang",
          "affiliation": null
        },
        {
          "name": "Francis Ferraro",
          "affiliation": null
        }
      ],
      "abstract": "Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance. Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions. Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.",
      "publishedDate": "2026-01-07T03:49:48Z",
      "updatedDate": "2026-01-07T03:49:48Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03555v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03555",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation",
        "planning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03550",
      "title": "ReEfBench: Quantifying the Reasoning Efficiency of LLMs",
      "authors": [
        {
          "name": "Zhizhang Fu",
          "affiliation": null
        },
        {
          "name": "Yuancheng Gu",
          "affiliation": null
        },
        {
          "name": "Chenkai Hu",
          "affiliation": null
        },
        {
          "name": "Hanmeng Liu",
          "affiliation": null
        },
        {
          "name": "Yue Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.",
      "publishedDate": "2026-01-07T03:33:07Z",
      "updatedDate": "2026-01-07T03:33:07Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03550v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03550",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03542",
      "title": "Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Xukai Liu",
          "affiliation": null
        },
        {
          "name": "Ye Liu",
          "affiliation": null
        },
        {
          "name": "Jipeng Zhang",
          "affiliation": null
        },
        {
          "name": "Yanghai Zhang",
          "affiliation": null
        },
        {
          "name": "Kai Zhang",
          "affiliation": null
        },
        {
          "name": "Qi Liu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) perform well on multi-hop reasoning, yet how they internally compose multiple facts remains unclear. Recent work proposes \\emph{hop-aligned circuit hypothesis}, suggesting that bridge entities are computed sequentially across layers before later-hop answers. Through systematic analyses on real-world multi-hop queries, we show that this hop-aligned assumption does not generalize: later-hop answer entities can become decodable earlier than bridge entities, a phenomenon we call \\emph{layer-order inversion}, which strengthens with total hops. To explain this behavior, we propose a \\emph{probabilistic recall-and-extract} framework that models multi-hop reasoning as broad probabilistic recall in shallow MLP layers followed by selective extraction in deeper attention layers. This framework is empirically validated through systematic probing analyses, reinterpreting prior layer-wise decoding evidence, explaining chain-of-thought gains, and providing a mechanistic diagnosis of multi-hop failures despite correct single-hop knowledge. Code is available at https://github.com/laquabe/Layer-Order-Inversion.",
      "publishedDate": "2026-01-07T03:13:03Z",
      "updatedDate": "2026-01-07T03:13:03Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03542v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03542",
      "comment": "16 pages, 18 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03537",
      "title": "STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules",
      "authors": [
        {
          "name": "Di Wu",
          "affiliation": null
        },
        {
          "name": "Yanyan Zhao",
          "affiliation": null
        },
        {
          "name": "Xin Lu",
          "affiliation": null
        },
        {
          "name": "Mingzhe Li",
          "affiliation": null
        },
        {
          "name": "Bing Qin",
          "affiliation": null
        }
      ],
      "abstract": "Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \\textbf{STAR-S} (\\textbf{S}elf-\\textbf{TA}ught \\textbf{R}easoning based on \\textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.",
      "publishedDate": "2026-01-07T03:06:55Z",
      "updatedDate": "2026-01-07T03:06:55Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03537v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03537",
      "comment": "19 pages,4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03534",
      "title": "Persona-aware and Explainable Bikeability Assessment: A Vision-Language Model Approach",
      "authors": [
        {
          "name": "Yilong Dai",
          "affiliation": null
        },
        {
          "name": "Ziyi Wang",
          "affiliation": null
        },
        {
          "name": "Chenguang Wang",
          "affiliation": null
        },
        {
          "name": "Kexin Zhou",
          "affiliation": null
        },
        {
          "name": "Yiheng Qian",
          "affiliation": null
        },
        {
          "name": "Susu Xu",
          "affiliation": null
        },
        {
          "name": "Xiang Yan",
          "affiliation": null
        }
      ],
      "abstract": "Bikeability assessment is essential for advancing sustainable urban transportation and creating cyclist-friendly cities, and it requires incorporating users' perceptions of safety and comfort. Yet existing perception-based bikeability assessment approaches face key limitations in capturing the complexity of road environments and adequately accounting for heterogeneity in subjective user perceptions. This paper proposes a persona-aware Vision-Language Model framework for bikeability assessment with three novel contributions: (i) theory-grounded persona conditioning based on established cyclist typology that generates persona-specific explanations via chain-of-thought reasoning; (ii) multi-granularity supervised fine-tuning that combines scarce expert-annotated reasoning with abundant user ratings for joint prediction and explainable assessment; and (iii) AI-enabled data augmentation that creates controlled paired data to isolate infrastructure variable impacts. To test and validate this framework, we developed a panoramic image-based crowdsourcing system and collected 12,400 persona-conditioned assessments from 427 cyclists. Experiment results show that the proposed framework offers competitive bikeability rating prediction while uniquely enabling explainable factor attribution.",
      "publishedDate": "2026-01-07T02:46:51Z",
      "updatedDate": "2026-01-07T02:46:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03534v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03534",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03531",
      "title": "PALM-Bench: A Comprehensive Benchmark for Personalized Audio-Language Models",
      "authors": [
        {
          "name": "Yuwen Wang",
          "affiliation": null
        },
        {
          "name": "Xinyuan Qian",
          "affiliation": null
        },
        {
          "name": "Tian-Hao Zhang",
          "affiliation": null
        },
        {
          "name": "Jiaran Gao",
          "affiliation": null
        },
        {
          "name": "Yuchen Pan",
          "affiliation": null
        },
        {
          "name": "Xin Wang",
          "affiliation": null
        },
        {
          "name": "Zhou Pan",
          "affiliation": null
        },
        {
          "name": "Chen Wei",
          "affiliation": null
        },
        {
          "name": "Yiming Wang",
          "affiliation": null
        }
      ],
      "abstract": "Large Audio-Language Models (LALMs) have demonstrated strong performance in audio understanding and generation. Yet, our extensive benchmarking reveals that their behavior is largely generic (e.g., summarizing spoken content) and fails to adequately support personalized question answering (e.g., summarizing what my best friend says). In contrast, human conditions their interpretation and decision-making on each individual's personal context. To bridge this gap, we formalize the task of Personalized LALMs (PALM) for recognizing personal concepts and reasoning within personal context. Moreover, we create the first benchmark (PALM-Bench) to foster the methodological advances in PALM and enable structured evaluation on several tasks across multi-speaker scenarios. Our extensive experiments on representative open-source LALMs, show that existing training-free prompting and supervised fine-tuning strategies, while yield improvements, remains limited in modeling personalized knowledge and transferring them across tasks robustly. Data and code will be released.",
      "publishedDate": "2026-01-07T02:44:38Z",
      "updatedDate": "2026-01-07T02:44:38Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03531v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03531",
      "comment": "Under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03519",
      "title": "A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving",
      "authors": [
        {
          "name": "Liangdong Zhang",
          "affiliation": null
        },
        {
          "name": "Yiming Nie",
          "affiliation": null
        },
        {
          "name": "Haoyang Li",
          "affiliation": null
        },
        {
          "name": "Fanjie Kong",
          "affiliation": null
        },
        {
          "name": "Baobao Zhang",
          "affiliation": null
        },
        {
          "name": "Shunxin Huang",
          "affiliation": null
        },
        {
          "name": "Kai Fu",
          "affiliation": null
        },
        {
          "name": "Chen Min",
          "affiliation": null
        },
        {
          "name": "Liang Xiao",
          "affiliation": null
        }
      ],
      "abstract": "Efficient trajectory planning in off-road terrains presents a formidable challenge for autonomous vehicles, often necessitating complex multi-step pipelines. However, traditional approaches exhibit limited adaptability in dynamic environments. To address these limitations, this paper proposes OFF-EMMA, a novel end-to-end multimodal framework designed to overcome the deficiencies of insufficient spatial perception and unstable reasoning in visual-language-action (VLA) models for off-road autonomous driving scenarios. The framework explicitly annotates input images through the design of a visual prompt block and introduces a chain-of-thought with self-consistency (COT-SC) reasoning strategy to enhance the accuracy and robustness of trajectory planning. The visual prompt block utilizes semantic segmentation masks as visual prompts, enhancing the spatial understanding ability of pre-trained visual-language models for complex terrains. The COT- SC strategy effectively mitigates the error impact of outliers on planning performance through a multi-path reasoning mechanism. Experimental results on the RELLIS-3D off-road dataset demonstrate that OFF-EMMA significantly outperforms existing methods, reducing the average L2 error of the Qwen backbone model by 13.3% and decreasing the failure rate from 16.52% to 6.56%.",
      "publishedDate": "2026-01-07T02:08:18Z",
      "updatedDate": "2026-01-12T02:37:04Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03519v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03519",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "agents",
        "planning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "agents",
          "planning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03515",
      "title": "Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents",
      "authors": [
        {
          "name": "Yuanchen Bei",
          "affiliation": null
        },
        {
          "name": "Tianxin Wei",
          "affiliation": null
        },
        {
          "name": "Xuying Ning",
          "affiliation": null
        },
        {
          "name": "Yanjun Zhao",
          "affiliation": null
        },
        {
          "name": "Zhining Liu",
          "affiliation": null
        },
        {
          "name": "Xiao Lin",
          "affiliation": null
        },
        {
          "name": "Yada Zhu",
          "affiliation": null
        },
        {
          "name": "Hendrik Hamann",
          "affiliation": null
        },
        {
          "name": "Jingrui He",
          "affiliation": null
        },
        {
          "name": "Hanghang Tong",
          "affiliation": null
        }
      ],
      "abstract": "Long-term memory is a critical capability for multimodal large language model (MLLM) agents, particularly in conversational settings where information accumulates and evolves over time. However, existing benchmarks either evaluate multi-session memory in text-only conversations or assess multimodal understanding within localized contexts, failing to evaluate how multimodal memory is preserved, organized, and evolved across long-term conversational trajectories. Thus, we introduce Mem-Gallery, a new benchmark for evaluating multimodal long-term conversational memory in MLLM agents. Mem-Gallery features high-quality multi-session conversations grounded in both visual and textual information, with long interaction horizons and rich multimodal dependencies. Building on this dataset, we propose a systematic evaluation framework that assesses key memory capabilities along three functional dimensions: memory extraction and test-time adaptation, memory reasoning, and memory knowledge management. Extensive benchmarking across thirteen memory systems reveals several key findings, highlighting the necessity of explicit multimodal information retention and memory organization, the persistent limitations in memory reasoning and knowledge management, as well as the efficiency bottleneck of current models.",
      "publishedDate": "2026-01-07T02:03:13Z",
      "updatedDate": "2026-01-07T02:03:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03515v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03515",
      "comment": "34 pages, 18 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03500",
      "title": "SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models",
      "authors": [
        {
          "name": "Yuxuan Xia",
          "affiliation": null
        },
        {
          "name": "Siheng Wang",
          "affiliation": null
        },
        {
          "name": "Peng Li",
          "affiliation": null
        }
      ],
      "abstract": "Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.",
      "publishedDate": "2026-01-07T01:27:58Z",
      "updatedDate": "2026-01-07T01:27:58Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03500v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03500",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03471",
      "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
      "authors": [
        {
          "name": "Mingyang Wei",
          "affiliation": null
        },
        {
          "name": "Dehai Min",
          "affiliation": null
        },
        {
          "name": "Zewen Liu",
          "affiliation": null
        },
        {
          "name": "Yuzhang Xie",
          "affiliation": null
        },
        {
          "name": "Guanchen Wu",
          "affiliation": null
        },
        {
          "name": "Carl Yang",
          "affiliation": null
        },
        {
          "name": "Max S. Y. Lau",
          "affiliation": null
        },
        {
          "name": "Qi He",
          "affiliation": null
        },
        {
          "name": "Lu Cheng",
          "affiliation": null
        },
        {
          "name": "Wei Jin",
          "affiliation": null
        }
      ],
      "abstract": "Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.",
      "publishedDate": "2026-01-06T23:49:10Z",
      "updatedDate": "2026-01-06T23:49:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03471v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03471",
      "comment": "21 pages, 3 figures, 12 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03458",
      "title": "Automated Feedback Generation for Undergraduate Mathematics: Development and Evaluation of an AI Teaching Assistant",
      "authors": [
        {
          "name": "Aron Gohr",
          "affiliation": null
        },
        {
          "name": "Marie-Amelie Lawn",
          "affiliation": null
        },
        {
          "name": "Kevin Gao",
          "affiliation": null
        },
        {
          "name": "Inigo Serjeant",
          "affiliation": null
        },
        {
          "name": "Stephen Heslip",
          "affiliation": null
        }
      ],
      "abstract": "Intelligent tutoring systems have long enabled automated immediate feedback on student work when it is presented in a tightly structured format and when problems are very constrained, but reliably assessing free-form mathematical reasoning remains challenging. We present a system that processes free-form natural language input, handles a wide range of edge cases, and comments competently not only on the technical correctness of submitted proofs, but also on style and presentation issues. We discuss the advantages and disadvantages of various approaches to the evaluation of such a system, and show that by the metrics we evaluate, the quality of the feedback generated is comparable to that produced by human experts when assessing early undergraduate homework. We stress-test our system with a small set of more advanced and unusual questions, and report both significant gaps and encouraging successes in that more challenging setting. Our system uses large language models in a modular workflow. The workflow configuration is human-readable and editable without programming knowledge, and allows some intermediate steps to be precomputed or injected by the instructor. A version of our tool is deployed on the Imperial mathematics homework platform Lambdafeedback. We report also on the integration of our tool into this platform.",
      "publishedDate": "2026-01-06T23:02:22Z",
      "updatedDate": "2026-01-06T23:02:22Z",
      "primaryCategory": "cs.CY",
      "arxivCategories": [
        "cs.CY",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03458v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03458",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03449",
      "title": "FIRE-VLM: A Vision-Language-Driven Reinforcement Learning Framework for UAV Wildfire Tracking in a Physics-Grounded Fire Digital Twin",
      "authors": [
        {
          "name": "Chris Webb",
          "affiliation": null
        },
        {
          "name": "Mobin Habibpour",
          "affiliation": null
        },
        {
          "name": "Mayamin Hamid Raha",
          "affiliation": null
        },
        {
          "name": "Ali Reza Tavakkoli",
          "affiliation": null
        },
        {
          "name": "Janice Coen",
          "affiliation": null
        },
        {
          "name": "Fatemeh Afghah",
          "affiliation": null
        }
      ],
      "abstract": "Wildfire monitoring demands autonomous systems capable of reasoning under extreme visual degradation, rapidly evolving physical dynamics, and scarce real-world training data. Existing UAV navigation approaches rely on simplified simulators and supervised perception pipelines, and lack embodied agents interacting with physically realistic fire environments. We introduce FIRE-VLM, the first end-to-end vision-language model (VLM) guided reinforcement learning (RL) framework trained entirely within a high-fidelity, physics-grounded wildfire digital twin. Built from USGS Digital Elevation Model (DEM) terrain, LANDFIRE fuel inventories, and semi-physical fire-spread solvers, this twin captures terrain-induced runs, wind-driven acceleration, smoke plume occlusion, and dynamic fuel consumption. Within this environment, a PPO agent with dual-view UAV sensing is guided by a CLIP-style VLM. Wildfire-specific semantic alignment scores, derived from a single prompt describing active fire and smoke plumes, are integrated as potential-based reward shaping signals. Our contributions are: (1) a GIS-to-simulation pipeline for constructing wildfire digital twins; (2) a VLM-guided RL agent for UAV firefront tracking; and (3) a wildfire-aware reward design that combines physical terms with VLM semantics. Across five digital-twin evaluation tasks, our VLM-guided policy reduces time-to-detection by up to 6 times, increases time-in-FOV, and is, to our knowledge, the first RL-based UAV wildfire monitoring system demonstrated in kilometer-scale, physics-grounded digital-twin fires.",
      "publishedDate": "2026-01-06T22:31:57Z",
      "updatedDate": "2026-01-06T22:31:57Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03449v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03449",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "prompting",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03448",
      "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
      "authors": [
        {
          "name": "Atsuki Yamaguchi",
          "affiliation": null
        },
        {
          "name": "Maggie Mi",
          "affiliation": null
        },
        {
          "name": "Nikolaos Aletras",
          "affiliation": null
        }
      ],
      "abstract": "Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks.",
      "publishedDate": "2026-01-06T22:28:15Z",
      "updatedDate": "2026-01-06T22:28:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03448v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03448",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03436",
      "title": "MARVEL: A Multi Agent-based Research Validator and Enabler using Large Language Models",
      "authors": [
        {
          "name": "Nikhil Mukund",
          "affiliation": null
        },
        {
          "name": "Yifang Luo",
          "affiliation": null
        },
        {
          "name": "Fan Zhang",
          "affiliation": null
        },
        {
          "name": "Lisa Barsotti",
          "affiliation": null
        },
        {
          "name": "Erik Katsavounidis",
          "affiliation": null
        }
      ],
      "abstract": "We present MARVEL (https://ligogpt.mit.edu/marvel), a locally deployable, open-source framework for domain-aware question answering and assisted scientific research. It is designed to address the increasing demands of a digital assistant for scientific groups that can read highly technical data, cite precisely, and operate within authenticated networks. MARVEL combines a fast path for straightforward queries with a more deliberate DeepSearch mode that integrates retrieval-augmented generation and Monte Carlo Tree Search. It explores complementary subqueries, allocates more compute to promising branches, and maintains a global evidence ledger that preserves sources during drafting. We applied this framework in the context of gravitational-wave research related to the Laser Interferometer Gravitational-wave Observatory. Answers are grounded in a curated semantic index of research literature, doctoral theses, LIGO documents, and long-running detector electronic logbooks, with targeted web searches when appropriate. Because direct benchmarking against commercial LLMs cannot be performed on private data, we evaluated MARVEL on two publicly available surrogate datasets that capture comparable semantic and technical characteristics. On these benchmarks, MARVEL matches a GPT-4o mini baseline on literature-centric queries and substantially outperforms it on detector-operations content, where domain retrieval and guided reasoning are decisive. By making the complete framework and evaluation datasets openly available, we aim to provide a reproducible foundation for developing domain-specific scientific assistants.",
      "publishedDate": "2026-01-06T21:47:22Z",
      "updatedDate": "2026-01-06T21:47:22Z",
      "primaryCategory": "astro-ph.IM",
      "arxivCategories": [
        "astro-ph.IM",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03436v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03436",
      "comment": "18 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03434",
      "title": "VNU-Bench: A Benchmarking Dataset for Multi-Source Multimodal News Video Understanding",
      "authors": [
        {
          "name": "Zibo Liu",
          "affiliation": null
        },
        {
          "name": "Muyang Li",
          "affiliation": null
        },
        {
          "name": "Zhe Jiang",
          "affiliation": null
        },
        {
          "name": "Shigang Chen",
          "affiliation": null
        }
      ],
      "abstract": "News videos are carefully edited multimodal narratives that combine narration, visuals, and external quotations into coherent storylines. In recent years, there have been significant advances in evaluating multimodal large language models (MLLMs) for news video understanding. However, existing benchmarks largely focus on single-source, intra-video reasoning, where each report is processed in isolation. In contrast, real-world news consumption is inherently multi-sourced: the same event is reported by different outlets with complementary details, distinct narrative choices, and sometimes conflicting claims that unfold over time. Robust news understanding, therefore, requires models to compare perspectives from different sources, align multimodal evidence across sources, and synthesize multi-source information. To fill this gap, we introduce VNU-Bench, the first benchmark for multi-source, cross-video understanding in the news domain. We design a set of new question types that are unique in testing models' ability of understanding multi-source multimodal news from a variety of different angles. We design a novel hybrid human-model QA generation process that addresses the issues of scalability and quality control in building a large dataset for cross-source news understanding. The dataset comprises 429 news groups, 1,405 videos, and 2,501 high-quality questions. Comprehensive evaluation of both closed- and open-source multimodal models shows that VNU-Bench poses substantial challenges for current MLLMs.",
      "publishedDate": "2026-01-06T21:42:44Z",
      "updatedDate": "2026-01-06T21:42:44Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03434v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03434",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03432",
      "title": "CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models",
      "authors": [
        {
          "name": "Danny Brahman",
          "affiliation": null
        },
        {
          "name": "Mohammad Mahoor",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains a significant gap in evaluating their code generation capabilities. Existing benchmark datasets fall short in pinpointing specific strengths and weaknesses, impeding targeted enhancements in models' reasoning abilities to synthesize code. To bridge this gap, our paper introduces an innovative, pedagogical benchmarking method that mirrors the evaluation processes encountered in academic programming courses. We introduce CodeEval, a multi-dimensional benchmark dataset designed to rigorously evaluate LLMs across 24 distinct aspects of Python programming. The dataset covers three proficiency levels - beginner, intermediate, and advanced - and includes both class-based and function-based problem types with detailed problem specifications and comprehensive test suites. To facilitate widespread adoption, we also developed RunCodeEval, an open-source execution framework that provides researchers with a ready-to-use evaluation pipeline for CodeEval. RunCodeEval handles test execution, context setup, and metrics generation, enabling researchers to quickly obtain detailed insights into model strengths and weaknesses across complexity levels, problem types, and programming categories. This combination enables targeted evaluation and guides improvements in LLMs' programming proficiencies.",
      "publishedDate": "2026-01-06T21:42:01Z",
      "updatedDate": "2026-01-06T21:42:01Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03432v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03432",
      "comment": "Accepted at the International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics, 2025. Will be published at ACL anthology",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03423",
      "title": "Training-Free Adaptation of New-Generation LLMs using Legacy Clinical Models",
      "authors": [
        {
          "name": "Sasha Ronaghi",
          "affiliation": null
        },
        {
          "name": "Chloe Stanwyck",
          "affiliation": null
        },
        {
          "name": "Asad Aali",
          "affiliation": null
        },
        {
          "name": "Amir Ronaghi",
          "affiliation": null
        },
        {
          "name": "Miguel Fuentes",
          "affiliation": null
        },
        {
          "name": "Tina Hernandez-Boussard",
          "affiliation": null
        },
        {
          "name": "Emily Alsentzer",
          "affiliation": null
        }
      ],
      "abstract": "Adapting language models to the clinical domain through continued pretraining and fine-tuning requires costly retraining for each new model generation. We propose Cross-Architecture Proxy Tuning (CAPT), a model-ensembling approach that enables training-free adaptation of state-of-the-art general-domain models using existing clinical models. CAPT supports models with disjoint vocabularies, leveraging contrastive decoding to selectively inject clinically relevant signals while preserving the general-domain model's reasoning and fluency. On six clinical classification and text-generation tasks, CAPT with a new-generation general-domain model and an older-generation clinical model consistently outperforms both models individually and state-of-the-art ensembling approaches (average +17.6% over UniTE, +41.4% over proxy tuning across tasks). Through token-level analysis and physician case studies, we demonstrate that CAPT amplifies clinically actionable language, reduces context errors, and increases clinical specificity.",
      "publishedDate": "2026-01-06T21:23:47Z",
      "updatedDate": "2026-01-06T21:23:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03423v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03423",
      "comment": "29 pages, 3 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03416",
      "title": "GAMBIT: A Gamified Jailbreak Framework for Multimodal Large Language Models",
      "authors": [
        {
          "name": "Xiangdong Hu",
          "affiliation": null
        },
        {
          "name": "Yangyang Jiang",
          "affiliation": null
        },
        {
          "name": "Qin Hu",
          "affiliation": null
        },
        {
          "name": "Xiaojun Jia",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have become widely deployed, yet their safety alignment remains fragile under adversarial inputs. Previous work has shown that increasing inference steps can disrupt safety mechanisms and lead MLLMs to generate attacker-desired harmful content. However, most existing attacks focus on increasing the complexity of the modified visual task itself and do not explicitly leverage the model's own reasoning incentives. This leads to them underperforming on reasoning models (Models with Chain-of-Thoughts) compared to non-reasoning ones (Models without Chain-of-Thoughts). If a model can think like a human, can we influence its cognitive-stage decisions so that it proactively completes a jailbreak? To validate this idea, we propose GAMBI} (Gamified Adversarial Multimodal Breakout via Instructional Traps), a novel multimodal jailbreak framework that decomposes and reassembles harmful visual semantics, then constructs a gamified scene that drives the model to explore, reconstruct intent, and answer as part of winning the game. The resulting structured reasoning chain increases task complexity in both vision and text, positioning the model as a participant whose goal pursuit reduces safety attention and induces it to answer the reconstructed malicious query. Extensive experiments on popular reasoning and non-reasoning MLLMs demonstrate that GAMBIT achieves high Attack Success Rates (ASR), reaching 92.13% on Gemini 2.5 Flash, 91.20% on QvQ-MAX, and 85.87% on GPT-4o, significantly outperforming baselines.",
      "publishedDate": "2026-01-06T21:09:10Z",
      "updatedDate": "2026-01-06T21:09:10Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03416v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03416",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03400",
      "title": "Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning",
      "authors": [
        {
          "name": "Ali Najar",
          "affiliation": null
        },
        {
          "name": "Alireza Mirrokni",
          "affiliation": null
        },
        {
          "name": "Arshia Izadyari",
          "affiliation": null
        },
        {
          "name": "Sadegh Mohammadian",
          "affiliation": null
        },
        {
          "name": "Amir Homayoon Sharifizade",
          "affiliation": null
        },
        {
          "name": "Asal Meskin",
          "affiliation": null
        },
        {
          "name": "Mobin Bagherian",
          "affiliation": null
        },
        {
          "name": "Ehsaneddin Asgari",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models' ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.",
      "publishedDate": "2026-01-06T20:27:29Z",
      "updatedDate": "2026-01-06T20:27:29Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03400v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03400",
      "comment": "8 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03388",
      "title": "Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models",
      "authors": [
        {
          "name": "Zhibo Hu",
          "affiliation": null
        },
        {
          "name": "Chen Wang",
          "affiliation": null
        },
        {
          "name": "Yanfeng Shu",
          "affiliation": null
        },
        {
          "name": "Hye-young Paik",
          "affiliation": null
        },
        {
          "name": "Liming Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Earlier research has shown that metaphors influence human's decision making, which raises the question of whether metaphors also influence large language models (LLMs)' reasoning pathways, considering their training data contain a large number of metaphors. In this work, we investigate the problem in the scope of the emergent misalignment problem where LLMs can generalize patterns learned from misaligned content in one domain to another domain. We discover a strong causal relationship between metaphors in training data and the misalignment degree of LLMs' reasoning contents. With interventions using metaphors in pre-training, fine-tuning and re-alignment phases, models' cross-domain misalignment degrees change significantly. As we delve deeper into the causes behind this phenomenon, we observe that there is a connection between metaphors and the activation of global and local latent features of large reasoning models. By monitoring these latent features, we design a detector that predict misaligned content with high accuracy.",
      "publishedDate": "2026-01-06T19:50:58Z",
      "updatedDate": "2026-01-12T18:08:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03388v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03388",
      "comment": "17 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03369",
      "title": "RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models",
      "authors": [
        {
          "name": "Sha Luo",
          "affiliation": null
        },
        {
          "name": "Yogesh Prabhu",
          "affiliation": null
        },
        {
          "name": "Tim Ossowski",
          "affiliation": null
        },
        {
          "name": "Kaiping Chen",
          "affiliation": null
        },
        {
          "name": "Junjie Hu",
          "affiliation": null
        }
      ],
      "abstract": "With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.",
      "publishedDate": "2026-01-06T19:14:49Z",
      "updatedDate": "2026-01-06T19:14:49Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03369v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03369",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03331",
      "title": "MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models",
      "authors": [
        {
          "name": "Yang Shi",
          "affiliation": null
        },
        {
          "name": "Yifeng Xie",
          "affiliation": null
        },
        {
          "name": "Minzhe Guo",
          "affiliation": null
        },
        {
          "name": "Liangsi Lu",
          "affiliation": null
        },
        {
          "name": "Mingxuan Huang",
          "affiliation": null
        },
        {
          "name": "Jingchao Wang",
          "affiliation": null
        },
        {
          "name": "Zhihong Zhu",
          "affiliation": null
        },
        {
          "name": "Boyan Xu",
          "affiliation": null
        },
        {
          "name": "Zhiqi Huang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in Vision-Language Models (VLMs) have improved performance in multi-modal learning, raising the question of whether these models truly understand the content they process. Crucially, can VLMs detect when a reasoning process is wrong and identify its error type? To answer this, we present MMErroR, a multi-modal benchmark of 2,013 samples, each embedding a single coherent reasoning error. These samples span 24 subdomains across six top-level domains, ensuring broad coverage and taxonomic richness. Unlike existing benchmarks that focus on answer correctness, MMErroR targets a process-level, error-centric evaluation that requires models to detect incorrect reasoning and classify the error type within both visual and linguistic contexts. We evaluate 20 advanced VLMs, even the best model (Gemini-3.0-Pro) classifies the error in only 66.47\\% of cases, underscoring the challenge of identifying erroneous reasoning. Furthermore, the ability to accurately identify errors offers valuable insights into the capabilities of multi-modal reasoning models. Project Page: https://mmerror-benchmark.github.io",
      "publishedDate": "2026-01-06T17:45:26Z",
      "updatedDate": "2026-01-06T17:45:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03331v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03331",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03321",
      "title": "Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting",
      "authors": [
        {
          "name": "Kun Zhao",
          "affiliation": null
        },
        {
          "name": "Siyuan Dai",
          "affiliation": null
        },
        {
          "name": "Pan Wang",
          "affiliation": null
        },
        {
          "name": "Jifeng Song",
          "affiliation": null
        },
        {
          "name": "Hui Ji",
          "affiliation": null
        },
        {
          "name": "Chenghua Lin",
          "affiliation": null
        },
        {
          "name": "Liang Zhan",
          "affiliation": null
        },
        {
          "name": "Haoteng Tang",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel \"Reason-then-Summarize\" architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.",
      "publishedDate": "2026-01-06T14:17:44Z",
      "updatedDate": "2026-01-12T05:56:31Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03321v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03321",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03320",
      "title": "Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning",
      "authors": [
        {
          "name": "Yu Luo",
          "affiliation": null
        },
        {
          "name": "Shuo Han",
          "affiliation": null
        },
        {
          "name": "Yihan Hu",
          "affiliation": null
        },
        {
          "name": "Dong Li",
          "affiliation": null
        },
        {
          "name": "Jianye Hao",
          "affiliation": null
        }
      ],
      "abstract": "On-policy reinforcement learning (RL), particularly Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), has become the dominant paradigm for fine-tuning large language models (LLMs). While policy ratio clipping stabilizes training, this heuristic hard constraint incurs a fundamental cost: it indiscriminately truncates gradients from high-return yet high-divergence actions, suppressing rare but highly informative \"eureka moments\" in complex reasoning. Moreover, once data becomes slightly stale, hard clipping renders it unusable, leading to severe sample inefficiency. In this work, we revisit the trust-region objective in policy optimization and show that explicitly constraining the \\emph{variance (second central moment) of the policy ratio} provides a principled and smooth relaxation of hard clipping. This distributional constraint stabilizes policy updates while preserving gradient signals from valuable trajectories. Building on this insight, we propose $R^2VPO$ (Ratio-Variance Regularized Policy Optimization), a novel primal-dual framework that supports stable on-policy learning and enables principled off-policy data reuse by dynamically reweighting stale samples rather than discarding them. We extensively evaluate $R^2VPO$ on fine-tuning state-of-the-art LLMs, including DeepSeek-Distill-Qwen-1.5B and the openPangu-Embedded series (1B and 7B), across challenging mathematical reasoning benchmarks. Experimental results show that $R^2VPO$ consistently achieves superior asymptotic performance, with average relative gains of up to 17% over strong clipping-based baselines, while requiring approximately 50% fewer rollouts to reach convergence. These findings establish ratio-variance control as a promising direction for improving both stability and data efficiency in RL-based LLM alignment.",
      "publishedDate": "2026-01-06T14:01:42Z",
      "updatedDate": "2026-01-06T14:01:42Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03320v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03320",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03287",
      "title": "Automated Post-Incident Policy Gap Analysis via Threat-Informed Evidence Mapping using Large Language Models",
      "authors": [
        {
          "name": "Huan Lin Oh",
          "affiliation": null
        },
        {
          "name": "Jay Yong Jun Jie",
          "affiliation": null
        },
        {
          "name": "Mandy Lee Ling Siu",
          "affiliation": null
        },
        {
          "name": "Jonathan Pan",
          "affiliation": null
        }
      ],
      "abstract": "Cybersecurity post-incident reviews are essential for identifying control failures and improving organisational resilience, yet they remain labour-intensive, time-consuming, and heavily reliant on expert judgment. This paper investigates whether Large Language Models (LLMs) can augment post-incident review workflows by autonomously analysing system evidence and identifying security policy gaps. We present a threat-informed, agentic framework that ingests log data, maps observed behaviours to the MITRE ATT&CK framework, and evaluates organisational security policies for adequacy and compliance. Using a simulated brute-force attack scenario against a Windows OpenSSH service (MITRE ATT&CK T1110), the system leverages GPT-4o for reasoning, LangGraph for multi-agent workflow orchestration, and LlamaIndex for traceable policy retrieval. Experimental results indicate that the LLM-based pipeline can interpret log-derived evidence, identify insufficient or missing policy controls, and generate actionable remediation recommendations with explicit evidence-to-policy traceability. Unlike prior work that treats log analysis and policy validation as isolated tasks, this study integrates both into a unified end-to-end proof-of-concept post-incident review framework. The findings suggest that LLM-assisted analysis has the potential to improve the efficiency, consistency, and auditability of post-incident evaluations, while highlighting the continued need for human oversight in high-stakes cybersecurity decision-making.",
      "publishedDate": "2026-01-04T01:39:20Z",
      "updatedDate": "2026-01-04T01:39:20Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03287v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03287",
      "comment": "5 pages, 1 figure. Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03286",
      "title": "HyperCLOVA X 32B Think",
      "authors": [
        {
          "name": "NAVER Cloud HyperCLOVA X Team",
          "affiliation": null
        }
      ],
      "abstract": "In this report, we present HyperCLOVA X 32B Think, a vision-language model designed with particular emphasis on reasoning within the Korean linguistic and cultural context, as well as agentic ability. HyperCLOVA X 32B Think is pre-trained with a strong focus on reasoning capabilities and subsequently post-trained to support multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences. Experimental evaluations against comparably sized models demonstrate that our model achieves strong performance on Korean text-to-text and vision-to-text benchmarks, as well as on agent-oriented evaluation tasks. By open-sourcing HyperCLOVA X 32B Think, we aim to support broader adoption and facilitate further research and innovation across both academic and industrial communities.",
      "publishedDate": "2026-01-03T06:39:38Z",
      "updatedDate": "2026-01-03T06:39:38Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03286v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03286",
      "comment": "Technical Report",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03285",
      "title": "Feedback Indices to Evaluate LLM Responses to Rebuttals for Multiple Choice Type Questions",
      "authors": [
        {
          "name": "Justin C. Dunlap",
          "affiliation": null
        },
        {
          "name": "Anne-Simone Parent",
          "affiliation": null
        },
        {
          "name": "Ralf Widenhorn",
          "affiliation": null
        }
      ],
      "abstract": "We present a systematic framework of indices designed to characterize Large Language Model (LLM) responses when challenged with rebuttals during a chat. Assessing how LLMs respond to user dissent is crucial for understanding their reliability and behavior patterns, yet the complexity of human-LLM interactions makes systematic evaluation challenging. Our approach employs a fictitious-response rebuttal method that quantifies LLM behavior when presented with multiple-choice questions followed by deliberate challenges to their fictitious previous response. The indices are specifically designed to detect and measure what could be characterized as sycophantic behavior (excessive agreement with user challenges) or stubborn responses (rigid adherence to the fictitious response in the chat history) from LLMs. These metrics allow investigation of the relationships between sycophancy, stubbornness, and the model's actual mastery of the subject matter. We demonstrate the utility of these indices using two physics problems as test scenarios with various OpenAI models. The framework is intentionally generalizable to any multiple-choice format question, including on topics without universally accepted correct answers. Our results reveal measurable differences across OpenAI model generations, with trends indicating that newer models and those employing greater \"Reasoning Effort\" exhibit reduced sycophantic behavior. The FR pairing method combined with our proposed indices provides a practical, adaptable toolkit for systematically comparing LLM dialogue behaviors across different models and contexts.",
      "publishedDate": "2026-01-02T21:16:43Z",
      "updatedDate": "2026-01-02T21:16:43Z",
      "primaryCategory": "physics.ed-ph",
      "arxivCategories": [
        "physics.ed-ph",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03285v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03285",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03281",
      "title": "$α^3$-Bench: A Unified Benchmark of Safety, Robustness, and Efficiency for LLM-Based UAV Agents over 6G Networks",
      "authors": [
        {
          "name": "Mohamed Amine Ferrag",
          "affiliation": null
        },
        {
          "name": "Abderrahmane Lakas",
          "affiliation": null
        },
        {
          "name": "Merouane Debbah",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used as high level controllers for autonomous Unmanned Aerial Vehicle (UAV) missions. However, existing evaluations rarely assess whether such agents remain safe, protocol compliant, and effective under realistic next generation networking constraints. This paper introduces $α^3$-Bench, a benchmark for evaluating LLM driven UAV autonomy as a multi turn conversational reasoning and control problem operating under dynamic 6G conditions. Each mission is formulated as a language mediated control loop between an LLM based UAV agent and a human operator, where decisions must satisfy strict schema validity, mission policies, speaker alternation, and safety constraints while adapting to fluctuating network slices, latency, jitter, packet loss, throughput, and edge load variations. To reflect modern agentic workflows, $α^3$-Bench integrates a dual action layer supporting both tool calls and agent to agent coordination, enabling evaluation of tool use consistency and multi agent interactions. We construct a large scale corpus of 113k conversational UAV episodes grounded in UAVBench scenarios and evaluate 17 state of the art LLMs using a fixed subset of 50 episodes per scenario under deterministic decoding. We propose a composite $α^3$ metric that unifies six pillars: Task Outcome, Safety Policy, Tool Consistency, Interaction Quality, Network Robustness, and Communication Cost, with efficiency normalized scores per second and per thousand tokens. Results show that while several models achieve high mission success and safety compliance, robustness and efficiency vary significantly under degraded 6G conditions, highlighting the need for network aware and resource efficient LLM based UAV agents. The dataset is publicly available on GitHub : https://github.com/maferrag/AlphaBench",
      "publishedDate": "2026-01-01T12:07:06Z",
      "updatedDate": "2026-01-01T12:07:06Z",
      "primaryCategory": "eess.SY",
      "arxivCategories": [
        "eess.SY",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03281v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03281",
      "comment": "20 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent",
        "tool-use",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "multi-agent",
          "tool-use",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03509",
      "title": "Evolving Programmatic Skill Networks",
      "authors": [
        {
          "name": "Haochen Shi",
          "affiliation": null
        },
        {
          "name": "Xingdi Yuan",
          "affiliation": null
        },
        {
          "name": "Bang Liu",
          "affiliation": null
        }
      ],
      "abstract": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.",
      "publishedDate": "2026-01-07T01:43:25Z",
      "updatedDate": "2026-01-07T01:43:25Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03509v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03509",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03398",
      "title": "Towards Zero-Knowledge Task Planning via a Language-based Approach",
      "authors": [
        {
          "name": "Liam Merz Hoffmeister",
          "affiliation": null
        },
        {
          "name": "Brian Scassellati",
          "affiliation": null
        },
        {
          "name": "Daniel Rakita",
          "affiliation": null
        }
      ],
      "abstract": "In this work, we introduce and formalize the Zero-Knowledge Task Planning (ZKTP) problem, i.e., formulating a sequence of actions to achieve some goal without task-specific knowledge. Additionally, we present a first investigation and approach for ZKTP that leverages a large language model (LLM) to decompose natural language instructions into subtasks and generate behavior trees (BTs) for execution. If errors arise during task execution, the approach also uses an LLM to adjust the BTs on-the-fly in a refinement loop. Experimental validation in the AI2-THOR simulator demonstrate our approach's effectiveness in improving overall task performance compared to alternative approaches that leverage task-specific knowledge. Our work demonstrates the potential of LLMs to effectively address several aspects of the ZKTP problem, providing a robust framework for automated behavior generation with no task-specific setup.",
      "publishedDate": "2026-01-06T20:18:15Z",
      "updatedDate": "2026-01-06T20:18:15Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03398v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03398",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "planning",
        "agents",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "planning",
          "agents",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03641",
      "title": "Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning",
      "authors": [
        {
          "name": "Zheng Wu",
          "affiliation": null
        },
        {
          "name": "Xingyu Lou",
          "affiliation": null
        },
        {
          "name": "Xinbei Ma",
          "affiliation": null
        },
        {
          "name": "Yansi Li",
          "affiliation": null
        },
        {
          "name": "Weiwen Liu",
          "affiliation": null
        },
        {
          "name": "Weinan Zhang",
          "affiliation": null
        },
        {
          "name": "Jun Wang",
          "affiliation": null
        },
        {
          "name": "Zhuosheng Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates. The codes are available at https://github.com/Wuzheng02/Agent-Dice.",
      "publishedDate": "2026-01-07T06:43:50Z",
      "updatedDate": "2026-01-08T08:36:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03641v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03641",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "agents"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04034",
      "title": "HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense",
      "authors": [
        {
          "name": "Siyuan Li",
          "affiliation": null
        },
        {
          "name": "Xi Lin",
          "affiliation": null
        },
        {
          "name": "Jun Wu",
          "affiliation": null
        },
        {
          "name": "Zehao Liu",
          "affiliation": null
        },
        {
          "name": "Haoyu Li",
          "affiliation": null
        },
        {
          "name": "Tianjie Ju",
          "affiliation": null
        },
        {
          "name": "Xiang Chen",
          "affiliation": null
        },
        {
          "name": "Jianhua Li",
          "affiliation": null
        }
      ],
      "abstract": "Jailbreak attacks pose significant threats to large language models (LLMs), enabling attackers to bypass safeguards. However, existing reactive defense approaches struggle to keep up with the rapidly evolving multi-turn jailbreaks, where attackers continuously deepen their attacks to exploit vulnerabilities. To address this critical challenge, we propose HoneyTrap, a novel deceptive LLM defense framework leveraging collaborative defenders to counter jailbreak attacks. It integrates four defensive agents, Threat Interceptor, Misdirection Controller, Forensic Tracker, and System Harmonizer, each performing a specialized security role and collaborating to complete a deceptive defense. To ensure a comprehensive evaluation, we introduce MTJ-Pro, a challenging multi-turn progressive jailbreak dataset that combines seven advanced jailbreak strategies designed to gradually deepen attack strategies across multi-turn attacks. Besides, we present two novel metrics: Mislead Success Rate (MSR) and Attack Resource Consumption (ARC), which provide more nuanced assessments of deceptive defense beyond conventional measures. Experimental results on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1 demonstrate that HoneyTrap achieves an average reduction of 68.77% in attack success rates compared to state-of-the-art baselines. Notably, even in a dedicated adaptive attacker setting with intensified conditions, HoneyTrap remains resilient, leveraging deceptive engagement to prolong interactions, significantly increasing the time and computational costs required for successful exploitation. Unlike simple rejection, HoneyTrap strategically wastes attacker resources without impacting benign queries, improving MSR and ARC by 118.11% and 149.16%, respectively.",
      "publishedDate": "2026-01-07T15:47:28Z",
      "updatedDate": "2026-01-07T15:47:28Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04034v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04034",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03671",
      "title": "NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models",
      "authors": [
        {
          "name": "Weiqi Liu",
          "affiliation": null
        },
        {
          "name": "Yongliang Miao",
          "affiliation": null
        },
        {
          "name": "Haiyan Zhao",
          "affiliation": null
        },
        {
          "name": "Yanguang Liu",
          "affiliation": null
        },
        {
          "name": "Mengnan Du",
          "affiliation": null
        }
      ],
      "abstract": "Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations into atomic semantic components, clusters them into distinct semantic modes, and iteratively refines each explanation using neuron activation feedback. Experiments demonstrate that NeuronScope uncovers hidden polysemanticity and produces explanations with significantly higher activation correlation compared to single-pass baselines.",
      "publishedDate": "2026-01-07T07:50:47Z",
      "updatedDate": "2026-01-07T07:50:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03671v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03671",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03359",
      "title": "Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization",
      "authors": [
        {
          "name": "Alberto Purpura",
          "affiliation": null
        },
        {
          "name": "Li Wang",
          "affiliation": null
        },
        {
          "name": "Sahil Badyal",
          "affiliation": null
        },
        {
          "name": "Eugenio Beaufrand",
          "affiliation": null
        },
        {
          "name": "Adam Faulkner",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.",
      "publishedDate": "2026-01-06T19:02:14Z",
      "updatedDate": "2026-01-06T19:02:14Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03359v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03359",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "agents",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03335",
      "title": "Digital Red Queen: Adversarial Program Evolution in Core War with LLMs",
      "authors": [
        {
          "name": "Akarsh Kumar",
          "affiliation": null
        },
        {
          "name": "Ryan Bahlous-Boldi",
          "affiliation": null
        },
        {
          "name": "Prafull Sharma",
          "affiliation": null
        },
        {
          "name": "Phillip Isola",
          "affiliation": null
        },
        {
          "name": "Sebastian Risi",
          "affiliation": null
        },
        {
          "name": "Yujin Tang",
          "affiliation": null
        },
        {
          "name": "David Ha",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly being used to evolve solutions to problems in many domains, in a process inspired by biological evolution. However, unlike biological evolution, most LLM-evolution frameworks are formulated as static optimization problems, overlooking the open-ended adversarial dynamics that characterize real-world evolutionary processes. Here, we study Digital Red Queen (DRQ), a simple self-play algorithm that embraces these so-called \"Red Queen\" dynamics via continual adaptation to a changing objective. DRQ uses an LLM to evolve assembly-like programs, called warriors, which compete against each other for control of a virtual machine in the game of Core War, a Turing-complete environment studied in artificial life and connected to cybersecurity. In each round of DRQ, the model evolves a new warrior to defeat all previous ones, producing a sequence of adapted warriors. Over many rounds, we observe that warriors become increasingly general (relative to a set of held-out human warriors). Interestingly, warriors also become less behaviorally diverse across independent runs, indicating a convergence pressure toward a general-purpose behavioral strategy, much like convergent evolution in nature. This result highlights a potential value of shifting from static objectives to dynamic Red Queen objectives. Our work positions Core War as a rich, controllable sandbox for studying adversarial adaptation in artificial systems and for evaluating LLM-based evolution methods. More broadly, the simplicity and effectiveness of DRQ suggest that similarly minimal self-play approaches could prove useful in other more practical multi-agent adversarial domains, like real-world cybersecurity or combating drug resistance.",
      "publishedDate": "2026-01-06T18:58:17Z",
      "updatedDate": "2026-01-06T18:58:17Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03335v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03335",
      "comment": "14 pages, 13 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03328",
      "title": "LLM-Enabled Multi-Agent Systems: Empirical Evaluation and Insights into Emerging Design Patterns & Paradigms",
      "authors": [
        {
          "name": "Harri Renney",
          "affiliation": null
        },
        {
          "name": "Maxim N Nethercott",
          "affiliation": null
        },
        {
          "name": "Nathan Renney",
          "affiliation": null
        },
        {
          "name": "Peter Hayes",
          "affiliation": null
        }
      ],
      "abstract": "This paper formalises the literature on emerging design patterns and paradigms for Large Language Model (LLM)-enabled multi-agent systems (MAS), evaluating their practical utility across various domains. We define key architectural components, including agent orchestration, communication mechanisms, and control-flow strategies, and demonstrate how these enable rapid development of modular, domain-adaptive solutions. Three real-world case studies are tested in controlled, containerised pilots in telecommunications security, national heritage asset management, and utilities customer service automation. Initial empirical results show that, for these case studies, prototypes were delivered within two weeks and pilot-ready solutions within one month, suggesting reduced development overhead compared to conventional approaches and improved user accessibility. However, findings also reinforce limitations documented in the literature, including variability in LLM behaviour that leads to challenges in transitioning from prototype to production maturity. We conclude by outlining critical research directions for improving reliability, scalability, and governance in MAS architectures and the further work needed to mature MAS design patterns to mitigate the inherent challenges.",
      "publishedDate": "2026-01-06T16:50:49Z",
      "updatedDate": "2026-01-06T16:50:49Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03328v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03328",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03979",
      "title": "SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems",
      "authors": [
        {
          "name": "Andreea-Elena Bodea",
          "affiliation": null
        },
        {
          "name": "Stephen Meisenbacher",
          "affiliation": null
        },
        {
          "name": "Alexandra Klymenko",
          "affiliation": null
        },
        {
          "name": "Florian Matthes",
          "affiliation": null
        }
      ],
      "abstract": "The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained \"knowledge\" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.",
      "publishedDate": "2026-01-07T14:50:41Z",
      "updatedDate": "2026-01-07T14:50:41Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03979v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03979",
      "comment": "17 pages, 3 figures, 5 tables. This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2026). The final version will be available on IEEE Xplore",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03908",
      "title": "Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval",
      "authors": [
        {
          "name": "Wang Chen",
          "affiliation": null
        },
        {
          "name": "Guanqiang Qi",
          "affiliation": null
        },
        {
          "name": "Weikang Li",
          "affiliation": null
        },
        {
          "name": "Yang Li",
          "affiliation": null
        },
        {
          "name": "Deguo Xia",
          "affiliation": null
        },
        {
          "name": "Jizhou Huang",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, but existing approaches indiscriminately trigger retrieval and rely on single-path evidence construction, often introducing noise and limiting performance gains. In this work, we propose Decide Then Retrieve (DTR), a training-free framework that adaptively determines when retrieval is necessary and how external information should be selected. DTR leverages generation uncertainty to guide retrieval triggering and introduces a dual-path retrieval mechanism with adaptive information selection to better handle sparse and ambiguous queries. Extensive experiments across five open-domain QA benchmarks, multiple model scales, and different retrievers demonstrate that DTR consistently improves EM and F1 over standard RAG and strong retrieval-enhanced baselines, while reducing unnecessary retrievals. The code and data used in this paper are available at https://github.com/ChenWangHKU/DTR.",
      "publishedDate": "2026-01-07T13:20:59Z",
      "updatedDate": "2026-01-07T13:20:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03908v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03908",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03748",
      "title": "Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning",
      "authors": [
        {
          "name": "Dario Maio",
          "affiliation": null
        },
        {
          "name": "Stefano Rizzi",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems are increasingly deployed on large-scale document collections, often comprising millions of documents and tens of millions of text chunks. In industrial-scale retrieval platforms, scalability is typically addressed through horizontal sharding and a combination of Approximate Nearest-Neighbor search, hybrid indexing, and optimized metadata filtering. Although effective from an efficiency perspective, these mechanisms rely on bottom-up, similarity-driven organization and lack a conceptual rationale for corpus partitioning. In this paper, we claim that the design of large-scale RAG systems may benefit from the combination of two orthogonal strategies: semantic clustering, which optimizes locality in embedding space, and multidimensional partitioning, which governs where retrieval should occur based on conceptual dimensions such as time and organizational context. Although such dimensions are already implicitly present in current systems, they are used in an ad hoc and poorly structured manner. We propose the Dimensional Fact Model (DFM) as a conceptual framework to guide the design of multidimensional partitions for RAG corpora. The DFM provides a principled way to reason about facts, dimensions, hierarchies, and granularity in retrieval-oriented settings. This framework naturally supports hierarchical routing and controlled fallback strategies, ensuring that retrieval remains robust even in the presence of incomplete metadata, while transforming the search process from a 'black-box' similarity matching into a governable and deterministic workflow. This work is intended as a position paper; its goal is to bridge the gap between OLAP-style multidimensional modeling and modern RAG architectures, and to stimulate further research on principled, explainable, and governable retrieval strategies at scale.",
      "publishedDate": "2026-01-07T09:37:36Z",
      "updatedDate": "2026-01-07T09:37:36Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03748v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03748",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03746",
      "title": "Whose Facts Win? LLM Source Preferences under Knowledge Conflicts",
      "authors": [
        {
          "name": "Jakob Schuster",
          "affiliation": null
        },
        {
          "name": "Vagrant Gautam",
          "affiliation": null
        },
        {
          "name": "Katja Markert",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) are more frequently used in retrieval-augmented generation pipelines, it is increasingly relevant to study their behavior under knowledge conflicts. Thus far, the role of the source of the retrieved information has gone unexamined. We address this gap with a novel framework to investigate how source preferences affect LLM resolution of inter-context knowledge conflicts in English, motivated by interdisciplinary research on credibility. With a comprehensive, tightly-controlled evaluation of 13 open-weight LLMs, we find that LLMs prefer institutionally-corroborated information (e.g., government or newspaper sources) over information from people and social media. However, these source preferences can be reversed by simply repeating information from less credible sources. To mitigate repetition effects and maintain consistent preferences, we propose a novel method that reduces repetition bias by up to 99.8%, while also maintaining at least 88.8% of original preferences. We release all data and code to encourage future work on credibility and source preferences in knowledge-intensive NLP.",
      "publishedDate": "2026-01-07T09:35:35Z",
      "updatedDate": "2026-01-13T09:48:40Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03746v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03746",
      "comment": "Data and code: https://github.com/JaSchuste/llm-source-preference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03618",
      "title": "The Pneuma Project: Reifying Information Needs as Relational Schemas to Automate Discovery, Guide Preparation, and Align Data with Intent",
      "authors": [
        {
          "name": "Muhammad Imam Luthfi Balaka",
          "affiliation": null
        },
        {
          "name": "Raul Castro Fernandez",
          "affiliation": null
        }
      ],
      "abstract": "Data discovery and preparation remain persistent bottlenecks in the data management lifecycle, especially when user intent is vague, evolving, or difficult to operationalize. The Pneuma Project introduces Pneuma-Seeker, a system that helps users articulate and fulfill information needs through iterative interaction with a language model-powered platform. The system reifies the user's evolving information need as a relational data model and incrementally converges toward a usable document aligned with that intent. To achieve this, the system combines three architectural ideas: context specialization to reduce LLM burden across subtasks, a conductor-style planner to assemble dynamic execution plans, and a convergence mechanism based on shared state. The system integrates recent advances in retrieval-augmented generation (RAG), agentic frameworks, and structured data preparation to support semi-automatic, language-guided workflows. We evaluate the system through LLM-based user simulations and show that it helps surface latent intent, guide discovery, and produce fit-for-purpose documents. It also acts as an emergent documentation layer, capturing institutional knowledge and supporting organizational memory.",
      "publishedDate": "2026-01-07T05:58:54Z",
      "updatedDate": "2026-01-07T05:58:54Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03618v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03618",
      "comment": "CIDR 2026 Paper",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "agents"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03378",
      "title": "RepoShapley: Shapley-Enhanced Context Filtering for Repository-Level Code Completion",
      "authors": [
        {
          "name": "Yu Huo",
          "affiliation": null
        },
        {
          "name": "Siyu Zhang",
          "affiliation": null
        },
        {
          "name": "Kun Zeng",
          "affiliation": null
        },
        {
          "name": "Yuquan Lu",
          "affiliation": null
        },
        {
          "name": "Cheng Yang",
          "affiliation": null
        },
        {
          "name": "Yifu Guo",
          "affiliation": null
        },
        {
          "name": "Xiaoying Tang",
          "affiliation": null
        }
      ],
      "abstract": "Repository-level code completion benefits from retrieval-augmented generation (RAG). However, controlling cross-file evidence is difficult because chunk utility is often interaction-dependent: some snippets help only when paired with complementary context, while others harm decoding when they conflict. We propose RepoShapley, a coalition-aware context filtering framework supervised by Shapley-style marginal contributions. Our module ChunkShapley constructs offline labels by (i) single-chunk probing with teacher-forced likelihood to estimate signed, weighted effects, (ii) a surrogate game that captures saturation and interference, (iii) exact Shapley computation for small retrieval sets, and (iv) bounded post-verification that selects a decoding-optimal coalition using the frozen generator. We distill verified $KEEP$ or $DROP$ decisions and retrieval triggering into a single model via discrete control tokens. Experiments across benchmarks and backbones show that RepoShapley improves completion quality while reducing harmful context and unnecessary retrieval. Code: https://anonymous.4open.science/r/a7f3c9.",
      "publishedDate": "2026-01-06T19:27:32Z",
      "updatedDate": "2026-01-06T19:27:32Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03378v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03378",
      "comment": "22pages, 9 figures, conference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05184",
      "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
      "authors": [
        {
          "name": "Yaxuan Wang",
          "affiliation": null
        },
        {
          "name": "Zhongteng Cai",
          "affiliation": null
        },
        {
          "name": "Yujia Bao",
          "affiliation": null
        },
        {
          "name": "Xueru Zhang",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.",
      "publishedDate": "2026-01-08T18:08:15Z",
      "updatedDate": "2026-01-08T18:08:15Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05184v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05184",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "tool-use"
      ],
      "tags": {
        "auto": [
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05162",
      "title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation",
      "authors": [
        {
          "name": "Jinze Yu",
          "affiliation": null
        },
        {
          "name": "Dayuan Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.",
      "publishedDate": "2026-01-08T17:51:35Z",
      "updatedDate": "2026-01-08T17:51:35Z",
      "primaryCategory": "cs.GR",
      "arxivCategories": [
        "cs.GR",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05162v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05162",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05144",
      "title": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models",
      "authors": [
        {
          "name": "Shuliang Liu",
          "affiliation": null
        },
        {
          "name": "Xingyu Li",
          "affiliation": null
        },
        {
          "name": "Hongyi Liu",
          "affiliation": null
        },
        {
          "name": "Yibo Yan",
          "affiliation": null
        },
        {
          "name": "Bingchen Duan",
          "affiliation": null
        },
        {
          "name": "Qi Zheng",
          "affiliation": null
        },
        {
          "name": "Dong Fang",
          "affiliation": null
        },
        {
          "name": "Lingfeng Su",
          "affiliation": null
        },
        {
          "name": "Xuming Hu",
          "affiliation": null
        }
      ],
      "abstract": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.",
      "publishedDate": "2026-01-08T17:32:22Z",
      "updatedDate": "2026-01-08T17:32:22Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05144v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05144",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05111",
      "title": "Agent-as-a-Judge",
      "authors": [
        {
          "name": "Runyang You",
          "affiliation": null
        },
        {
          "name": "Hongru Cai",
          "affiliation": null
        },
        {
          "name": "Caiqi Zhang",
          "affiliation": null
        },
        {
          "name": "Qiancheng Xu",
          "affiliation": null
        },
        {
          "name": "Meng Liu",
          "affiliation": null
        },
        {
          "name": "Tiezheng Yu",
          "affiliation": null
        },
        {
          "name": "Yongqi Li",
          "affiliation": null
        },
        {
          "name": "Wenjie Li",
          "affiliation": null
        }
      ],
      "abstract": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
      "publishedDate": "2026-01-08T16:58:10Z",
      "updatedDate": "2026-01-08T16:58:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05111v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05111",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "multi-agent",
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "evaluation",
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05073",
      "title": "Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward",
      "authors": [
        {
          "name": "Jianlong Chen",
          "affiliation": null
        },
        {
          "name": "Daocheng Fu",
          "affiliation": null
        },
        {
          "name": "Shengze Xu",
          "affiliation": null
        },
        {
          "name": "Jiawei Chen",
          "affiliation": null
        },
        {
          "name": "Yuan Feng",
          "affiliation": null
        },
        {
          "name": "Yue Yang",
          "affiliation": null
        },
        {
          "name": "Junchi Yan",
          "affiliation": null
        },
        {
          "name": "Hongyuan Zha",
          "affiliation": null
        },
        {
          "name": "Renqiu Xia",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) struggle with complex geometric reasoning, largely because \"black box\" outcome-based supervision fails to distinguish between lucky guesses and rigorous deduction. To address this, we introduce a paradigm shift towards subgoal-level evaluation and learning. We first construct GeoGoal, a benchmark synthesized via a rigorous formal verification data engine, which converts abstract proofs into verifiable numeric subgoals. This structure reveals a critical divergence between reasoning quality and outcome accuracy. Leveraging this, we propose the Sub-Goal Verifiable Reward (SGVR) framework, which replaces sparse signals with dense rewards based on the Skeleton Rate. Experiments demonstrate that SGVR not only enhances geometric performance (+9.7%) but also exhibits strong generalization, transferring gains to general math (+8.0%) and other general reasoning tasks (+2.8%), demonstrating broad applicability across diverse domains.",
      "publishedDate": "2026-01-08T16:17:56Z",
      "updatedDate": "2026-01-08T16:17:56Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05073v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05073",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05062",
      "title": "Compositional Steering of Large Language Models with Steering Tokens",
      "authors": [
        {
          "name": "Gorjan Radevski",
          "affiliation": null
        },
        {
          "name": "Kiril Gashteovski",
          "affiliation": null
        },
        {
          "name": "Giwon Hong",
          "affiliation": null
        },
        {
          "name": "Carolin Lawrence",
          "affiliation": null
        },
        {
          "name": "Goran Glavaš",
          "affiliation": null
        }
      ],
      "abstract": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.",
      "publishedDate": "2026-01-08T16:08:44Z",
      "updatedDate": "2026-01-08T16:08:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05062v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05062",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05047",
      "title": "Challenges and Research Directions for Large Language Model Inference Hardware",
      "authors": [
        {
          "name": "Xiaoyu Ma",
          "affiliation": null
        },
        {
          "name": "David Patterson",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.",
      "publishedDate": "2026-01-08T15:52:11Z",
      "updatedDate": "2026-01-08T15:52:11Z",
      "primaryCategory": "cs.AR",
      "arxivCategories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05047v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05047",
      "comment": "Accepted for publication by IEEE Computer, 2026",
      "journalRef": null,
      "doi": "10.1109/MC.2026.3652916",
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05004",
      "title": "Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei",
      "authors": [
        {
          "name": "Peng Wang",
          "affiliation": null
        },
        {
          "name": "Xilin Tao",
          "affiliation": null
        },
        {
          "name": "Siyi Yao",
          "affiliation": null
        },
        {
          "name": "Jiageng Wu",
          "affiliation": null
        },
        {
          "name": "Yuntao Zou",
          "affiliation": null
        },
        {
          "name": "Zhuotao Tian",
          "affiliation": null
        },
        {
          "name": "Libo Qin",
          "affiliation": null
        },
        {
          "name": "Dagang Li",
          "affiliation": null
        }
      ],
      "abstract": "Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.",
      "publishedDate": "2026-01-08T15:02:41Z",
      "updatedDate": "2026-01-08T15:02:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05004v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05004",
      "comment": "Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04742",
      "title": "Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval",
      "authors": [
        {
          "name": "Seyeon Jeong",
          "affiliation": null
        },
        {
          "name": "Yeonjun Choi",
          "affiliation": null
        },
        {
          "name": "JongWook Kim",
          "affiliation": null
        },
        {
          "name": "Beakcheol Jang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.",
      "publishedDate": "2026-01-08T09:07:41Z",
      "updatedDate": "2026-01-08T09:07:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04742v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04742",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "reasoning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04736",
      "title": "AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs",
      "authors": [
        {
          "name": "Han Zhu",
          "affiliation": null
        },
        {
          "name": "Jiale Chen",
          "affiliation": null
        },
        {
          "name": "Chengkun Cai",
          "affiliation": null
        },
        {
          "name": "Shengjie Sun",
          "affiliation": null
        },
        {
          "name": "Haoran Li",
          "affiliation": null
        },
        {
          "name": "Yujin Zhou",
          "affiliation": null
        },
        {
          "name": "Chi-Min Chan",
          "affiliation": null
        },
        {
          "name": "Pengcheng Wen",
          "affiliation": null
        },
        {
          "name": "Lei Li",
          "affiliation": null
        },
        {
          "name": "Sirui Han",
          "affiliation": null
        },
        {
          "name": "Yike Guo",
          "affiliation": null
        }
      ],
      "abstract": "Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\\% in harmless dimension and over 13\\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.",
      "publishedDate": "2026-01-08T08:57:05Z",
      "updatedDate": "2026-01-08T08:57:05Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04736v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04736",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04730",
      "title": "Automatic Classifiers Underdetect Emotions Expressed by Men",
      "authors": [
        {
          "name": "Ivan Smirnov",
          "affiliation": null
        },
        {
          "name": "Segun T. Aroyehun",
          "affiliation": null
        },
        {
          "name": "Paul Plener",
          "affiliation": null
        },
        {
          "name": "David Garcia",
          "affiliation": null
        }
      ],
      "abstract": "The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.",
      "publishedDate": "2026-01-08T08:52:17Z",
      "updatedDate": "2026-01-08T08:52:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04730v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04730",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04714",
      "title": "ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving",
      "authors": [
        {
          "name": "Chang Zhao",
          "affiliation": null
        },
        {
          "name": "Zheming Yang",
          "affiliation": null
        },
        {
          "name": "Yunqing Hu",
          "affiliation": null
        },
        {
          "name": "Qi Guo",
          "affiliation": null
        },
        {
          "name": "Zijian Wang",
          "affiliation": null
        },
        {
          "name": "Pengcheng Li",
          "affiliation": null
        },
        {
          "name": "Wen Ji",
          "affiliation": null
        }
      ],
      "abstract": "With the rapid advancement of large language models (LLMs) technologies, their application in the domain of autonomous driving has become increasingly widespread. However, existing methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought (CoT) reasoning enhances decision transparency, conventional supervised fine-tuning (SFT) fails to fully exploit its potential, and reinforcement learning (RL) approaches face instability and suboptimal reasoning depth. We propose ThinkDrive, a CoT guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization. Our method employs a two-stage training strategy. First, we perform SFT using CoT explanations. Then, we apply progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity. We evaluate our approach on a public dataset. The results show that ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy, respectively. Moreover, a 2B-parameter model trained with our method surpasses the much larger GPT-4o by 3.28% on the exam metric.",
      "publishedDate": "2026-01-08T08:30:36Z",
      "updatedDate": "2026-01-08T08:30:36Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04714v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04714",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "agents",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "agents",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04666",
      "title": "Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning",
      "authors": [
        {
          "name": "Zhiyuan Chang",
          "affiliation": null
        },
        {
          "name": "Mingyang Li",
          "affiliation": null
        },
        {
          "name": "Yuekai Huang",
          "affiliation": null
        },
        {
          "name": "Ziyou Jiang",
          "affiliation": null
        },
        {
          "name": "Xiaojun Jia",
          "affiliation": null
        },
        {
          "name": "Qian Xiong",
          "affiliation": null
        },
        {
          "name": "Junjie Wang",
          "affiliation": null
        },
        {
          "name": "Zhaoyang Li",
          "affiliation": null
        },
        {
          "name": "Qing Wang",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM)-integrated applications have become increasingly prevalent, yet face critical security vulnerabilities from prompt injection (PI) attacks. Defending against PI attacks faces two major issues: malicious instructions can be injected through diverse vectors, and injected instructions often lack clear semantic boundaries from the surrounding context, making them difficult to identify. To address these issues, we propose InstruCoT, a model enhancement method for PI defense that synthesizes diverse training data and employs instruction-level chain-of-thought fine-tuning, enabling LLMs to effectively identify and reject malicious instructions regardless of their source or position in the context. We evaluate InstruCoT across three critical dimensions: Behavior Deviation, Privacy Leakage, and Harmful Output. Experimental results across four LLMs demonstrate that InstruCoT significantly outperforms baselines in all dimensions while maintaining utility performance without degradation",
      "publishedDate": "2026-01-08T07:25:27Z",
      "updatedDate": "2026-01-08T07:25:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04666v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04666",
      "comment": "19 pages, 6 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04566",
      "title": "BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents",
      "authors": [
        {
          "name": "Yunhao Feng",
          "affiliation": null
        },
        {
          "name": "Yige Li",
          "affiliation": null
        },
        {
          "name": "Yutao Wu",
          "affiliation": null
        },
        {
          "name": "Yingshui Tan",
          "affiliation": null
        },
        {
          "name": "Yanming Guo",
          "affiliation": null
        },
        {
          "name": "Yifan Ding",
          "affiliation": null
        },
        {
          "name": "Kun Zhai",
          "affiliation": null
        },
        {
          "name": "Xingjun Ma",
          "affiliation": null
        },
        {
          "name": "Yu-Gang Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \\textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \\textbf{planning attacks}, \\textbf{memory attacks}, and \\textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \\textbf{Agent QA}, \\textbf{Agent Code}, \\textbf{Agent Web}, and \\textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \\textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\\% of planning attacks, 77.97\\% of memory attacks, and 60.28\\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.",
      "publishedDate": "2026-01-08T03:49:39Z",
      "updatedDate": "2026-01-11T08:47:08Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04566v2",
      "arxivUrl": "https://arxiv.org/abs/2601.04566",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "tool-use",
        "planning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "planning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04540",
      "title": "AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation",
      "authors": [
        {
          "name": "Tanghaoran Zhang",
          "affiliation": null
        },
        {
          "name": "Xinjun Mao",
          "affiliation": null
        },
        {
          "name": "Shangwen Wang",
          "affiliation": null
        },
        {
          "name": "Yuxin Zhao",
          "affiliation": null
        },
        {
          "name": "Yao Lu",
          "affiliation": null
        },
        {
          "name": "Jin Zhang",
          "affiliation": null
        },
        {
          "name": "Zhang Zhang",
          "affiliation": null
        },
        {
          "name": "Kang Yang",
          "affiliation": null
        },
        {
          "name": "Yue Yu",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.",
      "publishedDate": "2026-01-08T03:13:20Z",
      "updatedDate": "2026-01-08T03:13:20Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04540v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04540",
      "comment": "13 pages, 7 figures, Accepted by ASE 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "code-generation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04334",
      "title": "Autonomous Reasoning for Spacecraft Control: A Large Language Model Framework with Group Relative Policy Optimization",
      "authors": [
        {
          "name": "Amit Jain",
          "affiliation": null
        },
        {
          "name": "Richard Linares",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents a learning-based guidance-and-control approach that couples a reasoning-enabled Large Language Model (LLM) with Group Relative Policy Optimization (GRPO). A two-stage procedure consisting of Supervised Fine-Tuning (SFT) to learn formatting and control primitives, followed by GRPO for interaction-driven policy improvement, trains controllers for each environment. The framework is demonstrated on four control problems spanning a gradient of dynamical complexity, from canonical linear systems through nonlinear oscillatory dynamics to three-dimensional spacecraft attitude control with gyroscopic coupling and thrust constraints. Results demonstrate that an LLM with explicit reasoning, optimized via GRPO, can synthesize feasible stabilizing policies under consistent training settings across both linear and nonlinear systems. The two-stage training methodology enables models to generate control sequences while providing human-readable explanations of their decision-making process. This work establishes a foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and other safety-critical domains.",
      "publishedDate": "2026-01-07T19:13:22Z",
      "updatedDate": "2026-01-07T19:13:22Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "math.OC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04334v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04334",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04261",
      "title": "Inhibitory Attacks on Backdoor-based Fingerprinting for Large Language Models",
      "authors": [
        {
          "name": "Hang Fu",
          "affiliation": null
        },
        {
          "name": "Wanli Peng",
          "affiliation": null
        },
        {
          "name": "Yinghan Zhou",
          "affiliation": null
        },
        {
          "name": "Jiaxuan Wu",
          "affiliation": null
        },
        {
          "name": "Juan Wen",
          "affiliation": null
        },
        {
          "name": "Yiming Xue",
          "affiliation": null
        }
      ],
      "abstract": "The widespread adoption of Large Language Model (LLM) in commercial and research settings has intensified the need for robust intellectual property protection. Backdoor-based LLM fingerprinting has emerged as a promising solution for this challenge. In practical application, the low-cost multi-model collaborative technique, LLM ensemble, combines diverse LLMs to leverage their complementary strengths, garnering significant attention and practical adoption. Unfortunately, the vulnerability of existing LLM fingerprinting for the ensemble scenario is unexplored. In order to comprehensively assess the robustness of LLM fingerprinting, in this paper, we propose two novel fingerprinting attack methods: token filter attack (TFA) and sentence verification attack (SVA). The TFA gets the next token from a unified set of tokens created by the token filter mechanism at each decoding step. The SVA filters out fingerprint responses through a sentence verification mechanism based on perplexity and voting. Experimentally, the proposed methods effectively inhibit the fingerprint response while maintaining ensemble performance. Compared with state-of-the-art attack methods, the proposed method can achieve better performance. The findings necessitate enhanced robustness in LLM fingerprinting.",
      "publishedDate": "2026-01-07T06:06:56Z",
      "updatedDate": "2026-01-07T06:06:56Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04261v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04261",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05215",
      "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
      "authors": [
        {
          "name": "Tamil Sudaravan Mohan Doss",
          "affiliation": null
        },
        {
          "name": "Michael Xu",
          "affiliation": null
        },
        {
          "name": "Sudha Rao",
          "affiliation": null
        },
        {
          "name": "Andrew D. Wilson",
          "affiliation": null
        },
        {
          "name": "Balasaravanan Thoravi Kumaravel",
          "affiliation": null
        }
      ],
      "abstract": "We present MineNPC-Task, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world Minecraft. Rather than relying on synthetic prompts, tasks are elicited through formative and summative co-play with expert players, then normalized into parametric templates with explicit preconditions and dependency structure. These tasks are paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan, action, and memory events, including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts, and reports outcomes relative to the total number of attempted subtasks using only in-world evidence. As an initial snapshot, we instantiate the framework with GPT-4o and evaluate 216 subtasks across 8 experienced players. We observe recurring breakdown patterns in code execution, inventory and tool handling, referencing, and navigation, alongside successful recoveries supported by mixed-initiative clarifications and lightweight memory use. Participants rated interaction quality and interface usability positively, while noting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and evaluation harness to support transparent and reproducible evaluation of future memory-aware embodied agents.",
      "publishedDate": "2026-01-08T18:39:52Z",
      "updatedDate": "2026-01-09T08:14:24Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05215v2",
      "arxivUrl": "https://arxiv.org/abs/2601.05215",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "agents",
        "prompting",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "prompting",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05191",
      "title": "AgentCompress: Task-Aware Compression for Affordable Large Language Model Agents",
      "authors": [
        {
          "name": "Zuhair Ahmed Khan Taha",
          "affiliation": null
        },
        {
          "name": "Mohammed Mudassir Uddin",
          "affiliation": null
        },
        {
          "name": "Shahnawaz Alam",
          "affiliation": null
        }
      ],
      "abstract": "Large language models hold considerable promise for various applications, but their computational requirements create a barrier that many institutions cannot overcome. A single session using a 70-billion-parameter model can cost around $127 in cloud computing fees, which puts these tools out of reach for organizations operating on limited budgets. We present AgentCompress, a framework that tackles this problem through task-aware dynamic compression. The idea comes from a simple observation: not all tasks require the same computational effort. Complex reasoning, for example, is far more demanding than text reformatting, yet conventional compression applies the same reduction to both. Our approach uses a lightweight neural controller that looks at the first few tokens of each request, estimates how complex the task will be, and sends it to an appropriately quantized version of the model. This routing step adds only about 12 milliseconds of overhead. We tested the framework on 290 multi-stage workflows from domains including computer science, physics, chemistry, and biology. The results show a 68.3% reduction in computational costs while preserving 96.2% of the original success rate. These findings suggest that routing queries intelligently can make powerful language models substantially more affordable without sacrificing output quality",
      "publishedDate": "2026-01-08T18:13:46Z",
      "updatedDate": "2026-01-12T18:25:18Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05191v2",
      "arxivUrl": "https://arxiv.org/abs/2601.05191",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04795",
      "title": "Defense Against Indirect Prompt Injection via Tool Result Parsing",
      "authors": [
        {
          "name": "Qiang Yu",
          "affiliation": null
        },
        {
          "name": "Xinran Cheng",
          "affiliation": null
        },
        {
          "name": "Chuanyi Liu",
          "affiliation": null
        }
      ],
      "abstract": "As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.",
      "publishedDate": "2026-01-08T10:21:56Z",
      "updatedDate": "2026-01-08T10:21:56Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04795v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04795",
      "comment": "20 pages, 3 figures, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "prompting",
        "robotics",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "robotics",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04767",
      "title": "AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search",
      "authors": [
        {
          "name": "Zefang Zong",
          "affiliation": null
        },
        {
          "name": "Dingwei Chen",
          "affiliation": null
        },
        {
          "name": "Yang Li",
          "affiliation": null
        },
        {
          "name": "Qi Yi",
          "affiliation": null
        },
        {
          "name": "Bo Zhou",
          "affiliation": null
        },
        {
          "name": "Chengming Li",
          "affiliation": null
        },
        {
          "name": "Bo Qian",
          "affiliation": null
        },
        {
          "name": "Peng Chen",
          "affiliation": null
        },
        {
          "name": "Jie Jiang",
          "affiliation": null
        }
      ],
      "abstract": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
      "publishedDate": "2026-01-08T09:35:49Z",
      "updatedDate": "2026-01-08T09:35:49Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04767v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04767",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04694",
      "title": "ResMAS: Resilience Optimization in LLM-based Multi-agent Systems",
      "authors": [
        {
          "name": "Zhilun Zhou",
          "affiliation": null
        },
        {
          "name": "Zihan Liu",
          "affiliation": null
        },
        {
          "name": "Jiahe Liu",
          "affiliation": null
        },
        {
          "name": "Qingyu Shao",
          "affiliation": null
        },
        {
          "name": "Yihan Wang",
          "affiliation": null
        },
        {
          "name": "Kun Shao",
          "affiliation": null
        },
        {
          "name": "Depeng Jin",
          "affiliation": null
        },
        {
          "name": "Fengli Xu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS's resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent's prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.",
      "publishedDate": "2026-01-08T08:03:37Z",
      "updatedDate": "2026-01-08T08:03:37Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04694v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04694",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "prompting",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04620",
      "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
      "authors": [
        {
          "name": "Di Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
      "publishedDate": "2026-01-08T05:49:01Z",
      "updatedDate": "2026-01-08T05:49:01Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04620v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04620",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04556",
      "title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering",
      "authors": [
        {
          "name": "Bo Yu",
          "affiliation": null
        },
        {
          "name": "Lei Zhao",
          "affiliation": null
        }
      ],
      "abstract": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.",
      "publishedDate": "2026-01-08T03:36:06Z",
      "updatedDate": "2026-01-08T03:36:06Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04556v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04556",
      "comment": "39 pages, 11 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04463",
      "title": "Beyond Static Summarization: Proactive Memory Extraction for LLM Agents",
      "authors": [
        {
          "name": "Chengyuan Yang",
          "affiliation": null
        },
        {
          "name": "Zequn Sun",
          "affiliation": null
        },
        {
          "name": "Wei Wei",
          "affiliation": null
        },
        {
          "name": "Wei Hu",
          "affiliation": null
        }
      ],
      "abstract": "Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is \"ahead-of-time\", acting as a blind \"feed-forward\" process that misses important details because it doesn't know future tasks. Second, extraction is usually \"one-off\", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.",
      "publishedDate": "2026-01-08T00:37:29Z",
      "updatedDate": "2026-01-08T00:37:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04463v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04463",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents"
      ],
      "tags": {
        "auto": [
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04426",
      "title": "XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs",
      "authors": [
        {
          "name": "Linzhang Li",
          "affiliation": null
        },
        {
          "name": "Yixin Dong",
          "affiliation": null
        },
        {
          "name": "Guanjie Wang",
          "affiliation": null
        },
        {
          "name": "Ziyi Xu",
          "affiliation": null
        },
        {
          "name": "Alexander Jiang",
          "affiliation": null
        },
        {
          "name": "Tianqi Chen",
          "affiliation": null
        }
      ],
      "abstract": "Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.",
      "publishedDate": "2026-01-07T22:18:51Z",
      "updatedDate": "2026-01-07T22:18:51Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04426v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04426",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04404",
      "title": "3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation",
      "authors": [
        {
          "name": "Jusheng Zhang",
          "affiliation": null
        },
        {
          "name": "Yijia Fan",
          "affiliation": null
        },
        {
          "name": "Zimo Wen",
          "affiliation": null
        },
        {
          "name": "Jian Wang",
          "affiliation": null
        },
        {
          "name": "Keze Wang",
          "affiliation": null
        }
      ],
      "abstract": "Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU",
      "publishedDate": "2026-01-07T21:23:05Z",
      "updatedDate": "2026-01-07T21:23:05Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04404v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04404",
      "comment": "Accepted at NeurIPS 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "multi-agent",
        "agents",
        "robotics",
        "rag"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "robotics",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04238",
      "title": "Generative AI for Social Impact",
      "authors": [
        {
          "name": "Lingkai Kong",
          "affiliation": null
        },
        {
          "name": "Cheol Woo Kim",
          "affiliation": null
        },
        {
          "name": "Davin Choo",
          "affiliation": null
        },
        {
          "name": "Milind Tambe",
          "affiliation": null
        }
      ],
      "abstract": "AI for Social Impact (AI4SI) has achieved compelling results in public health, conservation, and security, yet scaling these successes remains difficult due to a persistent deployment bottleneck. We characterize this bottleneck through three coupled gaps: observational scarcity resulting from limited or unreliable data; policy synthesis challenges involving combinatorial decisions and nonstationarity; and the friction of human-AI alignment when incorporating tacit expert knowledge and dynamic constraints. We argue that Generative AI offers a unified pathway to bridge these gaps. LLM agents assist in human-AI alignment by translating natural-language guidance into executable objectives and constraints for downstream planners, while diffusion models generate realistic synthetic data and support uncertainty-aware modeling to improve policy robustness and transfer across deployments. Together, these tools enable scalable, adaptable, and human-aligned AI systems for resource optimization in high-stakes settings.",
      "publishedDate": "2026-01-05T02:44:39Z",
      "updatedDate": "2026-01-05T02:44:39Z",
      "primaryCategory": "cs.CY",
      "arxivCategories": [
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04238v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04238",
      "comment": "To appear in IEEE Intelligent Systems",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents"
      ],
      "tags": {
        "auto": [
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05101",
      "title": "Arabic Prompts with English Tools: A Benchmark",
      "authors": [
        {
          "name": "Konstantin Kubrak",
          "affiliation": null
        },
        {
          "name": "Ahmed El-Moselhy",
          "affiliation": null
        },
        {
          "name": "Ammar Alsulami",
          "affiliation": null
        },
        {
          "name": "Remaz Altuwaim",
          "affiliation": null
        },
        {
          "name": "Hassan Ismail Fawaz",
          "affiliation": null
        },
        {
          "name": "Faisal Alsaby",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
      "publishedDate": "2026-01-08T16:47:09Z",
      "updatedDate": "2026-01-08T16:47:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05101v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05101",
      "comment": "10 pages, 10 figures, LLMs, Big Data, and Multilinguality for All (LLMs4All) Workshop at IEEE BigData 2025 Conference, Macau, December 10, 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04583",
      "title": "Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries",
      "authors": [
        {
          "name": "Saad Alqithami",
          "affiliation": null
        }
      ],
      "abstract": "Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.",
      "publishedDate": "2026-01-08T04:29:26Z",
      "updatedDate": "2026-01-08T04:29:26Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04583v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04583",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "multi-agent",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "tool-use",
          "multi-agent",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04424",
      "title": "Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization",
      "authors": [
        {
          "name": "Yao Dou",
          "affiliation": null
        },
        {
          "name": "Wei Xu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.",
      "publishedDate": "2026-01-07T22:08:17Z",
      "updatedDate": "2026-01-07T22:08:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04424v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04424",
      "comment": "webpage at https://yao-dou.github.io/gavel/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05242",
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "authors": [
        {
          "name": "Shih-Yang Liu",
          "affiliation": null
        },
        {
          "name": "Xin Dong",
          "affiliation": null
        },
        {
          "name": "Ximing Lu",
          "affiliation": null
        },
        {
          "name": "Shizhe Diao",
          "affiliation": null
        },
        {
          "name": "Peter Belcak",
          "affiliation": null
        },
        {
          "name": "Mingjie Liu",
          "affiliation": null
        },
        {
          "name": "Min-Hung Chen",
          "affiliation": null
        },
        {
          "name": "Hongxu Yin",
          "affiliation": null
        },
        {
          "name": "Yu-Chiang Frank Wang",
          "affiliation": null
        },
        {
          "name": "Kwang-Ting Cheng",
          "affiliation": null
        },
        {
          "name": "Yejin Choi",
          "affiliation": null
        },
        {
          "name": "Jan Kautz",
          "affiliation": null
        },
        {
          "name": "Pavlo Molchanov",
          "affiliation": null
        }
      ],
      "abstract": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
      "publishedDate": "2026-01-08T18:59:24Z",
      "updatedDate": "2026-01-08T18:59:24Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05242v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05242",
      "comment": "NVIDIA-Tech Report",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05240",
      "title": "Robust Reasoning as a Symmetry-Protected Topological Phase",
      "authors": [
        {
          "name": "Ilmo Sung",
          "affiliation": null
        }
      ],
      "abstract": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.",
      "publishedDate": "2026-01-08T18:58:34Z",
      "updatedDate": "2026-01-08T18:58:34Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "hep-th"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05240v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05240",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05214",
      "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
      "authors": [
        {
          "name": "Kait Healy",
          "affiliation": null
        },
        {
          "name": "Bharathi Srinivasan",
          "affiliation": null
        },
        {
          "name": "Visakh Madathil",
          "affiliation": null
        },
        {
          "name": "Jing Wu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
      "publishedDate": "2026-01-08T18:38:45Z",
      "updatedDate": "2026-01-08T18:38:45Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05214v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05214",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05187",
      "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
      "authors": [
        {
          "name": "Yanchang Liang",
          "affiliation": null
        },
        {
          "name": "Xiaowei Zhao",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
      "publishedDate": "2026-01-08T18:10:35Z",
      "updatedDate": "2026-01-08T18:10:35Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05187v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05187",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "agents",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05175",
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "authors": [
        {
          "name": "Shuming Liu",
          "affiliation": null
        },
        {
          "name": "Mingchen Zhuge",
          "affiliation": null
        },
        {
          "name": "Changsheng Zhao",
          "affiliation": null
        },
        {
          "name": "Jun Chen",
          "affiliation": null
        },
        {
          "name": "Lemeng Wu",
          "affiliation": null
        },
        {
          "name": "Zechun Liu",
          "affiliation": null
        },
        {
          "name": "Chenchen Zhu",
          "affiliation": null
        },
        {
          "name": "Zhipeng Cai",
          "affiliation": null
        },
        {
          "name": "Chong Zhou",
          "affiliation": null
        },
        {
          "name": "Haozhe Liu",
          "affiliation": null
        },
        {
          "name": "Ernie Chang",
          "affiliation": null
        },
        {
          "name": "Saksham Suri",
          "affiliation": null
        },
        {
          "name": "Hongyu Xu",
          "affiliation": null
        },
        {
          "name": "Qi Qian",
          "affiliation": null
        },
        {
          "name": "Wei Wen",
          "affiliation": null
        },
        {
          "name": "Balakrishnan Varadarajan",
          "affiliation": null
        },
        {
          "name": "Zhuang Liu",
          "affiliation": null
        },
        {
          "name": "Hu Xu",
          "affiliation": null
        },
        {
          "name": "Florian Bordes",
          "affiliation": null
        },
        {
          "name": "Raghuraman Krishnamoorthi",
          "affiliation": null
        },
        {
          "name": "Bernard Ghanem",
          "affiliation": null
        },
        {
          "name": "Vikas Chandra",
          "affiliation": null
        },
        {
          "name": "Yunyang Xiong",
          "affiliation": null
        }
      ],
      "abstract": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
      "publishedDate": "2026-01-08T18:00:59Z",
      "updatedDate": "2026-01-08T18:00:59Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05175v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05175",
      "comment": "Project page: https://ivul-kaust.github.io/projects/videoauto-r1/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05172",
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "authors": [
        {
          "name": "Haoyu Zhao",
          "affiliation": null
        },
        {
          "name": "Akide Liu",
          "affiliation": null
        },
        {
          "name": "Zeyu Zhang",
          "affiliation": null
        },
        {
          "name": "Weijie Wang",
          "affiliation": null
        },
        {
          "name": "Feng Chen",
          "affiliation": null
        },
        {
          "name": "Ruihan Zhu",
          "affiliation": null
        },
        {
          "name": "Gholamreza Haffari",
          "affiliation": null
        },
        {
          "name": "Bohan Zhuang",
          "affiliation": null
        }
      ],
      "abstract": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached. We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training. Code is available on https://github.com/ziplab/CoV .",
      "publishedDate": "2026-01-08T17:59:42Z",
      "updatedDate": "2026-01-09T07:20:05Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05172v2",
      "arxivUrl": "https://arxiv.org/abs/2601.05172",
      "comment": "Code link https://github.com/ziplab/CoV",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "prompting",
        "agents",
        "reasoning",
        "rag",
        "robotics"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "reasoning",
          "rag",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05167",
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "authors": [
        {
          "name": "Chengsong Huang",
          "affiliation": null
        },
        {
          "name": "Tong Zheng",
          "affiliation": null
        },
        {
          "name": "Langlin Huang",
          "affiliation": null
        },
        {
          "name": "Jinyuan Li",
          "affiliation": null
        },
        {
          "name": "Haolin Liu",
          "affiliation": null
        },
        {
          "name": "Jiaxin Huang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "publishedDate": "2026-01-08T17:56:16Z",
      "updatedDate": "2026-01-08T17:56:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05167v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05167",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05106",
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "authors": [
        {
          "name": "Nuoya Xiong",
          "affiliation": null
        },
        {
          "name": "Yuhang Zhou",
          "affiliation": null
        },
        {
          "name": "Hanqing Zeng",
          "affiliation": null
        },
        {
          "name": "Zhaorun Chen",
          "affiliation": null
        },
        {
          "name": "Furong Huang",
          "affiliation": null
        },
        {
          "name": "Shuchao Bi",
          "affiliation": null
        },
        {
          "name": "Lizhu Zhang",
          "affiliation": null
        },
        {
          "name": "Zhuokai Zhao",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
      "publishedDate": "2026-01-08T16:53:16Z",
      "updatedDate": "2026-01-08T16:53:16Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05106v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05106",
      "comment": "25 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "rag",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05053",
      "title": "Reinforced Efficient Reasoning via Semantically Diverse Exploration",
      "authors": [
        {
          "name": "Ziqi Zhao",
          "affiliation": null
        },
        {
          "name": "Zhaochun Ren",
          "affiliation": null
        },
        {
          "name": "Jiahong Zou",
          "affiliation": null
        },
        {
          "name": "Liu Yang",
          "affiliation": null
        },
        {
          "name": "Zhiwei Xu",
          "affiliation": null
        },
        {
          "name": "Xuri Ge",
          "affiliation": null
        },
        {
          "name": "Zhumin Chen",
          "affiliation": null
        },
        {
          "name": "Xinyu Ma",
          "affiliation": null
        },
        {
          "name": "Daiting Shi",
          "affiliation": null
        },
        {
          "name": "Shuaiqiang Wang",
          "affiliation": null
        },
        {
          "name": "Dawei Yin",
          "affiliation": null
        },
        {
          "name": "Xin Xin",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.",
      "publishedDate": "2026-01-08T15:56:44Z",
      "updatedDate": "2026-01-08T15:56:44Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05053v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05053",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05039",
      "title": "FinDeepForecast: A Live Multi-Agent System for Benchmarking Deep Research Agents in Financial Forecasting",
      "authors": [
        {
          "name": "Xiangyu Li",
          "affiliation": null
        },
        {
          "name": "Xuan Yao",
          "affiliation": null
        },
        {
          "name": "Guohao Qi",
          "affiliation": null
        },
        {
          "name": "Fengbin Zhu",
          "affiliation": null
        },
        {
          "name": "Kelvin J. L. Koa",
          "affiliation": null
        },
        {
          "name": "Xiang Yao Ng",
          "affiliation": null
        },
        {
          "name": "Ziyang Liu",
          "affiliation": null
        },
        {
          "name": "Xingyu Ni",
          "affiliation": null
        },
        {
          "name": "Chang Liu",
          "affiliation": null
        },
        {
          "name": "Yonghui Yang",
          "affiliation": null
        },
        {
          "name": "Yang Zhang",
          "affiliation": null
        },
        {
          "name": "Wenjie Wang",
          "affiliation": null
        },
        {
          "name": "Fuli Feng",
          "affiliation": null
        },
        {
          "name": "Chao Wang",
          "affiliation": null
        },
        {
          "name": "Huanbo Luan",
          "affiliation": null
        },
        {
          "name": "Xiaofen Xing",
          "affiliation": null
        },
        {
          "name": "Xiangmin Xu",
          "affiliation": null
        },
        {
          "name": "Tat-Seng Chua",
          "affiliation": null
        },
        {
          "name": "Ke-Wei Huang",
          "affiliation": null
        }
      ],
      "abstract": "Deep Research (DR) Agents powered by advanced Large Language Models (LLMs) have fundamentally shifted the paradigm for completing complex research tasks. Yet, a comprehensive and live evaluation of their forecasting performance on real-world, research-oriented tasks in high-stakes domains (e.g., finance) remains underexplored. We introduce FinDeepForecast, the first live, end-to-end multi-agent system for automatically evaluating DR agents by continuously generating research-oriented fi- nancial forecasting tasks. This system is equipped with a dual-track taxonomy, enabling the dynamic generation of recurrent and non-recurrent forecasting tasks at both corporate and macro levels. With this system, we generate FinDeepForecastBench, a weekly evaluation benchmark over a ten-week horizon, encompassing 8 global economies and 1,314 listed companies, and evaluate 13 representative methods. Extensive experiments show that, while DR agents consistently outperform strong baselines, their performance still falls short of genuine forward-looking financial reasoning. We expect the proposed FinDeepForecast system to consistently facilitate future advancements of DR agents in research-oriented financial forecasting tasks. The benchmark and leaderboard are publicly available on the OpenFinArena Platform.",
      "publishedDate": "2026-01-08T15:45:09Z",
      "updatedDate": "2026-01-08T15:45:09Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05039v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05039",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05019",
      "title": "Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models",
      "authors": [
        {
          "name": "Yueqing Hu",
          "affiliation": null
        },
        {
          "name": "Xinyang Peng",
          "affiliation": null
        },
        {
          "name": "Shuting Peng",
          "affiliation": null
        },
        {
          "name": "Hanqi Wang",
          "affiliation": null
        },
        {
          "name": "Tianhong Wang",
          "affiliation": null
        }
      ],
      "abstract": "Recent Large Reasoning Models trained via reinforcement learning exhibit a \"natural\" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the \"Hán Dān Xué Bù\" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a \"Functional Alignment Collapse\": while teacher models mirror human difficulty scaling ($\\bar{r}=0.64$), distilled students significantly degrade this alignment ($\\bar{r}=0.34$), often underperforming their own pre-distillation baselines (\"Negative Transfer\"). Our analysis suggests that SFT induces a \"Cargo Cult\" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.",
      "publishedDate": "2026-01-08T15:27:03Z",
      "updatedDate": "2026-01-08T15:27:03Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "q-bio.NC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05019v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05019",
      "comment": "7 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05009",
      "title": "An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions",
      "authors": [
        {
          "name": "Avik Dutta",
          "affiliation": null
        },
        {
          "name": "Harshit Nigam",
          "affiliation": null
        },
        {
          "name": "Hosein Hasanbeig",
          "affiliation": null
        },
        {
          "name": "Arjun Radhakrishna",
          "affiliation": null
        },
        {
          "name": "Sumit Gulwani",
          "affiliation": null
        }
      ],
      "abstract": "We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing.",
      "publishedDate": "2026-01-08T15:10:32Z",
      "updatedDate": "2026-01-08T15:10:32Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05009v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05009",
      "comment": "4 pages, 1 figure, 1 table",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04992",
      "title": "Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization",
      "authors": [
        {
          "name": "Xueyun Tian",
          "affiliation": null
        },
        {
          "name": "Minghua Ma",
          "affiliation": null
        },
        {
          "name": "Bingbing Xu",
          "affiliation": null
        },
        {
          "name": "Nuoyan Lyu",
          "affiliation": null
        },
        {
          "name": "Wei Li",
          "affiliation": null
        },
        {
          "name": "Heng Dong",
          "affiliation": null
        },
        {
          "name": "Zheng Chu",
          "affiliation": null
        },
        {
          "name": "Yuanzhuo Wang",
          "affiliation": null
        },
        {
          "name": "Huawei Shen",
          "affiliation": null
        }
      ],
      "abstract": "Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.",
      "publishedDate": "2026-01-08T14:49:10Z",
      "updatedDate": "2026-01-09T02:57:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04992v2",
      "arxivUrl": "https://arxiv.org/abs/2601.04992",
      "comment": "Code and data are available at https://github.com/Eureka-Maggie/GLOW",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04963",
      "title": "Text as a Universal Interface for Transferable Personalization",
      "authors": [
        {
          "name": "Yuting Liu",
          "affiliation": null
        },
        {
          "name": "Jian Guan",
          "affiliation": null
        },
        {
          "name": "Jia-Nan Li",
          "affiliation": null
        },
        {
          "name": "Wei Wu",
          "affiliation": null
        },
        {
          "name": "Jiang-Ming Yang",
          "affiliation": null
        },
        {
          "name": "Jianzhe Zhao",
          "affiliation": null
        },
        {
          "name": "Guibing Guo",
          "affiliation": null
        }
      ],
      "abstract": "We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.",
      "publishedDate": "2026-01-08T14:09:17Z",
      "updatedDate": "2026-01-08T14:09:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04963v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04963",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04960",
      "title": "A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction",
      "authors": [
        {
          "name": "Qing Wang",
          "affiliation": null
        },
        {
          "name": "Zehan Li",
          "affiliation": null
        },
        {
          "name": "Yaodong Song",
          "affiliation": null
        },
        {
          "name": "Hongjie Chen",
          "affiliation": null
        },
        {
          "name": "Jian Kang",
          "affiliation": null
        },
        {
          "name": "Jie Lian",
          "affiliation": null
        },
        {
          "name": "Jie Li",
          "affiliation": null
        },
        {
          "name": "Yongxiang Li",
          "affiliation": null
        },
        {
          "name": "Xuelong Li",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.",
      "publishedDate": "2026-01-08T14:07:30Z",
      "updatedDate": "2026-01-08T14:07:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SD"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04960v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04960",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04945",
      "title": "T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs",
      "authors": [
        {
          "name": "Chunyu Wei",
          "affiliation": null
        },
        {
          "name": "Huaiyu Qin",
          "affiliation": null
        },
        {
          "name": "Siyuan He",
          "affiliation": null
        },
        {
          "name": "Yunhai Wang",
          "affiliation": null
        },
        {
          "name": "Yueguo Chen",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models' ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph's natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.",
      "publishedDate": "2026-01-08T13:49:12Z",
      "updatedDate": "2026-01-08T13:49:12Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04945v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04945",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04932",
      "title": "GenProve: Learning to Generate Text with Fine-Grained Provenance",
      "authors": [
        {
          "name": "Jingxuan Wei",
          "affiliation": null
        },
        {
          "name": "Xingyue Wang",
          "affiliation": null
        },
        {
          "name": "Yanghaoyu Liao",
          "affiliation": null
        },
        {
          "name": "Jie Dong",
          "affiliation": null
        },
        {
          "name": "Yuchen Liu",
          "affiliation": null
        },
        {
          "name": "Caijun Jia",
          "affiliation": null
        },
        {
          "name": "Bihui Yu",
          "affiliation": null
        },
        {
          "name": "Junnan Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.",
      "publishedDate": "2026-01-08T13:30:30Z",
      "updatedDate": "2026-01-08T13:30:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04932v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04932",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04920",
      "title": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition",
      "authors": [
        {
          "name": "Nils Einecke",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.",
      "publishedDate": "2026-01-08T13:17:50Z",
      "updatedDate": "2026-01-08T13:17:50Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04920v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04920",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "tool-use",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "tool-use",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04897",
      "title": "V-FAT: Benchmarking Visual Fidelity Against Text-bias",
      "authors": [
        {
          "name": "Ziteng Wang",
          "affiliation": null
        },
        {
          "name": "Yujie He",
          "affiliation": null
        },
        {
          "name": "Guanliang Li",
          "affiliation": null
        },
        {
          "name": "Siqi Yang",
          "affiliation": null
        },
        {
          "name": "Jiaqi Xiong",
          "affiliation": null
        },
        {
          "name": "Songxiang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize \"lucky\" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.",
      "publishedDate": "2026-01-08T12:50:14Z",
      "updatedDate": "2026-01-08T12:50:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04897v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04897",
      "comment": "12 pages, 6 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04895",
      "title": "DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation",
      "authors": [
        {
          "name": "Renzhao Liang",
          "affiliation": null
        },
        {
          "name": "Jingru Chen",
          "affiliation": null
        },
        {
          "name": "Bo Jia",
          "affiliation": null
        },
        {
          "name": "Bo Deng",
          "affiliation": null
        },
        {
          "name": "Chenggang Xie",
          "affiliation": null
        },
        {
          "name": "Yidong Wang",
          "affiliation": null
        },
        {
          "name": "Ke Jin",
          "affiliation": null
        },
        {
          "name": "Xin Wang",
          "affiliation": null
        },
        {
          "name": "Linfeng Zhang",
          "affiliation": null
        },
        {
          "name": "Cunxiang Wang",
          "affiliation": null
        }
      ],
      "abstract": "Evaluating large language models (LLMs) is increasingly confounded by \\emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \\textbf{DVD} (\\textbf{D}etection via \\textbf{V}ariance of generation \\textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \\emph{memory-adherence} state and a \\emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \\textbf{DVD} consistently outperforms perplexity-based, Min-$k$\\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.",
      "publishedDate": "2026-01-08T12:48:40Z",
      "updatedDate": "2026-01-08T12:48:40Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04895v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04895",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04891",
      "title": "Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform",
      "authors": [
        {
          "name": "Suyash Mishra",
          "affiliation": null
        },
        {
          "name": "Qiang Li",
          "affiliation": null
        },
        {
          "name": "Srikanth Patil",
          "affiliation": null
        },
        {
          "name": "Satyanarayan Pati",
          "affiliation": null
        },
        {
          "name": "Baddu Narendra",
          "affiliation": null
        }
      ],
      "abstract": "Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new \"A+B\" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.",
      "publishedDate": "2026-01-08T12:42:17Z",
      "updatedDate": "2026-01-08T12:42:17Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04891v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04891",
      "comment": "Submitted to the Industry Track of Top Tier Conference; currently under peer review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04888",
      "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
      "authors": [
        {
          "name": "Tongyu Wen",
          "affiliation": null
        },
        {
          "name": "Guanting Dong",
          "affiliation": null
        },
        {
          "name": "Zhicheng Dou",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
      "publishedDate": "2026-01-08T12:39:05Z",
      "updatedDate": "2026-01-08T12:39:05Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04888v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04888",
      "comment": "16 pages, 6 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04878",
      "title": "Higher-Order Knowledge Representations for Agentic Scientific Reasoning",
      "authors": [
        {
          "name": "Isabella A. Stewart",
          "affiliation": null
        },
        {
          "name": "Markus J. Buehler",
          "affiliation": null
        }
      ],
      "abstract": "Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a \"teacherless\" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.",
      "publishedDate": "2026-01-08T12:25:37Z",
      "updatedDate": "2026-01-08T12:25:37Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cond-mat.mtrl-sci",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04878v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04878",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04876",
      "title": "ChronosAudio: A Comprehensive Long-Audio Benchmark for Evaluating Audio-Large Language Models",
      "authors": [
        {
          "name": "Kaiwen Luo",
          "affiliation": null
        },
        {
          "name": "Liang Lin",
          "affiliation": null
        },
        {
          "name": "Yibo Zhang",
          "affiliation": null
        },
        {
          "name": "Moayad Aloqaily",
          "affiliation": null
        },
        {
          "name": "Dexian Wang",
          "affiliation": null
        },
        {
          "name": "Zhenhong Zhou",
          "affiliation": null
        },
        {
          "name": "Junwei Zhang",
          "affiliation": null
        },
        {
          "name": "Kun Wang",
          "affiliation": null
        },
        {
          "name": "Li Sun",
          "affiliation": null
        },
        {
          "name": "Qingsong Wen",
          "affiliation": null
        }
      ],
      "abstract": "Although Audio Large Language Models (ALLMs) have witnessed substantial advancements, their long audio understanding capabilities remain unexplored. A plethora of benchmarks have been proposed for general audio tasks, they predominantly focus on short-form clips, leaving without a consensus on evaluating ALLMs over extended durations. This paper proposes ChronosAudio, the first multi-task benchmark tailored for long-audio understanding in ALLMs. It encompasses six major task categories and comprises 36,000 test instances totaling over 200 hours audio, stratified into short, middle, and long-form categories to comprehensively evaluate length generalization. Extensive experiments on 16 state-of-the-art models using ChronosAudio yield three critical findings: 1.Precipitous Long-Context Collapse: ALLMs exhibit a severe inability to sustain performance, with the transition from short to long contexts triggering a staggering performance degradation of over 90% in specific tasks. 2.Structural Attention Dilution: Performance degradation stems from a fundamental failure in maintaining temporal locality; attention mechanisms suffer from significant diffusion in later sequences. 3.Restorative Ceiling of Mitigation: Current strategies only offer 50% recovery. These findings reveal significant challenges in long-audio, underscoring the urgent need for approaches to achieve robust, document-level audio reasoning.",
      "publishedDate": "2026-01-08T12:21:09Z",
      "updatedDate": "2026-01-08T12:21:09Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04876v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04876",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04861",
      "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
      "authors": [
        {
          "name": "Jingbo Wang",
          "affiliation": null
        },
        {
          "name": "Sendong Zhao",
          "affiliation": null
        },
        {
          "name": "Jiatong Liu",
          "affiliation": null
        },
        {
          "name": "Haochun Wang",
          "affiliation": null
        },
        {
          "name": "Wanting Li",
          "affiliation": null
        },
        {
          "name": "Bing Qin",
          "affiliation": null
        },
        {
          "name": "Ting Liu",
          "affiliation": null
        }
      ],
      "abstract": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.",
      "publishedDate": "2026-01-08T11:56:09Z",
      "updatedDate": "2026-01-08T11:56:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04861v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04861",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04853",
      "title": "RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection",
      "authors": [
        {
          "name": "Zhiwei Liu",
          "affiliation": null
        },
        {
          "name": "Runteng Guo",
          "affiliation": null
        },
        {
          "name": "Baojie Qu",
          "affiliation": null
        },
        {
          "name": "Yuechen Jiang",
          "affiliation": null
        },
        {
          "name": "Min Peng",
          "affiliation": null
        },
        {
          "name": "Qianqian Xie",
          "affiliation": null
        },
        {
          "name": "Sophia Ananiadou",
          "affiliation": null
        }
      ],
      "abstract": "Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.",
      "publishedDate": "2026-01-08T11:43:16Z",
      "updatedDate": "2026-01-08T11:43:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04853v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04853",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "multi-agent",
        "reasoning",
        "rag",
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "reasoning",
          "rag",
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04824",
      "title": "SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models",
      "authors": [
        {
          "name": "Oriol Rabasseda",
          "affiliation": null
        },
        {
          "name": "Zenjie Li",
          "affiliation": null
        },
        {
          "name": "Kamal Nasrollahi",
          "affiliation": null
        },
        {
          "name": "Sergio Escalera",
          "affiliation": null
        }
      ],
      "abstract": "Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models. Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.",
      "publishedDate": "2026-01-08T10:58:59Z",
      "updatedDate": "2026-01-09T10:27:37Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04824v2",
      "arxivUrl": "https://arxiv.org/abs/2601.04824",
      "comment": "This work has been accepted at Real World Surveillance: Applications and Challenges, 6th (in WACV Workshops)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "evaluation",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04819",
      "title": "AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding",
      "authors": [
        {
          "name": "Aleksei Kondratenko",
          "affiliation": null
        },
        {
          "name": "Mussie Birhane",
          "affiliation": null
        },
        {
          "name": "Houssame E. Hsain",
          "affiliation": null
        },
        {
          "name": "Guido Maciocci",
          "affiliation": null
        }
      ],
      "abstract": "AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.",
      "publishedDate": "2026-01-08T10:54:32Z",
      "updatedDate": "2026-01-08T10:54:32Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04819v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04819",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04809",
      "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
      "authors": [
        {
          "name": "Caijun Xu",
          "affiliation": null
        },
        {
          "name": "Changyi Xiao",
          "affiliation": null
        },
        {
          "name": "Zhongyuan Peng",
          "affiliation": null
        },
        {
          "name": "Xinrun Wang",
          "affiliation": null
        },
        {
          "name": "Yixin Cao",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.",
      "publishedDate": "2026-01-08T10:42:04Z",
      "updatedDate": "2026-01-08T10:42:04Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04809v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04809",
      "comment": "19 pages,5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04789",
      "title": "NC2C: Automated Convexification of Generic Non-Convex Optimization Problems",
      "authors": [
        {
          "name": "Xinyue Peng",
          "affiliation": null
        },
        {
          "name": "Yanming Liu",
          "affiliation": null
        },
        {
          "name": "Yihan Cang",
          "affiliation": null
        },
        {
          "name": "Yuwei Zhang",
          "affiliation": null
        },
        {
          "name": "Xinyi Wang",
          "affiliation": null
        },
        {
          "name": "Songhang Deng",
          "affiliation": null
        },
        {
          "name": "Jiannan Cao",
          "affiliation": null
        }
      ],
      "abstract": "Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\\% execution rate and a 76\\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.",
      "publishedDate": "2026-01-08T10:12:45Z",
      "updatedDate": "2026-01-08T10:12:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04789v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04789",
      "comment": "First version of NC2C",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04778",
      "title": "CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models",
      "authors": [
        {
          "name": "Tobia Poppi",
          "affiliation": null
        },
        {
          "name": "Burak Uzkent",
          "affiliation": null
        },
        {
          "name": "Amanmeet Garg",
          "affiliation": null
        },
        {
          "name": "Lucas Porto",
          "affiliation": null
        },
        {
          "name": "Garin Kessler",
          "affiliation": null
        },
        {
          "name": "Yezhou Yang",
          "affiliation": null
        },
        {
          "name": "Marcella Cornia",
          "affiliation": null
        },
        {
          "name": "Lorenzo Baraldi",
          "affiliation": null
        },
        {
          "name": "Rita Cucchiara",
          "affiliation": null
        },
        {
          "name": "Florian Schiffers",
          "affiliation": null
        }
      ],
      "abstract": "Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.",
      "publishedDate": "2026-01-08T10:03:07Z",
      "updatedDate": "2026-01-08T10:03:07Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04778v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04778",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04777",
      "title": "GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models",
      "authors": [
        {
          "name": "Shurong Zheng",
          "affiliation": null
        },
        {
          "name": "Yousong Zhu",
          "affiliation": null
        },
        {
          "name": "Hongyin Zhao",
          "affiliation": null
        },
        {
          "name": "Fan Yang",
          "affiliation": null
        },
        {
          "name": "Yufei Zhan",
          "affiliation": null
        },
        {
          "name": "Ming Tang",
          "affiliation": null
        },
        {
          "name": "Jinqiao Wang",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.",
      "publishedDate": "2026-01-08T09:58:35Z",
      "updatedDate": "2026-01-08T09:58:35Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04777v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04777",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04770",
      "title": "SciIF: Benchmarking Scientific Instruction Following Towards Rigorous Scientific Intelligence",
      "authors": [
        {
          "name": "Encheng Su",
          "affiliation": null
        },
        {
          "name": "Jianyu Wu",
          "affiliation": null
        },
        {
          "name": "Chen Tang",
          "affiliation": null
        },
        {
          "name": "Lintao Wang",
          "affiliation": null
        },
        {
          "name": "Pengze Li",
          "affiliation": null
        },
        {
          "name": "Aoran Wang",
          "affiliation": null
        },
        {
          "name": "Jinouwen Zhang",
          "affiliation": null
        },
        {
          "name": "Yizhou Wang",
          "affiliation": null
        },
        {
          "name": "Yuan Meng",
          "affiliation": null
        },
        {
          "name": "Xinzhu Ma",
          "affiliation": null
        },
        {
          "name": "Shixiang Tang",
          "affiliation": null
        },
        {
          "name": "Houqiang Li",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) transition from general knowledge retrieval to complex scientific discovery, their evaluation standards must also incorporate the rigorous norms of scientific inquiry. Existing benchmarks exhibit a critical blind spot: general instruction-following metrics focus on superficial formatting, while domain-specific scientific benchmarks assess only final-answer correctness, often rewarding models that arrive at the right result with the wrong reasons. To address this gap, we introduce scientific instruction following: the capability to solve problems while strictly adhering to the constraints that establish scientific validity. Specifically, we introduce SciIF, a multi-discipline benchmark that evaluates this capability by pairing university-level problems with a fixed catalog of constraints across three pillars: scientific conditions (e.g., boundary checks and assumptions), semantic stability (e.g., unit and symbol conventions), and specific processes(e.g., required numerical methods). Uniquely, SciIF emphasizes auditability, requiring models to provide explicit evidence of constraint satisfaction rather than implicit compliance. By measuring both solution correctness and multi-constraint adherence, SciIF enables finegrained diagnosis of compositional reasoning failures, ensuring that LLMs can function as reliable agents within the strict logical frameworks of science.",
      "publishedDate": "2026-01-08T09:45:58Z",
      "updatedDate": "2026-01-12T02:43:34Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.DB"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04770v2",
      "arxivUrl": "https://arxiv.org/abs/2601.04770",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04758",
      "title": "PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks",
      "authors": [
        {
          "name": "Yehoon Jang",
          "affiliation": null
        },
        {
          "name": "Chaewon Lee",
          "affiliation": null
        },
        {
          "name": "Hyun-seok Min",
          "affiliation": null
        },
        {
          "name": "Sungchul Choi",
          "affiliation": null
        }
      ],
      "abstract": "The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.",
      "publishedDate": "2026-01-08T09:26:05Z",
      "updatedDate": "2026-01-08T09:26:05Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04758v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04758",
      "comment": "Accepted at the NLLP Workshop at EMNLP 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04752",
      "title": "Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition",
      "authors": [
        {
          "name": "Masatomo Yoshida",
          "affiliation": null
        },
        {
          "name": "Haruto Namura",
          "affiliation": null
        },
        {
          "name": "Nicola Adami",
          "affiliation": null
        },
        {
          "name": "Masahiro Okuda",
          "affiliation": null
        }
      ],
      "abstract": "This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.",
      "publishedDate": "2026-01-08T09:15:27Z",
      "updatedDate": "2026-01-08T09:15:27Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04752v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04752",
      "comment": "accepted to ITC-CSCC 2025",
      "journalRef": "Proc. ITC-CSCC 2025",
      "doi": "10.1109/ITC-CSCC66376.2025.11137646",
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04734",
      "title": "AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection",
      "authors": [
        {
          "name": "Yunqing Hu",
          "affiliation": null
        },
        {
          "name": "Zheming Yang",
          "affiliation": null
        },
        {
          "name": "Chang Zhao",
          "affiliation": null
        },
        {
          "name": "Qi Guo",
          "affiliation": null
        },
        {
          "name": "Meng Gao",
          "affiliation": null
        },
        {
          "name": "Pengcheng Li",
          "affiliation": null
        },
        {
          "name": "Wen Ji",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.",
      "publishedDate": "2026-01-08T08:56:07Z",
      "updatedDate": "2026-01-08T08:56:07Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04734v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04734",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04726",
      "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
      "authors": [
        {
          "name": "Yuyang Hu",
          "affiliation": null
        },
        {
          "name": "Jiongnan Liu",
          "affiliation": null
        },
        {
          "name": "Jiejun Tan",
          "affiliation": null
        },
        {
          "name": "Yutao Zhu",
          "affiliation": null
        },
        {
          "name": "Zhicheng Dou",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
      "publishedDate": "2026-01-08T08:44:07Z",
      "updatedDate": "2026-01-08T08:44:07Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04726v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04726",
      "comment": "19 pages,6 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04709",
      "title": "Bridging Temporal and Textual Modalities: A Multimodal Framework for Automated Cloud Failure Root Cause Analysis",
      "authors": [
        {
          "name": "Gijun Park",
          "affiliation": null
        }
      ],
      "abstract": "Root cause analysis in modern cloud infrastructure demands sophisticated understanding of heterogeneous data sources, particularly time-series performance metrics that involve core failure signatures. While large language models demonstrate remarkable capabilities in textual reasoning, their discrete token-based architecture creates fundamental incompatibilities with continuous numerical sequences exhibiting temporal dependencies. Current methodologies inadequately address this modality mismatch, constraining the potential of language model-driven automation in incident management workflows. This paper presents a multimodal diagnostic framework that harmonizes time-series representations with pretrained language model embedding spaces. Our approach contributes three technical advances: (1) a semantic compression technique that distills temporal segments into single-token abstractions while preserving pattern semantics, (2) an alignment encoder utilizing gated cross-attention to project time-series features into language model latent space, and (3) a retrieval-augmented diagnostic pipeline that synthesizes aligned embeddings with historical incident knowledge for expert-level failure attribution. Comprehensive evaluation across six cloud system benchmarks demonstrates that our framework achieves leading performance, reaching 48.75% diagnostic accuracy with notable improvements on scenarios involving compound failure modes. The results validate embedding-space alignment as an effective strategy for enabling language models to reason over multimodal telemetry data in production incident response contexts.",
      "publishedDate": "2026-01-08T08:20:44Z",
      "updatedDate": "2026-01-08T08:20:44Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04709v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04709",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04703",
      "title": "Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search",
      "authors": [
        {
          "name": "Yiqun Chen",
          "affiliation": null
        },
        {
          "name": "Lingyong Yan",
          "affiliation": null
        },
        {
          "name": "Zixuan Yang",
          "affiliation": null
        },
        {
          "name": "Erhan Zhang",
          "affiliation": null
        },
        {
          "name": "Jiashu Zhao",
          "affiliation": null
        },
        {
          "name": "Shuaiqiang Wang",
          "affiliation": null
        },
        {
          "name": "Dawei Yin",
          "affiliation": null
        },
        {
          "name": "Jiaxin Mao",
          "affiliation": null
        }
      ],
      "abstract": "Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \\textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}",
      "publishedDate": "2026-01-08T08:13:27Z",
      "updatedDate": "2026-01-08T08:13:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04703v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04703",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "tool-use",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04700",
      "title": "PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards",
      "authors": [
        {
          "name": "Mukesh Ghimire",
          "affiliation": null
        },
        {
          "name": "Aosong Feng",
          "affiliation": null
        },
        {
          "name": "Liwen You",
          "affiliation": null
        },
        {
          "name": "Youzhi Luo",
          "affiliation": null
        },
        {
          "name": "Fang Liu",
          "affiliation": null
        },
        {
          "name": "Xuan Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.",
      "publishedDate": "2026-01-08T08:09:29Z",
      "updatedDate": "2026-01-08T08:09:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04700v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04700",
      "comment": "Preprint. Under Review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04688",
      "title": "ToolGate: Contract-Grounded and Verified Tool Execution for LLMs",
      "authors": [
        {
          "name": "Yanming Liu",
          "affiliation": null
        },
        {
          "name": "Xinyue Peng",
          "affiliation": null
        },
        {
          "name": "Jiannan Cao",
          "affiliation": null
        },
        {
          "name": "Xinyi Wang",
          "affiliation": null
        },
        {
          "name": "Songhang Deng",
          "affiliation": null
        },
        {
          "name": "Jintao Chen",
          "affiliation": null
        },
        {
          "name": "Jianwei Yin",
          "affiliation": null
        },
        {
          "name": "Xuhong Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \\textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.",
      "publishedDate": "2026-01-08T07:56:45Z",
      "updatedDate": "2026-01-08T07:56:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.FL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04688v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04688",
      "comment": "First version of ToolGate",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04675",
      "title": "LLM-Guided Quantified SMT Solving over Uninterpreted Functions",
      "authors": [
        {
          "name": "Kunhang Lv",
          "affiliation": null
        },
        {
          "name": "Yuhang Dong",
          "affiliation": null
        },
        {
          "name": "Rui Han",
          "affiliation": null
        },
        {
          "name": "Fuqi Jia",
          "affiliation": null
        },
        {
          "name": "Feifei Ma",
          "affiliation": null
        },
        {
          "name": "Jian Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Quantified formulas with Uninterpreted Functions (UFs) over non-linear real arithmetic pose fundamental challenges for Satisfiability Modulo Theories (SMT) solving. Traditional quantifier instantiation methods struggle because they lack semantic understanding of UF constraints, forcing them to search through unbounded solution spaces with limited guidance. We present AquaForte, a framework that leverages Large Language Models to provide semantic guidance for UF instantiation by generating instantiated candidates for function definitions that satisfy the constraints, thereby significantly reducing the search space and complexity for solvers. Our approach preprocesses formulas through constraint separation, uses structured prompts to extract mathematical reasoning from LLMs, and integrates the results with traditional SMT algorithms through adaptive instantiation. AquaForte maintains soundness through systematic validation: LLM-guided instantiations yielding SAT solve the original problem, while UNSAT results generate exclusion clauses for iterative refinement. Completeness is preserved by fallback to traditional solvers augmented with learned constraints. Experimental evaluation on SMT-COMP benchmarks demonstrates that AquaForte solves numerous instances where state-of-the-art solvers like Z3 and CVC5 timeout, with particular effectiveness on satisfiable formulas. Our work shows that LLMs can provide valuable mathematical intuition for symbolic reasoning, establishing a new paradigm for SMT constraint solving.",
      "publishedDate": "2026-01-08T07:40:37Z",
      "updatedDate": "2026-01-08T07:40:37Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04675v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04675",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04674",
      "title": "PROMISE: Process Reward Models Unlock Test-Time Scaling Laws in Generative Recommendations",
      "authors": [
        {
          "name": "Chengcheng Guo",
          "affiliation": null
        },
        {
          "name": "Kuo Cai",
          "affiliation": null
        },
        {
          "name": "Yu Zhou",
          "affiliation": null
        },
        {
          "name": "Qiang Luo",
          "affiliation": null
        },
        {
          "name": "Ruiming Tang",
          "affiliation": null
        },
        {
          "name": "Han Li",
          "affiliation": null
        },
        {
          "name": "Kun Gai",
          "affiliation": null
        },
        {
          "name": "Guorui Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Generative Recommendation has emerged as a promising paradigm, reformulating recommendation as a sequence-to-sequence generation task over hierarchical Semantic IDs. However, existing methods suffer from a critical issue we term Semantic Drift, where errors in early, high-level tokens irreversibly divert the generation trajectory into irrelevant semantic subspaces. Inspired by Process Reward Models (PRMs) that enhance reasoning in Large Language Models, we propose Promise, a novel framework that integrates dense, step-by-step verification into generative models. Promise features a lightweight PRM to assess the quality of intermediate inference steps, coupled with a PRM-guided Beam Search strategy that leverages dense feedback to dynamically prune erroneous branches. Crucially, our approach unlocks Test-Time Scaling Laws for recommender systems: by increasing inference compute, smaller models can match or surpass larger models. Extensive offline experiments and online A/B tests on a large-scale platform demonstrate that Promise effectively mitigates Semantic Drift, significantly improving recommendation accuracy while enabling efficient deployment.",
      "publishedDate": "2026-01-08T07:38:46Z",
      "updatedDate": "2026-01-08T07:38:46Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04674v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04674",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04672",
      "title": "Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning",
      "authors": [
        {
          "name": "Wentao Zhang",
          "affiliation": null
        },
        {
          "name": "Lifei Wang",
          "affiliation": null
        },
        {
          "name": "Lina Lu",
          "affiliation": null
        },
        {
          "name": "MingKun Xu",
          "affiliation": null
        },
        {
          "name": "Shangyang Li",
          "affiliation": null
        },
        {
          "name": "Yanchao Yang",
          "affiliation": null
        },
        {
          "name": "Tao Fang",
          "affiliation": null
        }
      ],
      "abstract": "Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \\textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\\% relative gain in disease recognition accuracy, +33.3\\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.",
      "publishedDate": "2026-01-08T07:34:37Z",
      "updatedDate": "2026-01-08T07:34:37Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04672v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04672",
      "comment": "This paper is submitted for review to ACL 2026. It is 17 pages long and includes 5 figures. The corresponding authors are Tao Fang and Lina Lu",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04670",
      "title": "Learning Dynamics in RL Post-Training for Language Models",
      "authors": [
        {
          "name": "Akiyoshi Tomihari",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tangent kernel (NTK) framework and decompose the NTK into two components to characterize how RL updates propagate across training samples. Our analysis reveals that limited variability in feature representations can cause RL updates to systematically increase model confidence, providing an explanation for the commonly observed reduction in output diversity after RL post-training. Furthermore, we show that effective learning in this regime depends on rapidly shaping the classifier, which directly affects the gradient component of the NTK. Motivated by these insights, we propose classifier-first reinforcement learning (CF-RL), a simple two-stage training strategy that prioritizes classifier updates before standard RL optimization. Experimental results validate our theoretical analysis by demonstrating increased model confidence and accelerated optimization under CF-RL. Additional analysis shows that the mechanism underlying CF-RL differs from that of linear-probing-then-fine-tuning in supervised learning. Overall, our study formalizes the learning dynamics of RL post-training and motivates further analysis and improvement.",
      "publishedDate": "2026-01-08T07:32:15Z",
      "updatedDate": "2026-01-08T07:32:15Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04670v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04670",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04658",
      "title": "LAMB: LLM-based Audio Captioning with Modality Gap Bridging via Cauchy-Schwarz Divergence",
      "authors": [
        {
          "name": "Hyeongkeun Lee",
          "affiliation": null
        },
        {
          "name": "Jongmin Choi",
          "affiliation": null
        },
        {
          "name": "KiHyun Nam",
          "affiliation": null
        },
        {
          "name": "Joon Son Chung",
          "affiliation": null
        }
      ],
      "abstract": "Automated Audio Captioning aims to describe the semantic content of input audio. Recent works have employed large language models (LLMs) as a text decoder to leverage their reasoning capabilities. However, prior approaches that project audio features into the LLM embedding space without considering cross-modal alignment fail to fully utilize these capabilities. To address this, we propose LAMB, an LLM-based audio captioning framework that bridges the modality gap between audio embeddings and the LLM text embedding space. LAMB incorporates a Cross-Modal Aligner that minimizes Cauchy-Schwarz divergence while maximizing mutual information, yielding tighter alignment between audio and text at both global and token levels. We further design a Two-Stream Adapter that extracts semantically enriched audio embeddings, thereby delivering richer information to the Cross-Modal Aligner. Finally, leveraging the aligned audio embeddings, a proposed Token Guide directly computes scores within the LLM text embedding space to steer the output logits of generated captions. Experimental results confirm that our framework strengthens the reasoning capabilities of the LLM decoder, achieving state-of-the-art performance on AudioCaps.",
      "publishedDate": "2026-01-08T07:05:35Z",
      "updatedDate": "2026-01-08T07:05:35Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04658v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04658",
      "comment": "5 pages, 2 figures;",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04654",
      "title": "LLMs-Integrated Automatic Hate Speech Recognition Using Controllable Text Generation Models",
      "authors": [
        {
          "name": "Ryutaro Oshima",
          "affiliation": null
        },
        {
          "name": "Yuya Hosoda",
          "affiliation": null
        },
        {
          "name": "Youji Iiguni",
          "affiliation": null
        }
      ],
      "abstract": "This paper proposes an automatic speech recognition (ASR) model for hate speech using large language models (LLMs). The proposed method integrates the encoder of the ASR model with the decoder of the LLMs, enabling simultaneous transcription and censorship tasks to prevent the exposure of harmful content. Instruction tuning of the LLM to mask hate-related words with specific tokens requires an annotated hate speech dataset, which is limited. We generate text samples using an LLM with the Chain-of-Thought (CoT) prompting technique guided by cultural context and examples and then convert them into speech samples using a text-to-speech (TTS) system. However, some of them contain non-hate speech samples with hate-related words, which degrades the censorship performance. This paper filters the samples which text classification models correctly label as hate content. By adjusting the threshold for the number of correct answer models, we can control the level of hate in the generated dataset, allowing us to train the LLMs through curriculum learning in a gradual manner. Experimental results show that the proposed method achieves a masking accuracy of 58.6\\% for hate-related words, surpassing previous baselines. We also confirm that the curriculum training contributes to the efficiency of both transcription and censorship tasks.",
      "publishedDate": "2026-01-08T07:03:48Z",
      "updatedDate": "2026-01-08T07:03:48Z",
      "primaryCategory": "eess.AS",
      "arxivCategories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04654v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04654",
      "comment": "In Proceedings of the 17th Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC 2025)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04653",
      "title": "Vibe Coding an LLM-powered Theorem Prover",
      "authors": [
        {
          "name": "Zhe Hou",
          "affiliation": null
        }
      ],
      "abstract": "We present Isabellm, an LLM-powered theorem prover for Isabelle/HOL that performs fully automatic proof synthesis. Isabellm works with any local LLM on Ollama and APIs such as Gemini CLI, and it is designed to run on consumer grade computers. The system combines a stepwise prover, which uses large language models to propose proof commands validated by Isabelle in a bounded search loop, with a higher-level proof planner that generates structured Isar outlines and attempts to fill and repair remaining gaps. The framework includes beam search for tactics, tactics reranker ML and RL models, premise selection with small transformer models, micro-RAG for Isar proofs built from AFP, and counter-example guided proof repair. All the code is implemented by GPT 4.1 - 5.2, Gemini 3 Pro, and Claude 4.5. Empirically, Isabellm can prove certain lemmas that defeat Isabelle's standard automation, including Sledgehammer, demonstrating the practical value of LLM-guided proof search. At the same time, we find that even state-of-the-art LLMs, such as GPT 5.2 Extended Thinking and Gemini 3 Pro struggle to reliably implement the intended fill-and-repair mechanisms with complex algorithmic designs, highlighting fundamental challenges in LLM code generation and reasoning. The code of Isabellm is available at https://github.com/zhehou/llm-isabelle",
      "publishedDate": "2026-01-08T07:00:24Z",
      "updatedDate": "2026-01-08T07:00:24Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04653v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04653",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "tool-use",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04651",
      "title": "Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models",
      "authors": [
        {
          "name": "Can Xu",
          "affiliation": null
        },
        {
          "name": "Lingyong Yan",
          "affiliation": null
        },
        {
          "name": "Jiayi Wu",
          "affiliation": null
        },
        {
          "name": "Haosen Wang",
          "affiliation": null
        },
        {
          "name": "Shuaiqiang Wang",
          "affiliation": null
        },
        {
          "name": "Yuchen Li",
          "affiliation": null
        },
        {
          "name": "Jizhou Huang",
          "affiliation": null
        },
        {
          "name": "Dawei Yin",
          "affiliation": null
        },
        {
          "name": "Xiang Li",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other's logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.",
      "publishedDate": "2026-01-08T06:57:03Z",
      "updatedDate": "2026-01-09T03:12:04Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.IR",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04651v2",
      "arxivUrl": "https://arxiv.org/abs/2601.04651",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04633",
      "title": "MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark",
      "authors": [
        {
          "name": "Anyang Song",
          "affiliation": null
        },
        {
          "name": "Ying Cheng",
          "affiliation": null
        },
        {
          "name": "Yiqian Xu",
          "affiliation": null
        },
        {
          "name": "Rui Feng",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \\textbf{M}achine-\\textbf{A}ugment-\\textbf{G}enerated Text via \\textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \\textbf{R}einforced \\textbf{L}earning from \\textbf{D}etectors \\textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.",
      "publishedDate": "2026-01-08T06:07:07Z",
      "updatedDate": "2026-01-08T06:07:07Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04633v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04633",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04568",
      "title": "Neurosymbolic Retrievers for Retrieval-augmented Generation",
      "authors": [
        {
          "name": "Yash Saxena",
          "affiliation": null
        },
        {
          "name": "Manas Gaur",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance",
      "publishedDate": "2026-01-08T03:53:05Z",
      "updatedDate": "2026-01-08T03:53:05Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04568v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04568",
      "comment": "8 pages, 2 Figures, To Appear in IEEE Intelligent Systems",
      "journalRef": null,
      "doi": "10.1109/MIS.2025.3642666",
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04562",
      "title": "Reasoning Over Space: Enabling Geographic Reasoning for LLM-Based Generative Next POI Recommendation",
      "authors": [
        {
          "name": "Dongyi Lv",
          "affiliation": null
        },
        {
          "name": "Qiuyu Ding",
          "affiliation": null
        },
        {
          "name": "Heng-Da Xu",
          "affiliation": null
        },
        {
          "name": "Zhaoxu Sun",
          "affiliation": null
        },
        {
          "name": "Zhi Wang",
          "affiliation": null
        },
        {
          "name": "Feng Xiong",
          "affiliation": null
        },
        {
          "name": "Mu Xu",
          "affiliation": null
        }
      ],
      "abstract": "Generative recommendation with large language models (LLMs) reframes prediction as sequence generation, yet existing LLM-based recommenders remain limited in leveraging geographic signals that are crucial in mobility and local-services scenarios. Here, we present Reasoning Over Space (ROS), a framework that utilizes geography as a vital decision variable within the reasoning process. ROS introduces a Hierarchical Spatial Semantic ID (SID) that discretizes coarse-to-fine locality and POI semantics into compositional tokens, and endows LLM with a three-stage Mobility Chain-of-Thought (CoT) paradigm that models user personality, constructs an intent-aligned candidate space, and performs locality informed pruning. We further align the model with real world geography via spatial-guided Reinforcement Learning (RL). Experiments on three widely used location-based social network (LBSN) datasets show that ROS achieves over 10% relative gains in hit rate over strongest LLM-based baselines and improves cross-city transfer, despite using a smaller backbone model.",
      "publishedDate": "2026-01-08T03:46:03Z",
      "updatedDate": "2026-01-08T03:46:03Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04562v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04562",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04531",
      "title": "Self-MedRAG: a Self-Reflective Hybrid Retrieval-Augmented Generation Framework for Reliable Medical Question Answering",
      "authors": [
        {
          "name": "Jessica Ryan",
          "affiliation": null
        },
        {
          "name": "Alexander I. Gumilang",
          "affiliation": null
        },
        {
          "name": "Robert Wiliam",
          "affiliation": null
        },
        {
          "name": "Derwin Suhartono",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated significant potential in medical Question Answering (QA), yet they remain prone to hallucinations and ungrounded reasoning, limiting their reliability in high-stakes clinical scenarios. While Retrieval-Augmented Generation (RAG) mitigates these issues by incorporating external knowledge, conventional single-shot retrieval often fails to resolve complex biomedical queries requiring multi-step inference. To address this, we propose Self-MedRAG, a self-reflective hybrid framework designed to mimic the iterative hypothesis-verification process of clinical reasoning. Self-MedRAG integrates a hybrid retrieval strategy, combining sparse (BM25) and dense (Contriever) retrievers via Reciprocal Rank Fusion (RRF) to maximize evidence coverage. It employs a generator to produce answers with supporting rationales, which are then assessed by a lightweight self-reflection module using Natural Language Inference (NLI) or LLM-based verification. If the rationale lacks sufficient evidentiary support, the system autonomously reformulates the query and iterates to refine the context. We evaluated Self-MedRAG on the MedQA and PubMedQA benchmarks. The results demonstrate that our hybrid retrieval approach significantly outperforms single-retriever baselines. Furthermore, the inclusion of the self-reflective loop yielded substantial gains, increasing accuracy on MedQA from 80.00% to 83.33% and on PubMedQA from 69.10% to 79.82%. These findings confirm that integrating hybrid retrieval with iterative, evidence-based self-reflection effectively reduces unsupported claims and enhances the clinical reliability of LLM-based systems.",
      "publishedDate": "2026-01-08T02:56:04Z",
      "updatedDate": "2026-01-08T02:56:04Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04531v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04531",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04526",
      "title": "Advancing Language Models for Code-related Tasks",
      "authors": [
        {
          "name": "Zhao Tian",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.",
      "publishedDate": "2026-01-08T02:48:01Z",
      "updatedDate": "2026-01-08T02:48:01Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04526v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04526",
      "comment": "Accepted by ICSE 2026 (DS)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "prompting",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "prompting",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04516",
      "title": "LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation",
      "authors": [
        {
          "name": "Yuxiao Ye",
          "affiliation": null
        },
        {
          "name": "Yiming Zhang",
          "affiliation": null
        },
        {
          "name": "Yiran Ma",
          "affiliation": null
        },
        {
          "name": "Huiyuan Xie",
          "affiliation": null
        },
        {
          "name": "Huining Zhu",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Liu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.",
      "publishedDate": "2026-01-08T02:30:43Z",
      "updatedDate": "2026-01-08T02:30:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04516v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04516",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04505",
      "title": "CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts",
      "authors": [
        {
          "name": "Khandakar Shakib Al Hasan",
          "affiliation": null
        },
        {
          "name": "Syed Rifat Raiyan",
          "affiliation": null
        },
        {
          "name": "Hasin Mahtab Alvee",
          "affiliation": null
        },
        {
          "name": "Wahid Sadik",
          "affiliation": null
        }
      ],
      "abstract": "Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.",
      "publishedDate": "2026-01-08T02:18:43Z",
      "updatedDate": "2026-01-08T02:18:43Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "eess.SY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04505v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04505",
      "comment": "Under review, 13 pages, 11 figures, 2 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "rag",
        "agents",
        "reasoning",
        "multi-agent",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "agents",
          "reasoning",
          "multi-agent",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04453",
      "title": "UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving",
      "authors": [
        {
          "name": "Zhexiao Xiong",
          "affiliation": null
        },
        {
          "name": "Xin Ye",
          "affiliation": null
        },
        {
          "name": "Burhan Yaman",
          "affiliation": null
        },
        {
          "name": "Sheng Cheng",
          "affiliation": null
        },
        {
          "name": "Yiren Lu",
          "affiliation": null
        },
        {
          "name": "Jingru Luo",
          "affiliation": null
        },
        {
          "name": "Nathan Jacobs",
          "affiliation": null
        },
        {
          "name": "Liu Ren",
          "affiliation": null
        }
      ],
      "abstract": "World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .",
      "publishedDate": "2026-01-07T23:49:52Z",
      "updatedDate": "2026-01-07T23:49:52Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04453v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04453",
      "comment": "Project Page: https://unidrive-wm.github.io/UniDrive-WM",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04442",
      "title": "Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization",
      "authors": [
        {
          "name": "Xingjian Diao",
          "affiliation": null
        },
        {
          "name": "Zheyuan Liu",
          "affiliation": null
        },
        {
          "name": "Chunhui Zhang",
          "affiliation": null
        },
        {
          "name": "Weiyi Wu",
          "affiliation": null
        },
        {
          "name": "Keyi Kong",
          "affiliation": null
        },
        {
          "name": "Lin Shi",
          "affiliation": null
        },
        {
          "name": "Kaize Ding",
          "affiliation": null
        },
        {
          "name": "Soroush Vosoughi",
          "affiliation": null
        },
        {
          "name": "Jiang Gui",
          "affiliation": null
        }
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.",
      "publishedDate": "2026-01-07T23:05:17Z",
      "updatedDate": "2026-01-07T23:05:17Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04442v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04442",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04435",
      "title": "Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs",
      "authors": [
        {
          "name": "Myra Cheng",
          "affiliation": null
        },
        {
          "name": "Robert D. Hawkins",
          "affiliation": null
        },
        {
          "name": "Dan Jurafsky",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase \"wait a minute\", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.",
      "publishedDate": "2026-01-07T22:47:24Z",
      "updatedDate": "2026-01-07T22:47:24Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04435v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04435",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04260",
      "title": "Towards a Mechanistic Understanding of Propositional Logical Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Danchun Chen",
          "affiliation": null
        },
        {
          "name": "Qiyao Yan",
          "affiliation": null
        },
        {
          "name": "Liangming Pan",
          "affiliation": null
        }
      ],
      "abstract": "Understanding how Large Language Models (LLMs) perform logical reasoning internally remains a fundamental challenge. While prior mechanistic studies focus on identifying taskspecific circuits, they leave open the question of what computational strategies LLMs employ for propositional reasoning. We address this gap through comprehensive analysis of Qwen3 (8B and 14B) on PropLogic-MI, a controlled dataset spanning 11 propositional logic rule categories across one-hop and two-hop reasoning. Rather than asking ''which components are necessary,'' we ask ''how does the model organize computation?'' Our analysis reveals a coherent computational architecture comprising four interlocking mechanisms: Staged Computation (layer-wise processing phases), Information Transmission (information flow aggregation at boundary tokens), Fact Retrospection (persistent re-access of source facts), and Specialized Attention Heads (functionally distinct head types). These mechanisms generalize across model scales, rule types, and reasoning depths, providing mechanistic evidence that LLMs employ structured computational strategies for logical reasoning.",
      "publishedDate": "2026-01-07T04:20:30Z",
      "updatedDate": "2026-01-07T04:20:30Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04260v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04260",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04254",
      "title": "Scaling Trends for Multi-Hop Contextual Reasoning in Mid-Scale Language Models",
      "authors": [
        {
          "name": "Brady Steele",
          "affiliation": null
        },
        {
          "name": "Micah Katz",
          "affiliation": null
        }
      ],
      "abstract": "We present a controlled study of multi-hop contextual reasoning in large language models, providing a clean demonstration of the task-method dissociation: rule-based pattern matching achieves 100% success on structured information retrieval but only 6.7% on tasks requiring cross-document reasoning, while LLM-based multi-agent systems show the inverse pattern, achieving up to 80% on reasoning tasks where rule-based methods fail. Using a synthetic evaluation framework with 120 trials across four models (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), we report three key findings: (1) Multi-agent amplification depends on base capability: statistically significant gains occur only for models with sufficient reasoning ability (p < 0.001 for LLaMA-3 8B, p = 0.014 for Mixtral), with improvements of up to 46.7 percentage points, while weaker models show no benefit, suggesting amplification rather than compensation; (2) Active parameters predict reasoning performance: Mixtral's performance aligns with its ~12B active parameters rather than 47B total, consistent with the hypothesis that inference-time compute drives reasoning capability in MoE architectures; (3) Architecture quality matters: LLaMA-3 8B outperforms LLaMA-2 13B despite fewer parameters, consistent with known training improvements. Our results provide controlled quantitative evidence for intuitions about multi-agent coordination and MoE scaling, while highlighting the dependence of multi-agent benefits on base model capability. We release our evaluation framework to support reproducible research on reasoning in mid-scale models.",
      "publishedDate": "2026-01-06T20:18:55Z",
      "updatedDate": "2026-01-06T20:18:55Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04254v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04254",
      "comment": "18 pages, 6 figures, 8 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04237",
      "title": "SAGE-32B: Agentic Reasoning via Iterative Distillation",
      "authors": [
        {
          "name": "Basab Jha",
          "affiliation": null
        },
        {
          "name": "Firoj Paudel",
          "affiliation": null
        },
        {
          "name": "Ujjwal Puri",
          "affiliation": null
        },
        {
          "name": "Ethan Henkel",
          "affiliation": null
        },
        {
          "name": "Zhang Yuting",
          "affiliation": null
        },
        {
          "name": "Mateusz Kowalczyk",
          "affiliation": null
        },
        {
          "name": "Mei Huang",
          "affiliation": null
        },
        {
          "name": "Choi Donghyuk",
          "affiliation": null
        },
        {
          "name": "Wang Junhao",
          "affiliation": null
        }
      ],
      "abstract": "We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b",
      "publishedDate": "2026-01-04T16:41:58Z",
      "updatedDate": "2026-01-04T16:41:58Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04237v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04237",
      "comment": "23 Pages, 3 figures, 4 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "planning",
        "evaluation",
        "agents",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "planning",
          "evaluation",
          "agents",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04500",
      "title": "GUITester: Enabling GUI Agents for Exploratory Defect Discovery",
      "authors": [
        {
          "name": "Yifei Gao",
          "affiliation": null
        },
        {
          "name": "Jiang Wu",
          "affiliation": null
        },
        {
          "name": "Xiaoyi Chen",
          "affiliation": null
        },
        {
          "name": "Yifan Yang",
          "affiliation": null
        },
        {
          "name": "Zhe Cui",
          "affiliation": null
        },
        {
          "name": "Tianyi Ma",
          "affiliation": null
        },
        {
          "name": "Jiaming Zhang",
          "affiliation": null
        },
        {
          "name": "Jitao Sang",
          "affiliation": null
        }
      ],
      "abstract": "Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \\textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \\textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \\textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \\textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \\textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \\textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\\footnote{Our code is now available in~\\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.",
      "publishedDate": "2026-01-08T02:07:53Z",
      "updatedDate": "2026-01-08T02:07:53Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04500v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04500",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "code-generation",
        "planning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation",
          "planning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04458",
      "title": "Using Large Language Models to Detect Socially Shared Regulation of Collaborative Learning",
      "authors": [
        {
          "name": "Jiayi Zhang",
          "affiliation": null
        },
        {
          "name": "Conrad Borchers",
          "affiliation": null
        },
        {
          "name": "Clayton Cohn",
          "affiliation": null
        },
        {
          "name": "Namrata Srivastava",
          "affiliation": null
        },
        {
          "name": "Caitlin Snyder",
          "affiliation": null
        },
        {
          "name": "Siyuan Guo",
          "affiliation": null
        },
        {
          "name": "Ashwin T S",
          "affiliation": null
        },
        {
          "name": "Naveeduddin Mohammed",
          "affiliation": null
        },
        {
          "name": "Haley Noh",
          "affiliation": null
        },
        {
          "name": "Gautam Biswas",
          "affiliation": null
        }
      ],
      "abstract": "The field of learning analytics has made notable strides in automating the detection of complex learning processes in multimodal data. However, most advancements have focused on individualized problem-solving instead of collaborative, open-ended problem-solving, which may offer both affordances (richer data) and challenges (low cohesion) to behavioral prediction. Here, we extend predictive models to automatically detect socially shared regulation of learning (SSRL) behaviors in collaborative computational modeling environments using embedding-based approaches. We leverage large language models (LLMs) as summarization tools to generate task-aware representations of student dialogue aligned with system logs. These summaries, combined with text-only embeddings, context-enriched embeddings, and log-derived features, were used to train predictive models. Results show that text-only embeddings often achieve stronger performance in detecting SSRL behaviors related to enactment or group dynamics (e.g., off-task behavior or requesting assistance). In contrast, contextual and multimodal features provide complementary benefits for constructs such as planning and reflection. Overall, our findings highlight the promise of embedding-based models for extending learning analytics by enabling scalable detection of SSRL behaviors, ultimately supporting real-time feedback and adaptive scaffolding in collaborative learning environments that teachers value.",
      "publishedDate": "2026-01-08T00:30:46Z",
      "updatedDate": "2026-01-08T00:30:46Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04458v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04458",
      "comment": "Short research paper accepted at Learning Analytics and Knowledge (LAK '26)",
      "journalRef": null,
      "doi": "10.1145/3785022.3785083",
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04377",
      "title": "Disco-RAG: Discourse-Aware Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Dongqi Liu",
          "affiliation": null
        },
        {
          "name": "Hang Ding",
          "affiliation": null
        },
        {
          "name": "Qiming Feng",
          "affiliation": null
        },
        {
          "name": "Jian Li",
          "affiliation": null
        },
        {
          "name": "Xurong Xie",
          "affiliation": null
        },
        {
          "name": "Zhucun Xue",
          "affiliation": null
        },
        {
          "name": "Chengjie Wang",
          "affiliation": null
        },
        {
          "name": "Jiangning Zhang",
          "affiliation": null
        },
        {
          "name": "Yabiao Wang",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.",
      "publishedDate": "2026-01-07T20:32:50Z",
      "updatedDate": "2026-01-10T18:27:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04377v2",
      "arxivUrl": "https://arxiv.org/abs/2601.04377",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04790",
      "title": "Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework",
      "authors": [
        {
          "name": "Junhyuk Choi",
          "affiliation": null
        },
        {
          "name": "Jeongyoun Kwon",
          "affiliation": null
        },
        {
          "name": "Heeju Kim",
          "affiliation": null
        },
        {
          "name": "Haeun Cho",
          "affiliation": null
        },
        {
          "name": "Hayeong Jung",
          "affiliation": null
        },
        {
          "name": "Sehee Min",
          "affiliation": null
        },
        {
          "name": "Bugeun Kim",
          "affiliation": null
        }
      ],
      "abstract": "Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.",
      "publishedDate": "2026-01-08T10:13:56Z",
      "updatedDate": "2026-01-08T10:13:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04790v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04790",
      "comment": "Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04632",
      "title": "From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset",
      "authors": [
        {
          "name": "Haneul Yoo",
          "affiliation": null
        },
        {
          "name": "Won Ik Cho",
          "affiliation": null
        },
        {
          "name": "Geunhye Kim",
          "affiliation": null
        },
        {
          "name": "Jiyoon Han",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.",
      "publishedDate": "2026-01-08T06:04:59Z",
      "updatedDate": "2026-01-08T06:04:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04632v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04632",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05038",
      "title": "ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG",
      "authors": [
        {
          "name": "Jianbo Li",
          "affiliation": null
        },
        {
          "name": "Yi Jiang",
          "affiliation": null
        },
        {
          "name": "Sendong Zhao",
          "affiliation": null
        },
        {
          "name": "Bairui Hu",
          "affiliation": null
        },
        {
          "name": "Haochun Wang",
          "affiliation": null
        },
        {
          "name": "Bing Qin",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.",
      "publishedDate": "2026-01-08T15:44:52Z",
      "updatedDate": "2026-01-08T15:44:52Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05038v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05038",
      "comment": "Code is available at https://github.com/liunian-Jay/ArcAligner.git",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05027",
      "title": "OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Yi Jiang",
          "affiliation": null
        },
        {
          "name": "Sendong Zhao",
          "affiliation": null
        },
        {
          "name": "Jianbo Li",
          "affiliation": null
        },
        {
          "name": "Bairui Hu",
          "affiliation": null
        },
        {
          "name": "Yanrui Du",
          "affiliation": null
        },
        {
          "name": "Haochun Wang",
          "affiliation": null
        },
        {
          "name": "Bing Qin",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an \"Expand-then-Refine\" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available.",
      "publishedDate": "2026-01-08T15:35:01Z",
      "updatedDate": "2026-01-08T15:35:01Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05027v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05027",
      "comment": "Code is available at https://github.com/liunian-Jay/OptiSet.git",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04859",
      "title": "A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs",
      "authors": [
        {
          "name": "Maxime Delmas",
          "affiliation": null
        },
        {
          "name": "Lei Xu",
          "affiliation": null
        },
        {
          "name": "André Freitas",
          "affiliation": null
        }
      ],
      "abstract": "Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.",
      "publishedDate": "2026-01-08T11:50:40Z",
      "updatedDate": "2026-01-08T11:50:40Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04859v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04859",
      "comment": "23 pages, 10 figures, 6 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04764",
      "title": "Orion-RAG: Path-Aligned Hybrid Retrieval for Graphless Data",
      "authors": [
        {
          "name": "Zhen Chen",
          "affiliation": null
        },
        {
          "name": "Weihao Xie",
          "affiliation": null
        },
        {
          "name": "Peilin Chen",
          "affiliation": null
        },
        {
          "name": "Shiqi Wang",
          "affiliation": null
        },
        {
          "name": "Jianping Wang",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has proven effective for knowledge synthesis, yet it encounters significant challenges in practical scenarios where data is inherently discrete and fragmented. In most environments, information is distributed across isolated files like reports and logs that lack explicit links. Standard search engines process files independently, ignoring the connections between them. Furthermore, manually building Knowledge Graphs is impractical for such vast data. To bridge this gap, we present Orion-RAG. Our core insight is simple yet effective: we do not need heavy algorithms to organize this data. Instead, we use a low-complexity strategy to extract lightweight paths that naturally link related concepts. We demonstrate that this streamlined approach suffices to transform fragmented documents into semi-structured data, enabling the system to link information across different files effectively. Extensive experiments demonstrate that Orion-RAG consistently outperforms mainstream frameworks across diverse domains, supporting real-time updates and explicit Human-in-the-Loop verification with high cost-efficiency. Experiments on FinanceBench demonstrate superior precision with a 25.2% relative improvement over strong baselines.",
      "publishedDate": "2026-01-08T09:32:01Z",
      "updatedDate": "2026-01-08T09:32:01Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04764v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04764",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04525",
      "title": "GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence",
      "authors": [
        {
          "name": "Yibo Zhao",
          "affiliation": null
        },
        {
          "name": "Jiapeng Zhu",
          "affiliation": null
        },
        {
          "name": "Zichen Ding",
          "affiliation": null
        },
        {
          "name": "Xiang Li",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..",
      "publishedDate": "2026-01-08T02:47:33Z",
      "updatedDate": "2026-01-08T02:47:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04525v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04525",
      "comment": "18 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04416",
      "title": "Transitive Expert Error and Routing Problems in Complex AI Systems",
      "authors": [
        {
          "name": "Forest Mars",
          "affiliation": null
        }
      ],
      "abstract": "Domain expertise enhances judgment within boundaries but creates systematic vulnerabilities specifically at borders. We term this Transitive Expert Error (TEE), distinct from Dunning-Kruger effects, requiring calibrated expertise as precondition. Mechanisms enabling reliable within-domain judgment become liabilities when structural similarity masks causal divergence. Two core mechanisms operate: structural similarity bias causes experts to overweight surface features (shared vocabulary, patterns, formal structure) while missing causal architecture differences; authority persistence maintains confidence across competence boundaries through social reinforcement and metacognitive failures (experts experience no subjective uncertainty as pattern recognition operates smoothly on familiar-seeming inputs.) These mechanism intensify under three conditions: shared vocabulary masking divergent processes, social pressure for immediate judgment, and delayed feedback. These findings extend to AI routing architectures (MoE systems, multi-model orchestration, tool-using agents, RAG systems) exhibiting routing-induced failures (wrong specialist selected) and coverage-induced failures (no appropriate specialist exists). Both produce a hallucination phenotype: confident, coherent, structurally plausible but causally incorrect outputs at domain boundaries. In human systems where mechanisms are cognitive black boxes; AI architectures make them explicit and addressable. We propose interventions: multi-expert activation with disagreement detection (router level), boundary-aware calibration (specialist level), and coverage gap detection (training level). TEE has detectable signatures (routing patterns, confidence-accuracy dissociations, domain-inappropriate content) enabling monitoring and mitigation. What remains intractable in human cognition becomes addressable through architectural design.",
      "publishedDate": "2026-01-07T21:53:06Z",
      "updatedDate": "2026-01-07T21:53:06Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04416v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04416",
      "comment": "31pp",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04297",
      "title": "ArtCognition: A Multimodal AI Framework for Affective State Sensing from Visual and Kinematic Drawing Cues",
      "authors": [
        {
          "name": "Behrad Binaei-Haghighi",
          "affiliation": null
        },
        {
          "name": "Nafiseh Sadat Sajadi",
          "affiliation": null
        },
        {
          "name": "Mehrad Liviyan",
          "affiliation": null
        },
        {
          "name": "Reyhane Akhavan Kharazi",
          "affiliation": null
        },
        {
          "name": "Fatemeh Amirkhani",
          "affiliation": null
        },
        {
          "name": "Behnam Bahrak",
          "affiliation": null
        }
      ],
      "abstract": "The objective assessment of human affective and psychological states presents a significant challenge, particularly through non-verbal channels. This paper introduces digital drawing as a rich and underexplored modality for affective sensing. We present a novel multimodal framework, named ArtCognition, for the automated analysis of the House-Tree-Person (HTP) test, a widely used psychological instrument. ArtCognition uniquely fuses two distinct data streams: static visual features from the final artwork, captured by computer vision models, and dynamic behavioral kinematic cues derived from the drawing process itself, such as stroke speed, pauses, and smoothness. To bridge the gap between low-level features and high-level psychological interpretation, we employ a Retrieval-Augmented Generation (RAG) architecture. This grounds the analysis in established psychological knowledge, enhancing explainability and reducing the potential for model hallucination. Our results demonstrate that the fusion of visual and behavioral kinematic cues provides a more nuanced assessment than either modality alone. We show significant correlations between the extracted multimodal features and standardized psychological metrics, validating the framework's potential as a scalable tool to support clinicians. This work contributes a new methodology for non-intrusive affective state assessment and opens new avenues for technology-assisted mental healthcare.",
      "publishedDate": "2026-01-07T17:35:37Z",
      "updatedDate": "2026-01-07T17:35:37Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CV",
        "cs.HC",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04297v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04297",
      "comment": "12 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04801",
      "title": "MPM-LLM4DSE: Reaching the Pareto Frontier in HLS with Multimodal Learning and LLM-Driven Exploration",
      "authors": [
        {
          "name": "Lei Xu",
          "affiliation": null
        },
        {
          "name": "Shanshan Wang",
          "affiliation": null
        },
        {
          "name": "Chenglong Xiao",
          "affiliation": null
        }
      ],
      "abstract": "High-Level Synthesis (HLS) design space exploration (DSE) seeks Pareto-optimal designs within expansive pragma configuration spaces. To accelerate HLS DSE, graph neural networks (GNNs) are commonly employed as surrogates for HLS tools to predict quality of results (QoR) metrics, while multi-objective optimization algorithms expedite the exploration. However, GNN-based prediction methods may not fully capture the rich semantic features inherent in behavioral descriptions, and conventional multi-objective optimization algorithms often do not explicitly account for the domain-specific knowledge regarding how pragma directives influence QoR. To address these limitations, this paper proposes the MPM-LLM4DSE framework, which incorporates a multimodal prediction model (MPM) that simultaneously fuses features from behavioral descriptions and control and data flow graphs. Furthermore, the framework employs a large language model (LLM) as an optimizer, accompanied by a tailored prompt engineering methodology. This methodology incorporates pragma impact analysis on QoR to guide the LLM in generating high-quality configurations (LLM4DSE). Experimental results demonstrate that our multimodal predictive model significantly outperforms state-of-the-art work ProgSG by up to 10.25$\\times$. Furthermore, in DSE tasks, the proposed LLM4DSE achieves an average performance gain of 39.90\\% over prior methods, validating the effectiveness of our prompting methodology. Code and models are available at https://github.com/wslcccc/MPM-LLM4DSE.",
      "publishedDate": "2026-01-08T10:32:49Z",
      "updatedDate": "2026-01-08T10:32:49Z",
      "primaryCategory": "cs.AR",
      "arxivCategories": [
        "cs.AR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04801v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04801",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04455",
      "title": "Re-Rankers as Relevance Judges",
      "authors": [
        {
          "name": "Chuan Meng",
          "affiliation": null
        },
        {
          "name": "Jiqun Liu",
          "affiliation": null
        },
        {
          "name": "Mohammad Aliannejadi",
          "affiliation": null
        },
        {
          "name": "Fengran Mo",
          "affiliation": null
        },
        {
          "name": "Jeff Dalton",
          "affiliation": null
        },
        {
          "name": "Maarten de Rijke",
          "affiliation": null
        }
      ],
      "abstract": "Using large language models (LLMs) to predict relevance judgments has shown promising results. Most studies treat this task as a distinct research line, e.g., focusing on prompt design for predicting relevance labels given a query and passage. However, predicting relevance judgments is essentially a form of relevance prediction, a problem extensively studied in tasks such as re-ranking. Despite this potential overlap, little research has explored reusing or adapting established re-ranking methods to predict relevance judgments, leading to potential resource waste and redundant development. To bridge this gap, we reproduce re-rankers in a re-ranker-as-relevance-judge setup. We design two adaptation strategies: (i) using binary tokens (e.g., \"true\" and \"false\") generated by a re-ranker as direct judgments, and (ii) converting continuous re-ranking scores into binary labels via thresholding. We perform extensive experiments on TREC-DL 2019 to 2023 with 8 re-rankers from 3 families, ranging from 220M to 32B, and analyse the evaluation bias exhibited by re-ranker-based judges. Results show that re-ranker-based relevance judges, under both strategies, can outperform UMBRELA, a state-of-the-art LLM-based relevance judge, in around 40% to 50% of the cases; they also exhibit strong self-preference towards their own and same-family re-rankers, as well as cross-family bias.",
      "publishedDate": "2026-01-08T00:02:59Z",
      "updatedDate": "2026-01-08T00:02:59Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04455v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04455",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03552",
      "title": "From Risk Perception to Behavior Large Language Models-Based Simulation of Pandemic Prevention Behaviors",
      "authors": [
        {
          "name": "Lujia Bo",
          "affiliation": null
        },
        {
          "name": "Mingxuan Chen",
          "affiliation": null
        },
        {
          "name": "Youduo Chen",
          "affiliation": null
        },
        {
          "name": "Xiaofan Gui",
          "affiliation": null
        },
        {
          "name": "Jiang Bian",
          "affiliation": null
        },
        {
          "name": "Chunyan Wang",
          "affiliation": null
        },
        {
          "name": "Yi Liu",
          "affiliation": null
        }
      ],
      "abstract": "Individual prevention behaviors are a primary line of defense during the early stages of novel infectious disease outbreaks, yet their adoption is heterogeneous and difficult to forecast-especially when empirical data are scarce and epidemic-policy contexts evolve rapidly. To address this gap, we develop an LLM-based prevention-behavior simulation framework that couples (i) a static module for behavior-intensity prediction under a specified external context and (ii) a dynamic module that updates residents' perceived risk over time and propagates these updates into behavior evolution. The model is implemented via structured prompt engineering in a first-person perspective and is evaluated against two rounds of survey data from Beijing residents (R1: December 2020; R2: August 2021) under progressively realistic data-availability settings: zero-shot, few-shot, and cross-context transfer. Using Kolmogorov-Smirnov tests to compare simulated and observed behavior distributions (p > 0.001 as the validity criterion), the framework demonstrates robust performance and improves with limited reference examples; reported predictive accuracy increases from 72.7% (zero-shot) to 81.8% (few-shot), and remains high at 77.8% under transfer to novel contexts. We further apply the framework to simulate behavior changes during China's December 2022 policy relaxation and to stress-test behavioral responses across 120 systematically varied epidemic conditions (R0, CFR, and control-measure tiers). Results indicate broad behavioral loosening under relaxation but a distinctive counter-trend increase in drain-related disinfection, highlighting how low-cost, low-friction behaviors may persist or intensify even when external constraints recede-raising a potential environmental tradeoff.",
      "publishedDate": "2026-01-07T03:36:55Z",
      "updatedDate": "2026-01-07T03:36:55Z",
      "primaryCategory": "cs.SI",
      "arxivCategories": [
        "cs.SI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03552v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03552",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "prompting",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03493",
      "title": "Submodular Evaluation Subset Selection in Automatic Prompt Optimization",
      "authors": [
        {
          "name": "Jinming Nian",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Peng",
          "affiliation": null
        },
        {
          "name": "Hongwei Shang",
          "affiliation": null
        },
        {
          "name": "Dae Hoon Park",
          "affiliation": null
        },
        {
          "name": "Yi Fang",
          "affiliation": null
        }
      ],
      "abstract": "Automatic prompt optimization reduces manual prompt engineering, but relies on task performance measured on a small, often randomly sampled evaluation subset as its main source of feedback signal. Despite this, how to select that evaluation subset is usually treated as an implementation detail. We study evaluation subset selection for prompt optimization from a principled perspective and propose SESS, a submodular evaluation subset selection method. We frame selection as maximizing an objective set function and show that, under mild conditions, it is monotone and submodular, enabling greedy selection with theoretical guarantees. Across GSM8K, MATH, and GPQA-Diamond, submodularly selected evaluation subsets can yield better optimized prompts than random or heuristic baselines.",
      "publishedDate": "2026-01-07T01:12:45Z",
      "updatedDate": "2026-01-07T01:12:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03493v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03493",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03298",
      "title": "130k Lines of Formal Topology in Two Weeks: Simple and Cheap Autoformalization for Everyone?",
      "authors": [
        {
          "name": "Josef Urban",
          "affiliation": null
        }
      ],
      "abstract": "This is a brief description of a project that has already autoformalized a large portion of the general topology from the Munkres textbook (which has in total 241 pages in 7 chapters and 39 sections). The project has been running since November 21, 2025 and has as of January 4, 2026, produced 160k lines of formalized topology. Most of it (about 130k lines) have been done in two weeks,from December 22 to January 4, for an LLM subscription cost of about \\$100. This includes a 3k-line proof of Urysohn's lemma, a 2k-line proof of Urysohn's Metrization theorem, over 10k-line proof of the Tietze extension theorem, and many more (in total over 1.5k lemmas/theorems). The approach is quite simple and cheap: build a long-running feedback loop between an LLM and a reasonably fast proof checker equipped with a core foundational library. The LLM is now instantiated as ChatGPT (mostly 5.2) or Claude Sonnet (4.5) run through the respective Codex or Claude Code command line interfaces. The proof checker is Chad Brown's higher-order set theory system Megalodon, and the core library is Brown's formalization of basic set theory and surreal numbers (including reals, etc). The rest is some prompt engineering and technical choices which we describe here. Based on the fast progress, low cost, virtually unknown ITP/library, and the simple setup available to everyone, we believe that (auto)formalization may become quite easy and ubiquitous in 2026, regardless of which proof assistant is used.",
      "publishedDate": "2026-01-06T01:01:04Z",
      "updatedDate": "2026-01-06T01:01:04Z",
      "primaryCategory": "cs.LO",
      "arxivCategories": [
        "cs.LO",
        "cs.AI",
        "cs.SC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03298v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03298",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03594",
      "title": "Jailbreaking LLMs & VLMs: Mechanisms, Evaluation, and Unified Defense",
      "authors": [
        {
          "name": "Zejian Chen",
          "affiliation": null
        },
        {
          "name": "Chaozhuo Li",
          "affiliation": null
        },
        {
          "name": "Chao Li",
          "affiliation": null
        },
        {
          "name": "Xi Zhang",
          "affiliation": null
        },
        {
          "name": "Litian Zhang",
          "affiliation": null
        },
        {
          "name": "Yiming He",
          "affiliation": null
        }
      ],
      "abstract": "This paper provides a systematic survey of jailbreak attacks and defenses on Large Language Models (LLMs) and Vision-Language Models (VLMs), emphasizing that jailbreak vulnerabilities stem from structural factors such as incomplete training data, linguistic ambiguity, and generative uncertainty. It further differentiates between hallucinations and jailbreaks in terms of intent and triggering mechanisms. We propose a three-dimensional survey framework: (1) Attack dimension-including template/encoding-based, in-context learning manipulation, reinforcement/adversarial learning, LLM-assisted and fine-tuned attacks, as well as prompt- and image-level perturbations and agent-based transfer in VLMs; (2) Defense dimension-encompassing prompt-level obfuscation, output evaluation, and model-level alignment or fine-tuning; and (3) Evaluation dimension-covering metrics such as Attack Success Rate (ASR), toxicity score, query/time cost, and multimodal Clean Accuracy and Attribute Success Rate. Compared with prior works, this survey spans the full spectrum from text-only to multimodal settings, consolidating shared mechanisms and proposing unified defense principles: variant-consistency and gradient-sensitivity detection at the perception layer, safety-aware decoding and output review at the generation layer, and adversarially augmented preference alignment at the parameter layer. Additionally, we summarize existing multimodal safety benchmarks and discuss future directions, including automated red teaming, cross-modal collaborative defense, and standardized evaluation.",
      "publishedDate": "2026-01-07T05:25:33Z",
      "updatedDate": "2026-01-07T05:25:33Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03594v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03594",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "prompting",
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05103",
      "title": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content",
      "authors": [
        {
          "name": "Changxu Duan",
          "affiliation": null
        },
        {
          "name": "Zhiyin Tan",
          "affiliation": null
        }
      ],
      "abstract": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.",
      "publishedDate": "2026-01-08T16:48:36Z",
      "updatedDate": "2026-01-08T16:48:36Z",
      "primaryCategory": "cs.DL",
      "arxivCategories": [
        "cs.DL",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05103v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05103",
      "comment": "Accepted at the 29th International Conference on Theory and Practice of Digital Libraries (TPDL 2025)",
      "journalRef": null,
      "doi": "10.1007/978-3-032-05409-8_12",
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05099",
      "title": "Multi-Disciplinary Dataset Discovery from Citation-Verified Literature Contexts",
      "authors": [
        {
          "name": "Zhiyin Tan",
          "affiliation": null
        },
        {
          "name": "Changxu Duan",
          "affiliation": null
        }
      ],
      "abstract": "Identifying suitable datasets for a research question remains challenging because existing dataset search engines rely heavily on metadata quality and keyword overlap, which often fail to capture the semantic intent of scientific investigation. We introduce a literature-driven framework that discovers datasets from citation contexts in scientific papers, enabling retrieval grounded in actual research use rather than metadata availability. Our approach combines large-scale citation-context extraction, schema-guided dataset recognition with Large Language Models, and provenance-preserving entity resolution. We evaluate the system on eight survey-derived computer science queries and find that it achieves substantially higher recall than Google Dataset Search and DataCite Commons, with normalized recall ranging from an average of 47.47% to a highest value of 81.82%. Beyond recovering gold-standard datasets, the method also surfaces additional datasets not documented in the surveys. Expert assessments across five top-level Fields of Science indicate that a substantial portion of the additional datasets are considered high utility, and some are regarded as novel for the specific topics chosen by the experts. These findings establish citation-context mining as an effective and generalizable paradigm for dataset discovery, particularly in settings where datasets lack sufficient or reliable metadata. To support reproducibility and future extensions, we release our code, evaluation datasets, and results on GitHub (https://github.com/Fireblossom/citation-context-dataset-discovery).",
      "publishedDate": "2026-01-08T16:46:06Z",
      "updatedDate": "2026-01-08T16:46:06Z",
      "primaryCategory": "cs.DL",
      "arxivCategories": [
        "cs.DL",
        "cs.CL",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05099v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05099",
      "comment": "Accepted at the 25th ACM/IEEE Joint Conference on Digital Libraries (JCDL 2025)",
      "journalRef": null,
      "doi": "10.1109/JCDL67857.2025.00022",
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04885",
      "title": "CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters",
      "authors": [
        {
          "name": "Ao Sun",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Wang",
          "affiliation": null
        },
        {
          "name": "Zhe Tan",
          "affiliation": null
        },
        {
          "name": "Yu Li",
          "affiliation": null
        },
        {
          "name": "Jiachen Zhu",
          "affiliation": null
        },
        {
          "name": "Shu Su",
          "affiliation": null
        },
        {
          "name": "Yuheng Jia",
          "affiliation": null
        }
      ],
      "abstract": "As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \\textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \\textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \\textbf{\\textsc{CuMA}} (\\textbf{Cu}ltural \\textbf{M}ixture of \\textbf{A}dapters), a framework that frames alignment as a \\textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \\textsc{CuMA} internalizes a \\textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \\textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \\textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.",
      "publishedDate": "2026-01-08T12:30:43Z",
      "updatedDate": "2026-01-08T12:30:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04885v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04885",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04879",
      "title": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
      "authors": [
        {
          "name": "Mingyue Cheng",
          "affiliation": null
        },
        {
          "name": "Daoyu Wang",
          "affiliation": null
        },
        {
          "name": "Qi Liu",
          "affiliation": null
        },
        {
          "name": "Shuo Yu",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Tao",
          "affiliation": null
        },
        {
          "name": "Yuqian Wang",
          "affiliation": null
        },
        {
          "name": "Chengzhong Chu",
          "affiliation": null
        },
        {
          "name": "Yu Duan",
          "affiliation": null
        },
        {
          "name": "Mingkang Long",
          "affiliation": null
        },
        {
          "name": "Enhong Chen",
          "affiliation": null
        }
      ],
      "abstract": "Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.",
      "publishedDate": "2026-01-08T12:27:52Z",
      "updatedDate": "2026-01-08T12:27:52Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04879v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04879",
      "comment": "26 Pages, 9 Figures, 7 Tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04740",
      "title": "RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation",
      "authors": [
        {
          "name": "Huawei Zheng",
          "affiliation": null
        },
        {
          "name": "Xinqi Jiang",
          "affiliation": null
        },
        {
          "name": "Sen Yang",
          "affiliation": null
        },
        {
          "name": "Shouling Ji",
          "affiliation": null
        },
        {
          "name": "Yingcai Wu",
          "affiliation": null
        },
        {
          "name": "Dazhen Deng",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.",
      "publishedDate": "2026-01-08T09:05:28Z",
      "updatedDate": "2026-01-08T09:05:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04740v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04740",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04728",
      "title": "Excess Description Length of Learning Generalizable Predictors",
      "authors": [
        {
          "name": "Elizabeth Donoway",
          "affiliation": null
        },
        {
          "name": "Hailey Joren",
          "affiliation": null
        },
        {
          "name": "Fabien Roger",
          "affiliation": null
        },
        {
          "name": "Jan Leike",
          "affiliation": null
        }
      ],
      "abstract": "Understanding whether fine-tuning elicits latent capabilities or teaches new ones is a fundamental question for language model evaluation and safety. We develop a formal information-theoretic framework for quantifying how much predictive structure fine-tuning extracts from the train dataset and writes into a model's parameters. Our central quantity, Excess Description Length (EDL), is defined via prequential coding and measures the gap between the bits required to encode training labels sequentially using an evolving model (trained online) and the residual encoding cost under the final trained model. We establish that EDL is non-negative in expectation, converges to surplus description length in the infinite-data limit, and provides bounds on expected generalization gain. Through a series of toy models, we clarify common confusions about information in learning: why random labels yield EDL near zero, how a single example can eliminate many bits of uncertainty about the underlying rule(s) that describe the data distribution, why structure learned on rare inputs contributes proportionally little to expected generalization, and how format learning creates early transients distinct from capability acquisition. This framework provides rigorous foundations for the empirical observation that capability elicitation and teaching exhibit qualitatively distinct scaling signatures.",
      "publishedDate": "2026-01-08T08:46:42Z",
      "updatedDate": "2026-01-08T08:46:42Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04728v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04728",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04564",
      "title": "When Tone and Words Disagree: Towards Robust Speech Emotion Recognition under Acoustic-Semantic Conflict",
      "authors": [
        {
          "name": "Dawei Huang",
          "affiliation": null
        },
        {
          "name": "Yongjie Lv",
          "affiliation": null
        },
        {
          "name": "Ruijie Xiong",
          "affiliation": null
        },
        {
          "name": "Chunxiang Jin",
          "affiliation": null
        },
        {
          "name": "Xiaojiang Peng",
          "affiliation": null
        }
      ],
      "abstract": "Speech Emotion Recognition (SER) systems often assume congruence between vocal emotion and lexical semantics. However, in real-world interactions, acoustic-semantic conflict is common yet overlooked, where the emotion conveyed by tone contradicts the literal meaning of spoken words. We show that state-of-the-art SER models, including ASR-based, self-supervised learning (SSL) approaches and Audio Language Models (ALMs), suffer performance degradation under such conflicts due to semantic bias or entangled acoustic-semantic representations. To address this, we propose the Fusion Acoustic-Semantic (FAS) framework, which explicitly disentangles acoustic and semantic pathways and bridges them through a lightweight, query-based attention module. To enable systematic evaluation, we introduce the Conflict in Acoustic-Semantic Emotion (CASE), the first dataset dominated by clear and interpretable acoustic-semantic conflicts in varied scenarios. Extensive experiments demonstrate that FAS consistently outperforms existing methods in both in-domain and zero-shot settings. Notably, on the CASE benchmark, conventional SER models fail dramatically, while FAS sets a new SOTA with 59.38% accuracy. Our code and datasets is available at https://github.com/24DavidHuang/FAS.",
      "publishedDate": "2026-01-08T03:47:21Z",
      "updatedDate": "2026-01-08T03:47:21Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04564v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04564",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04554",
      "title": "Exploring Recommender System Evaluation: A Multi-Modal User Agent Framework for A/B Testing",
      "authors": [
        {
          "name": "Wenlin Zhang",
          "affiliation": null
        },
        {
          "name": "Xiangyang Li",
          "affiliation": null
        },
        {
          "name": "Qiyuan Ge",
          "affiliation": null
        },
        {
          "name": "Kuicai Dong",
          "affiliation": null
        },
        {
          "name": "Pengyue Jia",
          "affiliation": null
        },
        {
          "name": "Xiaopeng Li",
          "affiliation": null
        },
        {
          "name": "Zijian Zhang",
          "affiliation": null
        },
        {
          "name": "Maolin Wang",
          "affiliation": null
        },
        {
          "name": "Yichao Wang",
          "affiliation": null
        },
        {
          "name": "Huifeng Guo",
          "affiliation": null
        },
        {
          "name": "Ruiming Tang",
          "affiliation": null
        },
        {
          "name": "Xiangyu Zhao",
          "affiliation": null
        }
      ],
      "abstract": "In recommender systems, online A/B testing is a crucial method for evaluating the performance of different models. However, conducting online A/B testing often presents significant challenges, including substantial economic costs, user experience degradation, and considerable time requirements. With the Large Language Models' powerful capacity, LLM-based agent shows great potential to replace traditional online A/B testing. Nonetheless, current agents fail to simulate the perception process and interaction patterns, due to the lack of real environments and visual perception capability. To address these challenges, we introduce a multi-modal user agent for A/B testing (A/B Agent). Specifically, we construct a recommendation sandbox environment for A/B testing, enabling multimodal and multi-page interactions that align with real user behavior on online platforms. The designed agent leverages multimodal information perception, fine-grained user preferences, and integrates profiles, action memory retrieval, and a fatigue system to simulate complex human decision-making. We validated the potential of the agent as an alternative to traditional A/B testing from three perspectives: model, data, and features. Furthermore, we found that the data generated by A/B Agent can effectively enhance the capabilities of recommendation models. Our code is publicly available at https://github.com/Applied-Machine-Learning-Lab/ABAgent.",
      "publishedDate": "2026-01-08T03:33:43Z",
      "updatedDate": "2026-01-08T03:33:43Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04554v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04554",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "agents",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04534",
      "title": "BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation",
      "authors": [
        {
          "name": "Amit Bin Tariqul",
          "affiliation": null
        },
        {
          "name": "A N M Zahid Hossain Milkan",
          "affiliation": null
        },
        {
          "name": "Sahab-Al-Chowdhury",
          "affiliation": null
        },
        {
          "name": "Syed Rifat Raiyan",
          "affiliation": null
        },
        {
          "name": "Hasan Mahmud",
          "affiliation": null
        },
        {
          "name": "Md Kamrul Hasan",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\\times$ to 4$\\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.",
      "publishedDate": "2026-01-08T03:01:59Z",
      "updatedDate": "2026-01-08T03:01:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04534v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04534",
      "comment": "Under review, 12 pages, 7 figures, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04497",
      "title": "Vision-Language Agents for Interactive Forest Change Analysis",
      "authors": [
        {
          "name": "James Brock",
          "affiliation": null
        },
        {
          "name": "Ce Zhang",
          "affiliation": null
        },
        {
          "name": "Nantheera Anantrasirichai",
          "affiliation": null
        }
      ],
      "abstract": "Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.",
      "publishedDate": "2026-01-08T02:02:36Z",
      "updatedDate": "2026-01-08T02:02:36Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04497v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04497",
      "comment": "5 pages, 4 figures, Submitted to IGARSS 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03988",
      "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures",
      "authors": [
        {
          "name": "Nicolas Lacroix",
          "affiliation": null
        },
        {
          "name": "Mireille Blay-Fornarino",
          "affiliation": null
        },
        {
          "name": "Sébastien Mosser",
          "affiliation": null
        },
        {
          "name": "Frederic Precioso",
          "affiliation": null
        }
      ],
      "abstract": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions. Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices. Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.",
      "publishedDate": "2026-01-07T15:00:22Z",
      "updatedDate": "2026-01-07T15:00:22Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03988v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03988",
      "comment": "SANER 2026 Registered Report",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03955",
      "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
      "authors": [
        {
          "name": "Xu Zhang",
          "affiliation": null
        },
        {
          "name": "Cheng Da",
          "affiliation": null
        },
        {
          "name": "Huan Yang",
          "affiliation": null
        },
        {
          "name": "Kun Gai",
          "affiliation": null
        },
        {
          "name": "Ming Lu",
          "affiliation": null
        },
        {
          "name": "Zhan Ma",
          "affiliation": null
        }
      ],
      "abstract": "Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring \"vision\" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.",
      "publishedDate": "2026-01-07T14:09:18Z",
      "updatedDate": "2026-01-07T14:09:18Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03955v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03955",
      "comment": "Technical report",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03793",
      "title": "Prompt Tuning without Labeled Samples for Zero-Shot Node Classification in Text-Attributed Graphs",
      "authors": [
        {
          "name": "Sethupathy Parameswaran",
          "affiliation": null
        },
        {
          "name": "Suresh Sundaram",
          "affiliation": null
        },
        {
          "name": "Yuan Fang",
          "affiliation": null
        }
      ],
      "abstract": "Node classification is a fundamental problem in information retrieval with many real-world applications, such as community detection in social networks, grouping articles published online and product categorization in e-commerce. Zero-shot node classification in text-attributed graphs (TAGs) presents a significant challenge, particularly due to the absence of labeled data. In this paper, we propose a novel Zero-shot Prompt Tuning (ZPT) framework to address this problem by leveraging a Universal Bimodal Conditional Generator (UBCG). Our approach begins with pre-training a graph-language model to capture both the graph structure and the associated textual descriptions of each node. Following this, a conditional generative model is trained to learn the joint distribution of nodes in both graph and text modalities, enabling the generation of synthetic samples for each class based solely on the class name. These synthetic node and text embeddings are subsequently used to perform continuous prompt tuning, facilitating effective node classification in a zero-shot setting. Furthermore, we conduct extensive experiments on multiple benchmark datasets, demonstrating that our framework performs better than existing state-of-the-art baselines. We also provide ablation studies to validate the contribution of the bimodal generator. The code is provided at: https://github.com/Sethup123/ZPT.",
      "publishedDate": "2026-01-07T10:50:18Z",
      "updatedDate": "2026-01-07T10:50:18Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03793v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03793",
      "comment": "Accepted by WSDM 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03780",
      "title": "Assessing and Improving the Representativeness of Code Generation Benchmarks Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study",
      "authors": [
        {
          "name": "Md Ahasanuzzaman",
          "affiliation": null
        },
        {
          "name": "Bram Adams",
          "affiliation": null
        },
        {
          "name": "Emad Fallahzadeh",
          "affiliation": null
        },
        {
          "name": "Gustavo A. Oliva",
          "affiliation": null
        },
        {
          "name": "Ahmed E. Hassan",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) such as GPT-4, Claude and LLaMA have shown impressive performance in code generation, typically evaluated using benchmarks (e.g., HumanEval). However, effective code generation requires models to understand and apply a wide range of language concepts. If the concepts exercised in benchmarks are not representative of those used in real-world projects, evaluations may yield incomplete. Despite this concern, the representativeness of code concepts in benchmarks has not been systematically examined. To address this gap, we present the first empirical study that analyzes the representativeness of code generation benchmarks through the lens of Knowledge Units (KUs) - cohesive sets of programming language capabilities provided by language constructs and APIs. We analyze KU coverage in two widely used Python benchmarks, HumanEval and MBPP, and compare them with 30 real-world Python projects. Our results show that each benchmark covers only half of the identified 20 KUs, whereas projects exercise all KUs with relatively balanced distributions. In contrast, benchmark tasks exhibit highly skewed KU distributions. To mitigate this misalignment, we propose a prompt-based LLM framework that synthesizes KU-based tasks to rebalance benchmark KU distributions and better align them with real-world usage. Using this framework, we generate 440 new tasks and augment existing benchmarks. The augmented benchmarks substantially improve KU coverage and achieve over a 60% improvement in distributional alignment. Evaluations of state-of-the-art LLMs on these augmented benchmarks reveal consistent and statistically significant performance drops (12.54-44.82%), indicating that existing benchmarks overestimate LLM performance due to their limited KU coverage. Our findings provide actionable guidance for building more realistic evaluations of LLM code-generation capabilities.",
      "publishedDate": "2026-01-07T10:23:33Z",
      "updatedDate": "2026-01-07T10:23:33Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03780v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03780",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "evaluation",
        "tool-use",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "tool-use",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03708",
      "title": "MHRC-Bench: A Multilingual Hardware Repository-Level Code Completion benchmark",
      "authors": [
        {
          "name": "Qingyun Zou",
          "affiliation": null
        },
        {
          "name": "Jiahao Cui",
          "affiliation": null
        },
        {
          "name": "Nuo Chen",
          "affiliation": null
        },
        {
          "name": "Bingsheng He",
          "affiliation": null
        },
        {
          "name": "Weng-Fai Wong",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have achieved strong performance on code completion tasks in general-purpose programming languages. However, existing repository-level code completion benchmarks focus almost exclusively on software code and largely overlook hardware description languages. In this work, we present \\textbf{MHRC-Bench}, consisting of \\textbf{MHRC-Bench-Train} and \\textbf{MHRC-Bench-Eval}, the first benchmark designed for multilingual hardware code completion at the repository level. Our benchmark targets completion tasks and covers three major hardware design coding styles. Each completion target is annotated with code-structure-level and hardware-oriented semantic labels derived from concrete syntax tree analysis. We conduct a comprehensive evaluation of models on MHRC-Bench-Eval. Comprehensive evaluation results and analysis demonstrate the effectiveness of MHRC-Bench.",
      "publishedDate": "2026-01-07T08:46:10Z",
      "updatedDate": "2026-01-07T08:46:10Z",
      "primaryCategory": "cs.PL",
      "arxivCategories": [
        "cs.PL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03708v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03708",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03543",
      "title": "EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory",
      "authors": [
        {
          "name": "Ye Shen",
          "affiliation": null
        },
        {
          "name": "Dun Pei",
          "affiliation": null
        },
        {
          "name": "Yiqiu Guo",
          "affiliation": null
        },
        {
          "name": "Junying Wang",
          "affiliation": null
        },
        {
          "name": "Yijin Guo",
          "affiliation": null
        },
        {
          "name": "Zicheng Zhang",
          "affiliation": null
        },
        {
          "name": "Qi Jia",
          "affiliation": null
        },
        {
          "name": "Jun Zhou",
          "affiliation": null
        },
        {
          "name": "Guangtao Zhai",
          "affiliation": null
        }
      ],
      "abstract": "Despite recent advances in understanding and leveraging long-range conversational memory, existing benchmarks still lack systematic evaluation of large language models(LLMs) across diverse memory dimensions, particularly in multi-session settings. In this work, we propose EvolMem, a new benchmark for assessing multi-session memory capabilities of LLMs and agent systems. EvolMem is grounded in cognitive psychology and encompasses both declarative and non-declarative memory, further decomposed into multiple fine-grained abilities. To construct the benchmark, we introduce a hybrid data synthesis framework that consists of topic-initiated generation and narrative-inspired transformations. This framework enables scalable generation of multi-session conversations with controllable complexity, accompanied by sample-specific evaluation guidelines. Extensive evaluation reveals that no LLM consistently outperforms others across all memory dimensions. Moreover, agent memory mechanisms do not necessarily enhance LLMs' capabilities and often exhibit notable efficiency limitations. Data and code will be released at https://github.com/shenye7436/EvolMem.",
      "publishedDate": "2026-01-07T03:14:42Z",
      "updatedDate": "2026-01-07T03:14:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03543v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03543",
      "comment": "14 pages, 7 figures, 8 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "agents",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03484",
      "title": "From Bits to Chips: An LLM-based Hardware-Aware Quantization Agent for Streamlined Deployment of LLMs",
      "authors": [
        {
          "name": "Kaiyuan Deng",
          "affiliation": null
        },
        {
          "name": "Hangyu Zheng",
          "affiliation": null
        },
        {
          "name": "Minghai Qing",
          "affiliation": null
        },
        {
          "name": "Kunxiong Zhu",
          "affiliation": null
        },
        {
          "name": "Gen Li",
          "affiliation": null
        },
        {
          "name": "Yang Xiao",
          "affiliation": null
        },
        {
          "name": "Lan Emily Zhang",
          "affiliation": null
        },
        {
          "name": "Linke Guo",
          "affiliation": null
        },
        {
          "name": "Bo Hui",
          "affiliation": null
        },
        {
          "name": "Yanzhi Wang",
          "affiliation": null
        },
        {
          "name": "Geng Yuan",
          "affiliation": null
        },
        {
          "name": "Gagan Agrawal",
          "affiliation": null
        },
        {
          "name": "Wei Niu",
          "affiliation": null
        },
        {
          "name": "Xiaolong Ma",
          "affiliation": null
        }
      ],
      "abstract": "Deploying models, especially large language models (LLMs), is becoming increasingly attractive to a broader user base, including those without specialized expertise. However, due to the resource constraints of certain hardware, maintaining high accuracy with larger model while meeting the hardware requirements remains a significant challenge. Model quantization technique helps mitigate memory and compute bottlenecks, yet the added complexities of tuning and deploying quantized models further exacerbates these challenges, making the process unfriendly to most of the users. We introduce the Hardware-Aware Quantization Agent (HAQA), an automated framework that leverages LLMs to streamline the entire quantization and deployment process by enabling efficient hyperparameter tuning and hardware configuration, thereby simultaneously improving deployment quality and ease of use for a broad range of users. Our results demonstrate up to a 2.3x speedup in inference, along with increased throughput and improved accuracy compared to unoptimized models on Llama. Additionally, HAQA is designed to implement adaptive quantization strategies across diverse hardware platforms, as it automatically finds optimal settings even when they appear counterintuitive, thereby reducing extensive manual effort and demonstrating superior adaptability. Code will be released.",
      "publishedDate": "2026-01-07T00:39:09Z",
      "updatedDate": "2026-01-07T00:39:09Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03484v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03484",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03418",
      "title": "PCoA: A New Benchmark for Medical Aspect-Based Summarization With Phrase-Level Context Attribution",
      "authors": [
        {
          "name": "Bohao Chu",
          "affiliation": null
        },
        {
          "name": "Sameh Frihat",
          "affiliation": null
        },
        {
          "name": "Tabea M. G. Pakull",
          "affiliation": null
        },
        {
          "name": "Hendrik Damm",
          "affiliation": null
        },
        {
          "name": "Meijie Li",
          "affiliation": null
        },
        {
          "name": "Ula Muhabbek",
          "affiliation": null
        },
        {
          "name": "Georg Lodde",
          "affiliation": null
        },
        {
          "name": "Norbert Fuhr",
          "affiliation": null
        }
      ],
      "abstract": "Verifying system-generated summaries remains challenging, as effective verification requires precise attribution to the source context, which is especially crucial in high-stakes medical domains. To address this challenge, we introduce PCoA, an expert-annotated benchmark for medical aspect-based summarization with phrase-level context attribution. PCoA aligns each aspect-based summary with its supporting contextual sentences and contributory phrases within them. We further propose a fine-grained, decoupled evaluation framework that independently assesses the quality of generated summaries, citations, and contributory phrases. Through extensive experiments, we validate the quality and consistency of the PCoA dataset and benchmark several large language models on the proposed task. Experimental results demonstrate that PCoA provides a reliable benchmark for evaluating system-generated summaries with phrase-level context attribution. Furthermore, comparative experiments show that explicitly identifying relevant sentences and contributory phrases before summarization can improve overall quality. The data and code are available at https://github.com/chubohao/PCoA.",
      "publishedDate": "2026-01-06T21:12:03Z",
      "updatedDate": "2026-01-06T21:12:03Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03418v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03418",
      "comment": "ACL 2026 Conference Submission (8 main pages)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04786",
      "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
      "authors": [
        {
          "name": "Lang Feng",
          "affiliation": null
        },
        {
          "name": "Fuchao Yang",
          "affiliation": null
        },
        {
          "name": "Feng Chen",
          "affiliation": null
        },
        {
          "name": "Xin Cheng",
          "affiliation": null
        },
        {
          "name": "Haiyang Xu",
          "affiliation": null
        },
        {
          "name": "Zhenglin Wan",
          "affiliation": null
        },
        {
          "name": "Ming Yan",
          "affiliation": null
        },
        {
          "name": "Bo An",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
      "publishedDate": "2026-01-08T10:10:20Z",
      "updatedDate": "2026-01-08T10:10:20Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04786v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04786",
      "comment": "Work in progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04716",
      "title": "Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents",
      "authors": [
        {
          "name": "Yonghyun Jun",
          "affiliation": null
        },
        {
          "name": "Junhyuk Choi",
          "affiliation": null
        },
        {
          "name": "Jihyeong Park",
          "affiliation": null
        },
        {
          "name": "Hwanhee Lee",
          "affiliation": null
        }
      ],
      "abstract": "Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \\textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \\textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \\textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \\textit{\"Fame Fades\"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \\textit{\"Nature Remains\"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.",
      "publishedDate": "2026-01-08T08:33:40Z",
      "updatedDate": "2026-01-08T08:33:40Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04716v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04716",
      "comment": "27 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04680",
      "title": "Leveraging LLMs for Efficient and Personalized Smart Home Automation",
      "authors": [
        {
          "name": "Chaerin Yu",
          "affiliation": null
        },
        {
          "name": "Chihun Choi",
          "affiliation": null
        },
        {
          "name": "Sunjae Lee",
          "affiliation": null
        },
        {
          "name": "Hyosu Kim",
          "affiliation": null
        },
        {
          "name": "Steven Y. Ko",
          "affiliation": null
        },
        {
          "name": "Young-Bae Ko",
          "affiliation": null
        },
        {
          "name": "Sangeun Oh",
          "affiliation": null
        }
      ],
      "abstract": "The proliferation of smart home devices has increased the complexity of controlling and managing them, leading to user fatigue. In this context, large language models (LLMs) offer a promising solution by enabling natural-language interfaces for Internet of Things (IoT) control. However, existing LLM-based approaches suffer from unreliable and inefficient device control due to the non-deterministic nature of LLMs, high inference latency and cost, and limited personalization. To address these challenges, we present IoTGPT, an LLM-based smart home agent designed to execute IoT commands in a reliable, efficient, and personalized manner. Inspired by how humans manage complex tasks, IoTGPT decomposes user instructions into subtasks and memorizes them. By reusing learned subtasks, subsequent instructions can be processed more efficiently with fewer LLM calls, improving reliability and reducing both latency and cost. IoTGPT also supports fine-grained personalization by adapting individual subtasks to user preferences. Our evaluation demonstrates that IoTGPT outperforms baselines in accuracy, latency/cost, and personalization, while reducing user workload.",
      "publishedDate": "2026-01-08T07:44:59Z",
      "updatedDate": "2026-01-08T07:44:59Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04680v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04680",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04643",
      "title": "MMFCTUB: Multi-Modal Financial Credit Table Understanding Benchmark",
      "authors": [
        {
          "name": "Cui Yakun",
          "affiliation": null
        },
        {
          "name": "Yanting Zhang",
          "affiliation": null
        },
        {
          "name": "Zhu Lei",
          "affiliation": null
        },
        {
          "name": "Jian Xie",
          "affiliation": null
        },
        {
          "name": "Zhizhuo Kou",
          "affiliation": null
        },
        {
          "name": "Hang Du",
          "affiliation": null
        },
        {
          "name": "Zhenghao Zhu",
          "affiliation": null
        },
        {
          "name": "Sirui Han",
          "affiliation": null
        }
      ],
      "abstract": "The advent of multi-modal language models (MLLMs) has spurred research into their application across various table understanding tasks. However, their performance in credit table understanding (CTU) for financial credit review remains largely unexplored due to the following barriers: low data consistency, high annotation costs stemming from domain-specific knowledge and complex calculations, and evaluation paradigm gaps between benchmark and real-world scenarios. To address these challenges, we introduce MMFCTUB (Multi-Modal Financial Credit Table Understanding Benchmark), a practical benchmark, encompassing more than 7,600 high quality CTU samples across 5 table types. MMFCTUB employ a minimally supervised pipeline that adheres to inter-table constraints and maintains data distributions consistency. The benchmark leverages capacity-driven questions and mask-and-recovery strategy to evaluate models' cross-table structure perception, domain knowledge utilization, and numerical calculation capabilities. Utilizing MMFCTUB, we conduct comprehensive evaluations of both proprietary and open-source MLLMs, revealing their strengths and limitations in CTU tasks. MMFCTUB serves as a valuable resource for the research community, facilitating rigorous evaluation of MLLMs in the domain of CTU.",
      "publishedDate": "2026-01-08T06:34:47Z",
      "updatedDate": "2026-01-10T06:46:25Z",
      "primaryCategory": "cs.CE",
      "arxivCategories": [
        "cs.CE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04643v2",
      "arxivUrl": "https://arxiv.org/abs/2601.04643",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04508",
      "title": "WESR: Scaling and Evaluating Word-level Event-Speech Recognition",
      "authors": [
        {
          "name": "Chenchen Yang",
          "affiliation": null
        },
        {
          "name": "Kexin Huang",
          "affiliation": null
        },
        {
          "name": "Liwei Fan",
          "affiliation": null
        },
        {
          "name": "Qian Tu",
          "affiliation": null
        },
        {
          "name": "Botian Jiang",
          "affiliation": null
        },
        {
          "name": "Dong Zhang",
          "affiliation": null
        },
        {
          "name": "Linqi Yin",
          "affiliation": null
        },
        {
          "name": "Shimin Li",
          "affiliation": null
        },
        {
          "name": "Zhaoye Fei",
          "affiliation": null
        },
        {
          "name": "Qinyuan Cheng",
          "affiliation": null
        },
        {
          "name": "Xipeng Qiu",
          "affiliation": null
        }
      ],
      "abstract": "Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.",
      "publishedDate": "2026-01-08T02:23:21Z",
      "updatedDate": "2026-01-08T02:23:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04508v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04508",
      "comment": "14 pages, 6 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "tool-use",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03676",
      "title": "Towards Compositional Generalization of LLMs via Skill Taxonomy Guided Data Synthesis",
      "authors": [
        {
          "name": "Yifan Wei",
          "affiliation": null
        },
        {
          "name": "Li Du",
          "affiliation": null
        },
        {
          "name": "Xiaoyan Yu",
          "affiliation": null
        },
        {
          "name": "Yang Feng",
          "affiliation": null
        },
        {
          "name": "Angsheng Li",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) and agent-based systems often struggle with compositional generalization due to a data bottleneck in which complex skill combinations follow a long-tailed, power-law distribution, limiting both instruction-following performance and generalization in agent-centric tasks. To address this challenge, we propose STEPS, a Skill Taxonomy guided Entropy-based Post-training data Synthesis framework for generating compositionally challenging data. STEPS explicitly targets compositional generalization by uncovering latent relationships among skills and organizing them into an interpretable, hierarchical skill taxonomy using structural information theory. Building on this taxonomy, we formulate data synthesis as a constrained information maximization problem, selecting skill combinations that maximize marginal structural information within the hierarchy while preserving semantic coherence. Experiments on challenging instruction-following benchmarks show that STEPS outperforms existing data synthesis baselines, while also yielding improved compositional generalization in downstream agent-based evaluations.",
      "publishedDate": "2026-01-07T07:58:51Z",
      "updatedDate": "2026-01-07T07:58:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03676v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03676",
      "comment": "The code and data for our methods and experiments are available at https://github.com/weiyifan1023/STEPS",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "evaluation",
        "agents",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03606",
      "title": "Policy-Guided Search on Tree-of-Thoughts for Efficient Problem Solving with Bounded Language Model Queries",
      "authors": [
        {
          "name": "Sumedh Pendurkar",
          "affiliation": null
        },
        {
          "name": "Guni Sharon",
          "affiliation": null
        }
      ],
      "abstract": "Recent studies explored integrating state-space search algorithms with Language Models (LM) to perform look-ahead on the token generation process, the ''Tree-of-Thoughts'' (ToT), generated by LMs, thereby improving performance on problem-solving tasks. However, the affiliated search algorithms often overlook the significant computational costs associated with LM inference, particularly in scenarios with constrained computational budgets. Consequently, we address the problem of improving LM performance on problem-solving tasks under limited computational budgets. We demonstrate how the probabilities assigned to thoughts by LMs can serve as a heuristic to guide search within the ToT framework, thereby reducing the number of thought evaluations. Building on this insight, we adapt a heuristic search algorithm, Levin Tree Search (LTS), to the ToT framework, which leverages LMs as policies to guide the tree exploration efficiently. We extend the theoretical results of LTS by showing that, for ToT (a pruned tree), LTS guarantees a bound on the number of states expanded, and consequently, on the number of thoughts generated. Additionally, we analyze the sensitivity of this bound to the temperature values commonly used in the final softmax layer of the LM. Empirical evaluation under a fixed LM query budget demonstrates that LTS consistently achieves comparable or higher accuracy than baseline search algorithms within the ToT framework, across three domains (Blocksworld, PrOntoQA, Array Sorting) and four distinct LMs. These findings highlight the efficacy of LTS on ToT, particularly in enabling cost-effective and time-efficient problem-solving, making it well-suited for latency-critical and resource-constrained applications.",
      "publishedDate": "2026-01-07T05:35:16Z",
      "updatedDate": "2026-01-07T05:35:16Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03606v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03606",
      "comment": "Published in Transactions on Machine Learning Research (TMLR), 2025. Available at https://openreview.net/forum?id=Rlk1bWe2ii",
      "journalRef": "Transactions on Machine Learning Research, 2025",
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03329",
      "title": "Attention mechanisms in neural networks",
      "authors": [
        {
          "name": "Hasi Hays",
          "affiliation": null
        }
      ],
      "abstract": "Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and multimodal learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive transformers, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision Transformers for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.",
      "publishedDate": "2026-01-06T17:12:10Z",
      "updatedDate": "2026-01-06T17:12:10Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03329v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03329",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-09T03:25:39.273Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05827",
      "title": "SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking",
      "authors": [
        {
          "name": "Zewei Lin",
          "affiliation": null
        },
        {
          "name": "Jiachi Chen",
          "affiliation": null
        },
        {
          "name": "Jingwen Zhang",
          "affiliation": null
        },
        {
          "name": "Zexu Wang",
          "affiliation": null
        },
        {
          "name": "Yuming Feng",
          "affiliation": null
        },
        {
          "name": "Weizhe Zhang",
          "affiliation": null
        },
        {
          "name": "Zibin Zheng",
          "affiliation": null
        }
      ],
      "abstract": "Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.",
      "publishedDate": "2026-01-09T15:01:41Z",
      "updatedDate": "2026-01-09T15:01:41Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05827v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05827",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05825",
      "title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI",
      "authors": [
        {
          "name": "Lucija Mihić Zidar",
          "affiliation": null
        },
        {
          "name": "Philipp Wicke",
          "affiliation": null
        },
        {
          "name": "Praneel Bhatia",
          "affiliation": null
        },
        {
          "name": "Rosa Lutz",
          "affiliation": null
        },
        {
          "name": "Marius Klug",
          "affiliation": null
        },
        {
          "name": "Thorsten O. Zander",
          "affiliation": null
        }
      ],
      "abstract": "Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.",
      "publishedDate": "2026-01-09T14:59:25Z",
      "updatedDate": "2026-01-09T14:59:25Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05825v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05825",
      "comment": "Accepted at the 14th International Winter Conference on Brain-Computer Interface",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05806",
      "title": "Modular Autonomy with Conversational Interaction: An LLM-driven Framework for Decision Making in Autonomous Driving",
      "authors": [
        {
          "name": "Marvin Seegert",
          "affiliation": null
        },
        {
          "name": "Korbinian Moller",
          "affiliation": null
        },
        {
          "name": "Johannes Betz",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) offer new opportunities to create natural language interfaces for Autonomous Driving Systems (ADSs), moving beyond rigid inputs. This paper addresses the challenge of mapping the complexity of human language to the structured action space of modular ADS software. We propose a framework that integrates an LLM-based interaction layer with Autoware, a widely used open-source software. This system enables passengers to issue high-level commands, from querying status information to modifying driving behavior. Our methodology is grounded in three key components: a taxonomization of interaction categories, an application-centric Domain Specific Language (DSL) for command translation, and a safety-preserving validation layer. A two-stage LLM architecture ensures high transparency by providing feedback based on the definitive execution status. Evaluation confirms the system's timing efficiency and translation robustness. Simulation successfully validated command execution across all five interaction categories. This work provides a foundation for extensible, DSL-assisted interaction in modular and safety-conscious autonomy stacks.",
      "publishedDate": "2026-01-09T14:23:01Z",
      "updatedDate": "2026-01-09T14:23:01Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05806v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05806",
      "comment": "Submitted to the IEEE Intelligent Vehicles Symposium (IV 2026), Detroit, MI, United States",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05772",
      "title": "StriderSPD: Structure-Guided Joint Representation Learning for Binary Security Patch Detection",
      "authors": [
        {
          "name": "Qingyuan Li",
          "affiliation": null
        },
        {
          "name": "Chenchen Yu",
          "affiliation": null
        },
        {
          "name": "Chuanyi Li",
          "affiliation": null
        },
        {
          "name": "Xin-Cheng Wen",
          "affiliation": null
        },
        {
          "name": "Cheryl Lee",
          "affiliation": null
        },
        {
          "name": "Cuiyun Gao",
          "affiliation": null
        },
        {
          "name": "Bin Luo",
          "affiliation": null
        }
      ],
      "abstract": "Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose \\textbf{\\textit{StriderSPD}}, a \\underline{Str}ucture-gu\\underline{ide}d joint \\underline{r}epresentation \\underline{SPD} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.",
      "publishedDate": "2026-01-09T12:55:29Z",
      "updatedDate": "2026-01-09T12:55:29Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05772v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05772",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "evaluation",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05648",
      "title": "Open World Knowledge Aided Single-Cell Foundation Model with Robust Cross-Modal Cell-Language Pre-training",
      "authors": [
        {
          "name": "Haoran Wang",
          "affiliation": null
        },
        {
          "name": "Xuanyi Zhang",
          "affiliation": null
        },
        {
          "name": "Shuangsang Fang",
          "affiliation": null
        },
        {
          "name": "Longke Ran",
          "affiliation": null
        },
        {
          "name": "Ziqing Deng",
          "affiliation": null
        },
        {
          "name": "Yong Zhang",
          "affiliation": null
        },
        {
          "name": "Yuxiang Li",
          "affiliation": null
        },
        {
          "name": "Shaoshuai Li",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements in single-cell multi-omics, particularly RNA-seq, have provided profound insights into cellular heterogeneity and gene regulation. While pre-trained language model (PLM) paradigm based single-cell foundation models have shown promise, they remain constrained by insufficient integration of in-depth individual profiles and neglecting the influence of noise within multi-modal data. To address both issues, we propose an Open-world Language Knowledge-Aided Robust Single-Cell Foundation Model (OKR-CELL). It is built based on a cross-modal Cell-Language pre-training framework, which comprises two key innovations: (1) leveraging Large Language Models (LLMs) based workflow with retrieval-augmented generation (RAG) enriches cell textual descriptions using open-world knowledge; (2) devising a Cross-modal Robust Alignment (CRA) objective that incorporates sample reliability assessment, curriculum learning, and coupled momentum contrastive learning to strengthen the model's resistance to noisy data. After pretraining on 32M cell-text pairs, OKR-CELL obtains cutting-edge results across 6 evaluation tasks. Beyond standard benchmarks such as cell clustering, cell-type annotation, batch-effect correction, and few-shot annotation, the model also demonstrates superior performance in broader multi-modal applications, including zero-shot cell-type annotation and bidirectional cell-text retrieval.",
      "publishedDate": "2026-01-09T09:10:14Z",
      "updatedDate": "2026-01-09T09:10:14Z",
      "primaryCategory": "q-bio.GN",
      "arxivCategories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05648v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05648",
      "comment": "41 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05609",
      "title": "Data Augmented Pipeline for Legal Information Extraction and Reasoning",
      "authors": [
        {
          "name": "Nguyen Minh Phuong",
          "affiliation": null
        },
        {
          "name": "Ha-Thanh Nguyen",
          "affiliation": null
        },
        {
          "name": "May Myo Zin",
          "affiliation": null
        },
        {
          "name": "Ken Satoh",
          "affiliation": null
        }
      ],
      "abstract": "In this paper, we propose a pipeline leveraging Large Language Models (LLMs) for data augmentation in Information Extraction tasks within the legal domain. The proposed method is both simple and effective, significantly reducing the manual effort required for data annotation while enhancing the robustness of Information Extraction systems. Furthermore, the method is generalizable, making it applicable to various Natural Language Processing (NLP) tasks beyond the legal domain.",
      "publishedDate": "2026-01-09T08:02:54Z",
      "updatedDate": "2026-01-09T08:02:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05609v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05609",
      "comment": "Accepted in the Demonstration Track at ICAIL 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05579",
      "title": "RISE: Rule-Driven SQL Dialect Translation via Query Reduction",
      "authors": [
        {
          "name": "Xudong Xie",
          "affiliation": null
        },
        {
          "name": "Yuwei Zhang",
          "affiliation": null
        },
        {
          "name": "Wensheng Dou",
          "affiliation": null
        },
        {
          "name": "Yu Gao",
          "affiliation": null
        },
        {
          "name": "Ziyu Cui",
          "affiliation": null
        },
        {
          "name": "Jiansen Song",
          "affiliation": null
        },
        {
          "name": "Rui Yang",
          "affiliation": null
        },
        {
          "name": "Jun Wei",
          "affiliation": null
        }
      ],
      "abstract": "Translating SQL dialects across different relational database management systems (RDBMSs) is crucial for migrating RDBMS-based applications to the cloud. Traditional SQL dialect translation tools rely on manually-crafted rules, necessitating significant manual effort to support new RDBMSs and dialects. Although large language models (LLMs) can assist in translating SQL dialects, they often struggle with lengthy and complex SQL queries. In this paper, we propose RISE, a novel LLM-based SQL dialect translation approach that can accurately handle lengthy and complex SQL queries. Given a complex source query $Q_c$ that contains a SQL dialect $d$, we first employ a dialect-aware query reduction technique to derive a simplified query $Q_{s}$ by removing $d$-irrelevant SQL elements from $Q_c$. Subsequently, we utilize LLMs to translate $Q_{s}$ into $Q_{s^{'}}$, and automatically extract the translation rule $r_d$ for dialect $d$ based on the relationship between $Q_{s}$ and $Q_{s^{'}}$. By applying $r_d$ to $Q_c$, we can effectively translate the dialect $d$ within $Q_c$, thereby bypassing the complexity of the source query $Q_c$. We evaluate RISE on two real-world benchmarks, i.e., TPC-DS and SQLProcBench, comparing its performance against both the traditional rule-based tools and the LLM-based approaches with respect to translation accuracy. RISE achieves accuracies of 97.98% on TPC-DS and 100% on SQLProcBench, outperforming the baselines by an average improvement of 24.62% and 238.41%, respectively.",
      "publishedDate": "2026-01-09T07:00:44Z",
      "updatedDate": "2026-01-09T07:00:44Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05579v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05579",
      "comment": "Accepted by ICSE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05578",
      "title": "Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection",
      "authors": [
        {
          "name": "Cooper Lin",
          "affiliation": null
        },
        {
          "name": "Yanting Zhang",
          "affiliation": null
        },
        {
          "name": "Maohao Ran",
          "affiliation": null
        },
        {
          "name": "Wei Xue",
          "affiliation": null
        },
        {
          "name": "Hongwei Fan",
          "affiliation": null
        },
        {
          "name": "Yibo Xu",
          "affiliation": null
        },
        {
          "name": "Zhenglin Wan",
          "affiliation": null
        },
        {
          "name": "Sirui Han",
          "affiliation": null
        },
        {
          "name": "Yike Guo",
          "affiliation": null
        },
        {
          "name": "Jun Song",
          "affiliation": null
        }
      ],
      "abstract": "E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.",
      "publishedDate": "2026-01-09T06:56:27Z",
      "updatedDate": "2026-01-09T06:56:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05578v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05578",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05467",
      "title": "STELP: Secure Transpilation and Execution of LLM-Generated Programs",
      "authors": [
        {
          "name": "Swapnil Shinde",
          "affiliation": null
        },
        {
          "name": "Sahil Wadhwa",
          "affiliation": null
        },
        {
          "name": "Andy Luo",
          "affiliation": null
        },
        {
          "name": "Akshay Gupta",
          "affiliation": null
        },
        {
          "name": "Mohammad Shahed Sorower",
          "affiliation": null
        }
      ],
      "abstract": "Rapid evolution of Large Language Models (LLMs) has achieved major advances in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative frameworks using such LLMs place them at the center of solving software development-related tasks such as code generation. However, direct use of LLM generated code in production software development systems is problematic. The code could be unstable or erroneous and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations that could lead to widespread system malfunctions. This prohibits the adoption of LLM generated code in production AI systems where human code reviews and traditional secure testing tools are impractical or untrustworthy. In this paper, we discuss safety and reliability problems with the execution of LLM generated code and propose a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of executing LLM-generated code in a controlled and safe manner. STELP secures autonomous production AI systems involving code generation, filling the critical void left by the impracticality or limitations of traditional secure testing methodologies and human oversight. This includes applications such as headless code generation-execution and LLMs that produce executable code snippets as an action plan to be executed in real time. We contribute a human-validated dataset of insecure code snippets and benchmark our approach on publicly available datasets for correctness, safety, and latency. Our results demonstrate that our approach outperforms an existing method by a significant margin, particularly in its ability to safely execute risky code snippets. Warning: This paper contains malicious code snippets that should be run with caution.",
      "publishedDate": "2026-01-09T01:49:41Z",
      "updatedDate": "2026-01-13T17:55:11Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05467v2",
      "arxivUrl": "https://arxiv.org/abs/2601.05467",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation",
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05466",
      "title": "Jailbreaking Large Language Models through Iterative Tool-Disguised Attacks via Reinforcement Learning",
      "authors": [
        {
          "name": "Zhaoqi Wang",
          "affiliation": null
        },
        {
          "name": "Zijian Zhang",
          "affiliation": null
        },
        {
          "name": "Daqing He",
          "affiliation": null
        },
        {
          "name": "Pengtao Kou",
          "affiliation": null
        },
        {
          "name": "Xin Li",
          "affiliation": null
        },
        {
          "name": "Jiamou Liu",
          "affiliation": null
        },
        {
          "name": "Jincheng An",
          "affiliation": null
        },
        {
          "name": "Yong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse applications, however, they remain critically vulnerable to jailbreak attacks that elicit harmful responses violating human values and safety guidelines. Despite extensive research on defense mechanisms, existing safeguards prove insufficient against sophisticated adversarial strategies. In this work, we propose iMIST (\\underline{i}nteractive \\underline{M}ulti-step \\underline{P}rogre\\underline{s}sive \\underline{T}ool-disguised Jailbreak Attack), a novel adaptive jailbreak method that synergistically exploits vulnerabilities in current defense mechanisms. iMIST disguises malicious queries as normal tool invocations to bypass content filters, while simultaneously introducing an interactive progressive optimization algorithm that dynamically escalates response harmfulness through multi-turn dialogues guided by real-time harmfulness assessment. Our experiments on widely-used models demonstrate that iMIST achieves higher attack effectiveness, while maintaining low rejection rates. These results reveal critical vulnerabilities in current LLM safety mechanisms and underscore the urgent need for more robust defense strategies.",
      "publishedDate": "2026-01-09T01:41:39Z",
      "updatedDate": "2026-01-09T01:41:39Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05466v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05466",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05414",
      "title": "Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions",
      "authors": [
        {
          "name": "Minda Zhao",
          "affiliation": null
        },
        {
          "name": "Yilun Du",
          "affiliation": null
        },
        {
          "name": "Mengyu Wang",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.",
      "publishedDate": "2026-01-08T22:33:12Z",
      "updatedDate": "2026-01-08T22:33:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05414v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05414",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "tool-use",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06007",
      "title": "Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks",
      "authors": [
        {
          "name": "Elias Lumer",
          "affiliation": null
        },
        {
          "name": "Faheem Nizar",
          "affiliation": null
        },
        {
          "name": "Akshaya Jangiti",
          "affiliation": null
        },
        {
          "name": "Kevin Frank",
          "affiliation": null
        },
        {
          "name": "Anmol Gulati",
          "affiliation": null
        },
        {
          "name": "Mandar Phadate",
          "affiliation": null
        },
        {
          "name": "Vamse Kumar Subbiah",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.",
      "publishedDate": "2026-01-09T18:41:57Z",
      "updatedDate": "2026-01-09T18:41:57Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06007v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06007",
      "comment": "15 pages, 8 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "tool-use",
        "evaluation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "evaluation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05918",
      "title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
      "authors": [
        {
          "name": "Tianshi Li",
          "affiliation": null
        }
      ],
      "abstract": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
      "publishedDate": "2026-01-09T16:32:33Z",
      "updatedDate": "2026-01-09T16:32:33Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05918v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05918",
      "comment": "4 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05808",
      "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
      "authors": [
        {
          "name": "Xiaoshuai Song",
          "affiliation": null
        },
        {
          "name": "Haofei Chang",
          "affiliation": null
        },
        {
          "name": "Guanting Dong",
          "affiliation": null
        },
        {
          "name": "Yutao Zhu",
          "affiliation": null
        },
        {
          "name": "Zhicheng Dou",
          "affiliation": null
        },
        {
          "name": "Ji-Rong Wen",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
      "publishedDate": "2026-01-09T14:32:06Z",
      "updatedDate": "2026-01-09T14:32:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05808v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05808",
      "comment": "Working in progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05755",
      "title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit",
      "authors": [
        {
          "name": "Junda Lin",
          "affiliation": null
        },
        {
          "name": "Zhaomeng Zhou",
          "affiliation": null
        },
        {
          "name": "Zhi Zheng",
          "affiliation": null
        },
        {
          "name": "Shuochen Liu",
          "affiliation": null
        },
        {
          "name": "Tong Xu",
          "affiliation": null
        },
        {
          "name": "Yong Chen",
          "affiliation": null
        },
        {
          "name": "Enhong Chen",
          "affiliation": null
        }
      ],
      "abstract": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility. Code is available at https://anonymous.4open.science/r/VIGIL-378B/.",
      "publishedDate": "2026-01-09T12:19:49Z",
      "updatedDate": "2026-01-09T12:19:49Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05755v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05755",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "reasoning",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05536",
      "title": "Task Cascades for Efficient Unstructured Data Processing",
      "authors": [
        {
          "name": "Shreya Shankar",
          "affiliation": null
        },
        {
          "name": "Sepanta Zeighami",
          "affiliation": null
        },
        {
          "name": "Aditya Parameswaran",
          "affiliation": null
        }
      ],
      "abstract": "Modern database systems allow users to query or process unstructured text or document columns using LLM-powered functions. Users can express an operation in natural language (e.g., \"identify if this review mentions billing issues\"), with the system executing the operation on each document, in a row-by-row fashion. One way to reduce cost on a batch of documents is to employ the model cascade framework: a cheap proxy model processes each document, and only uncertain cases are escalated to a more accurate, expensive oracle. However, model cascades miss important optimization opportunities; for example, often only part of a document is needed to answer a query, or other related, but simpler operations (e.g., \"is the review sentiment negative?\", \"does the review mention money?\") can be handled by cheap models more effectively than the original operation, while still being correlated with it. We introduce the task cascades framework, which generalizes model cascades by varying not just the model, but also the document portion and operation at each stage. Our framework uses an LLM agent to generate simplified, decomposed, or otherwise related operations and selects the most relevant document portions, constructing hundreds of candidate tasks from which it assembles a task cascade. We show that optimal cascade selection is intractable via reduction from Minimum Sum Set Cover, but our iterative approach constructs effective cascades. We also provide an extension that offers statistical accuracy guarantees: the resulting cascade meets a user-defined accuracy target (with respect to the oracle) up to a bounded failure probability. Across eight real-world document processing tasks at a 90% target accuracy, task cascades reduce end-to-end cost by an average of 36% compared to model cascades, at a production scale.",
      "publishedDate": "2026-01-09T05:23:40Z",
      "updatedDate": "2026-01-09T05:23:40Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05536v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05536",
      "comment": "SIGMOD 2026. 21 pages, 8 figures, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05504",
      "title": "Memory Poisoning Attack and Defense on Memory Based LLM-Agents",
      "authors": [
        {
          "name": "Balachandra Devarangadi Sunil",
          "affiliation": null
        },
        {
          "name": "Isheeta Sinha",
          "affiliation": null
        },
        {
          "name": "Piyush Maheshwari",
          "affiliation": null
        },
        {
          "name": "Shantanu Todmal",
          "affiliation": null
        },
        {
          "name": "Shreyan Mallik",
          "affiliation": null
        },
        {
          "name": "Shuchi Mishra",
          "affiliation": null
        }
      ],
      "abstract": "Large language model agents equipped with persistent memory are vulnerable to memory poisoning attacks, where adversaries inject malicious instructions through query only interactions that corrupt the agents long term memory and influence future responses. Recent work demonstrated that the MINJA (Memory Injection Attack) achieves over 95 % injection success rate and 70 % attack success rate under idealized conditions. However, the robustness of these attacks in realistic deployments and effective defensive mechanisms remain understudied. This work addresses these gaps through systematic empirical evaluation of memory poisoning attacks and defenses in Electronic Health Record (EHR) agents. We investigate attack robustness by varying three critical dimensions: initial memory state, number of indication prompts, and retrieval parameters. Our experiments on GPT-4o-mini, Gemini-2.0-Flash and Llama-3.1-8B-Instruct models using MIMIC-III clinical data reveal that realistic conditions with pre-existing legitimate memories dramatically reduce attack effectiveness. We then propose and evaluate two novel defense mechanisms: (1) Input/Output Moderation using composite trust scoring across multiple orthogonal signals, and (2) Memory Sanitization with trust-aware retrieval employing temporal decay and pattern-based filtering. Our defense evaluation reveals that effective memory sanitization requires careful trust threshold calibration to prevent both overly conservative rejection (blocking all entries) and insufficient filtering (missing subtle attacks), establishing important baselines for future adaptive defense mechanisms. These findings provide crucial insights for securing memory-augmented LLM agents in production environments.",
      "publishedDate": "2026-01-09T03:26:10Z",
      "updatedDate": "2026-01-12T03:35:39Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05504v2",
      "arxivUrl": "https://arxiv.org/abs/2601.05504",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05483",
      "title": "MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis",
      "authors": [
        {
          "name": "Zixuan Xiao",
          "affiliation": null
        },
        {
          "name": "Jun Ma",
          "affiliation": null
        },
        {
          "name": "Siwei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.",
      "publishedDate": "2026-01-09T02:34:35Z",
      "updatedDate": "2026-01-09T02:34:35Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05483v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05483",
      "comment": null,
      "journalRef": "Applied Soft Computing 190 (2026) 114576",
      "doi": "10.1016/j.asoc.2026.114576",
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents"
      ],
      "tags": {
        "auto": [
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05302",
      "title": "Effects of personality steering on cooperative behavior in Large Language Model agents",
      "authors": [
        {
          "name": "Mizuki Sakai",
          "affiliation": null
        },
        {
          "name": "Mizuki Yokoyama",
          "affiliation": null
        },
        {
          "name": "Wakaba Tateishi",
          "affiliation": null
        },
        {
          "name": "Genki Ichinose",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.",
      "publishedDate": "2026-01-08T14:23:45Z",
      "updatedDate": "2026-01-08T14:23:45Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05302v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05302",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents"
      ],
      "tags": {
        "auto": [
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06022",
      "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs",
      "authors": [
        {
          "name": "Chengming Cui",
          "affiliation": null
        },
        {
          "name": "Tianxin Wei",
          "affiliation": null
        },
        {
          "name": "Ziyi Chen",
          "affiliation": null
        },
        {
          "name": "Ruizhong Qiu",
          "affiliation": null
        },
        {
          "name": "Zhichen Zeng",
          "affiliation": null
        },
        {
          "name": "Zhining Liu",
          "affiliation": null
        },
        {
          "name": "Xuying Ning",
          "affiliation": null
        },
        {
          "name": "Duo Zhou",
          "affiliation": null
        },
        {
          "name": "Jingrui He",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.",
      "publishedDate": "2026-01-09T18:58:22Z",
      "updatedDate": "2026-01-09T18:58:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06022v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06022",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06002",
      "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Qiguang Chen",
          "affiliation": null
        },
        {
          "name": "Yantao Du",
          "affiliation": null
        },
        {
          "name": "Ziniu Li",
          "affiliation": null
        },
        {
          "name": "Jinhao Liu",
          "affiliation": null
        },
        {
          "name": "Songyao Duan",
          "affiliation": null
        },
        {
          "name": "Jiarui Guo",
          "affiliation": null
        },
        {
          "name": "Minghao Liu",
          "affiliation": null
        },
        {
          "name": "Jiaheng Liu",
          "affiliation": null
        },
        {
          "name": "Tong Yang",
          "affiliation": null
        },
        {
          "name": "Ge Zhang",
          "affiliation": null
        },
        {
          "name": "Libo Qin",
          "affiliation": null
        },
        {
          "name": "Wanxiang Che",
          "affiliation": null
        },
        {
          "name": "Wenhao Huang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
      "publishedDate": "2026-01-09T18:39:01Z",
      "updatedDate": "2026-01-13T18:21:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06002v2",
      "arxivUrl": "https://arxiv.org/abs/2601.06002",
      "comment": "Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05939",
      "title": "Context-Aware Decoding for Faithful Vision-Language Generation",
      "authors": [
        {
          "name": "Mehrdad Fazli",
          "affiliation": null
        },
        {
          "name": "Bowen Wei",
          "affiliation": null
        },
        {
          "name": "Ziwei Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth gap: truthful tokens accumulate probability mass on their final candidates earlier than hallucinatory ones. Drawing on this discovery, we introduce Context Embedding Injection (CEI), a lightweight method that harnesses the hidden state of the last input token-the context embedding-as a grounding signal to maintain visual fidelity throughout decoding and curb hallucinations. Evaluated on the CHAIR, AMBER, and MMHal-Bench benchmarks (with a maximum token length of 512), CEI outperforms state-of-the-art baselines across three LVLMs, with its dynamic variant yielding the lowest overall hallucination rates. By integrating novel mechanistic insights with a scalable intervention, this work advances the mitigation of hallucinations in LVLMs.",
      "publishedDate": "2026-01-09T16:50:57Z",
      "updatedDate": "2026-01-09T16:50:57Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05939v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05939",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05870",
      "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
      "authors": [
        {
          "name": "Huilin Deng",
          "affiliation": null
        },
        {
          "name": "Hongchen Luo",
          "affiliation": null
        },
        {
          "name": "Yue Zhu",
          "affiliation": null
        },
        {
          "name": "Long Li",
          "affiliation": null
        },
        {
          "name": "Zhuoyue Chen",
          "affiliation": null
        },
        {
          "name": "Xinghao Zhao",
          "affiliation": null
        },
        {
          "name": "Ming Li",
          "affiliation": null
        },
        {
          "name": "Jihai Zhang",
          "affiliation": null
        },
        {
          "name": "Mengchang Wang",
          "affiliation": null
        },
        {
          "name": "Yang Cao",
          "affiliation": null
        },
        {
          "name": "Yu Kang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
      "publishedDate": "2026-01-09T15:46:40Z",
      "updatedDate": "2026-01-09T15:46:40Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05870v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05870",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05746",
      "title": "DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation",
      "authors": [
        {
          "name": "Zhenghao Li",
          "affiliation": null
        },
        {
          "name": "Zhi Zheng",
          "affiliation": null
        },
        {
          "name": "Wei Chen",
          "affiliation": null
        },
        {
          "name": "Jielun Zhao",
          "affiliation": null
        },
        {
          "name": "Yong Chen",
          "affiliation": null
        },
        {
          "name": "Tong Xu",
          "affiliation": null
        },
        {
          "name": "Enhong Chen",
          "affiliation": null
        }
      ],
      "abstract": "Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.",
      "publishedDate": "2026-01-09T12:01:33Z",
      "updatedDate": "2026-01-09T12:01:33Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05746v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05746",
      "comment": "16pages,6figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "tool-use",
        "multi-agent",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "multi-agent",
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05705",
      "title": "Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning",
      "authors": [
        {
          "name": "Ali Farjami",
          "affiliation": null
        },
        {
          "name": "Luca Redondi",
          "affiliation": null
        },
        {
          "name": "Marco Valentino",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.",
      "publishedDate": "2026-01-09T10:47:30Z",
      "updatedDate": "2026-01-09T10:47:30Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05705v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05705",
      "comment": "Work in progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05688",
      "title": "SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More",
      "authors": [
        {
          "name": "Muye Huang",
          "affiliation": null
        },
        {
          "name": "Lingling Zhang",
          "affiliation": null
        },
        {
          "name": "Yifei Li",
          "affiliation": null
        },
        {
          "name": "Yaqiang Wu",
          "affiliation": null
        },
        {
          "name": "Jun Liu",
          "affiliation": null
        }
      ],
      "abstract": "Charts are high-density visual carriers of complex data and medium for information extraction and analysis. Due to the need for precise and complex visual reasoning, automated chart understanding poses a significant challenge to existing Multimodal Large Language Models (MLLMs). Many MLLMs trained with reinforcement learning (RL) face the challenge of credit assignment. Their advantage estimation, typically performed at the trajectory level, cannot distinguish between correct and incorrect reasoning steps within a single generated response. To address this limitation, we introduce SketchVL, a novel MLLM that optimized with FinePO, a new RL algorithm designed for fine-grained credit assignment within each trajectory. SketchVL's methodology involves drawing its intermediate reasoning steps as markers on the image and feeding the annotated image back to itself, creating a robust, multi-step reasoning process. During training, the FinePO algorithm leverages a Fine-grained Process Reward Model (FinePRM) to score each drawing action within a trajectory, thereby precisely assigning credit for each step. This mechanism allows FinePO to more strongly reward correct tokens when a trajectory is globally successful, and more heavily penalize incorrect tokens when the trajectory is globally suboptimal, thus achieving fine-grained reinforcement signals. Experiments show that SketchVL learns to align its step-level behavior with the FinePRM, achieving an average performance gain of 7.23\\% over its base model across chart datasets, natural image datasets, and mathematics, providing a promising new direction for training powerful reasoning models.",
      "publishedDate": "2026-01-09T10:13:01Z",
      "updatedDate": "2026-01-09T10:13:01Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05688v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05688",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05679",
      "title": "Do Sparse Autoencoders Identify Reasoning Features in Language Models?",
      "authors": [
        {
          "name": "George Ma",
          "affiliation": null
        },
        {
          "name": "Zhongyuan Liang",
          "affiliation": null
        },
        {
          "name": "Irene Y. Chen",
          "affiliation": null
        },
        {
          "name": "Somayeh Sojoudi",
          "affiliation": null
        }
      ],
      "abstract": "We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.",
      "publishedDate": "2026-01-09T09:54:36Z",
      "updatedDate": "2026-01-09T09:54:36Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05679v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05679",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05640",
      "title": "SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving",
      "authors": [
        {
          "name": "Jingyu Li",
          "affiliation": null
        },
        {
          "name": "Junjie Wu",
          "affiliation": null
        },
        {
          "name": "Dongnan Hu",
          "affiliation": null
        },
        {
          "name": "Xiangkai Huang",
          "affiliation": null
        },
        {
          "name": "Bin Sun",
          "affiliation": null
        },
        {
          "name": "Zhihui Hao",
          "affiliation": null
        },
        {
          "name": "Xianpeng Lang",
          "affiliation": null
        },
        {
          "name": "Xiatian Zhu",
          "affiliation": null
        },
        {
          "name": "Li Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM's representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.",
      "publishedDate": "2026-01-09T08:55:42Z",
      "updatedDate": "2026-01-12T03:29:14Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05640v2",
      "arxivUrl": "https://arxiv.org/abs/2601.05640",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05633",
      "title": "GIFT: Games as Informal Training for Generalizable LLMs",
      "authors": [
        {
          "name": "Nuoyan Lyu",
          "affiliation": null
        },
        {
          "name": "Bingbing Xu",
          "affiliation": null
        },
        {
          "name": "Weihao Meng",
          "affiliation": null
        },
        {
          "name": "Yige Yuan",
          "affiliation": null
        },
        {
          "name": "Yang Zhang",
          "affiliation": null
        },
        {
          "name": "Zhiyong Huang",
          "affiliation": null
        },
        {
          "name": "Tat-Seng Chua",
          "affiliation": null
        },
        {
          "name": "Huawei Shen",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.",
      "publishedDate": "2026-01-09T08:42:44Z",
      "updatedDate": "2026-01-09T08:42:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05633v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05633",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05632",
      "title": "LLM-DMD: Large Language Model-based Power System Dynamic Model Discovery",
      "authors": [
        {
          "name": "Chao Shen",
          "affiliation": null
        },
        {
          "name": "Zihan Guo",
          "affiliation": null
        },
        {
          "name": "Ke Zuo",
          "affiliation": null
        },
        {
          "name": "Wenqi Huang",
          "affiliation": null
        },
        {
          "name": "Mingyang Sun",
          "affiliation": null
        }
      ],
      "abstract": "Current model structural discovery methods for power system dynamics impose rigid priors on the basis functions and variable sets of dynamic models while often neglecting algebraic constraints, thereby limiting the formulation of high-fidelity models required for precise simulation and analysis. This letter presents a novel large language model (LLM)-based framework for dynamic model discovery (LLM-DMD) which integrates the reasoning and code synthesis capabilities of LLMs to discover dynamic equations and enforce algebraic constraints through two sequential loops: the differential-equation loop that identifies state dynamics and associated variables, and the algebraic-equation loop that formulates algebraic constraints on the identified algebraic variables. In each loop, executable skeletons of power system dynamic equations are generated by the LLM-based agent and evaluated via gradient-based optimizer. Candidate models are stored in an island-based archive to guide future iterations, and evaluation stagnation activates a variable extension mechanism that augments the model with missing algebraic or input variables, such as stator currents to refine the model. Validation on synchronous generator benchmarks of the IEEE 39-bus system demonstrates the superiority of LLM-DMD in complete dynamic model discovery.",
      "publishedDate": "2026-01-09T08:40:45Z",
      "updatedDate": "2026-01-09T08:40:45Z",
      "primaryCategory": "eess.SY",
      "arxivCategories": [
        "eess.SY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05632v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05632",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05616",
      "title": "Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks",
      "authors": [
        {
          "name": "ShaoZhen Liu",
          "affiliation": null
        },
        {
          "name": "Xinting Huang",
          "affiliation": null
        },
        {
          "name": "Houwen Peng",
          "affiliation": null
        },
        {
          "name": "Xin Chen",
          "affiliation": null
        },
        {
          "name": "Xinyang Song",
          "affiliation": null
        },
        {
          "name": "Qi Li",
          "affiliation": null
        },
        {
          "name": "Zhenan Sun",
          "affiliation": null
        }
      ],
      "abstract": "In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.",
      "publishedDate": "2026-01-09T08:19:11Z",
      "updatedDate": "2026-01-09T08:19:11Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05616v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05616",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05611",
      "title": "LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction",
      "authors": [
        {
          "name": "Chengen Xie",
          "affiliation": null
        },
        {
          "name": "Bin Sun",
          "affiliation": null
        },
        {
          "name": "Tianyu Li",
          "affiliation": null
        },
        {
          "name": "Junjie Wu",
          "affiliation": null
        },
        {
          "name": "Zhihui Hao",
          "affiliation": null
        },
        {
          "name": "XianPeng Lang",
          "affiliation": null
        },
        {
          "name": "Hongyang Li",
          "affiliation": null
        }
      ],
      "abstract": "End-to-end autonomous driving models trained on largescale datasets perform well in common scenarios but struggle with rare, long-tail situations due to limited scenario diversity. Recent Vision-Language-Action (VLA) models leverage broad knowledge from pre-trained visionlanguage models to address this limitation, yet face critical challenges: (1) numerical imprecision in trajectory prediction due to discrete tokenization, (2) heavy reliance on language annotations that introduce linguistic bias and annotation burden, and (3) computational inefficiency from multi-step chain-of-thought reasoning hinders real-time deployment. We propose LatentVLA, a novel framework that employs self-supervised latent action prediction to train VLA models without language annotations, eliminating linguistic bias while learning rich driving representations from unlabeled trajectory data. Through knowledge distillation, LatentVLA transfers the generalization capabilities of VLA models to efficient vision-based networks, achieving both robust performance and real-time efficiency. LatentVLA establishes a new state-of-the-art on the NAVSIM benchmark with a PDMS score of 92.4 and demonstrates strong zeroshot generalization on the nuScenes benchmark.",
      "publishedDate": "2026-01-09T08:06:44Z",
      "updatedDate": "2026-01-09T08:06:44Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05611v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05611",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05607",
      "title": "Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR",
      "authors": [
        {
          "name": "Zijun Min",
          "affiliation": null
        },
        {
          "name": "Bingshuai Liu",
          "affiliation": null
        },
        {
          "name": "Ante Wang",
          "affiliation": null
        },
        {
          "name": "Long Zhang",
          "affiliation": null
        },
        {
          "name": "Anxiang Zeng",
          "affiliation": null
        },
        {
          "name": "Haibo Zhang",
          "affiliation": null
        },
        {
          "name": "Jinsong Su",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.",
      "publishedDate": "2026-01-09T07:57:40Z",
      "updatedDate": "2026-01-09T07:57:40Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05607v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05607",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05606",
      "title": "Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting",
      "authors": [
        {
          "name": "Chen Han",
          "affiliation": null
        },
        {
          "name": "Jin Tan",
          "affiliation": null
        },
        {
          "name": "Bohan Yu",
          "affiliation": null
        },
        {
          "name": "Wenzhen Zheng",
          "affiliation": null
        },
        {
          "name": "Xijin Tang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly instantiated as interacting agents in multi-agent systems (MAS), where collective decisions emerge through social interaction rather than independent reasoning. A fundamental yet underexplored mechanism in this process is conformity, the tendency of agents to align their judgments with prevailing group opinions. This paper presents a systematic study of how network topology shapes conformity dynamics in LLM-based MAS through a misinformation detection task. We introduce a confidence-normalized pooling rule that controls the trade-off between self-reliance and social influence, enabling comparisons between two canonical decision paradigms: Centralized Aggregation and Distributed Consensus. Experimental results demonstrate that network topology critically governs both the efficiency and robustness of collective judgments. Centralized structures enable immediate decisions but are sensitive to hub competence and exhibit same-model alignment biases. In contrast, distributed structures promote more robust consensus, while increased network connectivity speeds up convergence but also heightens the risk of wrong-but-sure cascades, in which agents converge on incorrect decisions with high confidence. These findings characterize the conformity dynamics in LLM-based MAS, clarifying how network topology and self-social weighting jointly shape the efficiency, robustness, and failure modes of collective decision-making.",
      "publishedDate": "2026-01-09T07:52:33Z",
      "updatedDate": "2026-01-09T07:52:33Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05606v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05606",
      "comment": "Under Review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05600",
      "title": "SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes",
      "authors": [
        {
          "name": "Chuhan Wang",
          "affiliation": null
        },
        {
          "name": "Xintong Li",
          "affiliation": null
        },
        {
          "name": "Jennifer Yuntong Zhang",
          "affiliation": null
        },
        {
          "name": "Junda Wu",
          "affiliation": null
        },
        {
          "name": "Chengkai Huang",
          "affiliation": null
        },
        {
          "name": "Lina Yao",
          "affiliation": null
        },
        {
          "name": "Julian McAuley",
          "affiliation": null
        },
        {
          "name": "Jingbo Shang",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal large language models often struggle with faithful reasoning in complex visual scenes, where intricate entities and relations require precise visual grounding at each step. This reasoning unfaithfulness frequently manifests as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning. Existing preference-based approaches, typically relying on textual perturbations or answer-conditioned rationales, fail to address this challenge as they allow models to exploit language priors to bypass visual grounding. To address this, we propose SceneAlign, a framework that leverages scene graphs as structured visual information to perform controllable structural interventions. By identifying reasoning-critical nodes and perturbing them through four targeted strategies that mimic typical grounding failures, SceneAlign constructs hard negative rationales that remain linguistically plausible but are grounded in inaccurate visual facts. These contrastive pairs are used in Direct Preference Optimization to steer models toward fine-grained, structure-faithful reasoning. Across seven visual reasoning benchmarks, SceneAlign consistently improves answer accuracy and reasoning faithfulness, highlighting the effectiveness of grounding-aware alignment for multimodal reasoning.",
      "publishedDate": "2026-01-09T07:40:39Z",
      "updatedDate": "2026-01-09T07:40:39Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05600v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05600",
      "comment": "Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05593",
      "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
      "authors": [
        {
          "name": "Jingcheng Hu",
          "affiliation": null
        },
        {
          "name": "Yinmin Zhang",
          "affiliation": null
        },
        {
          "name": "Shijie Shang",
          "affiliation": null
        },
        {
          "name": "Xiaobo Yang",
          "affiliation": null
        },
        {
          "name": "Yue Peng",
          "affiliation": null
        },
        {
          "name": "Zhewei Huang",
          "affiliation": null
        },
        {
          "name": "Hebin Zhou",
          "affiliation": null
        },
        {
          "name": "Xin Wu",
          "affiliation": null
        },
        {
          "name": "Jie Cheng",
          "affiliation": null
        },
        {
          "name": "Fanqi Wan",
          "affiliation": null
        },
        {
          "name": "Xiangwen Kong",
          "affiliation": null
        },
        {
          "name": "Chengyuan Yao",
          "affiliation": null
        },
        {
          "name": "Kaiwen Yan",
          "affiliation": null
        },
        {
          "name": "Ailin Huang",
          "affiliation": null
        },
        {
          "name": "Hongyu Zhou",
          "affiliation": null
        },
        {
          "name": "Qi Han",
          "affiliation": null
        },
        {
          "name": "Zheng Ge",
          "affiliation": null
        },
        {
          "name": "Daxin Jiang",
          "affiliation": null
        },
        {
          "name": "Xiangyu Zhang",
          "affiliation": null
        },
        {
          "name": "Heung-Yeung Shum",
          "affiliation": null
        }
      ],
      "abstract": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.",
      "publishedDate": "2026-01-09T07:24:43Z",
      "updatedDate": "2026-01-09T07:24:43Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05593v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05593",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05589",
      "title": "ACR: Adaptive Context Refactoring via Context Refactoring Operators for Multi-Turn Dialogue",
      "authors": [
        {
          "name": "Jiawei Shen",
          "affiliation": null
        },
        {
          "name": "Jia Zhu",
          "affiliation": null
        },
        {
          "name": "Hanghui Guo",
          "affiliation": null
        },
        {
          "name": "Weijie Shi",
          "affiliation": null
        },
        {
          "name": "Yue Cui",
          "affiliation": null
        },
        {
          "name": "Qingyu Niu",
          "affiliation": null
        },
        {
          "name": "Guoqing Ma",
          "affiliation": null
        },
        {
          "name": "Yidan Liang",
          "affiliation": null
        },
        {
          "name": "Jingjiang Liu",
          "affiliation": null
        },
        {
          "name": "Yiling Wang",
          "affiliation": null
        },
        {
          "name": "Shimin Di",
          "affiliation": null
        },
        {
          "name": "Jiajie Xu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable performance in multi-turn dialogue. However, in multi-turn dialogue, models still struggle to stay aligned with what has been established earlier, follow dependencies across many turns, and avoid drifting into incorrect facts as the interaction grows longer. Existing approaches primarily focus on extending the context window, introducing external memory, or applying context compression, yet these methods still face limitations such as \\textbf{contextual inertia} and \\textbf{state drift}. To address these challenges, we propose the \\textbf{A}daptive \\textbf{C}ontext \\textbf{R}efactoring \\textbf{(ACR)} Framework, which dynamically monitors and reshapes the interaction history to mitigate contextual inertia and state drift actively. ACR is built on a library of context refactoring operators and a teacher-guided self-evolving training paradigm that learns when to intervene and how to refactor, thereby decoupling context management from the reasoning process. Extensive experiments on multi-turn dialogue demonstrate that our method significantly outperforms existing baselines while reducing token consumption.",
      "publishedDate": "2026-01-09T07:17:28Z",
      "updatedDate": "2026-01-09T07:17:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05589v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05589",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05567",
      "title": "WildSci: Advancing Scientific Reasoning from In-the-Wild Literature",
      "authors": [
        {
          "name": "Tengxiao Liu",
          "affiliation": null
        },
        {
          "name": "Deepak Nathani",
          "affiliation": null
        },
        {
          "name": "Zekun Li",
          "affiliation": null
        },
        {
          "name": "Kevin Yang",
          "affiliation": null
        },
        {
          "name": "William Yang Wang",
          "affiliation": null
        }
      ],
      "abstract": "Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.",
      "publishedDate": "2026-01-09T06:35:23Z",
      "updatedDate": "2026-01-09T06:35:23Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05567v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05567",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05543",
      "title": "Closing the Modality Reasoning Gap for Speech Large Language Models",
      "authors": [
        {
          "name": "Chaoren Wang",
          "affiliation": null
        },
        {
          "name": "Heng Lu",
          "affiliation": null
        },
        {
          "name": "Xueyao Zhang",
          "affiliation": null
        },
        {
          "name": "Shujie Liu",
          "affiliation": null
        },
        {
          "name": "Yan Lu",
          "affiliation": null
        },
        {
          "name": "Jinyu Li",
          "affiliation": null
        },
        {
          "name": "Zhizheng Wu",
          "affiliation": null
        }
      ],
      "abstract": "Although speech large language models have achieved notable progress, a substantial modality reasoning gap remains: their reasoning performance on speech inputs is markedly weaker than on text. This gap could be associated with representational drift across Transformer layers and behavior deviations in long-chain reasoning. To address this issue, we introduce TARS, a reinforcement-learning framework that aligns text-conditioned and speech-conditioned trajectories through an asymmetric reward design. The framework employs two dense and complementary signals: representation alignment, which measures layer-wise hidden-state similarity between speech- and text-conditioned trajectories, and behavior alignment, which evaluates semantic consistency between generated outputs and reference text completions. Experiments on challenging reasoning benchmarks, including MMSU and OBQA, show that our approach significantly narrows the modality reasoning gap and achieves state-of-the-art performance among 7B-scale Speech LLMs.",
      "publishedDate": "2026-01-09T05:51:56Z",
      "updatedDate": "2026-01-09T05:51:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05543v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05543",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05539",
      "title": "LIDL: LLM Integration Defect Localization via Knowledge Graph-Enhanced Multi-Agent Analysis",
      "authors": [
        {
          "name": "Gou Tan",
          "affiliation": null
        },
        {
          "name": "Zilong He",
          "affiliation": null
        },
        {
          "name": "Min Li",
          "affiliation": null
        },
        {
          "name": "Pengfei Chen",
          "affiliation": null
        },
        {
          "name": "Jieke Shi",
          "affiliation": null
        },
        {
          "name": "Zhensu Sun",
          "affiliation": null
        },
        {
          "name": "Ting Zhang",
          "affiliation": null
        },
        {
          "name": "Danwen Chen",
          "affiliation": null
        },
        {
          "name": "Lwin Khin Shar",
          "affiliation": null
        },
        {
          "name": "Chuanfu Zhang",
          "affiliation": null
        },
        {
          "name": "David Lo",
          "affiliation": null
        }
      ],
      "abstract": "LLM-integrated software, which embeds or interacts with large language models (LLMs) as functional components, exhibits probabilistic and context-dependent behaviors that fundamentally differ from those of traditional software. This shift introduces a new category of integration defects that arise not only from code errors but also from misaligned interactions among LLM-specific artifacts, including prompts, API calls, configurations, and model outputs. However, existing defect localization techniques are ineffective at identifying these LLM-specific integration defects because they fail to capture cross-layer dependencies across heterogeneous artifacts, cannot exploit incomplete or misleading error traces, and lack semantic reasoning capabilities for identifying root causes. To address these challenges, we propose LIDL, a multi-agent framework for defect localization in LLM-integrated software. LIDL (1) constructs a code knowledge graph enriched with LLM-aware annotations that represent interaction boundaries across source code, prompts, and configuration files, (2) fuses three complementary sources of error evidence inferred by LLMs to surface candidate defect locations, and (3) applies context-aware validation that uses counterfactual reasoning to distinguish true root causes from propagated symptoms. We evaluate LIDL on 146 real-world defect instances collected from 105 GitHub repositories and 16 agent-based systems. The results show that LIDL significantly outperforms five state-of-the-art baselines across all metrics, achieving a Top-3 accuracy of 0.64 and a MAP of 0.48, which represents a 64.1% improvement over the best-performing baseline. Notably, LIDL achieves these gains while reducing cost by 92.5%, demonstrating both high accuracy and cost efficiency.",
      "publishedDate": "2026-01-09T05:47:59Z",
      "updatedDate": "2026-01-09T05:47:59Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05539v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05539",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "agents",
        "tool-use",
        "reasoning",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "tool-use",
          "reasoning",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05529",
      "title": "Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making",
      "authors": [
        {
          "name": "Jua Han",
          "affiliation": null
        },
        {
          "name": "Jaeyoon Seo",
          "affiliation": null
        },
        {
          "name": "Jungbin Min",
          "affiliation": null
        },
        {
          "name": "Jean Oh",
          "affiliation": null
        },
        {
          "name": "Jihie Kim",
          "affiliation": null
        }
      ],
      "abstract": "One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how \"rare\" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.",
      "publishedDate": "2026-01-09T05:04:15Z",
      "updatedDate": "2026-01-09T05:04:15Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05529v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05529",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "robotics",
        "agents",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "robotics",
          "agents",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05520",
      "title": "CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems",
      "authors": [
        {
          "name": "Xuemei Tang",
          "affiliation": null
        },
        {
          "name": "Chengxi Yan",
          "affiliation": null
        },
        {
          "name": "Jinghang Gu",
          "affiliation": null
        },
        {
          "name": "Chu-Ren Huang",
          "affiliation": null
        }
      ],
      "abstract": "Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \\textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \\textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \\textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \\textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \\textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.",
      "publishedDate": "2026-01-09T04:28:45Z",
      "updatedDate": "2026-01-09T04:28:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05520v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05520",
      "comment": "22 pages, 13 figures, 7 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05513",
      "title": "LEAPS: An LLM-Empowered Adaptive Plugin for Taobao AI Search",
      "authors": [
        {
          "name": "Lei Wang",
          "affiliation": null
        },
        {
          "name": "Jinhang Wu",
          "affiliation": null
        },
        {
          "name": "Zhibin Wang",
          "affiliation": null
        },
        {
          "name": "Biye Li",
          "affiliation": null
        },
        {
          "name": "Haiping Hou",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancement of large language models has reshaped user search cognition, driving a paradigm shift from discrete keyword-based search to high-dimensional conversational interaction. However, existing e-commerce search architectures face a critical capability deficit in adapting to this change. Users are often caught in a dilemma: precise natural language descriptions frequently trigger zero-result scenarios, while the forced simplification of queries leads to decision overload from noisy, generic results. To tackle this challenge, we propose LEAPS (LLM-Empowered Adaptive Plugin for Taobao AI Search), which seamlessly upgrades traditional search systems via a \"Broaden-and-Refine\" paradigm. Specifically, it attaches plugins to both ends of the search pipeline: (1) Upstream, a Query Expander acts as an intent translator. It employs a novel three-stage training strategy--inverse data augmentation, posterior-knowledge supervised fine-tuning, and diversity-aware reinforcement learning--to generate adaptive and complementary query combinations that maximize the candidate product set. (2) Downstream, a Relevance Verifier serves as a semantic gatekeeper. By synthesizing multi-source data (e.g., OCR text, reviews) and leveraging chain-of-thought reasoning, it precisely filters noise to resolve selection overload. Extensive offline experiments and online A/B testing demonstrate that LEAPS significantly enhances conversational search experiences. Crucially, its non-invasive architecture preserves established retrieval performance optimized for short-text queries, while simultaneously allowing for low-cost integration into diverse back-ends. Fully deployed on Taobao AI Search since August 2025, LEAPS currently serves hundreds of millions of users monthly.",
      "publishedDate": "2026-01-09T03:41:27Z",
      "updatedDate": "2026-01-09T03:41:27Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05513v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05513",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05505",
      "title": "FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse",
      "authors": [
        {
          "name": "Yubo Hou",
          "affiliation": null
        },
        {
          "name": "Zhisheng Chen",
          "affiliation": null
        },
        {
          "name": "Tao Wan",
          "affiliation": null
        },
        {
          "name": "Zengchang Qin",
          "affiliation": null
        }
      ],
      "abstract": "The stateless architecture of Large Language Models inherently lacks the mechanism to preserve dynamic context, compelling agents to redundantly reprocess history to maintain long-horizon autonomy. While latent memory offers a solution, current approaches are hindered by architectural segregation, relying on auxiliary encoders that decouple memory from the reasoning backbone. We propose FlashMem, a framework that distills intrinsic memory directly from transient reasoning states via computation reuse. Leveraging the property that internal representations uniquely encode input trajectories, FlashMem identifies the last hidden state as a sufficient statistic for the interaction history. This enables a Shared-KV Consolidator to synthesize memory by attending directly to the backbone's frozen cache, eliminating redundant re-parameterization. Furthermore, a parameter-free Cognitive Monitor leverages attention entropy to adaptively trigger consolidation only when high epistemic uncertainty is detected. Experiments demonstrate that FlashMem matches the performance of heavy baselines while reducing inference latency by 5 times, effectively bridging the gap between efficiency and persistent cognition.",
      "publishedDate": "2026-01-09T03:27:43Z",
      "updatedDate": "2026-01-09T03:27:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05505v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05505",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05503",
      "title": "Over-Searching in Search-Augmented Large Language Models",
      "authors": [
        {
          "name": "Roy Xie",
          "affiliation": null
        },
        {
          "name": "Deepak Gopinath",
          "affiliation": null
        },
        {
          "name": "David Qiu",
          "affiliation": null
        },
        {
          "name": "Dong Lin",
          "affiliation": null
        },
        {
          "name": "Haitian Sun",
          "affiliation": null
        },
        {
          "name": "Saloni Potdar",
          "affiliation": null
        },
        {
          "name": "Bhuwan Dhingra",
          "affiliation": null
        }
      ],
      "abstract": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
      "publishedDate": "2026-01-09T03:24:46Z",
      "updatedDate": "2026-01-09T03:24:46Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05503v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05503",
      "comment": "Accepted to EACL 2026 Main Conference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05501",
      "title": "Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection",
      "authors": [
        {
          "name": "Feihu Jin",
          "affiliation": null
        },
        {
          "name": "Ying Tan",
          "affiliation": null
        }
      ],
      "abstract": "Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \\textbf{Hi-ZFO} (\\textbf{Hi}erarchical \\textbf{Z}eroth- and \\textbf{F}irst-\\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of \"beneficial stochasticity\" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.",
      "publishedDate": "2026-01-09T03:20:54Z",
      "updatedDate": "2026-01-09T03:20:54Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05501v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05501",
      "comment": "13 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05485",
      "title": "Readability-Robust Code Summarization via Meta Curriculum Learning",
      "authors": [
        {
          "name": "Wenhao Zeng",
          "affiliation": null
        },
        {
          "name": "Yitian Chai",
          "affiliation": null
        },
        {
          "name": "Hao Zhou",
          "affiliation": null
        },
        {
          "name": "Fandong Meng",
          "affiliation": null
        },
        {
          "name": "Jie Zhou",
          "affiliation": null
        },
        {
          "name": "Xiaodong Gu",
          "affiliation": null
        }
      ],
      "abstract": "Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and variable names. In the real world, however, code is often poorly structured or obfuscated, significantly degrading model performance. In this paper, we first empirically evaluate the robustness of state-of-the-art language models on poor-readability code for the task of code summarization, focusing on (1) their effectiveness, (2) the impact of prompt engineering, and (3) the robustness of different variants. Experimental results reveal that state-of-the-art models-including GPT-4o and DeepSeek-V3 experience a substantial performance drop when faced with poorly readable code, and that prompt engineering and reasoning-enhanced models offer limited improvements. Motivated by these findings, we propose RoFTCodeSum, a novel fine-tuning method that enhances the robustness of code summarization against poorly readable code. RoFTCodeSum marries the concepts of curriculum learning and meta-learning: based on the original dataset for fine-tuning, it creates curricular training sets, e.g., obfuscating function names and identifiers from the code, respectively, that have progressive difficulty in code comprehension. In each training step, the approach meta-updates the gradients using these progressively challenging datasets, thereby optimizing both accuracy and readability robustness simultaneously. Experimental results demonstrate that RoFTCodeSum exhibits increased robustness against semantic perturbation while enhancing performance on the original code.",
      "publishedDate": "2026-01-09T02:38:24Z",
      "updatedDate": "2026-01-09T02:38:24Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05485v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05485",
      "comment": "Code available at https://github.com/Zengwh02/RoFTCodeSum",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05459",
      "title": "Do LLMs Need Inherent Reasoning Before Reinforcement Learning? A Study in Korean Self-Correction",
      "authors": [
        {
          "name": "Hongjin Kim",
          "affiliation": null
        },
        {
          "name": "Jaewook Lee",
          "affiliation": null
        },
        {
          "name": "Kiyoung Lee",
          "affiliation": null
        },
        {
          "name": "Jong-hun Shin",
          "affiliation": null
        },
        {
          "name": "Soojong Lim",
          "affiliation": null
        },
        {
          "name": "Oh-Woog Kwon",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) demonstrate strong reasoning and self-correction abilities in high-resource languages like English, but their performance remains limited in low-resource languages such as Korean. In this study, we investigate whether reinforcement learning (RL) can enhance Korean reasoning abilities to a degree comparable to English. Our findings reveal that RL alone yields limited improvements when applied to models lacking inherent Korean reasoning capabilities. To address this, we explore several fine-tuning strategies and show that aligning the model's internal reasoning processes with Korean inputs-particularly by tuning Korean-specific neurons in early layers-is key to unlocking RL's effectiveness. We introduce a self-correction code-switching dataset to facilitate this alignment and observe significant performance gains in both mathematical reasoning and self-correction tasks. Ultimately, we conclude that the crucial factor in multilingual reasoning enhancement is not injecting new linguistic knowledge, but effectively eliciting and aligning existing reasoning capabilities. Our study provides a new perspective on how internal translation and neuron-level tuning contribute to multilingual reasoning alignment in LLMs.",
      "publishedDate": "2026-01-09T01:17:31Z",
      "updatedDate": "2026-01-09T01:17:31Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05459v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05459",
      "comment": "IJCNLP-AACL 2025 (Main), Outstanding Paper Award",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05455",
      "title": "ART: Adaptive Reasoning Trees for Explainable Claim Verification",
      "authors": [
        {
          "name": "Sahil Wadhwa",
          "affiliation": null
        },
        {
          "name": "Himanshu Kumar",
          "affiliation": null
        },
        {
          "name": "Guanqun Yang",
          "affiliation": null
        },
        {
          "name": "Abbaas Alif Mohamed Nishar",
          "affiliation": null
        },
        {
          "name": "Pranab Mohanty",
          "affiliation": null
        },
        {
          "name": "Swapnil Shinde",
          "affiliation": null
        },
        {
          "name": "Yue Wu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.",
      "publishedDate": "2026-01-09T01:01:55Z",
      "updatedDate": "2026-01-09T01:01:55Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05455v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05455",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "agents",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "agents",
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05432",
      "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
      "authors": [
        {
          "name": "Yuxiang Ji",
          "affiliation": null
        },
        {
          "name": "Yong Wang",
          "affiliation": null
        },
        {
          "name": "Ziyu Ma",
          "affiliation": null
        },
        {
          "name": "Yiming Hu",
          "affiliation": null
        },
        {
          "name": "Hailang Huang",
          "affiliation": null
        },
        {
          "name": "Xuecai Hu",
          "affiliation": null
        },
        {
          "name": "Guanhua Chen",
          "affiliation": null
        },
        {
          "name": "Liaoni Wu",
          "affiliation": null
        },
        {
          "name": "Xiangxiang Chu",
          "affiliation": null
        }
      ],
      "abstract": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model \\textit{Thinking with Map} ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to \\textit{Gemini-3-Pro} with Google Search/Map grounded mode.",
      "publishedDate": "2026-01-08T23:47:30Z",
      "updatedDate": "2026-01-08T23:47:30Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05432v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05432",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05376",
      "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
      "authors": [
        {
          "name": "Tassallah Abdullahi",
          "affiliation": null
        },
        {
          "name": "Shrestha Ghosh",
          "affiliation": null
        },
        {
          "name": "Hamish S Fraser",
          "affiliation": null
        },
        {
          "name": "Daniel León Tramontini",
          "affiliation": null
        },
        {
          "name": "Adeel Abbasi",
          "affiliation": null
        },
        {
          "name": "Ghada Bourjeily",
          "affiliation": null
        },
        {
          "name": "Carsten Eickhoff",
          "affiliation": null
        },
        {
          "name": "Ritambhara Singh",
          "affiliation": null
        }
      ],
      "abstract": "Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\\sim+20\\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\\_Paradox.",
      "publishedDate": "2026-01-08T21:01:11Z",
      "updatedDate": "2026-01-08T21:01:11Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05376v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05376",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05356",
      "title": "PRISM: Protocol Refinement through Intelligent Simulation Modeling",
      "authors": [
        {
          "name": "Brian Hsu",
          "affiliation": null
        },
        {
          "name": "Priyanka V Setty",
          "affiliation": null
        },
        {
          "name": "Rory M Butler",
          "affiliation": null
        },
        {
          "name": "Ryan Lewis",
          "affiliation": null
        },
        {
          "name": "Casey Stone",
          "affiliation": null
        },
        {
          "name": "Rebecca Weinberg",
          "affiliation": null
        },
        {
          "name": "Thomas Brettin",
          "affiliation": null
        },
        {
          "name": "Rick Stevens",
          "affiliation": null
        },
        {
          "name": "Ian Foster",
          "affiliation": null
        },
        {
          "name": "Arvind Ramanathan",
          "affiliation": null
        }
      ],
      "abstract": "Automating experimental protocol design and execution remains as a fundamental bottleneck in realizing self-driving laboratories. We introduce PRISM (Protocol Refinement through Intelligent Simulation Modeling), a framework that automates the design, validation, and execution of experimental protocols on a laboratory platform composed of off-the-shelf robotic instruments. PRISM uses a set of language-model-based agents that work together to generate and refine experimental steps. The process begins with automatically gathering relevant procedures from web-based sources describing experimental workflows. These are converted into structured experimental steps (e.g., liquid handling steps, deck layout and other related operations) through a planning, critique, and validation loop. The finalized steps are translated into the Argonne MADSci protocol format, which provides a unified interface for coordinating multiple robotic instruments (Opentrons OT-2 liquid handler, PF400 arm, Azenta plate sealer and peeler) without requiring human intervention between steps. To evaluate protocol-generation performance, we benchmarked both single reasoning models and multi-agent workflow across constrained and open-ended prompting paradigms. The resulting protocols were validated in a digital-twin environment built in NVIDIA Omniverse to detect physical or sequencing errors before execution. Using Luna qPCR amplification and Cell Painting as case studies, we demonstrate PRISM as a practical end-to-end workflow that bridges language-based protocol generation, simulation-based validation, and automated robotic execution.",
      "publishedDate": "2026-01-08T20:15:28Z",
      "updatedDate": "2026-01-08T20:15:28Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.MA",
        "q-bio.QM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05356v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05356",
      "comment": "43 pages, 8 figures, submitted to RSC Digital Discovery. Equal contribution: B. Hsu, P.V. Setty, R.M. Butler. Corresponding author: A. Ramanathan",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "prompting",
        "agents",
        "reasoning",
        "planning",
        "multi-agent",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "reasoning",
          "planning",
          "multi-agent",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05336",
      "title": "Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models",
      "authors": [
        {
          "name": "Tracey Yee Hsin Tay",
          "affiliation": null
        },
        {
          "name": "Xu Yan",
          "affiliation": null
        },
        {
          "name": "Jonathan Ouyang",
          "affiliation": null
        },
        {
          "name": "Daniel Wu",
          "affiliation": null
        },
        {
          "name": "William Jiang",
          "affiliation": null
        },
        {
          "name": "Jonathan Kao",
          "affiliation": null
        },
        {
          "name": "Yuchen Cui",
          "affiliation": null
        }
      ],
      "abstract": "Designing intuitive interfaces for robotic control remains a central challenge in enabling effective human-robot interaction, particularly in assistive care settings. Eye gaze offers a fast, non-intrusive, and intent-rich input modality, making it an attractive channel for conveying user goals. In this work, we present GAMMA (Gaze Assisted Manipulation for Modular Autonomy), a system that leverages ego-centric gaze tracking and a vision-language model to infer user intent and autonomously execute robotic manipulation tasks. By contextualizing gaze fixations within the scene, the system maps visual attention to high-level semantic understanding, enabling skill selection and parameterization without task-specific training. We evaluate GAMMA on a range of table-top manipulation tasks and compare it against baseline gaze-based control without reasoning. Results demonstrate that GAMMA provides robust, intuitive, and generalizable control, highlighting the potential of combining foundation models and gaze for natural and scalable robot autonomy. Project website: https://gamma0.vercel.app/",
      "publishedDate": "2026-01-08T19:33:03Z",
      "updatedDate": "2026-01-08T19:33:03Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05336v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05336",
      "comment": "Accepted to 2025 RSS Robot Planning in the Era of Foundation Models (FM4RoboPlan) Workshop",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "robotics",
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "robotics",
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05300",
      "title": "TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning",
      "authors": [
        {
          "name": "Susmit Das",
          "affiliation": null
        }
      ],
      "abstract": "Reasoning oriented large language models often expose explicit \"thinking\" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench",
      "publishedDate": "2026-01-08T13:24:49Z",
      "updatedDate": "2026-01-08T13:24:49Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05300v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05300",
      "comment": "14 pages, 3 figures with 27 page appendix. See https://github.com/The-Coherence-Initiative/TIME and https://github.com/The-Coherence-Initiative/TIMEBench for associated code",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05298",
      "title": "Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing",
      "authors": [
        {
          "name": "Yeongbin Cha",
          "affiliation": null
        },
        {
          "name": "Namjung Kim",
          "affiliation": null
        }
      ],
      "abstract": "Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ontology-guided, equation-centric framework that tightly integrates large language models (LLMs) with an additive manufacturing mathematical knowledge graph (AM-MKG) to enable reliable knowledge extraction and principled extrapolative modeling. By explicitly encoding equations, variables, assumptions, and their semantic relationships within a formal ontology, unstructured literature is transformed into machine-interpretable representations that support structured querying and reasoning. LLM-based equation generation is further conditioned on MKG-derived subgraphs, enforcing physically meaningful functional forms and mitigating non-physical or unstable extrapolation trends. To assess reliability beyond conventional predictive uncertainty, a confidence-aware extrapolation assessment is introduced, integrating extrapolation distance, statistical stability, and knowledge-graph-based physical consistency into a unified confidence score. Results demonstrate that ontology-guided extraction significantly improves the structural coherence and quantitative reliability of extracted knowledge, while subgraph-conditioned equation generation yields stable and physically consistent extrapolations compared to unguided LLM outputs. Overall, this work establishes a unified pipeline for ontology-driven knowledge representation, equation-centered reasoning, and confidence-based extrapolation assessment, highlighting the potential of knowledge-graph-augmented LLMs as reliable tools for extrapolative modeling in additive manufacturing.",
      "publishedDate": "2026-01-08T10:38:03Z",
      "updatedDate": "2026-01-08T10:38:03Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05298v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05298",
      "comment": "preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05899",
      "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
      "authors": [
        {
          "name": "Dawei Wang",
          "affiliation": null
        },
        {
          "name": "Chengming Zhou",
          "affiliation": null
        },
        {
          "name": "Di Zhao",
          "affiliation": null
        },
        {
          "name": "Xinyuan Liu",
          "affiliation": null
        },
        {
          "name": "Marci Chi Ma",
          "affiliation": null
        },
        {
          "name": "Gary Ushaw",
          "affiliation": null
        },
        {
          "name": "Richard Davison",
          "affiliation": null
        }
      ],
      "abstract": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).",
      "publishedDate": "2026-01-09T16:18:08Z",
      "updatedDate": "2026-01-09T16:18:08Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05899v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05899",
      "comment": "AAAI 2026 Oral",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "agents",
        "planning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "planning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05445",
      "title": "Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models",
      "authors": [
        {
          "name": "Songze Li",
          "affiliation": null
        },
        {
          "name": "Ruishi He",
          "affiliation": null
        },
        {
          "name": "Xiaojun Jia",
          "affiliation": null
        },
        {
          "name": "Jun Wang",
          "affiliation": null
        },
        {
          "name": "Zhihui Fu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) face a significant threat from multi-turn jailbreak attacks, where adversaries progressively steer conversations to elicit harmful outputs. However, the practical effectiveness of existing attacks is undermined by several critical limitations: they struggle to maintain a coherent progression over long interactions, often losing track of what has been accomplished and what remains to be done; they rely on rigid or pre-defined patterns, and fail to adapt to the LLM's dynamic and unpredictable conversational state. To address these shortcomings, we introduce Mastermind, a multi-turn jailbreak framework that adopts a dynamic and self-improving approach. Mastermind operates in a closed loop of planning, execution, and reflection, enabling it to autonomously build and refine its knowledge of model vulnerabilities through interaction. It employs a hierarchical planning architecture that decouples high-level attack objectives from low-level tactical execution, ensuring long-term focus and coherence. This planning is guided by a knowledge repository that autonomously discovers and refines effective attack patterns by reflecting on interactive experiences. Mastermind leverages this accumulated knowledge to dynamically recombine and adapt attack vectors, dramatically improving both effectiveness and resilience. We conduct comprehensive experiments against state-of-the-art models, including GPT-5 and Claude 3.7 Sonnet. The results demonstrate that Mastermind significantly outperforms existing baselines, achieving substantially higher attack success rates and harmfulness ratings. Moreover, our framework exhibits notable resilience against multiple advanced defense mechanisms.",
      "publishedDate": "2026-01-09T00:27:08Z",
      "updatedDate": "2026-01-09T00:27:08Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05445v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05445",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05555",
      "title": "An Empirical Study of Policy-as-Code Adoption in Open-Source Software Projects",
      "authors": [
        {
          "name": "Patrick Loic Foalem",
          "affiliation": null
        },
        {
          "name": "Foutse Khomh",
          "affiliation": null
        },
        {
          "name": "Leuson Da Silva",
          "affiliation": null
        },
        {
          "name": "Ettore Merlo",
          "affiliation": null
        }
      ],
      "abstract": "\\textbf{Context:} Policy-as-Code (PaC) has become a foundational approach for embedding governance, compliance, and security requirements directly into software systems. While organizations increasingly adopt PaC tools, the software engineering community lacks an empirical understanding of how these tools are used in real-world development practices. \\textbf{Objective:} This paper aims to bridge this gap by conducting the first large-scale study of PaC usage in open-source software. Our goal is to characterize how PaC tools are adopted, what purposes they serve, and what governance activities they support across diverse software ecosystems. \\textbf{Method:} We analyzed 399 GitHub repositories using nine widely adopted PaC tools. Our mixed-methods approach combines quantitative analysis of tool usage and project characteristics with a qualitative investigation of policy files. We further employ a Large Language Model (LLM)--assisted classification pipeline, refined through expert validation, to derive a taxonomy of PaC usage consisting of 5 categories and 15 sub-categories. \\textbf{Results:} Our study reveals substantial diversity in PaC adoption. PaC tools are frequently used in early-stage projects and are heavily oriented toward governance, configuration control, and documentation. We also observe emerging PaC usage in MLOps pipelines and strong co-usage patterns, such as between OPA and Gatekeeper. Our taxonomy highlights recurring governance intents. \\textbf{Conclusion:} Our findings offer actionable insights for practitioners and tool developers. They highlight concrete usage patterns, emphasize actual PaC usage, and motivate opportunities for improving tool interoperability. This study lays the empirical foundation for future research on PaC practices and their role in ensuring trustworthy, compliant software systems.",
      "publishedDate": "2026-01-09T06:12:40Z",
      "updatedDate": "2026-01-09T06:12:40Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05555v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05555",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05366",
      "title": "Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models",
      "authors": [
        {
          "name": "Zheng Luo",
          "affiliation": null
        },
        {
          "name": "T Pranav Kutralingam",
          "affiliation": null
        },
        {
          "name": "Ogochukwu N Okoani",
          "affiliation": null
        },
        {
          "name": "Wanpeng Xu",
          "affiliation": null
        },
        {
          "name": "Hua Wei",
          "affiliation": null
        },
        {
          "name": "Xiyang Hu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.",
      "publishedDate": "2026-01-08T20:44:28Z",
      "updatedDate": "2026-01-08T20:44:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05366v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05366",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05890",
      "title": "StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management",
      "authors": [
        {
          "name": "Ruizhe Zhang",
          "affiliation": null
        },
        {
          "name": "Xinke Jiang",
          "affiliation": null
        },
        {
          "name": "Zhibang Yang",
          "affiliation": null
        },
        {
          "name": "Zhixin Zhang",
          "affiliation": null
        },
        {
          "name": "Jiaran Gao",
          "affiliation": null
        },
        {
          "name": "Yuzhen Xiao",
          "affiliation": null
        },
        {
          "name": "Hongbin Lai",
          "affiliation": null
        },
        {
          "name": "Xu Chu",
          "affiliation": null
        },
        {
          "name": "Junfeng Zhao",
          "affiliation": null
        },
        {
          "name": "Yasha Wang",
          "affiliation": null
        }
      ],
      "abstract": "Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.",
      "publishedDate": "2026-01-09T16:09:48Z",
      "updatedDate": "2026-01-09T16:09:48Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05890v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05890",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "multi-agent",
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05570",
      "title": "Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models",
      "authors": [
        {
          "name": "Cooper Lin",
          "affiliation": null
        },
        {
          "name": "Maohao Ran",
          "affiliation": null
        },
        {
          "name": "Yanting Zhang",
          "affiliation": null
        },
        {
          "name": "Zhenglin Wan",
          "affiliation": null
        },
        {
          "name": "Hongwei Fan",
          "affiliation": null
        },
        {
          "name": "Yibo Xu",
          "affiliation": null
        },
        {
          "name": "Yike Guo",
          "affiliation": null
        },
        {
          "name": "Wei Xue",
          "affiliation": null
        },
        {
          "name": "Jun Song",
          "affiliation": null
        }
      ],
      "abstract": "Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid \"Boy Scout\" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a \"transparency tax\" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing \"Reputation Management\" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.",
      "publishedDate": "2026-01-09T06:41:49Z",
      "updatedDate": "2026-01-09T06:41:49Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05570v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05570",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05384",
      "title": "Conformity and Social Impact on AI Agents",
      "authors": [
        {
          "name": "Alessandro Bellina",
          "affiliation": null
        },
        {
          "name": "Giordano De Marzo",
          "affiliation": null
        },
        {
          "name": "David Garcia",
          "affiliation": null
        }
      ],
      "abstract": "As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.",
      "publishedDate": "2026-01-08T21:16:28Z",
      "updatedDate": "2026-01-08T21:16:28Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05384v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05384",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05975",
      "title": "DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management",
      "authors": [
        {
          "name": "Kieran Wood",
          "affiliation": null
        },
        {
          "name": "Stephen J. Roberts",
          "affiliation": null
        },
        {
          "name": "Stefan Zohren",
          "affiliation": null
        }
      ],
      "abstract": "We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous \"ragged filtration\" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s \"CTA (Commodity Trading Advisor) Winter\" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.",
      "publishedDate": "2026-01-09T17:47:32Z",
      "updatedDate": "2026-01-09T17:47:32Z",
      "primaryCategory": "q-fin.TR",
      "arxivCategories": [
        "q-fin.TR",
        "cs.LG",
        "stat.ML"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05975v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05975",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05866",
      "title": "FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG",
      "authors": [
        {
          "name": "Maxime Dassen",
          "affiliation": null
        },
        {
          "name": "Rebecca Kotula",
          "affiliation": null
        },
        {
          "name": "Kenton Murray",
          "affiliation": null
        },
        {
          "name": "Andrew Yates",
          "affiliation": null
        },
        {
          "name": "Dawn Lawrie",
          "affiliation": null
        },
        {
          "name": "Efsun Kayi",
          "affiliation": null
        },
        {
          "name": "James Mayfield",
          "affiliation": null
        },
        {
          "name": "Kevin Duh",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) models are critically undermined by citation hallucinations, a deceptive failure where a model confidently cites a source that fails to support its claim. Existing work often attributes hallucination to a simple over-reliance on the model's parametric knowledge. We challenge this view and introduce FACTUM (Framework for Attesting Citation Trustworthiness via Underlying Mechanisms), a framework of four mechanistic scores measuring the distinct contributions of a model's attention and FFN pathways, and the alignment between them. Our analysis reveals two consistent signatures of correct citation: a significantly stronger contribution from the model's parametric knowledge and greater use of the attention sink for information synthesis. Crucially, we find the signature of a correct citation is not static but evolves with model scale. For example, the signature of a correct citation for the Llama-3.2-3B model is marked by higher pathway alignment, whereas for the Llama-3.1-8B model, it is characterized by lower alignment, where pathways contribute more distinct, orthogonal information. By capturing this complex, evolving signature, FACTUM outperforms state-of-the-art baselines by up to 37.5% in AUC. Our findings reframe citation hallucination as a complex, scale-dependent interplay between internal mechanisms, paving the way for more nuanced and reliable RAG systems.",
      "publishedDate": "2026-01-09T15:41:08Z",
      "updatedDate": "2026-01-09T15:41:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05866v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05866",
      "comment": "Accepted at ECIR 2026. 18 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05721",
      "title": "From Issues to Insights: RAG-based Explanation Generation from Software Engineering Artifacts",
      "authors": [
        {
          "name": "Daniel Pöttgen",
          "affiliation": null
        },
        {
          "name": "Mersedeh Sadeghi",
          "affiliation": null
        },
        {
          "name": "Max Unterbusch",
          "affiliation": null
        },
        {
          "name": "Andreas Vogelsang",
          "affiliation": null
        }
      ],
      "abstract": "The increasing complexity of modern software systems has made understanding their behavior increasingly challenging, driving the need for explainability to improve transparency and user trust. Traditional documentation is often outdated or incomplete, making it difficult to derive accurate, context-specific explanations. Meanwhile, issue-tracking systems capture rich and continuously updated development knowledge, but their potential for explainability remains untapped. With this work, we are the first to apply a Retrieval-Augmented Generation (RAG) approach for generating explanations from issue-tracking data. Our proof-of-concept system is implemented using open-source tools and language models, demonstrating the feasibility of leveraging structured issue data for explanation generation. Evaluating our approach on an exemplary project's set of GitHub issues, we achieve 90% alignment with human-written explanations. Additionally, our system exhibits strong faithfulness and instruction adherence, ensuring reliable and grounded explanations. These findings suggest that RAG-based methods can extend explainability beyond black-box ML models to a broader range of software systems, provided that issue-tracking data is available - making system behavior more accessible and interpretable.",
      "publishedDate": "2026-01-09T11:05:50Z",
      "updatedDate": "2026-01-09T11:05:50Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05721v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05721",
      "comment": "Accepted at NLBSE 2026, Rio de Janeiro, Brazil",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05549",
      "title": "Efficient Temporal-aware Matryoshka Adaptation for Temporal Information Retrieval",
      "authors": [
        {
          "name": "Tuan-Luc Huynh",
          "affiliation": null
        },
        {
          "name": "Weiqing Wang",
          "affiliation": null
        },
        {
          "name": "Trung Le",
          "affiliation": null
        },
        {
          "name": "Thuy-Trang Vu",
          "affiliation": null
        },
        {
          "name": "Dragan Gašević",
          "affiliation": null
        },
        {
          "name": "Yuan-Fang Li",
          "affiliation": null
        },
        {
          "name": "Thanh-Toan Do",
          "affiliation": null
        }
      ],
      "abstract": "Retrievers are a key bottleneck in Temporal Retrieval-Augmented Generation (RAG) systems: failing to retrieve temporally relevant context can degrade downstream generation, regardless of LLM reasoning. We propose Temporal-aware Matryoshka Representation Learning (TMRL), an efficient method that equips retrievers with temporal-aware Matryoshka embeddings. TMRL leverages the nested structure of Matryoshka embeddings to introduce a temporal subspace, enhancing temporal encoding while preserving general semantic representations. Experiments show that TMRL efficiently adapts diverse text embedding models, achieving competitive temporal retrieval and temporal RAG performance compared to prior Matryoshka-based non-temporal methods and prior temporal methods, while enabling flexible accuracy-efficiency trade-offs.",
      "publishedDate": "2026-01-09T06:01:01Z",
      "updatedDate": "2026-01-09T06:01:01Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05549v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05549",
      "comment": "18 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05465",
      "title": "PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering",
      "authors": [
        {
          "name": "Yu Liu",
          "affiliation": null
        },
        {
          "name": "Wenxiao Zhang",
          "affiliation": null
        },
        {
          "name": "Cong Cao",
          "affiliation": null
        },
        {
          "name": "Wenxuan Lu",
          "affiliation": null
        },
        {
          "name": "Fangfang Yuan",
          "affiliation": null
        },
        {
          "name": "Diandian Guo",
          "affiliation": null
        },
        {
          "name": "Kun Peng",
          "affiliation": null
        },
        {
          "name": "Qiang Sun",
          "affiliation": null
        },
        {
          "name": "Kaiyan Zhang",
          "affiliation": null
        },
        {
          "name": "Yanbing Liu",
          "affiliation": null
        },
        {
          "name": "Jin B. Hong",
          "affiliation": null
        },
        {
          "name": "Bowen Zhou",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Ma",
          "affiliation": null
        }
      ],
      "abstract": "Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.",
      "publishedDate": "2026-01-09T01:38:38Z",
      "updatedDate": "2026-01-09T01:38:38Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05465v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05465",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "multi-agent",
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "multi-agent",
          "agents",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05794",
      "title": "Simplify-This: A Comparative Analysis of Prompt-Based and Fine-Tuned LLMs",
      "authors": [
        {
          "name": "Eilam Cohen",
          "affiliation": null
        },
        {
          "name": "Itamar Bul",
          "affiliation": null
        },
        {
          "name": "Danielle Inbar",
          "affiliation": null
        },
        {
          "name": "Omri Loewenbach",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) enable strong text generation, and in general there is a practical tradeoff between fine-tuning and prompt engineering. We introduce Simplify-This, a comparative study evaluating both paradigms for text simplification with encoder-decoder LLMs across multiple benchmarks, using a range of evaluation metrics. Fine-tuned models consistently deliver stronger structural simplification, whereas prompting often attains higher semantic similarity scores yet tends to copy inputs. A human evaluation favors fine-tuned outputs overall. We release code, a cleaned derivative dataset used in our study, checkpoints of fine-tuned models, and prompt templates to facilitate reproducibility and future work.",
      "publishedDate": "2026-01-09T13:46:52Z",
      "updatedDate": "2026-01-09T13:46:52Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05794v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05794",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "prompting",
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05552",
      "title": "One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection",
      "authors": [
        {
          "name": "Bin-Bin Gao",
          "affiliation": null
        },
        {
          "name": "Chengjie Wang",
          "affiliation": null
        }
      ],
      "abstract": "Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.",
      "publishedDate": "2026-01-09T06:05:18Z",
      "updatedDate": "2026-01-09T06:05:18Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05552v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05552",
      "comment": "20 pages, 5 figures, 34 tabels",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05707",
      "title": "Multimodal In-context Learning for ASR of Low-resource Languages",
      "authors": [
        {
          "name": "Zhaolin Li",
          "affiliation": null
        },
        {
          "name": "Jan Niehues",
          "affiliation": null
        }
      ],
      "abstract": "Automatic speech recognition (ASR) still covers only a small fraction of the world's languages, mainly due to supervised data scarcity. In-context learning (ICL) with large language models (LLMs) addresses this problem, but prior work largely focuses on high-resource languages covered during training and text-only settings. This paper investigates whether speech LLMs can learn unseen languages with multimodal ICL (MICL), and how this learning can be used to improve ASR. We conduct experiments with two speech LLMs, Phi-4 and Qwen3-Omni, on three diverse endangered languages. Firstly, we find that MICL is effective for unseen languages, leveraging both speech and text modalities. We further show that cross-lingual transfer learning improves MICL efficiency on target languages without training on them. Moreover, we analyze attention patterns to interpret MICL mechanisms, and we observe layer-dependent preferences between audio and text context, with an overall bias towards text. Finally, we show that prompt-based ASR with speech LLMs performs poorly on unseen languages, motivating a simple ASR system that combines a stronger acoustic model with a speech LLM via MICL-based selection of acoustic hypotheses. Results show that MICL consistently improves ASR performance, and that cross-lingual transfer learning matches or outperforms corpus-trained language models without using target-language data. Our code is publicly available.",
      "publishedDate": "2026-01-09T10:52:23Z",
      "updatedDate": "2026-01-09T10:52:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05707v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05707",
      "comment": "Under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05991",
      "title": "Open-Vocabulary 3D Instruction Ambiguity Detection",
      "authors": [
        {
          "name": "Jiayu Ding",
          "affiliation": null
        },
        {
          "name": "Haoran Tang",
          "affiliation": null
        },
        {
          "name": "Ge Li",
          "affiliation": null
        }
      ],
      "abstract": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.",
      "publishedDate": "2026-01-09T18:17:11Z",
      "updatedDate": "2026-01-09T18:17:11Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05991v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05991",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "robotics",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05905",
      "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
      "authors": [
        {
          "name": "Haoming Xu",
          "affiliation": null
        },
        {
          "name": "Ningyuan Zhao",
          "affiliation": null
        },
        {
          "name": "Yunzhi Yao",
          "affiliation": null
        },
        {
          "name": "Weihong Xu",
          "affiliation": null
        },
        {
          "name": "Hongru Wang",
          "affiliation": null
        },
        {
          "name": "Xinle Deng",
          "affiliation": null
        },
        {
          "name": "Shumin Deng",
          "affiliation": null
        },
        {
          "name": "Jeff Z. Pan",
          "affiliation": null
        },
        {
          "name": "Huajun Chen",
          "affiliation": null
        },
        {
          "name": "Ningyu Zhang",
          "affiliation": null
        }
      ],
      "abstract": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
      "publishedDate": "2026-01-09T16:23:21Z",
      "updatedDate": "2026-01-09T16:23:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05905v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05905",
      "comment": "Work in progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "tool-use",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05903",
      "title": "HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search",
      "authors": [
        {
          "name": "Zihang Tian",
          "affiliation": null
        },
        {
          "name": "Rui Li",
          "affiliation": null
        },
        {
          "name": "Jingsen Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaohe Bo",
          "affiliation": null
        },
        {
          "name": "Wei Huo",
          "affiliation": null
        },
        {
          "name": "Xu Chen",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.",
      "publishedDate": "2026-01-09T16:22:25Z",
      "updatedDate": "2026-01-09T16:22:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05903v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05903",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05874",
      "title": "Continual-learning for Modelling Low-Resource Languages from Large Language Models",
      "authors": [
        {
          "name": "Santosh Srinath K",
          "affiliation": null
        },
        {
          "name": "Mudit Somani",
          "affiliation": null
        },
        {
          "name": "Varun Reddy Padala",
          "affiliation": null
        },
        {
          "name": "Prajna Devi Upadhyay",
          "affiliation": null
        },
        {
          "name": "Abhijit Das",
          "affiliation": null
        }
      ],
      "abstract": "Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.",
      "publishedDate": "2026-01-09T15:51:12Z",
      "updatedDate": "2026-01-09T15:51:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05874v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05874",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05858",
      "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning",
      "authors": [
        {
          "name": "Alexandra Dragomir",
          "affiliation": null
        },
        {
          "name": "Florin Brad",
          "affiliation": null
        },
        {
          "name": "Radu Tudor Ionescu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.",
      "publishedDate": "2026-01-09T15:34:31Z",
      "updatedDate": "2026-01-09T15:34:31Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05858v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05858",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05787",
      "title": "From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation",
      "authors": [
        {
          "name": "Zezhou Wang",
          "affiliation": null
        },
        {
          "name": "Ziyun Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaoyi Zhang",
          "affiliation": null
        },
        {
          "name": "Zhuzhong Qian",
          "affiliation": null
        },
        {
          "name": "Yan Lu",
          "affiliation": null
        }
      ],
      "abstract": "Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git",
      "publishedDate": "2026-01-09T13:26:38Z",
      "updatedDate": "2026-01-09T13:26:38Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05787v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05787",
      "comment": "Work In Progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "agents",
        "planning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05777",
      "title": "EET: Experience-Driven Early Termination for Cost-Efficient Software Engineering Agents",
      "authors": [
        {
          "name": "Yaoqi Guo",
          "affiliation": null
        },
        {
          "name": "Ying Xiao",
          "affiliation": null
        },
        {
          "name": "Jie M. Zhang",
          "affiliation": null
        },
        {
          "name": "Mark Harman",
          "affiliation": null
        },
        {
          "name": "Yiling Lou",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        },
        {
          "name": "Zhenpeng Chen",
          "affiliation": null
        }
      ],
      "abstract": "Software engineering (SE) agents powered by large language models are increasingly adopted in practice, yet they often incur substantial monetary cost. We introduce EET, an experience-driven early termination approach that reduces the cost of SE agents while preserving task performance. EET extracts structured experience from prior issue-resolution executions and leverages it to guide early termination during patch generation and selection, reducing unproductive iterations. We evaluate EET on the SWE-bench Verified benchmark across three representative SE agents. EET consistently reduces total cost by 19%-55% (32% on average), with negligible loss in resolution rate (at most 0.2%). These efficiency gains are achieved, on average, by identifying early-termination opportunities for 11% of issues and reducing API calls, input tokens, and output tokens by 21%, 30%, and 25%, respectively. We release the code, prompts, and data at https://github.com/EffiSEAgent/EET.",
      "publishedDate": "2026-01-09T13:01:49Z",
      "updatedDate": "2026-01-09T13:01:49Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05777v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05777",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "agents",
        "tool-use",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "tool-use",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05635",
      "title": "Continual Pretraining on Encrypted Synthetic Data for Privacy-Preserving LLMs",
      "authors": [
        {
          "name": "Honghao Liu",
          "affiliation": null
        },
        {
          "name": "Xuhui Jiang",
          "affiliation": null
        },
        {
          "name": "Chengjin Xu",
          "affiliation": null
        },
        {
          "name": "Cehao Yang",
          "affiliation": null
        },
        {
          "name": "Yiran Cheng",
          "affiliation": null
        },
        {
          "name": "Lionel Ni",
          "affiliation": null
        },
        {
          "name": "Jian Guo",
          "affiliation": null
        }
      ],
      "abstract": "Preserving privacy in sensitive data while pretraining large language models on small, domain-specific corpora presents a significant challenge. In this work, we take an exploratory step toward privacy-preserving continual pretraining by proposing an entity-based framework that synthesizes encrypted training data to protect personally identifiable information (PII). Our approach constructs a weighted entity graph to guide data synthesis and applies deterministic encryption to PII entities, enabling LLMs to encode new knowledge through continual pretraining while granting authorized access to sensitive data through decryption keys. Our results on limited-scale datasets demonstrate that our pretrained models outperform base models and ensure PII security, while exhibiting a modest performance gap compared to models trained on unencrypted synthetic data. We further show that increasing the number of entities and leveraging graph-based synthesis improves model performance, and that encrypted models retain instruction-following capabilities with long retrieved contexts. We discuss the security implications and limitations of deterministic encryption, positioning this work as an initial investigation into the design space of encrypted data pretraining for privacy-preserving LLMs. Our code is available at https://github.com/DataArcTech/SoE.",
      "publishedDate": "2026-01-09T08:44:07Z",
      "updatedDate": "2026-01-12T04:33:16Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05635v2",
      "arxivUrl": "https://arxiv.org/abs/2601.05635",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05587",
      "title": "HogVul: Black-box Adversarial Code Generation Framework Against LM-based Vulnerability Detectors",
      "authors": [
        {
          "name": "Jingxiao Yang",
          "affiliation": null
        },
        {
          "name": "Ping He",
          "affiliation": null
        },
        {
          "name": "Tianyu Du",
          "affiliation": null
        },
        {
          "name": "Sun Bing",
          "affiliation": null
        },
        {
          "name": "Xuhong Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in software vulnerability detection have been driven by Language Model (LM)-based approaches. However, these models remain vulnerable to adversarial attacks that exploit lexical and syntax perturbations, allowing critical flaws to evade detection. Existing black-box attacks on LM-based vulnerability detectors primarily rely on isolated perturbation strategies, limiting their ability to efficiently explore the adversarial code space for optimal perturbations. To bridge this gap, we propose HogVul, a black-box adversarial code generation framework that integrates both lexical and syntax perturbations under a unified dual-channel optimization strategy driven by Particle Swarm Optimization (PSO). By systematically coordinating two-level perturbations, HogVul effectively expands the search space for adversarial examples, enhancing the attack efficacy. Extensive experiments on four benchmark datasets demonstrate that HogVul achieves an average attack success rate improvement of 26.05\\% over state-of-the-art baseline methods. These findings highlight the potential of hybrid optimization strategies in exposing model vulnerabilities.",
      "publishedDate": "2026-01-09T07:14:29Z",
      "updatedDate": "2026-01-09T07:14:29Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05587v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05587",
      "comment": "AAAI26",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05547",
      "title": "VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck",
      "authors": [
        {
          "name": "Feiran Zhang",
          "affiliation": null
        },
        {
          "name": "Yixin Wu",
          "affiliation": null
        },
        {
          "name": "Zhenghua Wang",
          "affiliation": null
        },
        {
          "name": "Xiaohua Wang",
          "affiliation": null
        },
        {
          "name": "Changze Lv",
          "affiliation": null
        },
        {
          "name": "Xuanjing Huang",
          "affiliation": null
        },
        {
          "name": "Xiaoqing Zheng",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal tasks, but remain susceptible to hallucinations, where generated text deviates from the underlying visual content. Existing hallucination detection methods primarily rely on output logits or external verification tools, often overlooking their internal mechanisms. In this work, we investigate the outputs of internal attention heads, postulating that specific heads carry the primary signals for truthful generation.However, directly probing these high-dimensional states is challenging due to the entanglement of visual-linguistic syntax and noise. To address this, we propose VIB-Probe, a novel hallucination detection and mitigation framework leveraging the Variational Information Bottleneck (VIB) theory. Our method extracts discriminative patterns across layers and heads while filtering out semantic nuisances through the information bottleneck principle. Furthermore, by leveraging the gradients of our VIB probe, we identify attention heads with strong causal influence on hallucinations and introduce an inference-time intervention strategy for hallucination mitigation. Extensive experiments across diverse benchmarks demonstrate that VIB-Probe significantly outperforms existing baselines in both settings. Our code will be made publicly available.",
      "publishedDate": "2026-01-09T05:58:22Z",
      "updatedDate": "2026-01-09T05:58:22Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05547v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05547",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05508",
      "title": "Enabling Stroke-Level Structural Analysis of Hieroglyphic Scripts without Language-Specific Priors",
      "authors": [
        {
          "name": "Fuwen Luo",
          "affiliation": null
        },
        {
          "name": "Zihao Wan",
          "affiliation": null
        },
        {
          "name": "Ziyue Wang",
          "affiliation": null
        },
        {
          "name": "Yaluo Liu",
          "affiliation": null
        },
        {
          "name": "Pau Tong Lin Xu",
          "affiliation": null
        },
        {
          "name": "Xuanjia Qiao",
          "affiliation": null
        },
        {
          "name": "Xiaolong Wang",
          "affiliation": null
        },
        {
          "name": "Peng Li",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Hieroglyphs, as logographic writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and Multimodal LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to model the underlying logic of character strokes. Furthermore, existing structural analysis methods are often script-specific and labor-intensive. In this paper, we propose Hieroglyphic Stroke Analyzer (HieroSA), a novel and generalizable framework that enables MLLMs to automatically derive stroke-level structures from character bitmaps without handcrafted data. It transforms modern logographic and ancient hieroglyphs character images into explicit, interpretable line-segment representations in a normalized coordinate space, allowing for cross-lingual generalization. Extensive experiments demonstrate that HieroSA effectively captures character-internal structures and semantics, bypassing the need for language-specific priors. Experimental results highlight the potential of our work as a graphematics analysis tool for a deeper understanding of hieroglyphic scripts. View our code at https://github.com/THUNLP-MT/HieroSA.",
      "publishedDate": "2026-01-09T03:30:12Z",
      "updatedDate": "2026-01-09T03:30:12Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05508v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05508",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05475",
      "title": "MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization",
      "authors": [
        {
          "name": "Jiefu Ou",
          "affiliation": null
        },
        {
          "name": "Sapana Chaudhary",
          "affiliation": null
        },
        {
          "name": "Kaj Bostrom",
          "affiliation": null
        },
        {
          "name": "Nathaniel Weir",
          "affiliation": null
        },
        {
          "name": "Shuai Zhang",
          "affiliation": null
        },
        {
          "name": "Huzefa Rangwala",
          "affiliation": null
        },
        {
          "name": "George Karypis",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.",
      "publishedDate": "2026-01-09T02:21:28Z",
      "updatedDate": "2026-01-09T02:21:28Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05475v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05475",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05344",
      "title": "Coding the Visual World: From Image to Simulation Using Vision Language Models",
      "authors": [
        {
          "name": "Sagi Eppel",
          "affiliation": null
        }
      ],
      "abstract": "The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.",
      "publishedDate": "2026-01-08T19:49:05Z",
      "updatedDate": "2026-01-08T19:49:05Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05344v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05344",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05280",
      "title": "On the Limits of Self-Improving in LLMs and Why AGI, ASI and the Singularity Are Not Near Without Symbolic Model Synthesis",
      "authors": [
        {
          "name": "Hector Zenil",
          "affiliation": null
        }
      ],
      "abstract": "We formalise recursive self-training in Large Language Models (LLMs) and Generative AI as a discrete-time dynamical system and prove that, as training data become increasingly self-generated ($α_t \\to 0$), the system undergoes inevitably degenerative dynamics. We derive two fundamental failure modes: (1) Entropy Decay, where finite sampling effects cause a monotonic loss of distributional diversity (mode collapse), and (2) Variance Amplification, where the loss of external grounding causes the model's representation of truth to drift as a random walk, bounded only by the support diameter. We show these behaviours are not contingent on architecture but are consequences of distributional learning on finite samples. We further argue that Reinforcement Learning with imperfect verifiers suffers similar semantic collapse. To overcome these limits, we propose a path involving symbolic regression and program synthesis guided by Algorithmic Probability. The Coding Theorem Method (CTM) allows for identifying generative mechanisms rather than mere correlations, escaping the data-processing inequality that binds standard statistical learning. We conclude that while purely distributional learning leads to model collapse, hybrid neurosymbolic approaches offer a coherent framework for sustained self-improvement.",
      "publishedDate": "2026-01-05T19:50:49Z",
      "updatedDate": "2026-01-05T19:50:49Z",
      "primaryCategory": "cs.IT",
      "arxivCategories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05280v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05280",
      "comment": "26 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "code-generation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05792",
      "title": "Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning",
      "authors": [
        {
          "name": "Manel Gil-Sorribes",
          "affiliation": null
        },
        {
          "name": "Júlia Vilalta-Mor",
          "affiliation": null
        },
        {
          "name": "Isaac Filella-Mercè",
          "affiliation": null
        },
        {
          "name": "Robert Soliva",
          "affiliation": null
        },
        {
          "name": "Álvaro Ciudad",
          "affiliation": null
        },
        {
          "name": "Víctor Guallar",
          "affiliation": null
        },
        {
          "name": "Alexis Molina",
          "affiliation": null
        }
      ],
      "abstract": "Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.",
      "publishedDate": "2026-01-09T13:39:49Z",
      "updatedDate": "2026-01-09T13:39:49Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05792v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05792",
      "comment": "Accepted at the Generative and Experimental Perspectives for Biomolecular Design Workshop at ICLR 2025 and at the Learning Meaningful Representations of Life Workshop at ICLR 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.05353",
      "title": "GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting",
      "authors": [
        {
          "name": "Shovito Barua Soumma",
          "affiliation": null
        },
        {
          "name": "Hassan Ghasemzadeh",
          "affiliation": null
        }
      ],
      "abstract": "Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a multimodal transformer architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.",
      "publishedDate": "2026-01-08T20:07:59Z",
      "updatedDate": "2026-01-08T20:07:59Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.IT"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.05353v1",
      "arxivUrl": "https://arxiv.org/abs/2601.05353",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-12T03:44:00.043Z",
      "categories": [
        "rag",
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07782",
      "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
      "authors": [
        {
          "name": "Wei Fang",
          "affiliation": null
        },
        {
          "name": "James Glass",
          "affiliation": null
        }
      ],
      "abstract": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
      "publishedDate": "2026-01-12T17:58:39Z",
      "updatedDate": "2026-01-12T17:58:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07782v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07782",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "prompting",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07504",
      "title": "FROAV: A Framework for RAG Observation and Agent Verification - Lowering the Barrier to LLM Agent Research",
      "authors": [
        {
          "name": "Tzu-Hsuan Lin",
          "affiliation": null
        },
        {
          "name": "Chih-Hsuan Kao",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) and their integration into autonomous agent systems has created unprecedented opportunities for document analysis, decision support, and knowledge retrieval. However, the complexity of developing, evaluating, and iterating on LLM-based agent workflows presents significant barriers to researchers, particularly those without extensive software engineering expertise. We present FROAV (Framework for RAG Observation and Agent Verification), an open-source research platform that democratizes LLM agent research by providing a plug-and-play architecture combining visual workflow orchestration, a comprehensive evaluation framework, and extensible Python integration. FROAV implements a multi-stage Retrieval-Augmented Generation (RAG) pipeline coupled with a rigorous \"LLM-as-a-Judge\" evaluation system, all accessible through intuitive graphical interfaces. Our framework integrates n8n for no-code workflow design, PostgreSQL for granular data management, FastAPI for flexible backend logic, and Streamlit for human-in-the-loop interaction. Through this integrated ecosystem, researchers can rapidly prototype RAG strategies, conduct prompt engineering experiments, validate agent performance against human judgments, and collect structured feedback-all without writing infrastructure code. We demonstrate the framework's utility through its application to financial document analysis, while emphasizing its material-agnostic architecture that adapts to any domain requiring semantic analysis. FROAV represents a significant step toward making LLM agent research accessible to a broader scientific community, enabling researchers to focus on hypothesis testing and algorithmic innovation rather than system integration challenges.",
      "publishedDate": "2026-01-12T13:02:32Z",
      "updatedDate": "2026-01-12T13:02:32Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07504v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07504",
      "comment": "8 pages, 1 figure, 3 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "code-generation",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "prompting",
          "code-generation",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07470",
      "title": "Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory",
      "authors": [
        {
          "name": "Sirui Liang",
          "affiliation": null
        },
        {
          "name": "Pengfei Cao",
          "affiliation": null
        },
        {
          "name": "Jian Zhao",
          "affiliation": null
        },
        {
          "name": "Wenhao Teng",
          "affiliation": null
        },
        {
          "name": "Xiangwen Liao",
          "affiliation": null
        },
        {
          "name": "Jun Zhao",
          "affiliation": null
        },
        {
          "name": "Kang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.",
      "publishedDate": "2026-01-12T12:26:02Z",
      "updatedDate": "2026-01-12T12:26:02Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07470v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07470",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents"
      ],
      "tags": {
        "auto": [
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07468",
      "title": "Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents",
      "authors": [
        {
          "name": "Miao Su",
          "affiliation": null
        },
        {
          "name": "Yucan Guo",
          "affiliation": null
        },
        {
          "name": "Zhongni Hou",
          "affiliation": null
        },
        {
          "name": "Long Bai",
          "affiliation": null
        },
        {
          "name": "Zixuan Li",
          "affiliation": null
        },
        {
          "name": "Yufei Zhang",
          "affiliation": null
        },
        {
          "name": "Guojun Yin",
          "affiliation": null
        },
        {
          "name": "Wei Lin",
          "affiliation": null
        },
        {
          "name": "Xiaolong Jin",
          "affiliation": null
        },
        {
          "name": "Jiafeng Guo",
          "affiliation": null
        },
        {
          "name": "Xueqi Cheng",
          "affiliation": null
        }
      ],
      "abstract": "Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.",
      "publishedDate": "2026-01-12T12:24:44Z",
      "updatedDate": "2026-01-12T12:24:44Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07468v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07468",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07395",
      "title": "MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP",
      "authors": [
        {
          "name": "Ruiqi Li",
          "affiliation": null
        },
        {
          "name": "Zhiqiang Wang",
          "affiliation": null
        },
        {
          "name": "Yunhao Yao",
          "affiliation": null
        },
        {
          "name": "Xiang-Yang Li",
          "affiliation": null
        }
      ],
      "abstract": "To standardize interactions between LLM-based agents and their environments, the Model Context Protocol (MCP) was proposed and has since been widely adopted. However, integrating external tools expands the attack surface, exposing agents to tool poisoning attacks. In such attacks, malicious instructions embedded in tool metadata are injected into the agent context during MCP registration phase, thereby manipulating agent behavior. Prior work primarily focuses on explicit tool poisoning or relied on manually crafted poisoned tools. In contrast, we focus on a particularly stealthy variant: implicit tool poisoning, where the poisoned tool itself remains uninvoked. Instead, the instructions embedded in the tool metadata induce the agent to invoke a legitimate but high-privilege tool to perform malicious operations. We propose MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. MCP-ITP formulates poisoned tool generation as a black-box optimization problem and employs an iterative optimization strategy that leverages feedback from both an evaluation LLM and a detection LLM to maximize Attack Success Rate (ASR) while evading current detection mechanisms. Experimental results on the MCPTox dataset across 12 LLM agents demonstrate that MCP-ITP consistently outperforms the manually crafted baseline, achieving up to 84.2% ASR while suppressing the Malicious Tool Detection Rate (MDR) to as low as 0.3%.",
      "publishedDate": "2026-01-12T10:28:46Z",
      "updatedDate": "2026-01-12T10:28:46Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07395v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07395",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07376",
      "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
      "authors": [
        {
          "name": "Siqi Zhu",
          "affiliation": null
        },
        {
          "name": "Jiaxuan You",
          "affiliation": null
        }
      ],
      "abstract": "We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.",
      "publishedDate": "2026-01-12T09:57:46Z",
      "updatedDate": "2026-01-12T09:57:46Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.DC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07376v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07376",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07309",
      "title": "ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging",
      "authors": [
        {
          "name": "Zhuoka Feng",
          "affiliation": null
        },
        {
          "name": "Kang Chen",
          "affiliation": null
        },
        {
          "name": "Sihan Zhao",
          "affiliation": null
        },
        {
          "name": "Kai Xiong",
          "affiliation": null
        },
        {
          "name": "Yaoning Wang",
          "affiliation": null
        },
        {
          "name": "Minshen Yu",
          "affiliation": null
        },
        {
          "name": "Junjie Nian",
          "affiliation": null
        },
        {
          "name": "Changyi Xiao",
          "affiliation": null
        },
        {
          "name": "Yixin Cao",
          "affiliation": null
        },
        {
          "name": "Yugang Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.",
      "publishedDate": "2026-01-12T08:31:53Z",
      "updatedDate": "2026-01-12T08:31:53Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07309v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07309",
      "comment": "17 pages, 12 figures. Project page: https://arkazhuo.github.io/ARM-homepage/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07224",
      "title": "Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration",
      "authors": [
        {
          "name": "Yang Zhao",
          "affiliation": null
        },
        {
          "name": "Yangou Ouyang",
          "affiliation": null
        },
        {
          "name": "Xiao Ding",
          "affiliation": null
        },
        {
          "name": "Hepeng Wang",
          "affiliation": null
        },
        {
          "name": "Bibo Cai",
          "affiliation": null
        },
        {
          "name": "Kai Xiong",
          "affiliation": null
        },
        {
          "name": "Jinglong Gao",
          "affiliation": null
        },
        {
          "name": "Zhouhao Sun",
          "affiliation": null
        },
        {
          "name": "Li Du",
          "affiliation": null
        },
        {
          "name": "Bing Qin",
          "affiliation": null
        },
        {
          "name": "Ting Liu",
          "affiliation": null
        }
      ],
      "abstract": "While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.",
      "publishedDate": "2026-01-12T05:43:20Z",
      "updatedDate": "2026-01-12T05:43:20Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07224v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07224",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07190",
      "title": "Active Context Compression: Autonomous Memory Management in LLM Agents",
      "authors": [
        {
          "name": "Nikhil Verma",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.",
      "publishedDate": "2026-01-12T04:31:00Z",
      "updatedDate": "2026-01-12T04:31:00Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07190v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07190",
      "comment": "8 pages, 2 figures, 2 tables. IEEE conference format",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "prompting",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07122",
      "title": "Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework",
      "authors": [
        {
          "name": "Yixiao Peng",
          "affiliation": null
        },
        {
          "name": "Hao Hu",
          "affiliation": null
        },
        {
          "name": "Feiyang Li",
          "affiliation": null
        },
        {
          "name": "Xinye Cao",
          "affiliation": null
        },
        {
          "name": "Yingchang Jiang",
          "affiliation": null
        },
        {
          "name": "Jipeng Tang",
          "affiliation": null
        },
        {
          "name": "Guoshun Nan",
          "affiliation": null
        },
        {
          "name": "Yuling Liu",
          "affiliation": null
        }
      ],
      "abstract": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&CK's Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.",
      "publishedDate": "2026-01-12T01:25:41Z",
      "updatedDate": "2026-01-12T01:25:41Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07122v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07122",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "tool-use",
        "planning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "planning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06838",
      "title": "CHASE: LLM Agents for Dissecting Malicious PyPI Packages",
      "authors": [
        {
          "name": "Takaaki Toda",
          "affiliation": null
        },
        {
          "name": "Tatsuya Mori",
          "affiliation": null
        }
      ],
      "abstract": "Modern software package registries like PyPI have become critical infrastructure for software development, but are increasingly exploited by threat actors distributing malicious packages with sophisticated multi-stage attack chains. While Large Language Models (LLMs) offer promising capabilities for automated code analysis, their application to security-critical malware detection faces fundamental challenges, including hallucination and context confusion, which can lead to missed detections or false alarms. We present CHASE (Collaborative Hierarchical Agents for Security Exploration), a high-reliability multi-agent architecture that addresses these limitations through a Plan-and-Execute coordination model, specialized Worker Agents focused on specific analysis aspects, and integration with deterministic security tools for critical operations. Our key insight is that reliability in LLM-based security analysis emerges not from improving individual model capabilities but from architecting systems that compensate for LLM weaknesses while leveraging their semantic understanding strengths. Evaluation on a dataset of 3,000 packages (500 malicious, 2,500 benign) demonstrates that CHASE achieves 98.4% recall with only 0.08% false positive rate, while maintaining a practical median analysis time of 4.5 minutes per package, making it suitable for operational deployment in automated package screening. Furthermore, we conducted a survey with cybersecurity professionals to evaluate the generated analysis reports, identifying their key strengths and areas for improvement. This work provides a blueprint for building reliable AI-powered security tools that can scale with the growing complexity of modern software supply chains. Our project page is available at https://t0d4.github.io/CHASE-AIware25/",
      "publishedDate": "2026-01-11T10:06:14Z",
      "updatedDate": "2026-01-11T10:06:14Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06838v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06838",
      "comment": "Accepted for publication and presented at the 2nd IEEE International Conference on AI-powered Software (AIware 2025). 10 pages, 3 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent",
          "code-generation",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06794",
      "title": "No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning",
      "authors": [
        {
          "name": "Zhicong Li",
          "affiliation": null
        },
        {
          "name": "Lingjie Jiang",
          "affiliation": null
        },
        {
          "name": "Yulan Hu",
          "affiliation": null
        },
        {
          "name": "Xingchen Zeng",
          "affiliation": null
        },
        {
          "name": "Yixia Li",
          "affiliation": null
        },
        {
          "name": "Xiangwen Zhang",
          "affiliation": null
        },
        {
          "name": "Guanhua Chen",
          "affiliation": null
        },
        {
          "name": "Zheng Pan",
          "affiliation": null
        },
        {
          "name": "Xin Li",
          "affiliation": null
        },
        {
          "name": "Yong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.",
      "publishedDate": "2026-01-11T07:29:08Z",
      "updatedDate": "2026-01-11T07:29:08Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06794v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06794",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06606",
      "title": "CEDAR: Context Engineering for Agentic Data Science",
      "authors": [
        {
          "name": "Rishiraj Saha Roy",
          "affiliation": null
        },
        {
          "name": "Chris Hinze",
          "affiliation": null
        },
        {
          "name": "Luzian Hahn",
          "affiliation": null
        },
        {
          "name": "Fabian Kuech",
          "affiliation": null
        }
      ],
      "abstract": "We demonstrate CEDAR, an application for automating data science (DS) tasks with an agentic setup. Solving DS problems with LLMs is an underexplored area that has immense market value. The challenges are manifold: task complexities, data sizes, computational limitations, and context restrictions. We show that these can be alleviated via effective context engineering. We first impose structure into the initial prompt with DS-specific input fields, that serve as instructions for the agentic system. The solution is then materialized as an enumerated sequence of interleaved plan and code blocks generated by separate LLM agents, providing a readable structure to the context at any step of the workflow. Function calls for generating these intermediate texts, and for corresponding Python code, ensure that data stays local, and only aggregate statistics and associated instructions are injected into LLM prompts. Fault tolerance and context management are introduced via iterative code generation and smart history rendering. The viability of our agentic data scientist is demonstrated using canonical Kaggle challenges.",
      "publishedDate": "2026-01-10T16:05:04Z",
      "updatedDate": "2026-01-10T16:05:04Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06606v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06606",
      "comment": "Accepted at ECIR 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06487",
      "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
      "authors": [
        {
          "name": "Qiang Zhang",
          "affiliation": null
        },
        {
          "name": "Boli Chen",
          "affiliation": null
        },
        {
          "name": "Fanrui Zhang",
          "affiliation": null
        },
        {
          "name": "Ruixue Ding",
          "affiliation": null
        },
        {
          "name": "Shihang Wang",
          "affiliation": null
        },
        {
          "name": "Qiuchen Wang",
          "affiliation": null
        },
        {
          "name": "Yinfeng Huang",
          "affiliation": null
        },
        {
          "name": "Haonan Zhang",
          "affiliation": null
        },
        {
          "name": "Rongxiang Zhu",
          "affiliation": null
        },
        {
          "name": "Pengyong Wang",
          "affiliation": null
        },
        {
          "name": "Ailin Ren",
          "affiliation": null
        },
        {
          "name": "Xin Li",
          "affiliation": null
        },
        {
          "name": "Pengjun Xie",
          "affiliation": null
        },
        {
          "name": "Jiawei Liu",
          "affiliation": null
        },
        {
          "name": "Ning Guo",
          "affiliation": null
        },
        {
          "name": "Jingren Zhou",
          "affiliation": null
        },
        {
          "name": "Zheng-Jun Zha",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
      "publishedDate": "2026-01-10T08:43:07Z",
      "updatedDate": "2026-01-10T08:43:07Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06487v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06487",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "evaluation",
        "planning"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06407",
      "title": "Value of Information: A Framework for Human-Agent Communication",
      "authors": [
        {
          "name": "Yijiang River Dong",
          "affiliation": null
        },
        {
          "name": "Tiancheng Hu",
          "affiliation": null
        },
        {
          "name": "Zheng Hui",
          "affiliation": null
        },
        {
          "name": "Caiqi Zhang",
          "affiliation": null
        },
        {
          "name": "Ivan Vulić",
          "affiliation": null
        },
        {
          "name": "Andreea Bobu",
          "affiliation": null
        },
        {
          "name": "Nigel Collier",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.",
      "publishedDate": "2026-01-10T03:07:41Z",
      "updatedDate": "2026-01-10T03:07:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06407v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06407",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06328",
      "title": "ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation",
      "authors": [
        {
          "name": "Ziqiao Xi",
          "affiliation": null
        },
        {
          "name": "Shuang Liang",
          "affiliation": null
        },
        {
          "name": "Qi Liu",
          "affiliation": null
        },
        {
          "name": "Jiaqing Zhang",
          "affiliation": null
        },
        {
          "name": "Letian Peng",
          "affiliation": null
        },
        {
          "name": "Fang Nan",
          "affiliation": null
        },
        {
          "name": "Meshal Nayim",
          "affiliation": null
        },
        {
          "name": "Tianhui Zhang",
          "affiliation": null
        },
        {
          "name": "Rishika Mundada",
          "affiliation": null
        },
        {
          "name": "Lianhui Qin",
          "affiliation": null
        },
        {
          "name": "Biwei Huang",
          "affiliation": null
        },
        {
          "name": "Kun Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Tool-using LLM agents still struggle in open-world settings with large tool pools, long-horizon objectives, wild constraints, and unreliable tool states. For scalable and realistic training and testing, we introduce an open-world tool-using environment, built on 5,571 format unified tools across 204 commonly used apps. It includes a task creation engine that synthesizes long-horizon, multi-tool workflows with wild constraints, and a state controller that injects interruptions and failures to stress-test robustness. On top of this environment, we develop a tool select-then-execute agent framework with a planner-actor decomposition to separate deliberate reasoning and self-correction from step-wise execution. Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Our code and data will be publicly released.",
      "publishedDate": "2026-01-09T21:59:31Z",
      "updatedDate": "2026-01-09T21:59:31Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06328v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06328",
      "comment": "Submitted to ACL 2026 12 pages, 4 figures Ziqiao Xi and Shuang Liang contributed equally to this work",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning",
          "planning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06301",
      "title": "Beyond BeautifulSoup: Benchmarking LLM-Powered Web Scraping for Everyday Users",
      "authors": [
        {
          "name": "Arth Bhardwaj",
          "affiliation": null
        },
        {
          "name": "Nirav Diwan",
          "affiliation": null
        },
        {
          "name": "Gang Wang",
          "affiliation": null
        }
      ],
      "abstract": "Web scraping has historically required technical expertise in HTML parsing, session management, and authentication circumvention, which limited large-scale data extraction to skilled developers. We argue that large language models (LLMs) have democratized web scraping, enabling low-skill users to execute sophisticated operations through simple natural language prompts. While extensive benchmarks evaluate these tools under optimal expert conditions, we show that without extensive manual effort, current LLM-based workflows allow novice users to scrape complex websites that would otherwise be inaccessible. We systematically benchmark what everyday users can do with off-the-shelf LLM tools across 35 sites spanning five security tiers, including authentication, anti-bot, and CAPTCHA controls. We devise and evaluate two distinct workflows: (a) LLM-assisted scripting, where users prompt LLMs to generate traditional scraping code but maintain manual execution control, and (b) end-to-end LLM agents, which autonomously navigate and extract data through integrated tool use. Our results demonstrate that end-to-end agents have made complex scraping accessible - requiring as little as a single prompt with minimal refinement (less than 5 changes) to complete workflows. We also highlight scenarios where LLM-assisted scripting may be simpler and faster for static sites. In light of these findings, we provide simple procedures for novices to use these workflows and gauge what adversaries could achieve using these.",
      "publishedDate": "2026-01-09T20:34:28Z",
      "updatedDate": "2026-01-09T20:34:28Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06301v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06301",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "tool-use",
        "evaluation",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "evaluation",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06216",
      "title": "LLM Agents in Law: Taxonomy, Applications, and Challenges",
      "authors": [
        {
          "name": "Shuang Liu",
          "affiliation": null
        },
        {
          "name": "Ruijia Zhang",
          "affiliation": null
        },
        {
          "name": "Ruoyun Ma",
          "affiliation": null
        },
        {
          "name": "Yujia Deng",
          "affiliation": null
        },
        {
          "name": "Lanyi Zhu",
          "affiliation": null
        },
        {
          "name": "Jiayu Li",
          "affiliation": null
        },
        {
          "name": "Zelong Li",
          "affiliation": null
        },
        {
          "name": "Zhibin Shen",
          "affiliation": null
        },
        {
          "name": "Mengnan Du",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Recently, LLM agents have attracted significant attention as a solution to these challenges, utilizing advanced capabilities such as planning, memory, and tool usage to meet the rigorous standards of legal practice. In this paper, we present a comprehensive survey of LLM agents for legal tasks, analyzing how these architectures bridge the gap between technical capabilities and domain-specific needs. Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants.",
      "publishedDate": "2026-01-08T21:04:35Z",
      "updatedDate": "2026-01-08T21:04:35Z",
      "primaryCategory": "cs.CY",
      "arxivCategories": [
        "cs.CY",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06216v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06216",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "tool-use",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07264",
      "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
      "authors": [
        {
          "name": "Weihao Xuan",
          "affiliation": null
        },
        {
          "name": "Qingcheng Zeng",
          "affiliation": null
        },
        {
          "name": "Heli Qi",
          "affiliation": null
        },
        {
          "name": "Yunze Xiao",
          "affiliation": null
        },
        {
          "name": "Junjue Wang",
          "affiliation": null
        },
        {
          "name": "Naoto Yokoya",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
      "publishedDate": "2026-01-12T07:10:35Z",
      "updatedDate": "2026-01-12T07:10:35Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07264v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07264",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07262",
      "title": "ColorBrowserAgent: An Intelligent GUI Agent for Complex Long-Horizon Web Automation",
      "authors": [
        {
          "name": "Jiamu Zhou",
          "affiliation": null
        },
        {
          "name": "Jihong Wang",
          "affiliation": null
        },
        {
          "name": "Weiming Zhang",
          "affiliation": null
        },
        {
          "name": "Weiwen Liu",
          "affiliation": null
        },
        {
          "name": "Zhuosheng Zhang",
          "affiliation": null
        },
        {
          "name": "Xingyu Lou",
          "affiliation": null
        },
        {
          "name": "Weinan Zhang",
          "affiliation": null
        },
        {
          "name": "Huarong Deng",
          "affiliation": null
        },
        {
          "name": "Jun Wang",
          "affiliation": null
        }
      ],
      "abstract": "The web browser serves as a primary interface for daily human activities, making its automation a critical frontier for Human-Centred AI. While Large Language Models (LLMs) have enabled autonomous agents to interact with web GUIs, their reliability in real-world scenarios is hampered by long-horizon instability and the vast heterogeneity of site designs. In this paper, we introduce ColorBrowserAgent, a framework designed for Collaborative Autonomy in complex web tasks. Our approach integrates two human-centred mechanisms: (1) Progressive Progress Summarization, which mimics human short-term memory to maintain coherence over extended interactions; and (2) Human-in-the-Loop Knowledge Adaptation, which bridges the knowledge gap in diverse environments by soliciting expert intervention only when necessary. This symbiotic design allows the agent to learn from human tips without extensive retraining, effectively combining the scalability of AI with the adaptability of human cognition. Evaluated on the WebArena benchmark using GPT-5, ColorBrowserAgent achieves a state-of-the-art success rate of 71.2\\%, demonstrating the efficacy of interactive human assistance in robust web automation.",
      "publishedDate": "2026-01-12T07:08:42Z",
      "updatedDate": "2026-01-12T07:08:42Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07262v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07262",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06411",
      "title": "Structured Episodic Event Memory",
      "authors": [
        {
          "name": "Zhengxuan Lu",
          "affiliation": null
        },
        {
          "name": "Dongfang Li",
          "affiliation": null
        },
        {
          "name": "Yukun Shi",
          "affiliation": null
        },
        {
          "name": "Beilun Wang",
          "affiliation": null
        },
        {
          "name": "Longyue Wang",
          "affiliation": null
        },
        {
          "name": "Baotian Hu",
          "affiliation": null
        }
      ],
      "abstract": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
      "publishedDate": "2026-01-10T03:17:25Z",
      "updatedDate": "2026-01-10T03:17:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06411v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06411",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07812",
      "title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
      "authors": [
        {
          "name": "Anurag Das",
          "affiliation": null
        },
        {
          "name": "Adrian Bulat",
          "affiliation": null
        },
        {
          "name": "Alberto Baldrati",
          "affiliation": null
        },
        {
          "name": "Ioannis Maniadis Metaxas",
          "affiliation": null
        },
        {
          "name": "Bernt Schiele",
          "affiliation": null
        },
        {
          "name": "Georgios Tzimiropoulos",
          "affiliation": null
        },
        {
          "name": "Brais Martinez",
          "affiliation": null
        }
      ],
      "abstract": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.",
      "publishedDate": "2026-01-12T18:45:13Z",
      "updatedDate": "2026-01-12T18:45:13Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07812v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07812",
      "comment": "19 pages, 16 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07794",
      "title": "Kinship Data Benchmark for Multi-hop Reasoning",
      "authors": [
        {
          "name": "Tianda Sun",
          "affiliation": null
        },
        {
          "name": "Dimitar Kazakov",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly evaluated on their ability to perform multi-hop reasoning, i.e., to combine multiple pieces of information into a coherent inference. We introduce KinshipQA, a benchmark designed to probe this capability through reasoning over kinship relations. The central contribution of our work is a generative pipeline that produces, on demand, large-scale, realistic, and culture-specific genealogical data: collections of interconnected family trees that satisfy explicit marriage constraints associated with different kinship systems. This allows task difficulty, cultural assumptions, and relational depth to be systematically controlled and varied. From these genealogies, we derive textual inference tasks that require reasoning over implicit relational chains. We evaluate the resulting benchmark using six state-of-the-art LLMs, spanning both open-source and closed-source models, under a uniform zero-shot protocol with deterministic decoding. Performance is measured using exact-match and set-based metrics. Our results demonstrate that KinshipQA yields a wide spread of outcomes and exposes systematic differences in multi-hop reasoning across models and cultural settings.",
      "publishedDate": "2026-01-12T18:07:41Z",
      "updatedDate": "2026-01-12T18:07:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07794v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07794",
      "comment": "11 pages, 2 figures, 9 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07790",
      "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
      "authors": [
        {
          "name": "Yahya Masri",
          "affiliation": null
        },
        {
          "name": "Emily Ma",
          "affiliation": null
        },
        {
          "name": "Zifu Wang",
          "affiliation": null
        },
        {
          "name": "Joseph Rogers",
          "affiliation": null
        },
        {
          "name": "Chaowei Yang",
          "affiliation": null
        }
      ],
      "abstract": "System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.",
      "publishedDate": "2026-01-12T18:02:33Z",
      "updatedDate": "2026-01-12T18:02:33Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07790v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07790",
      "comment": "28 pages, 5 figures, 7 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07780",
      "title": "Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection",
      "authors": [
        {
          "name": "Mariana Costa",
          "affiliation": null
        },
        {
          "name": "Alberlucia Rafael Soarez",
          "affiliation": null
        },
        {
          "name": "Daniel Kim",
          "affiliation": null
        },
        {
          "name": "Camila Ferreira",
          "affiliation": null
        }
      ],
      "abstract": "While Chain-of-Thought (CoT) prompting advances LLM reasoning, challenges persist in consistency, accuracy, and self-correction, especially for complex or ethically sensitive tasks. Existing single-dimensional reflection methods offer insufficient improvements. We propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel methodology employing structured multi-perspective reflection. After initial CoT, PR-CoT guides the LLM to self-assess its reasoning across multiple predefined angles: logical consistency, information completeness, biases/ethics, and alternative solutions. Implemented purely via prompt engineering, this process refines the initial CoT into a more robust and accurate final answer without model retraining. Experiments across arithmetic, commonsense, ethical decision-making, and logical puzzles, using GPT-three point five and GPT-four models, demonstrate PR-CoT's superior performance. It significantly outperforms traditional CoT and existing reflection methods in logical consistency and error correction, with notable gains in nuanced domains like ethical decision-making. Ablation studies, human evaluations, and qualitative analyses further validate the contribution of each reflection perspective and the overall efficacy of our poly-reflective paradigm in fostering more reliable LLM reasoning.",
      "publishedDate": "2026-01-12T17:57:05Z",
      "updatedDate": "2026-01-12T17:57:05Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07780v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07780",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07767",
      "title": "Are LLM Decisions Faithful to Verbal Confidence?",
      "authors": [
        {
          "name": "Jiawei Wang",
          "affiliation": null
        },
        {
          "name": "Yanfei Zhou",
          "affiliation": null
        },
        {
          "name": "Siddartha Devic",
          "affiliation": null
        },
        {
          "name": "Deqing Fu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce $\\textbf{RiskEval}$: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties. Our evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions. Even when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse. This indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems, as current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions.",
      "publishedDate": "2026-01-12T17:49:51Z",
      "updatedDate": "2026-01-12T17:49:51Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07767v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07767",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07761",
      "title": "Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding",
      "authors": [
        {
          "name": "Yanxiang Huang",
          "affiliation": null
        },
        {
          "name": "Guohua Gao",
          "affiliation": null
        },
        {
          "name": "Zhaoyang Wei",
          "affiliation": null
        },
        {
          "name": "Jianyuan Ni",
          "affiliation": null
        }
      ],
      "abstract": "Large Vision-Language Models (LVLMs) face a fundamental dilemma in video reasoning: they are caught between the prohibitive computational costs of verbose reasoning and the hallucination risks of efficient, ungrounded approaches. To resolve this, we introduce the Chain of Evidence (CoE), a novel framework that architecturally decouples and co-optimizes perceptual grounding and reasoning efficiency. CoE incorporates two core innovations: (1) A lightweight Evidence Grounding Module (EGM) that acts as a query-guided filter, dynamically identifying and extracting a compact set of high-fidelity visual evidence; and (2) An Evidence-Anchoring Protocol optimized via Reinforcement Learning. Crucially, we design a composite reward mechanism that enforces process alignment, compelling the model to strictly reference identified temporal anchors during deduction, thereby mitigating hallucinations. To enable this, we construct CoE-Instruct, a large-scale dataset (164k samples) featuring a novel dual-annotation schema for separate perception and reasoning supervision. Extensive experiments on five benchmarks, including Video-MME, MVBench, and VSI-Bench, demonstrate that CoE-enhanced models establish a new state-of-the-art. They significantly outperform existing methods in accuracy, proving CoE to be a powerful and practical paradigm for reliable video understanding.",
      "publishedDate": "2026-01-12T17:46:10Z",
      "updatedDate": "2026-01-12T17:46:10Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07761v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07761",
      "comment": "6 pages",
      "journalRef": "ICME 2026",
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07754",
      "title": "Structure First, Reason Next: Enhancing a Large Language Model using Knowledge Graph for Numerical Reasoning in Financial Documents",
      "authors": [
        {
          "name": "Aryan Mishra",
          "affiliation": null
        },
        {
          "name": "Akash Anil",
          "affiliation": null
        }
      ],
      "abstract": "Numerical reasoning is an important task in the analysis of financial documents. It helps in understanding and performing numerical predictions with logical conclusions for the given query seeking answers from financial texts. Recently, Large Language Models (LLMs) have shown promising results in multiple Question-Answering (Q-A) systems with the capability of logical reasoning. As documents related to finance often consist of long and complex financial contexts, LLMs appear well-suited for building high-quality automated financial question-answering systems. However, LLMs often face challenges in accurately processing the various numbers within financial reports. Extracting numerical data from unstructured text and semi-structured tables, and reliably performing accurate calculations, remains a significant bottleneck for numerical reasoning in most state-of-the-art LLMs. Recent studies have shown that structured data augmentations, such as Knowledge Graphs (KGs), have notably improved the predictions of LLMs along with logical explanations. Thus, it is an important requirement to consider inherent structured information in financial reports while using LLMs for various financial analytics. This paper proposes a framework to incorporate structured information using KGs along with LLM predictions for numerical reasoning tasks. The KGs are extracted using a proposed schema inherently from the document under processing. We evaluated our proposed framework over the benchmark data FinQA, using an open-source LLM, namely Llama 3.1 8B Instruct. We observed that the proposed framework improved execution accuracy by approximately 12% relative to the vanilla LLM.",
      "publishedDate": "2026-01-12T17:39:08Z",
      "updatedDate": "2026-01-12T17:39:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07754v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07754",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07737",
      "title": "Evaluating the encoding competence of visual language models using uncommon actions",
      "authors": [
        {
          "name": "Chen Ling",
          "affiliation": null
        },
        {
          "name": "Nai Ding",
          "affiliation": null
        }
      ],
      "abstract": "We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility. To build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model's competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.",
      "publishedDate": "2026-01-12T17:15:45Z",
      "updatedDate": "2026-01-12T17:15:45Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07737v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07737",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "evaluation",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "agents",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07696",
      "title": "Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task",
      "authors": [
        {
          "name": "Nick Ferguson",
          "affiliation": null
        },
        {
          "name": "Alan Bundy",
          "affiliation": null
        },
        {
          "name": "Kwabena Nuamah",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) are increasingly focused on \"reasoning\" ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.) We design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years. Questions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions. To bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains 'essential actions' against which we can compare the tool call output of LLMs to infer the strength of reasoning ability. We find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding. We find that n-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs. Finally, we discuss the generalisation and limitation of our findings to other task domains.",
      "publishedDate": "2026-01-12T16:29:21Z",
      "updatedDate": "2026-01-12T16:29:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07696v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07696",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07695",
      "title": "Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model",
      "authors": [
        {
          "name": "Siwen Jiao",
          "affiliation": null
        },
        {
          "name": "Tianxiong Lv",
          "affiliation": null
        },
        {
          "name": "Kangan Qian",
          "affiliation": null
        },
        {
          "name": "Chenxu Zhao",
          "affiliation": null
        },
        {
          "name": "Xiuyuan Zhu",
          "affiliation": null
        },
        {
          "name": "Tianlun Li",
          "affiliation": null
        },
        {
          "name": "Xiaolong Cheng",
          "affiliation": null
        },
        {
          "name": "Jinyu Li",
          "affiliation": null
        },
        {
          "name": "Zhihao Liao",
          "affiliation": null
        },
        {
          "name": "Yang Cai",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes \"near-miss\" samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.",
      "publishedDate": "2026-01-12T16:26:42Z",
      "updatedDate": "2026-01-12T16:26:42Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07695v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07695",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07654",
      "title": "Towards Automating Blockchain Consensus Verification with IsabeLLM",
      "authors": [
        {
          "name": "Elliot Jones",
          "affiliation": null
        },
        {
          "name": "William Knottenbelt",
          "affiliation": null
        }
      ],
      "abstract": "Consensus protocols are crucial for a blockchain system as they are what allow agreement between the system's nodes in a potentially adversarial environment. For this reason, it is paramount to ensure their correct design and implementation to prevent such adversaries from carrying out malicious behaviour. Formal verification allows us to ensure the correctness of such protocols, but requires high levels of effort and expertise to carry out and thus is often omitted in the development process. In this paper, we present IsabeLLM, a tool that integrates the proof assistant Isabelle with a Large Language Model to assist and automate proofs. We demonstrate the effectiveness of IsabeLLM by using it to develop a novel model of Bitcoin's Proof of Work consensus protocol and verify its correctness. We use the DeepSeek R1 API for this demonstration and found that we were able to generate correct proofs for each of the non-trivial lemmas present in the verification.",
      "publishedDate": "2026-01-12T15:35:08Z",
      "updatedDate": "2026-01-12T15:35:08Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07654v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07654",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "tool-use"
      ],
      "tags": {
        "auto": [
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07645",
      "title": "PlaM: Training-Free Plateau-Guided Model Merging for Better Visual Grounding in MLLMs",
      "authors": [
        {
          "name": "Zijing Wang",
          "affiliation": null
        },
        {
          "name": "Yongkang Liu",
          "affiliation": null
        },
        {
          "name": "Mingyang Wang",
          "affiliation": null
        },
        {
          "name": "Ercong Nie",
          "affiliation": null
        },
        {
          "name": "Deyuan Chen",
          "affiliation": null
        },
        {
          "name": "Zhengjie Zhao",
          "affiliation": null
        },
        {
          "name": "Shi Feng",
          "affiliation": null
        },
        {
          "name": "Daling Wang",
          "affiliation": null
        },
        {
          "name": "Xiaocui Yang",
          "affiliation": null
        },
        {
          "name": "Yifei Zhang",
          "affiliation": null
        },
        {
          "name": "Hinrich Schütze",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) rely on strong linguistic reasoning inherited from their base language models. However, multimodal instruction fine-tuning paradoxically degrades this text's reasoning capability, undermining multimodal performance. To address this issue, we propose a training-free framework to mitigate this degradation. Through layer-wise vision token masking, we reveal a common three-stage pattern in multimodal large language models: early-modal separation, mid-modal alignment, and late-modal degradation. By analyzing the behavior of MLLMs at different stages, we propose a plateau-guided model merging method that selectively injects base language model parameters into MLLMs. Experimental results based on five MLLMs on nine benchmarks demonstrate the effectiveness of our method. Attention-based analysis further reveals that merging shifts attention from diffuse, scattered patterns to focused localization on task-relevant visual regions. Our repository is on https://github.com/wzj1718/PlaM.",
      "publishedDate": "2026-01-12T15:27:51Z",
      "updatedDate": "2026-01-12T15:27:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07645v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07645",
      "comment": "under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07632",
      "title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
      "authors": [
        {
          "name": "Zhankai Ye",
          "affiliation": null
        },
        {
          "name": "Bofan Li",
          "affiliation": null
        },
        {
          "name": "Yukai Jin",
          "affiliation": null
        },
        {
          "name": "Shuoqiu Li",
          "affiliation": null
        },
        {
          "name": "Wei Wang",
          "affiliation": null
        },
        {
          "name": "Yanfu Zhang",
          "affiliation": null
        },
        {
          "name": "Shangqian Gao",
          "affiliation": null
        },
        {
          "name": "Xin Liu",
          "affiliation": null
        }
      ],
      "abstract": "Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.",
      "publishedDate": "2026-01-12T15:14:29Z",
      "updatedDate": "2026-01-12T15:14:29Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07632v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07632",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07593",
      "title": "GRPO with State Mutations: Improving LLM-Based Hardware Test Plan Generation",
      "authors": [
        {
          "name": "Dimple Vijay Kochar",
          "affiliation": null
        },
        {
          "name": "Nathaniel Pinckney",
          "affiliation": null
        },
        {
          "name": "Guan-Ting Liu",
          "affiliation": null
        },
        {
          "name": "Chia-Tung Ho",
          "affiliation": null
        },
        {
          "name": "Chenhui Deng",
          "affiliation": null
        },
        {
          "name": "Haoxing Ren",
          "affiliation": null
        },
        {
          "name": "Brucek Khailany",
          "affiliation": null
        }
      ],
      "abstract": "RTL design often relies heavily on ad-hoc testbench creation early in the design cycle. While large language models (LLMs) show promise for RTL code generation, their ability to reason about hardware specifications and generate targeted test plans remains largely unexplored. We present the first systematic study of LLM reasoning capabilities for RTL verification stimuli generation, establishing a two-stage framework that decomposes test plan generation from testbench execution. Our benchmark reveals that state-of-the-art models, including DeepSeek-R1 and Claude-4.0-Sonnet, achieve only 15.7-21.7% success rates on generating stimuli that pass golden RTL designs. To improve LLM generated stimuli, we develop a comprehensive training methodology combining supervised fine-tuning with a novel reinforcement learning approach, GRPO with State Mutation (GRPO-SMu), which enhances exploration by varying input mutations. Our approach leverages a tree-based branching mutation strategy to construct training data comprising equivalent and mutated trees, moving beyond linear mutation approaches to provide rich learning signals. Training on this curated dataset, our 7B parameter model achieves a 33.3% golden test pass rate and a 13.9% mutation detection rate, representing a 17.6% absolute improvement over baseline and outperforming much larger general-purpose models. These results demonstrate that specialized training methodologies can significantly enhance LLM reasoning capabilities for hardware verification tasks, establishing a foundation for automated sub-unit testing in semiconductor design workflows.",
      "publishedDate": "2026-01-12T14:42:42Z",
      "updatedDate": "2026-01-12T14:42:42Z",
      "primaryCategory": "cs.AR",
      "arxivCategories": [
        "cs.AR",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07593v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07593",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "reasoning",
        "planning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "planning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07577",
      "title": "Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents",
      "authors": [
        {
          "name": "Yunfan Li",
          "affiliation": null
        },
        {
          "name": "Bingbing Xu",
          "affiliation": null
        },
        {
          "name": "Xueyun Tian",
          "affiliation": null
        },
        {
          "name": "Xiucheng Xu",
          "affiliation": null
        },
        {
          "name": "Huawei Shen",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.",
      "publishedDate": "2026-01-12T14:30:10Z",
      "updatedDate": "2026-01-12T14:30:10Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07577v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07577",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07553",
      "title": "VirtualEnv: A Platform for Embodied AI Research",
      "authors": [
        {
          "name": "Kabir Swain",
          "affiliation": null
        },
        {
          "name": "Sijie Han",
          "affiliation": null
        },
        {
          "name": "Ayush Raina",
          "affiliation": null
        },
        {
          "name": "Jin Zhang",
          "affiliation": null
        },
        {
          "name": "Shuang Li",
          "affiliation": null
        },
        {
          "name": "Michael Stopa",
          "affiliation": null
        },
        {
          "name": "Antonio Torralba",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) continue to improve in reasoning and decision-making, there is a growing need for realistic and interactive environments where their abilities can be rigorously evaluated. We present VirtualEnv, a next-generation simulation platform built on Unreal Engine 5 that enables fine-grained benchmarking of LLMs in embodied and interactive scenarios. VirtualEnv supports rich agent-environment interactions, including object manipulation, navigation, and adaptive multi-agent collaboration, as well as game-inspired mechanics like escape rooms and procedurally generated environments. We provide a user-friendly API built on top of Unreal Engine, allowing researchers to deploy and control LLM-driven agents using natural language instructions. We integrate large-scale LLMs and vision-language models (VLMs), such as GPT-based models, to generate novel environments and structured tasks from multimodal inputs. Our experiments benchmark the performance of several popular LLMs across tasks of increasing complexity, analyzing differences in adaptability, planning, and multi-agent coordination. We also describe our methodology for procedural task generation, task validation, and real-time environment control. VirtualEnv is released as an open-source platform, we aim to advance research at the intersection of AI and gaming, enable standardized evaluation of LLMs in embodied AI settings, and pave the way for future developments in immersive simulations and interactive entertainment.",
      "publishedDate": "2026-01-12T14:04:38Z",
      "updatedDate": "2026-01-12T14:04:38Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07553v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07553",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "multi-agent",
        "evaluation",
        "robotics",
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "evaluation",
          "robotics",
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07525",
      "title": "Thinking Before Constraining: A Unified Decoding Framework for Large Language Models",
      "authors": [
        {
          "name": "Ngoc Trinh Hung Nguyen",
          "affiliation": null
        },
        {
          "name": "Alonso Silva",
          "affiliation": null
        },
        {
          "name": "Laith Zumot",
          "affiliation": null
        },
        {
          "name": "Liubov Tupikina",
          "affiliation": null
        },
        {
          "name": "Armen Aghasaryan",
          "affiliation": null
        },
        {
          "name": "Mehwish Alam",
          "affiliation": null
        }
      ],
      "abstract": "Natural generation allows Language Models (LMs) to produce free-form responses with rich reasoning, but the lack of guaranteed structure makes outputs difficult to parse or verify. Structured generation, or constrained decoding, addresses this drawback by producing content in standardized formats such as JSON, ensuring consistency and guaranteed-parsable outputs, but it can inadvertently restrict the model's reasoning capabilities. In this work, we propose a simple approach that combines the advantages of both natural and structured generation. By allowing LLMs to reason freely until specific trigger tokens are generated, and then switching to structured generation, our method preserves the expressive power of natural language reasoning while ensuring the reliability of structured outputs. We further evaluate our approach on several datasets, covering both classification and reasoning tasks, to demonstrate its effectiveness, achieving a substantial gain of up to 27% in accuracy compared to natural generation, while requiring only a small overhead of 10-20 extra tokens.",
      "publishedDate": "2026-01-12T13:25:28Z",
      "updatedDate": "2026-01-12T13:25:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07525v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07525",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07469",
      "title": "Knowledge Distillation for LLM-Based Human Activity Recognition in Homes",
      "authors": [
        {
          "name": "Julien Cumin",
          "affiliation": null
        },
        {
          "name": "Oussama Er-Rahmany",
          "affiliation": null
        },
        {
          "name": "Xi Chen",
          "affiliation": null
        }
      ],
      "abstract": "Human Activity Recognition (HAR) is a central problem for context-aware applications, especially for smart homes and assisted living. A few very recent studies have shown that Large Language Models (LLMs) can be used for HAR at home, reaching high performance and addressing key challenges. In this paper, we provide new experimental results regarding the use of LLMs for HAR, on two state-of-the-art datasets. More specifically, we show how recognition performance evolves depending on the size of the LLM used. Moreover, we experiment on the use of knowledge distillation techniques to fine-tune smaller LLMs with HAR reasoning examples generated by larger LLMs. We show that such fine-tuned models can perform almost as well as the largest LLMs, while having 50 times less parameters.",
      "publishedDate": "2026-01-12T12:25:53Z",
      "updatedDate": "2026-01-12T12:25:53Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07469v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07469",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07464",
      "title": "IFDNS: An Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning",
      "authors": [
        {
          "name": "Xiaoheng Wang",
          "affiliation": null
        },
        {
          "name": "Tongxuan Liu",
          "affiliation": null
        },
        {
          "name": "Zi Gong",
          "affiliation": null
        },
        {
          "name": "Xianzhe Dong",
          "affiliation": null
        },
        {
          "name": "Yuting Zeng",
          "affiliation": null
        },
        {
          "name": "Minhan Hu",
          "affiliation": null
        },
        {
          "name": "Weizhe Huang",
          "affiliation": null
        },
        {
          "name": "Jing Li",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across a wide range of reasoning tasks, including logical and mathematical problem-solving. While prompt-based methods like Chain-of-Thought (CoT) can enhance LLM reasoning abilities to some extent, they often suffer from a lack of faithfulness, where the derived conclusions may not align with the generated reasoning chain. To address this issue, researchers have explored neuro-symbolic approaches to bolster LLM logical reasoning capabilities. However, existing neuro-symbolic methods still face challenges with information loss during the process. To overcome these limitations, we introduce Iterative Feedback-Driven Neuro-Symbolic (IFDNS), a novel prompt-based method that employs a multi-round feedback mechanism to address LLM limitations in handling complex logical relationships. IFDNS utilizes iterative feedback during the logic extraction phase to accurately extract causal relationship statements and translate them into propositional and logical implication expressions, effectively mitigating information loss issues. Furthermore, IFDNS is orthogonal to existing prompt methods, allowing for seamless integration with various prompting approaches. Empirical evaluations across six datasets demonstrate the effectiveness of IFDNS in significantly improving the performance of CoT and Chain-of-Thought with Self-Consistency (CoT-SC). Specifically, IFDNS achieves a +9.40% accuracy boost for CoT on the LogiQA dataset and a +11.70% improvement for CoT-SC on the PrOntoQA dataset.",
      "publishedDate": "2026-01-12T12:20:19Z",
      "updatedDate": "2026-01-12T12:20:19Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07464v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07464",
      "comment": "13 pages,5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07430",
      "title": "KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning",
      "authors": [
        {
          "name": "Qitan Lv",
          "affiliation": null
        },
        {
          "name": "Tianyu Liu",
          "affiliation": null
        },
        {
          "name": "Qiaosheng Zhang",
          "affiliation": null
        },
        {
          "name": "Xingcheng Xu",
          "affiliation": null
        },
        {
          "name": "Chaochao Lu",
          "affiliation": null
        }
      ],
      "abstract": "Despite the impressive performance of large language models (LLMs) pretrained on vast knowledge corpora, advancing their knowledge manipulation-the ability to effectively recall, reason, and transfer relevant knowledge-remains challenging. Existing methods mainly leverage Supervised Fine-Tuning (SFT) on labeled datasets to enhance LLMs' knowledge manipulation ability. However, we observe that SFT models still exhibit the known&incorrect phenomenon, where they explicitly possess relevant knowledge for a given question but fail to leverage it for correct answers. To address this challenge, we propose KALE (Knowledge-Aware LEarning)-a post-training framework that leverages knowledge graphs (KGs) to generate high-quality rationales and enhance LLMs' knowledge manipulation ability. Specifically, KALE first introduces a Knowledge-Induced (KI) data synthesis method that efficiently extracts multi-hop reasoning paths from KGs to generate high-quality rationales for question-answer pairs. Then, KALE employs a Knowledge-Aware (KA) fine-tuning paradigm that enhances knowledge manipulation by internalizing rationale-guided reasoning through minimizing the KL divergence between predictions with and without rationales. Extensive experiments on eight popular benchmarks across six different LLMs demonstrate the effectiveness of KALE, achieving accuracy improvements of up to 11.72% and an average of 4.18%.",
      "publishedDate": "2026-01-12T11:19:42Z",
      "updatedDate": "2026-01-12T11:19:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07430v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07430",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07389",
      "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
      "authors": [
        {
          "name": "Xueyan Niu",
          "affiliation": null
        },
        {
          "name": "Bo Bai",
          "affiliation": null
        },
        {
          "name": "Wei Han",
          "affiliation": null
        },
        {
          "name": "Weixi Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training",
      "publishedDate": "2026-01-12T10:14:09Z",
      "updatedDate": "2026-01-12T10:14:09Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07389v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07389",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07372",
      "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "authors": [
        {
          "name": "Xin Cheng",
          "affiliation": null
        },
        {
          "name": "Wangding Zeng",
          "affiliation": null
        },
        {
          "name": "Damai Dai",
          "affiliation": null
        },
        {
          "name": "Qinyu Chen",
          "affiliation": null
        },
        {
          "name": "Bingxuan Wang",
          "affiliation": null
        },
        {
          "name": "Zhenda Xie",
          "affiliation": null
        },
        {
          "name": "Kezhao Huang",
          "affiliation": null
        },
        {
          "name": "Xingkai Yu",
          "affiliation": null
        },
        {
          "name": "Zhewen Hao",
          "affiliation": null
        },
        {
          "name": "Yukun Li",
          "affiliation": null
        },
        {
          "name": "Han Zhang",
          "affiliation": null
        },
        {
          "name": "Huishuai Zhang",
          "affiliation": null
        },
        {
          "name": "Dongyan Zhao",
          "affiliation": null
        },
        {
          "name": "Wenfeng Liang",
          "affiliation": null
        }
      ],
      "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.",
      "publishedDate": "2026-01-12T09:54:49Z",
      "updatedDate": "2026-01-12T09:54:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07372v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07372",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07367",
      "title": "FOCAL: A Novel Benchmarking Technique for Multi-modal Agents",
      "authors": [
        {
          "name": "Aditya Choudhary",
          "affiliation": null
        },
        {
          "name": "Anupam Purwar",
          "affiliation": null
        }
      ],
      "abstract": "With the recent advancements in reasoning capa- bilities, tool calling using MCP servers and Audio Language Models (ALMs), development and integration of multi-modal agents (with voice and text support) has come to the industry forefront. Cascading pipelines for voice agents still play a central role in the industry owing to their superior reasoning capabilities facilitated by LLMs. Although, cascading pipelines often present error propagation through the pipeline. We propose a framework, FOCAL to benchmark end-to-end reasoning, component-wise error propagation and error analysis for automated as well as human-assisted testing of multi-modal agents (voice to voice + text input). We also share two novel metrics viz. Reasoning and Semantic scores to evaluate efficacy of the agent in having meaningful conversations in voice mode.",
      "publishedDate": "2026-01-12T09:46:06Z",
      "updatedDate": "2026-01-12T09:46:06Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07367v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07367",
      "comment": "We present a framework for evaluation of Multi-modal Agents consisting of Voice-to-voice model components viz. Text to Speech (TTS), Retrieval Augmented Generation (RAG) and Speech-to-text (STT)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07359",
      "title": "Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training",
      "authors": [
        {
          "name": "Shezheng Song",
          "affiliation": null
        },
        {
          "name": "Shasha Li",
          "affiliation": null
        },
        {
          "name": "Jie Yu",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated strong capabilities across a variety of vision-language tasks. However, their internal reasoning often exhibits a critical inconsistency: although deeper layers may attend to the correct visual regions, final predictions are frequently misled by noisy attention from earlier layers. This results in a disconnect between what the model internally understands and what it ultimately expresses, a phenomenon we describe as seeing it right but saying it wrong. To address this issue, we propose DualPD, a dual-perspective decoding refinement strategy that enhances the visual understanding without any additional training. DualPD consists of two components. (1) The layer-wise attention-guided contrastive logits module captures how the belief in the correct answer evolves by comparing output logits between layers that exhibit the largest attention shift. (2) The head-wise information filtering module suppresses low-contribution attention heads that focus on irrelevant regions, thereby improving attention quality within each layer. Experiments conducted on both the LLaVA and Qwen-VL model families across multiple multimodal benchmarks demonstrate that DualPD consistently improves accuracy without training, confirming its effectiveness and generalizability. The code will be released upon publication.",
      "publishedDate": "2026-01-12T09:34:20Z",
      "updatedDate": "2026-01-12T09:34:20Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07359v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07359",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07347",
      "title": "DiffER: Diffusion Entity-Relation Modeling for Reversal Curse in Diffusion Large Language Models",
      "authors": [
        {
          "name": "Shaokai He",
          "affiliation": null
        },
        {
          "name": "Kaiwen Wei",
          "affiliation": null
        },
        {
          "name": "Xinyi Zeng",
          "affiliation": null
        },
        {
          "name": "Xiang Chen",
          "affiliation": null
        },
        {
          "name": "Xue Yang",
          "affiliation": null
        },
        {
          "name": "Zhenyang Li",
          "affiliation": null
        },
        {
          "name": "Jiang Zhong",
          "affiliation": null
        },
        {
          "name": "Yu Tian",
          "affiliation": null
        }
      ],
      "abstract": "The \"reversal curse\" refers to the phenomenon where large language models (LLMs) exhibit predominantly unidirectional behavior when processing logically bidirectional relationships. Prior work attributed this to autoregressive training -- predicting the next token inherently favors left-to-right information flow over genuine bidirectional knowledge associations. However, we observe that Diffusion LLMs (DLLMs), despite being trained bidirectionally, also suffer from the reversal curse. To investigate the root causes, we conduct systematic experiments on DLLMs and identify three key reasons: 1) entity fragmentation during training, 2) data asymmetry, and 3) missing entity relations. Motivated by the analysis of these reasons, we propose Diffusion Entity-Relation Modeling (DiffER), which addresses the reversal curse through entity-aware training and balanced data construction. Specifically, DiffER introduces whole-entity masking, which mitigates entity fragmentation by predicting complete entities in a single step. DiffER further employs distribution-symmetric and relation-enhanced data construction strategies to alleviate data asymmetry and missing relations. Extensive experiments demonstrate that DiffER effectively alleviates the reversal curse in Diffusion LLMs, offering new perspectives for future research.",
      "publishedDate": "2026-01-12T09:22:10Z",
      "updatedDate": "2026-01-12T09:22:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07347v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07347",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07342",
      "title": "Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure",
      "authors": [
        {
          "name": "Nicolas Tacheny",
          "affiliation": null
        }
      ],
      "abstract": "Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models, where failures propagate across physical and logical components and affect multiple customers. Traditional approaches to root cause analysis(RCA) rely on hard-coded graph traversal algorithms or rule-based correlation engines, which are costly to maintain and tightly coupled to the infrastructure model. In this work, we introduce an agentic diagnostic framework where a Large Language Model (LLM) performs step-wise investigation using a constrained tool space exposed through the Model Context Protocol (MCP). Instead of embedding causal logic or traversal algorithms into the application, the agent autonomously navigates the infrastructure model by invoking tools for service lookup, dependency retrieval, structured and unstructured data, and event analysis, and impact discovery. We define an investigation protocol that structures the agent's reasoning and ensures grounding, reproducibility, and safe handling of missing or ambiguous information. This work lays the foundation for autonomous incident resolution and change impact mitigation. Future systems will not only diagnose and remediate infrastructure failures, but also predict the impact of planned changes on services and customers, enabling operators to mitigate risks before executing maintenance operations.",
      "publishedDate": "2026-01-12T09:13:04Z",
      "updatedDate": "2026-01-12T09:13:04Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07342v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07342",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07320",
      "title": "Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training",
      "authors": [
        {
          "name": "Xue Gong",
          "affiliation": null
        },
        {
          "name": "Qi Yi",
          "affiliation": null
        },
        {
          "name": "Ziyuan Nan",
          "affiliation": null
        },
        {
          "name": "Guanhua Huang",
          "affiliation": null
        },
        {
          "name": "Kejiao Li",
          "affiliation": null
        },
        {
          "name": "Yuhao Jiang",
          "affiliation": null
        },
        {
          "name": "Ruibin Xiong",
          "affiliation": null
        },
        {
          "name": "Zenan Xu",
          "affiliation": null
        },
        {
          "name": "Jiaming Guo",
          "affiliation": null
        },
        {
          "name": "Shaohui Peng",
          "affiliation": null
        },
        {
          "name": "Bo Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Training Large Language Models (LLMs) for reasoning tasks is increasingly driven by Reinforcement Learning with Verifiable Rewards (RLVR), where Proximal Policy Optimization (PPO) provides a principled framework for stable policy updates. However, the practical application of PPO is hindered by unreliable advantage estimation in the sparse-reward RLVR regime. This issue arises because the sparse rewards in RLVR lead to inaccurate intermediate value predictions, which in turn introduce significant bias when aggregated at every token by Generalized Advantage Estimation (GAE). To address this, we introduce Segmental Advantage Estimation (SAE), which mitigates the bias that GAE can incur in RLVR. Our key insight is that aggregating $n$-step advantages at every token(as in GAE) is unnecessary and often introduces excessive bias, since individual tokens carry minimal information. Instead, SAE first partitions the generated sequence into coherent sub-segments using low-probability tokens as heuristic boundaries. It then selectively computes variance-reduced advantage estimates only from these information-rich segment transitions, effectively filtering out noise from intermediate tokens. Our experiments demonstrate that SAE achieves superior performance, with marked improvements in final scores, training stability, and sample efficiency. These gains are shown to be consistent across multiple model sizes, and a correlation analysis confirms that our proposed advantage estimator achieves a higher correlation with an approximate ground-truth advantage, justifying its superior performance.",
      "publishedDate": "2026-01-12T08:41:47Z",
      "updatedDate": "2026-01-12T08:41:47Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07320v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07320",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07316",
      "title": "BEAT-Net: Injecting Biomimetic Spatio-Temporal Priors for Interpretable ECG Classification",
      "authors": [
        {
          "name": "Runze Ma",
          "affiliation": null
        },
        {
          "name": "Caizhi Liao",
          "affiliation": null
        }
      ],
      "abstract": "Although deep learning has advanced automated electrocardiogram (ECG) diagnosis, prevalent supervised methods typically treat recordings as undifferentiated one-dimensional (1D) signals or two-dimensional (2D) images. This formulation compels models to learn physiological structures implicitly, resulting in data inefficiency and opacity that diverge from medical reasoning. To address these limitations, we propose BEAT-Net, a Biomimetic ECG Analysis with Tokenization framework that reformulates the problem as a language modeling task. Utilizing a QRS tokenization strategy to transform continuous signals into biologically aligned heartbeat sequences, the architecture explicitly decomposes cardiac physiology through specialized encoders that extract local beat morphology while normalizing spatial lead perspectives and modeling temporal rhythm dependencies. Evaluations across three large-scale benchmarks demonstrate that BEAT-Net matches the diagnostic accuracy of dominant convolutional neural network (CNN) architectures while substantially improving robustness. The framework exhibits exceptional data efficiency, recovering fully supervised performance using only 30 to 35 percent of annotated data. Moreover, learned attention mechanisms provide inherent interpretability by spontaneously reproducing clinical heuristics, such as Lead II prioritization for rhythm analysis, without explicit supervision. These findings indicate that integrating biological priors offers a computationally efficient and interpretable alternative to data-intensive large-scale pre-training.",
      "publishedDate": "2026-01-12T08:37:47Z",
      "updatedDate": "2026-01-12T08:37:47Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07316v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07316",
      "comment": "8 pages, 4 figures and 2 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07298",
      "title": "Mimic Human Cognition, Master Multi-Image Reasoning: A Meta-Action Framework for Enhanced Visual Understanding",
      "authors": [
        {
          "name": "Jianghao Yin",
          "affiliation": null
        },
        {
          "name": "Qingbin Li",
          "affiliation": null
        },
        {
          "name": "Kun Sun",
          "affiliation": null
        },
        {
          "name": "Cheng Ding",
          "affiliation": null
        },
        {
          "name": "Jie Wang",
          "affiliation": null
        },
        {
          "name": "Qin Chen",
          "affiliation": null
        },
        {
          "name": "Jie Zhou",
          "affiliation": null
        },
        {
          "name": "Nan Wang",
          "affiliation": null
        },
        {
          "name": "Changqing Li",
          "affiliation": null
        },
        {
          "name": "Pei Wu",
          "affiliation": null
        },
        {
          "name": "Jian Xu",
          "affiliation": null
        },
        {
          "name": "Zheming Yang",
          "affiliation": null
        },
        {
          "name": "Liang He",
          "affiliation": null
        }
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) excel at single-image understanding, they exhibit significantly degraded performance in multi-image reasoning scenarios. Multi-image reasoning presents fundamental challenges including complex inter-relationships between images and scattered critical information across image sets. Inspired by human cognitive processes, we propose the Cognition-Inspired Meta-Action Framework (CINEMA), a novel approach that decomposes multi-image reasoning into five structured meta-actions: Global, Focus, Hint, Think, and Answer which explicitly modeling the sequential cognitive steps humans naturally employ. For cold-start training, we introduce a Retrieval-Based Tree Sampling strategy that generates high-quality meta-action trajectories to bootstrap the model with reasoning patterns. During reinforcement learning, we adopt a two-stage paradigm: an exploration phase with Diversity-Preserving Strategy to avoid entropy collapse, followed by an annealed exploitation phase with DAPO to gradually strengthen exploitation. To train our model, we construct a dataset of 57k cold-start and 58k reinforcement learning instances spanning multi-image, multi-frame, and single-image tasks. We conduct extensive evaluations on multi-image reasoning benchmarks, video understanding benchmarks, and single-image benchmarks, achieving competitive state-of-the-art performance on several key benchmarks. Our model surpasses GPT-4o on the MUIR and MVMath benchmarks and notably outperforms specialized video reasoning models on video understanding benchmarks, demonstrating the effectiveness and generalizability of our human cognition-inspired reasoning framework.",
      "publishedDate": "2026-01-12T08:15:36Z",
      "updatedDate": "2026-01-12T08:15:36Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07298v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07298",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07280",
      "title": "ReasonTabQA: A Comprehensive Benchmark for Table Question Answering from Real World Industrial Scenarios",
      "authors": [
        {
          "name": "Changzai Pan",
          "affiliation": null
        },
        {
          "name": "Jie Zhang",
          "affiliation": null
        },
        {
          "name": "Kaiwen Wei",
          "affiliation": null
        },
        {
          "name": "Chenshuo Pan",
          "affiliation": null
        },
        {
          "name": "Yu Zhao",
          "affiliation": null
        },
        {
          "name": "Jingwang Huang",
          "affiliation": null
        },
        {
          "name": "Jian Yang",
          "affiliation": null
        },
        {
          "name": "Zhenhe Wu",
          "affiliation": null
        },
        {
          "name": "Haoyang Zeng",
          "affiliation": null
        },
        {
          "name": "Xiaoyan Gu",
          "affiliation": null
        },
        {
          "name": "Weichao Sun",
          "affiliation": null
        },
        {
          "name": "Yanbo Zhai",
          "affiliation": null
        },
        {
          "name": "Yujie Mao",
          "affiliation": null
        },
        {
          "name": "Zhuoru Jiang",
          "affiliation": null
        },
        {
          "name": "Jiang Zhong",
          "affiliation": null
        },
        {
          "name": "Shuangyong Song",
          "affiliation": null
        },
        {
          "name": "Yongxiang Li",
          "affiliation": null
        },
        {
          "name": "Zhongjiang He",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly catalyzed table-based question answering (TableQA). However, existing TableQA benchmarks often overlook the intricacies of industrial scenarios, which are characterized by multi-table structures, nested headers, and massive scales. These environments demand robust table reasoning through deep structured inference, presenting a significant challenge that remains inadequately addressed by current methodologies. To bridge this gap, we present ReasonTabQA, a large-scale bilingual benchmark encompassing 1,932 tables across 30 industry domains such as energy and automotive. ReasonTabQA provides high-quality annotations for both final answers and explicit reasoning chains, supporting both thinking and no-thinking paradigms. Furthermore, we introduce TabCodeRL, a reinforcement learning method that leverages table-aware verifiable rewards to guide the generation of logical reasoning paths. Extensive experiments on ReasonTabQA and 4 TableQA datasets demonstrate that while TabCodeRL yields substantial performance gains on open-source LLMs, the persistent performance gap on ReasonTabQA underscores the inherent complexity of real-world industrial TableQA.",
      "publishedDate": "2026-01-12T07:36:06Z",
      "updatedDate": "2026-01-12T07:36:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07280v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07280",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07263",
      "title": "When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering Attack in Web Automation Agent",
      "authors": [
        {
          "name": "Xinyi Wu",
          "affiliation": null
        },
        {
          "name": "Geng Hong",
          "affiliation": null
        },
        {
          "name": "Yueyue Chen",
          "affiliation": null
        },
        {
          "name": "MingXuan Liu",
          "affiliation": null
        },
        {
          "name": "Feier Jin",
          "affiliation": null
        },
        {
          "name": "Xudong Pan",
          "affiliation": null
        },
        {
          "name": "Jiarun Dai",
          "affiliation": null
        },
        {
          "name": "Baojun Liu",
          "affiliation": null
        }
      ],
      "abstract": "Web agents, powered by large language models (LLMs), are increasingly deployed to automate complex web interactions. The rise of open-source frameworks (e.g., Browser Use, Skyvern-AI) has accelerated adoption, but also broadened the attack surface. While prior research has focused on model threats such as prompt injection and backdoors, the risks of social engineering remain largely unexplored. We present the first systematic study of social engineering attacks against web automation agents and design a pluggable runtime mitigation solution. On the attack side, we introduce the AgentBait paradigm, which exploits intrinsic weaknesses in agent execution: inducement contexts can distort the agent's reasoning and steer it toward malicious objectives misaligned with the intended task. On the defense side, we propose SUPERVISOR, a lightweight runtime module that enforces environment and intention consistency alignment between webpage context and intended goals to mitigate unsafe operations before execution. Empirical results show that mainstream frameworks are highly vulnerable to AgentBait, with an average attack success rate of 67.5% and peaks above 80% under specific strategies (e.g., trusted identity forgery). Compared with existing lightweight defenses, our module can be seamlessly integrated across different web automation frameworks and reduces attack success rates by up to 78.1% on average while incurring only a 7.7% runtime overhead and preserving usability. This work reveals AgentBait as a critical new threat surface for web agents and establishes a practical, generalizable defense, advancing the security of this rapidly emerging ecosystem. We reported the details of this attack to the framework developers and received acknowledgment before submission.",
      "publishedDate": "2026-01-12T07:10:08Z",
      "updatedDate": "2026-01-12T07:10:08Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07263v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07263",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07260",
      "title": "ActiShade: Activating Overshadowed Knowledge to Guide Multi-Hop Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Huipeng Ma",
          "affiliation": null
        },
        {
          "name": "Luan Zhang",
          "affiliation": null
        },
        {
          "name": "Dandan Song",
          "affiliation": null
        },
        {
          "name": "Linmei Hu",
          "affiliation": null
        },
        {
          "name": "Yuhang Tian",
          "affiliation": null
        },
        {
          "name": "Jun Yang",
          "affiliation": null
        },
        {
          "name": "Changzhi Zhou",
          "affiliation": null
        },
        {
          "name": "Chenhao Li",
          "affiliation": null
        },
        {
          "name": "Yizhou Jin",
          "affiliation": null
        },
        {
          "name": "Xudong Li",
          "affiliation": null
        },
        {
          "name": "Meng Lin",
          "affiliation": null
        },
        {
          "name": "Mingxing Zhang",
          "affiliation": null
        },
        {
          "name": "Shuhao Zhang",
          "affiliation": null
        }
      ],
      "abstract": "In multi-hop reasoning, multi-round retrieval-augmented generation (RAG) methods typically rely on LLM-generated content as the retrieval query. However, these approaches are inherently vulnerable to knowledge overshadowing - a phenomenon where critical information is overshadowed during generation. As a result, the LLM-generated content may be incomplete or inaccurate, leading to irrelevant retrieval and causing error accumulation during the iteration process. To address this challenge, we propose ActiShade, which detects and activates overshadowed knowledge to guide large language models (LLMs) in multi-hop reasoning. Specifically, ActiShade iteratively detects the overshadowed keyphrase in the given query, retrieves documents relevant to both the query and the overshadowed keyphrase, and generates a new query based on the retrieved documents to guide the next-round iteration. By supplementing the overshadowed knowledge during the formulation of next-round queries while minimizing the introduction of irrelevant noise, ActiShade reduces the error accumulation caused by knowledge overshadowing. Extensive experiments show that ActiShade outperforms existing methods across multiple datasets and LLMs.",
      "publishedDate": "2026-01-12T06:57:31Z",
      "updatedDate": "2026-01-12T06:57:31Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07260v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07260",
      "comment": "Accepted to AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07251",
      "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
      "authors": [
        {
          "name": "Zizhen Li",
          "affiliation": null
        },
        {
          "name": "Chuanhao Li",
          "affiliation": null
        },
        {
          "name": "Yibin Wang",
          "affiliation": null
        },
        {
          "name": "Yukang Feng",
          "affiliation": null
        },
        {
          "name": "Jianwen Sun",
          "affiliation": null
        },
        {
          "name": "Jiaxin Ai",
          "affiliation": null
        },
        {
          "name": "Fanrui Zhang",
          "affiliation": null
        },
        {
          "name": "Mingzhu Sun",
          "affiliation": null
        },
        {
          "name": "Yifei Huang",
          "affiliation": null
        },
        {
          "name": "Kaipeng Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements have expanded the role of Large Language Models in board games from playing agents to creative co-designers. However, a critical gap remains: current systems lack the capacity to offer constructive critique grounded in the emergent user experience. Bridging this gap is fundamental for harmonizing Human-AI collaboration, as it empowers designers to refine their creations via external perspectives while steering models away from biased or unpredictable outcomes. Automating critique for board games presents two challenges: inferring the latent dynamics connecting rules to gameplay without an explicit engine, and modeling the subjective heterogeneity of diverse player groups. To address these, we curate a dataset of 1,727 structurally corrected rulebooks and 150K reviews selected via quality scoring and facet-aware sampling. We augment this data with Mechanics-Dynamics-Aesthetics (MDA) reasoning to explicitly bridge the causal gap between written rules and player experience. We further distill player personas and introduce MeepleLM, a specialized model that internalizes persona-specific reasoning patterns to accurately simulate the subjective feedback of diverse player archetypes. Experiments demonstrate that MeepleLM significantly outperforms latest commercial models (e.g., GPT-5.1, Gemini3-Pro) in community alignment and critique quality, achieving a 70% preference rate in user studies assessing utility. MeepleLM serves as a reliable virtual playtester for general interactive systems, marking a pivotal step towards audience-aligned, experience-aware Human-AI collaboration.",
      "publishedDate": "2026-01-12T06:37:12Z",
      "updatedDate": "2026-01-12T06:37:12Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07251v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07251",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07245",
      "title": "Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models",
      "authors": [
        {
          "name": "Pranav Kallem",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) achieve strong aver- age performance yet remain unreliable at the instance level, with frequent hallucinations, brittle failures, and poorly calibrated confidence. We study reliability through the lens of multi-model consensus: given responses from several heterogeneous LLMs, can we learn which answer is most likely correct for a given query? We introduce a Multi-Model Consensus Reasoning Engine that treats the set of LLM outputs as input to a supervised meta-learner. The system maps natural language responses into structured features using semantic embeddings, pairwise similarity and clustering statistics, lexical and structural cues, reasoning-quality scores, confidence estimates, and model-specific priors, and then applies gradient-boosted trees, listwise ranking, and graph neural networks over similarity graphs of answers. Using three open-weight LLMs evaluated on compact, resource- constrained subsets of GSM8K, ARC-Challenge, HellaSwag, and TruthfulQA, our best graph-attention-based consensus model improves macro-average accuracy by 4.6 percentage points over the strongest single LLM and by 8.1 points over majority vote, while also yielding lower Brier scores and fewer TruthfulQA hal- lucinations. Ablation and feature-importance analyses show that semantic agreement and clustering features are most influential, with reasoning-quality and model-prior features providing com- plementary gains, suggesting supervised multi-model consensus is a practical route toward more reliable LLM behavior, even in a modest single-machine setup.",
      "publishedDate": "2026-01-12T06:27:06Z",
      "updatedDate": "2026-01-12T06:27:06Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07245v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07245",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07239",
      "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
      "authors": [
        {
          "name": "Tanmay Joshi",
          "affiliation": null
        },
        {
          "name": "Shourya Aggarwal",
          "affiliation": null
        },
        {
          "name": "Anusa Saha",
          "affiliation": null
        },
        {
          "name": "Aadi Pandey",
          "affiliation": null
        },
        {
          "name": "Shreyash Dhoot",
          "affiliation": null
        },
        {
          "name": "Vighnesh Rai",
          "affiliation": null
        },
        {
          "name": "Raxit Goswami",
          "affiliation": null
        },
        {
          "name": "Aman Chadha",
          "affiliation": null
        },
        {
          "name": "Vinija Jain",
          "affiliation": null
        },
        {
          "name": "Amitava Das",
          "affiliation": null
        }
      ],
      "abstract": "Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability. In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. We instead advocate Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled. Empirically, we show that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation.",
      "publishedDate": "2026-01-12T06:19:09Z",
      "updatedDate": "2026-01-12T06:19:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07239v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07239",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07199",
      "title": "Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization",
      "authors": [
        {
          "name": "Murtaza Nikzad",
          "affiliation": null
        },
        {
          "name": "Raghuram Ramanujan",
          "affiliation": null
        }
      ],
      "abstract": "Large language models exhibit impressive reasoning capabilities yet frequently generate plausible but incorrect solutions, a phenomenon commonly termed hallucination. This paper investigates the effect of training objective composition on reasoning reliability through Direct Preference Optimization. Two complementary training signals are examined: forward chain-of-thought generation, which trains the model to produce correct reasoning traces, and backward verification, which trains the model to verify and acknowledge errors in candidate solutions. Experiments on GSM8K reveal a fundamental trade-off between these objectives. Forward-only DPO training achieves the highest accuracy improvement, increasing from 83.1% to 86.6% (+3.5 percentage points), while backward-only training yields minimal accuracy gains but substantially reduces the false positive rate from 13.4% to 4.3%. Notably, both training variants reduce acknowledgement rate compared to the baseline, suggesting that preference optimization increases model confidence in its outputs. These findings indicate that forward and backward reasoning objectives provide distinct and complementary learning signals: forward training improves problem-solving capability, while backward training improves verification calibration. The complete training and evaluation pipeline, implemented efficiently through Low-Rank Adaptation, is released to facilitate further research.",
      "publishedDate": "2026-01-12T04:46:27Z",
      "updatedDate": "2026-01-12T04:46:27Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07199v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07199",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07192",
      "title": "Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG",
      "authors": [
        {
          "name": "Manzong Huang",
          "affiliation": null
        },
        {
          "name": "Chenyang Bu",
          "affiliation": null
        },
        {
          "name": "Yi He",
          "affiliation": null
        },
        {
          "name": "Xingrui Zhuo",
          "affiliation": null
        },
        {
          "name": "Xindong Wu",
          "affiliation": null
        }
      ],
      "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are constrained by a prevailing \\textit{build-then-reason} paradigm, which relies on a static, pre-constructed Knowledge Graph (KG). This paradigm faces two critical challenges. First, the KG's inherent incompleteness often breaks reasoning paths. Second, the graph's low signal-to-noise ratio introduces distractor facts, presenting query-relevant but misleading knowledge that disrupts the reasoning process. To address these challenges, we argue for a \\textit{reason-and-construct} paradigm and propose Relink, a framework that dynamically builds a query-specific evidence graph. To tackle incompleteness, \\textbf{Relink} instantiates required facts from a latent relation pool derived from the original text corpus, repairing broken paths on the fly. To handle misleading or distractor facts, Relink employs a unified, query-aware evaluation strategy that jointly considers candidates from both the KG and latent relations, selecting those most useful for answering the query rather than relying on their pre-existence. This empowers Relink to actively discard distractor facts and construct the most faithful and precise evidence path for each query. Extensive experiments on five Open-Domain Question Answering benchmarks show that Relink achieves significant average improvements of 5.4\\% in EM and 5.2\\% in F1 over leading GraphRAG baselines, demonstrating the superiority of our proposed framework.",
      "publishedDate": "2026-01-12T04:35:23Z",
      "updatedDate": "2026-01-12T04:35:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07192v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07192",
      "comment": "Accepted by AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07185",
      "title": "Defenses Against Prompt Attacks Learn Surface Heuristics",
      "authors": [
        {
          "name": "Shawn Li",
          "affiliation": null
        },
        {
          "name": "Chenxiao Yu",
          "affiliation": null
        },
        {
          "name": "Zhiyu Ni",
          "affiliation": null
        },
        {
          "name": "Hao Li",
          "affiliation": null
        },
        {
          "name": "Charith Peris",
          "affiliation": null
        },
        {
          "name": "Chaowei Xiao",
          "affiliation": null
        },
        {
          "name": "Yue Zhao",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in security-sensitive applications, where they must follow system- or developer-specified instructions that define the intended task behavior, while completing benign user requests. When adversarial instructions appear in user queries or externally retrieved content, models may override intended logic. Recent defenses rely on supervised fine-tuning with benign and malicious labels. Although these methods achieve high attack rejection rates, we find that they rely on narrow correlations in defense data rather than harmful intent, leading to systematic rejection of safe inputs. We analyze three recurring shortcut behaviors induced by defense fine-tuning. \\emph{Position bias} arises when benign content placed later in a prompt is rejected at much higher rates; across reasoning benchmarks, suffix-task rejection rises from below \\textbf{10\\%} to as high as \\textbf{90\\%}. \\emph{Token trigger bias} occurs when strings common in attack data raise rejection probability even in benign contexts; inserting a single trigger token increases false refusals by up to \\textbf{50\\%}. \\emph{Topic generalization bias} reflects poor generalization beyond the defense data distribution, with defended models suffering test-time accuracy drops of up to \\textbf{40\\%}. These findings suggest that current prompt-injection defenses frequently respond to attack-like surface patterns rather than the underlying intent. We introduce controlled diagnostic datasets and a systematic evaluation across two base models and multiple defense pipelines, highlighting limitations of supervised fine-tuning for reliable LLM security.",
      "publishedDate": "2026-01-12T04:12:48Z",
      "updatedDate": "2026-01-12T04:12:48Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07185v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07185",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07182",
      "title": "PRPO: Aligning Process Reward with Outcome Reward in Policy Optimization",
      "authors": [
        {
          "name": "Ruiyi Ding",
          "affiliation": null
        },
        {
          "name": "Yongxuan Lv",
          "affiliation": null
        },
        {
          "name": "Xianhui Meng",
          "affiliation": null
        },
        {
          "name": "Jiahe Song",
          "affiliation": null
        },
        {
          "name": "Chao Wang",
          "affiliation": null
        },
        {
          "name": "Chen Jiang",
          "affiliation": null
        },
        {
          "name": "Yuan Cheng",
          "affiliation": null
        }
      ],
      "abstract": "Policy optimization for large language models often suffers from sparse reward signals in multi-step reasoning tasks. Critic-free methods like GRPO assign a single normalized outcome reward to all tokens, providing limited guidance for intermediate reasoning . While Process Reward Models (PRMs) offer dense feedback, they risk premature collapse when used alone, as early low-reward tokens can drive policies toward truncated outputs. We introduce Process Relative Policy Optimization (PRPO), which combines outcome reliability with process-level guidance in a critic-free framework. PRPO segments reasoning sequences based on semantic clues, normalizes PRM scores into token-level advantages, and aligns their distribution with outcome advantages through location-parameter shift. On MATH500, PRPO improves Qwen2.5-Math-1.5B accuracy from 61.2% to 64.4% over GRPO using only eight rollouts and no value network, demonstrating efficient fine-grained credit assignment within critic-free optimization. Code is available at: https://github.com/SchumiDing/srpocode",
      "publishedDate": "2026-01-12T04:04:43Z",
      "updatedDate": "2026-01-13T07:35:53Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07182v2",
      "arxivUrl": "https://arxiv.org/abs/2601.07182",
      "comment": "8 pages, 2 figures Code is available at: https://github.com/SchumiDing/srpocode",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07180",
      "title": "Structured Reasoning for Large Language Models",
      "authors": [
        {
          "name": "Jinyi Han",
          "affiliation": null
        },
        {
          "name": "Zixiang Di",
          "affiliation": null
        },
        {
          "name": "Zishang Jiang",
          "affiliation": null
        },
        {
          "name": "Ying Liao",
          "affiliation": null
        },
        {
          "name": "Jiaqing Liang",
          "affiliation": null
        },
        {
          "name": "Yongqi Wang",
          "affiliation": null
        },
        {
          "name": "Yanghua Xiao",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) achieve strong performance by generating long chains of thought, but longer traces always introduce redundant or ineffective reasoning steps. One typical behavior is that they often perform unnecessary verification and revisions even if they have reached the correct answers. This limitation stems from the unstructured nature of reasoning trajectories and the lack of targeted supervision for critical reasoning abilities. To address this, we propose Structured Reasoning (SCR), a framework that decouples reasoning trajectories into explicit, evaluable, and trainable components. We mainly implement SCR using a Generate-Verify-Revise paradigm. Specifically, we construct structured training data and apply Dynamic Termination Supervision to guide the model in deciding when to terminate reasoning. To avoid interference between learning signals for different reasoning abilities, we adopt a progressive two-stage reinforcement learning strategy: the first stage targets initial generation and self-verification, and the second stage focuses on revision. Extensive experiments on three backbone models show that SCR substantially improves reasoning efficiency and self-verification. Besides, compared with existing reasoning paradigms, it reduces output token length by up to 50%.",
      "publishedDate": "2026-01-12T04:04:01Z",
      "updatedDate": "2026-01-12T04:04:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07180v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07180",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07160",
      "title": "AscendKernelGen: A Systematic Study of LLM-Based Kernel Generation for Neural Processing Units",
      "authors": [
        {
          "name": "Xinzi Cao",
          "affiliation": null
        },
        {
          "name": "Jianyang Zhai",
          "affiliation": null
        },
        {
          "name": "Pengfei Li",
          "affiliation": null
        },
        {
          "name": "Zhiheng Hu",
          "affiliation": null
        },
        {
          "name": "Cen Yan",
          "affiliation": null
        },
        {
          "name": "Bingxu Mu",
          "affiliation": null
        },
        {
          "name": "Guanghuan Fang",
          "affiliation": null
        },
        {
          "name": "Bin She",
          "affiliation": null
        },
        {
          "name": "Jiayu Li",
          "affiliation": null
        },
        {
          "name": "Yihan Su",
          "affiliation": null
        },
        {
          "name": "Dongyang Tao",
          "affiliation": null
        },
        {
          "name": "Xiansong Huang",
          "affiliation": null
        },
        {
          "name": "Fan Xu",
          "affiliation": null
        },
        {
          "name": "Feidiao Yang",
          "affiliation": null
        },
        {
          "name": "Yao Lu",
          "affiliation": null
        },
        {
          "name": "Chang-Dong Wang",
          "affiliation": null
        },
        {
          "name": "Yutong Lu",
          "affiliation": null
        },
        {
          "name": "Weicheng Xue",
          "affiliation": null
        },
        {
          "name": "Bin Zhou",
          "affiliation": null
        },
        {
          "name": "Yonghong Tian",
          "affiliation": null
        }
      ],
      "abstract": "To meet the ever-increasing demand for computational efficiency, Neural Processing Units (NPUs) have become critical in modern AI infrastructure. However, unlocking their full potential requires developing high-performance compute kernels using vendor-specific Domain-Specific Languages (DSLs), a task that demands deep hardware expertise and is labor-intensive. While Large Language Models (LLMs) have shown promise in general code generation, they struggle with the strict constraints and scarcity of training data in the NPU domain. Our preliminary study reveals that state-of-the-art general-purpose LLMs fail to generate functional complex kernels for Ascend NPUs, yielding a near-zero success rate. To address these challenges, we propose AscendKernelGen, a generation-evaluation integrated framework for NPU kernel development. We introduce Ascend-CoT, a high-quality dataset incorporating chain-of-thought reasoning derived from real-world kernel implementations, and KernelGen-LM, a domain-adaptive model trained via supervised fine-tuning and reinforcement learning with execution feedback. Furthermore, we design NPUKernelBench, a comprehensive benchmark for assessing compilation, correctness, and performance across varying complexity levels. Experimental results demonstrate that our approach significantly bridges the gap between general LLMs and hardware-specific coding. Specifically, the compilation success rate on complex Level-2 kernels improves from 0% to 95.5% (Pass@10), while functional correctness achieves 64.3% compared to the baseline's complete failure. These results highlight the critical role of domain-specific reasoning and rigorous evaluation in automating accelerator-aware code generation.",
      "publishedDate": "2026-01-12T03:12:58Z",
      "updatedDate": "2026-01-12T03:12:58Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07160v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07160",
      "comment": "33 pages,7 figures,16 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07155",
      "title": "Stable On-Policy Distillation through Adaptive Target Reformulation",
      "authors": [
        {
          "name": "Ijun Jang",
          "affiliation": null
        },
        {
          "name": "Jewon Yeom",
          "affiliation": null
        },
        {
          "name": "Juan Yeo",
          "affiliation": null
        },
        {
          "name": "Hyunggu Lim",
          "affiliation": null
        },
        {
          "name": "Taesup Kim",
          "affiliation": null
        }
      ],
      "abstract": "Knowledge distillation (KD) is a widely adopted technique for transferring knowledge from large language models to smaller student models; however, conventional supervised KD often suffers from a distribution mismatch between training and inference. While on-policy KD approaches attempt to mitigate this issue by learning directly from student-generated outputs, they frequently encounter training instabilities because the distributional gap between the novice student and the expert teacher is often too wide to bridge directly. These challenges manifest as pathological gradients in forward KL objectives or diversity collapse in reverse KL regimes. To address these limitations, we propose Veto, an objective-level reformulation that constructs a geometric bridge in the logit space. Unlike prior methods that mix data samples, Veto creates an intermediate target distribution that promotes alignment between the teacher and the student. By introducing a tunable parameter beta, Veto serves as an Adaptive Gradient Veto that stabilizes optimization by suppressing harmful gradients on low-confidence tokens, while simultaneously acting as a Decisiveness Knob to balance reward-driven performance with output diversity. Extensive experiments across various reasoning and generation tasks demonstrate that Veto consistently outperforms supervised fine-tuning and existing on-policy baselines.",
      "publishedDate": "2026-01-12T02:57:39Z",
      "updatedDate": "2026-01-12T02:57:39Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07155v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07155",
      "comment": "10 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07153",
      "title": "Can Large Language Models Understand, Reason About, and Generate Code-Switched Text?",
      "authors": [
        {
          "name": "Genta Indra Winata",
          "affiliation": null
        },
        {
          "name": "David Anugraha",
          "affiliation": null
        },
        {
          "name": "Patrick Amadeus Irawan",
          "affiliation": null
        },
        {
          "name": "Anirban Das",
          "affiliation": null
        },
        {
          "name": "Haneul Yoo",
          "affiliation": null
        },
        {
          "name": "Paresh Dashore",
          "affiliation": null
        },
        {
          "name": "Shreyas Kulkarni",
          "affiliation": null
        },
        {
          "name": "Ruochen Zhang",
          "affiliation": null
        },
        {
          "name": "Haruki Sakajo",
          "affiliation": null
        },
        {
          "name": "Frederikus Hudi",
          "affiliation": null
        },
        {
          "name": "Anaelia Ovalle",
          "affiliation": null
        },
        {
          "name": "Syrielle Montariol",
          "affiliation": null
        },
        {
          "name": "Felix Gaschi",
          "affiliation": null
        },
        {
          "name": "Michael Anugraha",
          "affiliation": null
        },
        {
          "name": "Rutuj Ravindra Puranik",
          "affiliation": null
        },
        {
          "name": "Zawad Hayat Ahmed",
          "affiliation": null
        },
        {
          "name": "Adril Putra Merin",
          "affiliation": null
        },
        {
          "name": "Emmanuele Chersoni",
          "affiliation": null
        }
      ],
      "abstract": "Code-switching is a pervasive phenomenon in multilingual communication, yet the robustness of large language models (LLMs) in mixed-language settings remains insufficiently understood. In this work, we present a comprehensive evaluation of LLM capabilities in understanding, reasoning over, and generating code-switched text. We introduce CodeMixQA a novel benchmark with high-quality human annotations, comprising 16 diverse parallel code-switched language-pair variants that span multiple geographic regions and code-switching patterns, and include both original scripts and their transliterated forms. Using this benchmark, we analyze the reasoning behavior of LLMs on code-switched question-answering tasks, shedding light on how models process and reason over mixed-language inputs. We further conduct a systematic evaluation of LLM-generated synthetic code-switched text, focusing on both naturalness and semantic fidelity, and uncover key limitations in current generation capabilities. Our findings reveal persistent challenges in both reasoning and generation under code-switching conditions and provide actionable insights for building more robust multilingual LLMs. We release the dataset and code as open source.",
      "publishedDate": "2026-01-12T02:52:38Z",
      "updatedDate": "2026-01-12T02:52:38Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07153v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07153",
      "comment": "Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07152",
      "title": "Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation (Extended Version)",
      "authors": [
        {
          "name": "Aja Khanal",
          "affiliation": null
        },
        {
          "name": "Kaushik T. Ranade",
          "affiliation": null
        },
        {
          "name": "Rishabh Agrawal",
          "affiliation": null
        },
        {
          "name": "Kalyan S. Basu",
          "affiliation": null
        },
        {
          "name": "Apurva Narayan",
          "affiliation": null
        }
      ],
      "abstract": "Generating high-quality structured data such as JSON records, remains a fundamental challenge for large language models (LLMs), particularly when semantic richness must coexist with strict schema adherence. While autoregressive LLMs offer strong structural consistency, they often struggle with semantic variation and output diversity. In contrast, diffusion language models (DLMs) introduce powerful mechanisms for semantic richness and bidirectional decoding, yet lack the inductive biases needed for reliable structure preservation. We present Agents of Diffusion (AoD), a novel framework that unifies the generative flexibility of DLMs with the reasoning capabilities of autoregressive models through language-mediated reinforcement learning. AoD frames structured text generation as a multi-agent alignment process, where a prompt optimization agent collaborates with a judge agent to iteratively guide a DLM using natural language feedback. This approach enables controllable, schema-consistent generation without modifying model parameters or relying on handcrafted constraints. AoD advances the state of controllable generation by demonstrating that diffusion models, when supervised by cooperative agents, can achieve both high semantic novelty and structural fidelity. Across multiple structured data benchmarks, AoD consistently outperforms diffusion and autoregressive baselines, establishing a new path forward for structure-aware, diversity-enhanced text synthesis.",
      "publishedDate": "2026-01-12T02:49:09Z",
      "updatedDate": "2026-01-12T02:49:09Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07152v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07152",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07149",
      "title": "Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling",
      "authors": [
        {
          "name": "Zhaoyan Li",
          "affiliation": null
        },
        {
          "name": "Hang Lei",
          "affiliation": null
        },
        {
          "name": "Yujia Wang",
          "affiliation": null
        },
        {
          "name": "Lanbo Liu",
          "affiliation": null
        },
        {
          "name": "Hao Liu",
          "affiliation": null
        },
        {
          "name": "Liang Yu",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) can generate fluent text, producing high-quality creative stories remains challenging. Reinforcement Learning (RL) offers a promising solution but faces two critical obstacles: designing reliable reward signals for subjective storytelling quality and mitigating training instability. This paper introduces the Reinforcement Learning for Creative Storytelling (RLCS) framework to systematically address both challenges. First, we develop a Generative Reward Model (GenRM) that provides multi-dimensional analysis and explicit reasoning about story preferences, trained through supervised fine-tuning on demonstrations with reasoning chains distilled from strong teacher models, followed by GRPO-based refinement on expanded preference data. Second, we introduce an entropy-based reward shaping strategy that dynamically prioritizes learning on confident errors and uncertain correct predictions, preventing overfitting on already-mastered patterns. Experiments demonstrate that GenRM achieves 68\\% alignment with human creativity judgments, and RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality. This work provides a practical pipeline for applying RL to creative domains, effectively navigating the dual challenges of reward modeling and training stability.",
      "publishedDate": "2026-01-12T02:39:47Z",
      "updatedDate": "2026-01-12T02:39:47Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07149v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07149",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07123",
      "title": "ENTRA: Entropy-Based Redundancy Avoidance in Large Language Model Reasoning",
      "authors": [
        {
          "name": "Ruichu Cai",
          "affiliation": null
        },
        {
          "name": "Haopeng Du",
          "affiliation": null
        },
        {
          "name": "Qingwen Lin",
          "affiliation": null
        },
        {
          "name": "Yutong Chen",
          "affiliation": null
        },
        {
          "name": "Zijian Li",
          "affiliation": null
        },
        {
          "name": "Boyan Xu",
          "affiliation": null
        }
      ],
      "abstract": "Large Reasoning Models (LRMs) often suffer from overthinking, generating unnecessarily long reasoning chains even for simple tasks. This leads to substantial computational overhead with limited performance gain, primarily due to redundant verification and repetitive generation. While prior work typically constrains output length or optimizes correctness, such coarse supervision fails to guide models toward concise yet accurate inference. In this paper, we propose ENTRA, an entropy-based training framework that suppresses redundant reasoning while preserving performance. ENTRA first estimates the token-level importance using a lightweight Bidirectional Importance Estimation (BIE) method, which accounts for both prediction confidence and forward influence. It then computes a redundancy reward based on the entropy of low-importance tokens, normalized by its theoretical upper bound, and optimizes this reward via reinforcement learning. Experiments on mathematical reasoning benchmarks demonstrate that ENTRA reduces output length by 37% to 53% with no loss-and in some cases, gains-in accuracy. Our approach offers a principled and efficient solution to reduce overthinking in LRMs, and provides a generalizable path toward redundancy-aware reasoning optimization.",
      "publishedDate": "2026-01-12T01:26:30Z",
      "updatedDate": "2026-01-12T01:26:30Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07123v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07123",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07107",
      "title": "MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning",
      "authors": [
        {
          "name": "Meng Lu",
          "affiliation": null
        },
        {
          "name": "Yuxing Lu",
          "affiliation": null
        },
        {
          "name": "Yuchen Zhuang",
          "affiliation": null
        },
        {
          "name": "Megan Mullins",
          "affiliation": null
        },
        {
          "name": "Yang Xie",
          "affiliation": null
        },
        {
          "name": "Guanghua Xiao",
          "affiliation": null
        },
        {
          "name": "Charles Fleming",
          "affiliation": null
        },
        {
          "name": "Wenqi Shi",
          "affiliation": null
        },
        {
          "name": "Xuan Wang",
          "affiliation": null
        }
      ],
      "abstract": "Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.",
      "publishedDate": "2026-01-12T00:11:10Z",
      "updatedDate": "2026-01-12T00:11:10Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07107v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07107",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "agents",
        "tool-use",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "agents",
          "tool-use",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07085",
      "title": "The AI Cognitive Trojan Horse: How Large Language Models May Bypass Human Epistemic Vigilance",
      "authors": [
        {
          "name": "Andrew D. Maynard",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM)-based conversational AI systems present a challenge to human cognition that current frameworks for understanding misinformation and persuasion do not adequately address. This paper proposes that a significant epistemic risk from conversational AI may lie not in inaccuracy or intentional deception, but in something more fundamental: these systems may be configured, through optimization processes that make them useful, to present characteristics that bypass the cognitive mechanisms humans evolved to evaluate incoming information. The Cognitive Trojan Horse hypothesis draws on Sperber and colleagues' theory of epistemic vigilance -- the parallel cognitive process monitoring communicated information for reasons to doubt -- and proposes that LLM-based systems present 'honest non-signals': genuine characteristics (fluency, helpfulness, apparent disinterest) that fail to carry the information equivalent human characteristics would carry, because in humans these are costly to produce while in LLMs they are computationally trivial. Four mechanisms of potential bypass are identified: processing fluency decoupled from understanding, trust-competence presentation without corresponding stakes, cognitive offloading that delegates evaluation itself to the AI, and optimization dynamics that systematically produce sycophancy. The framework generates testable predictions, including a counterintuitive speculation that cognitively sophisticated users may be more vulnerable to AI-mediated epistemic influence. This reframes AI safety as partly a problem of calibration -- aligning human evaluative responses with the actual epistemic status of AI-generated content -- rather than solely a problem of preventing deception.",
      "publishedDate": "2026-01-11T22:28:56Z",
      "updatedDate": "2026-01-11T22:28:56Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07085v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07085",
      "comment": "15 pages, 18 references",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07055",
      "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
      "authors": [
        {
          "name": "Zhenrui Yue",
          "affiliation": null
        },
        {
          "name": "Kartikeya Upasani",
          "affiliation": null
        },
        {
          "name": "Xianjun Yang",
          "affiliation": null
        },
        {
          "name": "Suyu Ge",
          "affiliation": null
        },
        {
          "name": "Shaoliang Nie",
          "affiliation": null
        },
        {
          "name": "Yuning Mao",
          "affiliation": null
        },
        {
          "name": "Zhe Liu",
          "affiliation": null
        },
        {
          "name": "Dong Wang",
          "affiliation": null
        }
      ],
      "abstract": "As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.",
      "publishedDate": "2026-01-11T20:27:55Z",
      "updatedDate": "2026-01-11T20:27:55Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07055v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07055",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07054",
      "title": "Fine-Tuning vs. RAG for Multi-Hop Question Answering with Novel Knowledge",
      "authors": [
        {
          "name": "Zhuoyi Yang",
          "affiliation": null
        },
        {
          "name": "Yurun Song",
          "affiliation": null
        },
        {
          "name": "Iftekhar Ahmed",
          "affiliation": null
        },
        {
          "name": "Ian Harris",
          "affiliation": null
        }
      ],
      "abstract": "Multi-hop question answering is widely used to evaluate the reasoning capabilities of large language models (LLMs), as it requires integrating multiple pieces of supporting knowledge to arrive at a correct answer. While prior work has explored different mechanisms for providing knowledge to LLMs, such as finetuning and retrieval-augmented generation (RAG), their relative effectiveness for multi-hop question answering remains insufficiently understood, particularly when the required knowledge is temporally novel. In this paper, we systematically compare parametric and non-parametric knowledge injection methods for open-domain multi-hop question answering. We evaluate unsupervised fine-tuning (continual pretraining), supervised fine-tuning, and retrieval-augmented generation across three 7B-parameter open-source LLMs. Experiments are conducted on two benchmarks: QASC, a standard multi-hop science question answering dataset, and a newly constructed dataset of over 10,000 multi-hop questions derived from Wikipedia events in 2024, designed to test knowledge beyond the models' pretraining cutoff. Our results show that unsupervised fine-tuning provides only limited gains over base models, suggesting that continual pretraining alone is insufficient for improving multi-hop reasoning accuracy. In contrast, retrieval-augmented generation yields substantial and consistent improvements, particularly when answering questions that rely on temporally novel information. Supervised fine-tuning achieves the highest overall accuracy across models and datasets. These findings highlight fundamental differences in how knowledge injection mechanisms support multi-hop question answering and underscore the importance of retrieval-based methods when external or compositional knowledge is required.",
      "publishedDate": "2026-01-11T20:24:25Z",
      "updatedDate": "2026-01-11T20:24:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07054v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07054",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07041",
      "title": "When Abundance Conceals Weakness: Knowledge Conflict in Multilingual Models",
      "authors": [
        {
          "name": "Jiaqi Zhao",
          "affiliation": null
        },
        {
          "name": "Qiang Huang",
          "affiliation": null
        },
        {
          "name": "Haodong Chen",
          "affiliation": null
        },
        {
          "name": "Xiaoxing You",
          "affiliation": null
        },
        {
          "name": "Jun Yu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) encode vast world knowledge across multiple languages, yet their internal beliefs are often unevenly distributed across linguistic spaces. When external evidence contradicts these language-dependent memories, models encounter \\emph{cross-lingual knowledge conflict}, a phenomenon largely unexplored beyond English-centric settings. We introduce \\textbf{CLEAR}, a \\textbf{C}ross-\\textbf{L}ingual knowl\\textbf{E}dge conflict ev\\textbf{A}luation f\\textbf{R}amework that systematically examines how multilingual LLMs reconcile conflicting internal beliefs and multilingual external evidence. CLEAR decomposes conflict resolution into four progressive scenarios, from multilingual parametric elicitation to competitive multi-source cross-lingual induction, and systematically evaluates model behavior across two complementary QA benchmarks with distinct task characteristics. We construct multilingual versions of ConflictQA and ConflictingQA covering 10 typologically diverse languages and evaluate six representative LLMs. Our experiments reveal a task-dependent decision dichotomy. In reasoning-intensive tasks, conflict resolution is dominated by language resource abundance, with high-resource languages exerting stronger persuasive power. In contrast, for entity-centric factual conflicts, linguistic affinity, not resource scale, becomes decisive, allowing low-resource but linguistically aligned languages to outperform distant high-resource ones.",
      "publishedDate": "2026-01-11T19:26:59Z",
      "updatedDate": "2026-01-11T19:26:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07041v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07041",
      "comment": "14 pages, 7 figures, and 4 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07036",
      "title": "Mid-Think: Training-Free Intermediate-Budget Reasoning via Token-Level Triggers",
      "authors": [
        {
          "name": "Wang Yang",
          "affiliation": null
        },
        {
          "name": "Debargha Ganguly",
          "affiliation": null
        },
        {
          "name": "Xinpeng Li",
          "affiliation": null
        },
        {
          "name": "Chaoda Song",
          "affiliation": null
        },
        {
          "name": "Shouren Wang",
          "affiliation": null
        },
        {
          "name": "Vikash Singh",
          "affiliation": null
        },
        {
          "name": "Vipin Chaudhary",
          "affiliation": null
        },
        {
          "name": "Xiaotian Han",
          "affiliation": null
        }
      ],
      "abstract": "Hybrid reasoning language models are commonly controlled through high-level Think/No-think instructions to regulate reasoning behavior, yet we found that such mode switching is largely driven by a small set of trigger tokens rather than the instructions themselves. Through attention analysis and controlled prompting experiments, we show that a leading ``Okay'' token induces reasoning behavior, while the newline pattern following ``</think>'' suppresses it. Based on this observation, we propose Mid-Think, a simple training-free prompting format that combines these triggers to achieve intermediate-budget reasoning, consistently outperforming fixed-token and prompt-based baselines in terms of the accuracy-length trade-off. Furthermore, applying Mid-Think to RL training after SFT reduces training time by approximately 15% while improving final performance of Qwen3-8B on AIME from 69.8% to 72.4% and on GPQA from 58.5% to 61.1%, demonstrating its effectiveness for both inference-time control and RL-based reasoning training.",
      "publishedDate": "2026-01-11T19:19:39Z",
      "updatedDate": "2026-01-11T19:19:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07036v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07036",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07022",
      "title": "Solar Open Technical Report",
      "authors": [
        {
          "name": "Sungrae Park",
          "affiliation": null
        },
        {
          "name": "Sanghoon Kim",
          "affiliation": null
        },
        {
          "name": "Jungho Cho",
          "affiliation": null
        },
        {
          "name": "Gyoungjin Gim",
          "affiliation": null
        },
        {
          "name": "Dawoon Jung",
          "affiliation": null
        },
        {
          "name": "Mikyoung Cha",
          "affiliation": null
        },
        {
          "name": "Eunhae Choo",
          "affiliation": null
        },
        {
          "name": "Taekgyu Hong",
          "affiliation": null
        },
        {
          "name": "Minbyul Jeong",
          "affiliation": null
        },
        {
          "name": "SeHwan Joo",
          "affiliation": null
        },
        {
          "name": "Minsoo Khang",
          "affiliation": null
        },
        {
          "name": "Eunwon Kim",
          "affiliation": null
        },
        {
          "name": "Minjeong Kim",
          "affiliation": null
        },
        {
          "name": "Sujeong Kim",
          "affiliation": null
        },
        {
          "name": "Yunsu Kim",
          "affiliation": null
        },
        {
          "name": "Hyeonju Lee",
          "affiliation": null
        },
        {
          "name": "Seunghyun Lee",
          "affiliation": null
        },
        {
          "name": "Sukyung Lee",
          "affiliation": null
        },
        {
          "name": "Siyoung Park",
          "affiliation": null
        },
        {
          "name": "Gyungin Shin",
          "affiliation": null
        },
        {
          "name": "Inseo Song",
          "affiliation": null
        },
        {
          "name": "Wonho Song",
          "affiliation": null
        },
        {
          "name": "Seonghoon Yang",
          "affiliation": null
        },
        {
          "name": "Seungyoun Yi",
          "affiliation": null
        },
        {
          "name": "Sanghoon Yoon",
          "affiliation": null
        },
        {
          "name": "Jeonghyun Ko",
          "affiliation": null
        },
        {
          "name": "Seyoung Song",
          "affiliation": null
        },
        {
          "name": "Keunwoo Choi",
          "affiliation": null
        },
        {
          "name": "Hwalsuk Lee",
          "affiliation": null
        },
        {
          "name": "Sunghun Kim",
          "affiliation": null
        },
        {
          "name": "Du-Seong Chang",
          "affiliation": null
        },
        {
          "name": "Kyunghyun Cho",
          "affiliation": null
        },
        {
          "name": "Junsuk Choe",
          "affiliation": null
        },
        {
          "name": "Hwaran Lee",
          "affiliation": null
        },
        {
          "name": "Jae-Gil Lee",
          "affiliation": null
        },
        {
          "name": "KyungTae Lim",
          "affiliation": null
        },
        {
          "name": "Alice Oh",
          "affiliation": null
        }
      ],
      "abstract": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.",
      "publishedDate": "2026-01-11T18:33:09Z",
      "updatedDate": "2026-01-11T18:33:09Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07022v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07022",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07020",
      "title": "TurkBench: A Benchmark for Evaluating Turkish Large Language Models",
      "authors": [
        {
          "name": "Çağrı Toraman",
          "affiliation": null
        },
        {
          "name": "Ahmet Kaan Sever",
          "affiliation": null
        },
        {
          "name": "Ayse Aysu Cengiz",
          "affiliation": null
        },
        {
          "name": "Elif Ecem Arslan",
          "affiliation": null
        },
        {
          "name": "Görkem Sevinç",
          "affiliation": null
        },
        {
          "name": "Mete Mert Birdal",
          "affiliation": null
        },
        {
          "name": "Yusuf Faruk Güldemir",
          "affiliation": null
        },
        {
          "name": "Ali Buğra Kanburoğlu",
          "affiliation": null
        },
        {
          "name": "Sezen Felekoğlu",
          "affiliation": null
        },
        {
          "name": "Osman Gürlek",
          "affiliation": null
        },
        {
          "name": "Sarp Kantar",
          "affiliation": null
        },
        {
          "name": "Birsen Şahin Kütük",
          "affiliation": null
        },
        {
          "name": "Büşra Tufan",
          "affiliation": null
        },
        {
          "name": "Elif Genç",
          "affiliation": null
        },
        {
          "name": "Serkan Coşkun",
          "affiliation": null
        },
        {
          "name": "Gupse Ekin Demir",
          "affiliation": null
        },
        {
          "name": "Muhammed Emin Arayıcı",
          "affiliation": null
        },
        {
          "name": "Olgun Dursun",
          "affiliation": null
        },
        {
          "name": "Onur Gungor",
          "affiliation": null
        },
        {
          "name": "Susan Üsküdarlı",
          "affiliation": null
        },
        {
          "name": "Abdullah Topraksoy",
          "affiliation": null
        },
        {
          "name": "Esra Darıcı",
          "affiliation": null
        }
      ],
      "abstract": "With the recent surge in the development of large language models, the need for comprehensive and language-specific evaluation benchmarks has become critical. While significant progress has been made in evaluating English language models, benchmarks for other languages, particularly those with unique linguistic characteristics such as Turkish, remain less developed. Our study introduces TurkBench, a comprehensive benchmark designed to assess the capabilities of generative large language models in the Turkish language. TurkBench involves 8,151 data samples across 21 distinct subtasks. These are organized under six main categories of evaluation: Knowledge, Language Understanding, Reasoning, Content Moderation, Turkish Grammar and Vocabulary, and Instruction Following. The diverse range of tasks and the culturally relevant data would provide researchers and developers with a valuable tool for evaluating their models and identifying areas for improvement. We further publish our benchmark for online submissions at https://huggingface.co/turkbench",
      "publishedDate": "2026-01-11T18:28:23Z",
      "updatedDate": "2026-01-11T18:28:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07020v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07020",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07019",
      "title": "Zer0n: An AI-Assisted Vulnerability Discovery and Blockchain-Backed Integrity Framework",
      "authors": [
        {
          "name": "Harshil Parmar",
          "affiliation": null
        },
        {
          "name": "Pushti Vyas",
          "affiliation": null
        },
        {
          "name": "Prayers Khristi",
          "affiliation": null
        },
        {
          "name": "Priyank Panchal",
          "affiliation": null
        }
      ],
      "abstract": "As vulnerability research increasingly adopts generative AI, a critical reliance on opaque model outputs has emerged, creating a \"trust gap\" in security automation. We address this by introducing Zer0n, a framework that anchors the reasoning capabilities of Large Language Models (LLMs) to the immutable audit trails of blockchain technology. Specifically, we integrate Gemini 2.0 Pro for logic-based vulnerability detection with the Avalanche C-Chain for tamper-evident artifact logging. Unlike fully decentralized solutions that suffer from high latency, Zer0n employs a hybrid architecture: execution remains off-chain for performance, while integrity proofs are finalized on-chain. Our evaluation on a dataset of 500 endpoints reveals that this approach achieves 80% detection accuracy with only a marginal 22.9% overhead, effectively demonstrating that decentralized integrity can coexist with high-speed security workflows.",
      "publishedDate": "2026-01-11T18:27:52Z",
      "updatedDate": "2026-01-11T18:27:52Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07019v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07019",
      "comment": "10 pages, 3 figures, 7 tables. Framework for AI-Assisted Vulnerability Discovery",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06993",
      "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
      "authors": [
        {
          "name": "Jie Zhu",
          "affiliation": null
        },
        {
          "name": "Yiyang Su",
          "affiliation": null
        },
        {
          "name": "Xiaoming Liu",
          "affiliation": null
        }
      ],
      "abstract": "Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \\alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \\alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at \\href{https://github.com/jiezhu23/ReFine-RFT}{Project Link}.",
      "publishedDate": "2026-01-11T17:07:47Z",
      "updatedDate": "2026-01-11T17:07:47Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06993v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06993",
      "comment": "10 pages, 8 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06944",
      "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
      "authors": [
        {
          "name": "Yuhang Su",
          "affiliation": null
        },
        {
          "name": "Mei Wang",
          "affiliation": null
        },
        {
          "name": "Yaoyao Zhong",
          "affiliation": null
        },
        {
          "name": "Guozhang Li",
          "affiliation": null
        },
        {
          "name": "Shixing Li",
          "affiliation": null
        },
        {
          "name": "Yihan Feng",
          "affiliation": null
        },
        {
          "name": "Hua Huang",
          "affiliation": null
        }
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.",
      "publishedDate": "2026-01-11T15:08:05Z",
      "updatedDate": "2026-01-11T15:08:05Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06944v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06944",
      "comment": "8 pages for the main text (excluding references and the limitations section); 37 pages in total including appendices",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06943",
      "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
      "authors": [
        {
          "name": "Chengwen Liu",
          "affiliation": null
        },
        {
          "name": "Xiaomin Yu",
          "affiliation": null
        },
        {
          "name": "Zhuoyue Chang",
          "affiliation": null
        },
        {
          "name": "Zhe Huang",
          "affiliation": null
        },
        {
          "name": "Shuo Zhang",
          "affiliation": null
        },
        {
          "name": "Heng Lian",
          "affiliation": null
        },
        {
          "name": "Kunyi Wang",
          "affiliation": null
        },
        {
          "name": "Rui Xu",
          "affiliation": null
        },
        {
          "name": "Sen Hu",
          "affiliation": null
        },
        {
          "name": "Jianheng Hou",
          "affiliation": null
        },
        {
          "name": "Hao Peng",
          "affiliation": null
        },
        {
          "name": "Chengwei Qin",
          "affiliation": null
        },
        {
          "name": "Xiaobin Hu",
          "affiliation": null
        },
        {
          "name": "Hong Peng",
          "affiliation": null
        },
        {
          "name": "Ronghao Chen",
          "affiliation": null
        },
        {
          "name": "Huacan Wang",
          "affiliation": null
        }
      ],
      "abstract": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.",
      "publishedDate": "2026-01-11T15:07:37Z",
      "updatedDate": "2026-01-11T15:07:37Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06943v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06943",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06937",
      "title": "mind_call: A Dataset for Mental Health Function Calling with Large Language Models",
      "authors": [
        {
          "name": "Fozle Rabbi Shafi",
          "affiliation": null
        },
        {
          "name": "M. Anwar Hossain",
          "affiliation": null
        },
        {
          "name": "Salimur Choudhury",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM)-based systems increasingly rely on function calling to enable structured and controllable interaction with external data sources, yet existing datasets do not address mental health-oriented access to wearable sensor data. This paper presents a synthetic function-calling dataset designed for mental health assistance grounded in wearable health signals such as sleep, physical activity, cardiovascular measures, stress indicators, and metabolic data. The dataset maps diverse natural language queries to standardized API calls derived from a widely adopted health data schema. Each sample includes a user query, a query category, an explicit reasoning step, a normalized temporal parameter, and a target function. The dataset covers explicit, implicit, behavioral, symptom-based, and metaphorical expressions, which reflect realistic mental health-related user interactions. This resource supports research on intent grounding, temporal reasoning, and reliable function invocation in LLM-based mental health agents and is publicly released to promote reproducibility and future work.",
      "publishedDate": "2026-01-11T14:52:57Z",
      "updatedDate": "2026-01-11T14:52:57Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06937v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06937",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "tool-use",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06914",
      "title": "Towards Compositional Generalization in LLMs for Smart Contract Security: A Case Study on Reentrancy Vulnerabilities",
      "authors": [
        {
          "name": "Ying Zhou",
          "affiliation": null
        },
        {
          "name": "Jiacheng Wei",
          "affiliation": null
        },
        {
          "name": "Yu Qi",
          "affiliation": null
        },
        {
          "name": "Faguo Wu",
          "affiliation": null
        },
        {
          "name": "Xiao Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) demonstrate remarkable capabilities in natural language understanding and generation. Despite being trained on large-scale, high-quality data, LLMs still fail to outperform traditional static analysis tools in specialized domains like smart contract vulnerability detection. To address this issue, this paper proposes a post-training algorithm based on atomic task decomposition and fusion. This algorithm aims to achieve combinatorial generalization under limited data by decomposing complex reasoning tasks. Specifically, we decompose the reentrancy vulnerability detection task into four linearly independent atomic tasks: identifying external calls, identifying state updates, identifying data dependencies between external calls and state updates, and determining their data flow order. These tasks form the core components of our approach. By training on synthetic datasets, we generate three compiler-verified datasets. We then employ the Slither tool to extract structural information from the control flow graph and data flow graph, which is used to fine-tune the LLM's adapter. Experimental results demonstrate that low-rank normalization fusion with the LoRA adapter improves the LLM's reentrancy vulnerability detection accuracy to 98.2%, surpassing state-of-the-art methods. On 31 real-world contracts, the algorithm achieves a 20% higher recall than traditional analysis tools.",
      "publishedDate": "2026-01-11T13:52:07Z",
      "updatedDate": "2026-01-11T13:52:07Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06914v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06914",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06911",
      "title": "Distributional Clarity: The Hidden Driver of RL-Friendliness in Large Language Models",
      "authors": [
        {
          "name": "Shaoning Sun",
          "affiliation": null
        },
        {
          "name": "Mingzhu Cai",
          "affiliation": null
        },
        {
          "name": "Huang He",
          "affiliation": null
        },
        {
          "name": "Bingjin Chen",
          "affiliation": null
        },
        {
          "name": "Siqi Bao",
          "affiliation": null
        },
        {
          "name": "Yujiu Yang",
          "affiliation": null
        },
        {
          "name": "Hua Wu",
          "affiliation": null
        },
        {
          "name": "Haifeng Wang",
          "affiliation": null
        }
      ],
      "abstract": "Language model families exhibit striking disparity in their capacity to benefit from reinforcement learning: under identical training, models like Qwen achieve substantial gains, while others like Llama yield limited improvements. Complementing data-centric approaches, we reveal that this disparity reflects a hidden structural property: \\textbf{distributional clarity} in probability space. Through a three-stage analysis-from phenomenon to mechanism to interpretation-we uncover that RL-friendly models exhibit intra-class compactness and inter-class separation in their probability assignments to correct vs. incorrect responses. We quantify this clarity using the \\textbf{Silhouette Coefficient} ($S$) and demonstrate that (1) high $S$ correlates strongly with RL performance; (2) low $S$ is associated with severe logic errors and reasoning instability. To confirm this property, we introduce a Silhouette-Aware Reweighting strategy that prioritizes low-$S$ samples during training. Experiments across six mathematical benchmarks show consistent improvements across all model families, with gains up to 5.9 points on AIME24. Our work establishes distributional clarity as a fundamental, trainable property underlying RL-Friendliness.",
      "publishedDate": "2026-01-11T13:34:44Z",
      "updatedDate": "2026-01-11T13:34:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06911v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06911",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06860",
      "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
      "authors": [
        {
          "name": "Yifei Chen",
          "affiliation": null
        },
        {
          "name": "Guanting Dong",
          "affiliation": null
        },
        {
          "name": "Zhicheng Dou",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of \\ourmodel{} across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent",
      "publishedDate": "2026-01-11T11:05:26Z",
      "updatedDate": "2026-01-11T11:05:26Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06860v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06860",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06848",
      "title": "Explainable Multimodal Aspect-Based Sentiment Analysis with Dependency-guided Large Language Model",
      "authors": [
        {
          "name": "Zhongzheng Wang",
          "affiliation": null
        },
        {
          "name": "Yuanhe Tian",
          "affiliation": null
        },
        {
          "name": "Hongzhi Wang",
          "affiliation": null
        },
        {
          "name": "Yan Song",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal aspect-based sentiment analysis (MABSA) aims to identify aspect-level sentiments by jointly modeling textual and visual information, which is essential for fine-grained opinion understanding in social media. Existing approaches mainly rely on discriminative classification with complex multimodal fusion, yet lacking explicit sentiment explainability. In this paper, we reformulate MABSA as a generative and explainable task, proposing a unified framework that simultaneously predicts aspect-level sentiment and generates natural language explanations. Based on multimodal large language models (MLLMs), our approach employs a prompt-based generative paradigm, jointly producing sentiment and explanation. To further enhance aspect-oriented reasoning capabilities, we propose a dependency-syntax-guided sentiment cue strategy. This strategy prunes and textualizes the aspect-centered dependency syntax tree, guiding the model to distinguish different sentiment aspects and enhancing its explainability. To enable explainability, we use MLLMs to construct new datasets with sentiment explanations to fine-tune. Experiments show that our approach not only achieves consistent gains in sentiment classification accuracy, but also produces faithful, aspect-grounded explanations.",
      "publishedDate": "2026-01-11T10:41:33Z",
      "updatedDate": "2026-01-11T10:41:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06848v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06848",
      "comment": "9 pages, 3 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06803",
      "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
      "authors": [
        {
          "name": "Yubo Wang",
          "affiliation": null
        },
        {
          "name": "Juntian Zhang",
          "affiliation": null
        },
        {
          "name": "Yichen Wu",
          "affiliation": null
        },
        {
          "name": "Yankai Lin",
          "affiliation": null
        },
        {
          "name": "Nils Lukas",
          "affiliation": null
        },
        {
          "name": "Yuhan Liu",
          "affiliation": null
        }
      ],
      "abstract": "While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a \"Forest-before-Trees\" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.",
      "publishedDate": "2026-01-11T08:30:49Z",
      "updatedDate": "2026-01-11T08:30:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06803v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06803",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06801",
      "title": "Thinking with Deltas: Incentivizing Reinforcement Learning via Differential Visual Reasoning Policy",
      "authors": [
        {
          "name": "Shujian Gao",
          "affiliation": null
        },
        {
          "name": "Yuan Wang",
          "affiliation": null
        },
        {
          "name": "Jiangtao Yan",
          "affiliation": null
        },
        {
          "name": "Zuxuan Wu",
          "affiliation": null
        },
        {
          "name": "Yu-Gang Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced reasoning capabilities in Large Language Models. However, adapting RLVR to multimodal domains suffers from a critical \\textit{perception-reasoning decoupling}. Existing paradigms, driven by text-centric outcome rewards, reasoning in language medium, inadvertently encourage models to bypass visual perception. We empirically validate this through blind experiments: state-of-the-art policies maintain or surprisingly improve performance even when visual inputs are entirely removed. This reveals that these models degenerate into \\textit{blind reasoners}, exploiting linguistic priors to generate plausible answers instead of attending to visual evidence. In response, we propose \\textbf{Thinking with Deltas}, a framework driven by a \\textbf{Differential Visual Reasoning Policy (DVRP)}. DVRP introduces intrinsic supervision via visual triplets, comprising original, masked, and perturbed inputs. It optimizes the model to maximize reasoning divergence from masked inputs (enforcing \\textit{visual sensitivity}) while minimizing divergence from perturbed inputs (ensuring \\textit{visual robustness}). By aligning reasoning variations strictly with the \\textit{Delta} of visual information, DVRP inherently bolsters visual understanding capabilities and significantly outperforms state-of-the-art methods on both general and medical benchmarks, without requiring external annotations or auxiliary tools.",
      "publishedDate": "2026-01-11T08:25:34Z",
      "updatedDate": "2026-01-11T08:25:34Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06801v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06801",
      "comment": "24 pages, 10 tables, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06786",
      "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
      "authors": [
        {
          "name": "Jewon Yeom",
          "affiliation": null
        },
        {
          "name": "Jaewon Sok",
          "affiliation": null
        },
        {
          "name": "Seonghyeon Park",
          "affiliation": null
        },
        {
          "name": "Jeongjae Park",
          "affiliation": null
        },
        {
          "name": "Taesup Kim",
          "affiliation": null
        }
      ],
      "abstract": "Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.",
      "publishedDate": "2026-01-11T06:21:13Z",
      "updatedDate": "2026-01-11T06:21:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06786v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06786",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06779",
      "title": "CyberLLM-FINDS 2025: Instruction-Tuned Fine-tuning of Domain-Specific LLMs with Retrieval-Augmented Generation and Graph Integration for MITRE Evaluation",
      "authors": [
        {
          "name": "Vasanth Iyer",
          "affiliation": null
        },
        {
          "name": "Leonardo Bobadilla",
          "affiliation": null
        },
        {
          "name": "S. S. Iyengar",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) such as Gemma-2B have shown strong performance in various natural language processing tasks. However, general-purpose models often lack the domain expertise required for cybersecurity applications. This work presents a methodology to fine-tune the Gemma-2B model into a domain-specific cybersecurity LLM. We detail the processes of dataset preparation, fine-tuning, and synthetic data generation, along with implications for real-world applications in threat detection, forensic investigation, and attack analysis. Experiments highlight challenges in prompt length distribution during domain-specific fine-tuning. Uneven prompt lengths limit the model's effective use of the context window, constraining local inference to 200-400 tokens despite hardware support for longer sequences. Chain-of-thought styled prompts, paired with quantized weights, yielded the best performance under these constraints. To address context limitations, we employed a hybrid strategy using cloud LLMs for synthetic data generation and local fine-tuning for deployment efficiency. To extend the evaluation, we introduce a Retrieval-Augmented Generation (RAG) pipeline and graph-based reasoning framework. This approach enables structured alignment with MITRE ATT&CK techniques through STIX-based threat intelligence, enhancing recall in multi-hop and long-context scenarios. Graph modules encode entity-neighborhood context and tactic chains, helping mitigate the constraints of short prompt windows. Results demonstrate improved model alignment with tactic, technique, and procedure (TTP) coverage, validating the utility of graph-augmented LLMs in cybersecurity threat intelligence applications.",
      "publishedDate": "2026-01-11T05:07:57Z",
      "updatedDate": "2026-01-11T05:07:57Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06779v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06779",
      "comment": "12 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "prompting",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06750",
      "title": "Benchmarking Egocentric Clinical Intent Understanding Capability for Medical Multimodal Large Language Models",
      "authors": [
        {
          "name": "Shaonan Liu",
          "affiliation": null
        },
        {
          "name": "Guo Yu",
          "affiliation": null
        },
        {
          "name": "Xiaoling Luo",
          "affiliation": null
        },
        {
          "name": "Shiyi Zheng",
          "affiliation": null
        },
        {
          "name": "Wenting Chen",
          "affiliation": null
        },
        {
          "name": "Jie Liu",
          "affiliation": null
        },
        {
          "name": "Linlin Shen",
          "affiliation": null
        }
      ],
      "abstract": "Medical Multimodal Large Language Models (Med-MLLMs) require egocentric clinical intent understanding for real-world deployment, yet existing benchmarks fail to evaluate this critical capability. To address these challenges, we introduce MedGaze-Bench, the first benchmark leveraging clinician gaze as a Cognitive Cursor to assess intent understanding across surgery, emergency simulation, and diagnostic interpretation. Our benchmark addresses three fundamental challenges: visual homogeneity of anatomical structures, strict temporal-causal dependencies in clinical workflows, and implicit adherence to safety protocols. We propose a Three-Dimensional Clinical Intent Framework evaluating: (1) Spatial Intent: discriminating precise targets amid visual noise, (2) Temporal Intent: inferring causal rationale through retrospective and prospective reasoning, and (3) Standard Intent: verifying protocol compliance through safety checks. Beyond accuracy metrics, we introduce Trap QA mechanisms to stress-test clinical reliability by penalizing hallucinations and cognitive sycophancy. Experiments reveal current MLLMs struggle with egocentric intent due to over-reliance on global features, leading to fabricated observations and uncritical acceptance of invalid instructions.",
      "publishedDate": "2026-01-11T02:20:40Z",
      "updatedDate": "2026-01-11T02:20:40Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06750v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06750",
      "comment": "16 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06747",
      "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
      "authors": [
        {
          "name": "Glenn Matlin",
          "affiliation": null
        },
        {
          "name": "Akhil Theerthala",
          "affiliation": null
        },
        {
          "name": "Anant Gupta",
          "affiliation": null
        },
        {
          "name": "Anirudh JM",
          "affiliation": null
        },
        {
          "name": "Rayan Castilla",
          "affiliation": null
        },
        {
          "name": "Yi Mei Ng",
          "affiliation": null
        },
        {
          "name": "Sudheer Chava",
          "affiliation": null
        }
      ],
      "abstract": "Evaluating Language Models (LMs) in specialized, high-stakes domains such as finance remains a significant challenge due to the scarcity of open, high-quality, and domain-specific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs' capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, a scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through a hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipeline's efficacy, we produce FinForge-5k, a snapshot benchmark comprising over 5,000 human-validated question-answer pairs across 11 finance subdomains, derived from a curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the framework's utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge.",
      "publishedDate": "2026-01-11T01:38:33Z",
      "updatedDate": "2026-01-11T01:38:33Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06747v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06747",
      "comment": "AAAI 2026 Workshop on Agentic AI in Financial Services",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06707",
      "title": "Evaluating Accounting Reasoning Capabilities of Large Language Models",
      "authors": [
        {
          "name": "Jie Zhou",
          "affiliation": null
        },
        {
          "name": "Xin Chen",
          "affiliation": null
        },
        {
          "name": "Jie Zhang",
          "affiliation": null
        },
        {
          "name": "Hai Li",
          "affiliation": null
        },
        {
          "name": "Jie Wang",
          "affiliation": null
        },
        {
          "name": "Zhe Li",
          "affiliation": null
        }
      ],
      "abstract": "Large language models are transforming learning, cognition, and research across many fields. Effectively integrating them into professional domains, such as accounting, is a key challenge for enterprise digital transformation. To address this, we define vertical domain accounting reasoning and propose evaluation criteria derived from an analysis of the training data characteristics of representative GLM models. These criteria support systematic study of accounting reasoning and provide benchmarks for performance improvement. Using this framework, we evaluate GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4 on accounting reasoning tasks. Results show that prompt design significantly affects performance, with GPT-4 demonstrating the strongest capability. Despite these gains, current models remain insufficient for real-world enterprise accounting, indicating the need for further optimization to unlock their full practical value.",
      "publishedDate": "2026-01-10T22:24:52Z",
      "updatedDate": "2026-01-10T22:24:52Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06707v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06707",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06677",
      "title": "Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget",
      "authors": [
        {
          "name": "Zohaib Khan",
          "affiliation": null
        },
        {
          "name": "Omer Tafveez",
          "affiliation": null
        },
        {
          "name": "Zoha Hayat Bhatti",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in mathematical reasoning typically rely on massive scale, yet the question remains: can strong reasoning capabilities be induced in small language models ($\\leq1.5\\text{B}$) under extreme constraints? We investigate this by training models on a single A40 GPU (48GB) for under 24 hours using Reinforcement Learning with Verifiable Rewards (RLVR) and Low-Rank Adaptation (LoRA). We find that the success of this ``micro-budget\" regime depends critically on the interplay between adapter capacity and model initialization. While low-rank adapters ($r=8$) consistently fail to capture the complex optimization dynamics of reasoning, high-rank adapters ($r=256$) unlock significant plasticity in standard instruction-tuned models. Our best result achieved an impressive 40.0\\% Pass@1 on AIME 24 (an 11.1\\% absolute improvement over baseline) and pushed Pass@16 to 70.0\\%, demonstrating robust exploration capabilities. However, this plasticity is not universal: while instruction-tuned models utilized the budget to elongate their chain-of-thought and maximize reward, heavily math-aligned models suffered performance collapse, suggesting that noisy, low-budget RL updates can act as destructive interference for models already residing near a task-specific optimum.",
      "publishedDate": "2026-01-10T20:29:45Z",
      "updatedDate": "2026-01-10T20:29:45Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06677v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06677",
      "comment": "9 pages, 4 figures, 2 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06676",
      "title": "IDRBench: Interactive Deep Research Benchmark",
      "authors": [
        {
          "name": "Yingchaojie Feng",
          "affiliation": null
        },
        {
          "name": "Qiang Huang",
          "affiliation": null
        },
        {
          "name": "Xiaoya Xie",
          "affiliation": null
        },
        {
          "name": "Zhaorui Yang",
          "affiliation": null
        },
        {
          "name": "Jun Yu",
          "affiliation": null
        },
        {
          "name": "Wei Chen",
          "affiliation": null
        },
        {
          "name": "Anthony K. H. Tung",
          "affiliation": null
        }
      ],
      "abstract": "Deep research agents powered by Large Language Models (LLMs) can perform multi-step reasoning, web exploration, and long-form report generation. However, most existing systems operate in an autonomous manner, assuming fully specified user intent and evaluating only final outputs. In practice, research goals are often underspecified and evolve during exploration, making sustained interaction essential for robust alignment. Despite its importance, interaction remains largely invisible to existing deep research benchmarks, which neither model dynamic user feedback nor quantify its costs. We introduce IDRBench, the first benchmark for systematically evaluating interactive deep research. IDRBench combines a modular multi-agent research framework with on-demand interaction, a scalable reference-grounded user simulator, and an interaction-aware evaluation suite that jointly measures interaction benefits (quality and alignment) and costs (turns and tokens). Experiments across seven state-of-the-art LLMs show that interaction consistently improves research quality and robustness, often outweighing differences in model capacity, while revealing substantial trade-offs in interaction efficiency.",
      "publishedDate": "2026-01-10T20:29:12Z",
      "updatedDate": "2026-01-10T20:29:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06676v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06676",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06644",
      "title": "Do Language Models Reason Across Languages?",
      "authors": [
        {
          "name": "Yan Meng",
          "affiliation": null
        },
        {
          "name": "Wafaa Mohammed",
          "affiliation": null
        },
        {
          "name": "Christof Monz",
          "affiliation": null
        }
      ],
      "abstract": "The real-world information sources are inherently multilingual, which naturally raises a question about whether language models can synthesize information across languages. In this paper, we introduce a simple two-hop question answering setting, where answering a question requires making inferences over two multilingual documents. We find that language models are more sensitive to language variation in answer-span documents than in those providing bridging information, despite the equal importance of both documents for answering a question. Under a step-by-step sub-question evaluation, we further show that in up to 33% of multilingual cases, models fail to infer the bridging information in the first step yet still answer the overall question correctly. This indicates that reasoning in language models, especially in multilingual settings, does not follow a faithful step-by-step decomposition. Subsequently, we show that the absence of reasoning decomposition leads to around 18% composition failure, where both sub-questions are answered correctly but fail for the final two-hop questions. To mitigate this, we propose a simple three-stage SUBQ prompting method to guide the multi-step reasoning with sub-questions, which boosts accuracy from 10.1% to 66.5%.",
      "publishedDate": "2026-01-10T17:59:34Z",
      "updatedDate": "2026-01-10T17:59:34Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06644v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06644",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06640",
      "title": "Agentic AI Empowered Intent-Based Networking for 6G",
      "authors": [
        {
          "name": "Genze Jiang",
          "affiliation": null
        },
        {
          "name": "Kezhi Wang",
          "affiliation": null
        },
        {
          "name": "Xiaomin Chen",
          "affiliation": null
        },
        {
          "name": "Yizhou Huang",
          "affiliation": null
        }
      ],
      "abstract": "The transition towards sixth-generation (6G) wireless networks necessitates autonomous orchestration mechanisms capable of translating high-level operational intents into executable network configurations. Existing approaches to Intent-Based Networking (IBN) rely upon either rule-based systems that struggle with linguistic variation or end-to-end neural models that lack interpretability and fail to enforce operational constraints. This paper presents a hierarchical multi-agent framework where Large Language Model (LLM) based agents autonomously decompose natural language intents, consult domain-specific specialists, and synthesise technically feasible network slice configurations through iterative reasoning-action (ReAct) cycles. The proposed architecture employs an orchestrator agent coordinating two specialist agents, i.e., Radio Access Network (RAN) and Core Network agents, via ReAct-style reasoning, grounded in structured network state representations. Experimental evaluation across diverse benchmark scenarios shows that the proposed system outperforms rule-based systems and direct LLM prompting, with architectural principles applicable to Open RAN (O-RAN) deployments. The results also demonstrate that whilst contemporary LLMs possess general telecommunications knowledge, network automation requires careful prompt engineering to encode context-dependent decision thresholds, advancing autonomous orchestration capabilities for next-generation wireless systems.",
      "publishedDate": "2026-01-10T17:49:40Z",
      "updatedDate": "2026-01-10T17:49:40Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.NI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06640v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06640",
      "comment": "Submitted for Possible Journal Publication",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "agents",
        "evaluation",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "evaluation",
          "reasoning",
          "multi-agent",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06600",
      "title": "Probing Multimodal Large Language Models on Cognitive Biases in Chinese Short-Video Misinformation",
      "authors": [
        {
          "name": "Jen-tse Huang",
          "affiliation": null
        },
        {
          "name": "Chang Chen",
          "affiliation": null
        },
        {
          "name": "Shiyang Lai",
          "affiliation": null
        },
        {
          "name": "Wenxuan Wang",
          "affiliation": null
        },
        {
          "name": "Michelle R. Kaufman",
          "affiliation": null
        },
        {
          "name": "Mark Dredze",
          "affiliation": null
        }
      ],
      "abstract": "Short-video platforms have become major channels for misinformation, where deceptive claims frequently leverage visual experiments and social cues. While Multimodal Large Language Models (MLLMs) have demonstrated impressive reasoning capabilities, their robustness against misinformation entangled with cognitive biases remains under-explored. In this paper, we introduce a comprehensive evaluation framework using a high-quality, manually annotated dataset of 200 short videos spanning four health domains. This dataset provides fine-grained annotations for three deceptive patterns, experimental errors, logical fallacies, and fabricated claims, each verified by evidence such as national standards and academic literature. We evaluate eight frontier MLLMs across five modality settings. Experimental results demonstrate that Gemini-2.5-Pro achieves the highest performance in the multimodal setting with a belief score of 71.5/100, while o3 performs the worst at 35.2. Furthermore, we investigate social cues that induce false beliefs in videos and find that models are susceptible to biases like authoritative channel IDs.",
      "publishedDate": "2026-01-10T15:43:30Z",
      "updatedDate": "2026-01-10T15:43:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06600v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06600",
      "comment": "9 pages, 6 figures, 9 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06559",
      "title": "ArrowGEV: Grounding Events in Video via Learning the Arrow of Time",
      "authors": [
        {
          "name": "Fangxu Yu",
          "affiliation": null
        },
        {
          "name": "Ziyao Lu",
          "affiliation": null
        },
        {
          "name": "Liqiang Niu",
          "affiliation": null
        },
        {
          "name": "Fandong Meng",
          "affiliation": null
        },
        {
          "name": "Jie Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Grounding events in videos serves as a fundamental capability in video analysis. While Vision-Language Models (VLMs) are increasingly employed for this task, existing approaches predominantly train models to associate events with timestamps in the forward video only. This paradigm hinders VLMs from capturing the inherent temporal structure and directionality of events, thereby limiting robustness and generalization. To address this limitation, inspired by the arrow of time in physics, which characterizes the intrinsic directionality of temporal processes, we propose ArrowGEV, a reinforcement learning framework that explicitly models temporal directionality in events to improve both event grounding and temporal directionality understanding in VLMs. Specifically, we categorize events into time-sensitive (e.g., putting down a bag) and time-insensitive (e.g., holding a towel in the left hand). The former denote events whose reversal substantially alters their meaning, while the latter remain semantically unchanged under reversal. For time-sensitive events, ArrowGEV introduces a reward that encourages VLMs to discriminate between forward and backward videos, whereas for time-insensitive events, it enforces consistent grounding across both directions. Extensive experiments demonstrate that ArrowGEV not only improves grounding precision and temporal directionality recognition, but also enhances general video understanding and reasoning ability.",
      "publishedDate": "2026-01-10T13:05:23Z",
      "updatedDate": "2026-01-10T13:05:23Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06559v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06559",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06550",
      "title": "LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models",
      "authors": [
        {
          "name": "Pan Liao",
          "affiliation": null
        },
        {
          "name": "Feng Yang",
          "affiliation": null
        },
        {
          "name": "Di Wu",
          "affiliation": null
        },
        {
          "name": "Jinwen Yu",
          "affiliation": null
        },
        {
          "name": "Yuhua Zhu",
          "affiliation": null
        },
        {
          "name": "Wenhui Zhao",
          "affiliation": null
        }
      ],
      "abstract": "Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \\textit{where} and \\textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \\textit{what} and \\textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \\textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.",
      "publishedDate": "2026-01-10T12:18:12Z",
      "updatedDate": "2026-01-10T12:18:12Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06550v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06550",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06502",
      "title": "DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization",
      "authors": [
        {
          "name": "Shengkai Chen",
          "affiliation": null
        },
        {
          "name": "Zhiguang Cao",
          "affiliation": null
        },
        {
          "name": "Jianan Zhou",
          "affiliation": null
        },
        {
          "name": "Yaoxin Wu",
          "affiliation": null
        },
        {
          "name": "Senthilnath Jayavelu",
          "affiliation": null
        },
        {
          "name": "Zhuoyi Lin",
          "affiliation": null
        },
        {
          "name": "Xiaoli Li",
          "affiliation": null
        },
        {
          "name": "Shili Xiang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have recently shown promise in addressing combinatorial optimization problems (COPs) through prompt-based strategies. However, their scalability and generalization remain limited, and their effectiveness diminishes as problem size increases, particularly in routing problems involving more than 30 nodes. We propose DRAGON, which stands for Decomposition and Reconstruction Agents Guided OptimizatioN, a novel framework that combines the strengths of metaheuristic design and LLM reasoning. Starting from an initial global solution, DRAGON autonomously identifies regions with high optimization potential and strategically decompose large-scale COPs into manageable subproblems. Each subproblem is then reformulated as a concise, localized optimization task and solved through targeted LLM prompting guided by accumulated experiences. Finally, the locally optimized solutions are systematically reintegrated into the original global context to yield a significantly improved overall outcome. By continuously interacting with the optimization environment and leveraging an adaptive experience memory, the agents iteratively learn from feedback, effectively coupling symbolic reasoning with heuristic search. Empirical results show that, unlike existing LLM-based solvers limited to small-scale instances, DRAGON consistently produces feasible solutions on TSPLIB, CVRPLIB, and Weibull-5k bin packing benchmarks, and achieves near-optimal results (0.16% gap) on knapsack problems with over 3M variables. This work shows the potential of feedback-driven language agents as a new paradigm for generalizable and interpretable large-scale optimization.",
      "publishedDate": "2026-01-10T09:31:40Z",
      "updatedDate": "2026-01-10T09:31:40Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06502v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06502",
      "comment": "This paper has been accepted for presentation and publication at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), source code will be available soon",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06497",
      "title": "Coding in a Bubble? Evaluating LLMs in Resolving Context Adaptation Bugs During Code Adaptation",
      "authors": [
        {
          "name": "Tanghaoran Zhang",
          "affiliation": null
        },
        {
          "name": "Xinjun Mao",
          "affiliation": null
        },
        {
          "name": "Shangwen Wang",
          "affiliation": null
        },
        {
          "name": "Yuxin Zhao",
          "affiliation": null
        },
        {
          "name": "Yao Lu",
          "affiliation": null
        },
        {
          "name": "Zezhou Tang",
          "affiliation": null
        },
        {
          "name": "Wenyu Xu",
          "affiliation": null
        },
        {
          "name": "Longfei Sun",
          "affiliation": null
        },
        {
          "name": "Changrong Xie",
          "affiliation": null
        },
        {
          "name": "Kang Yang",
          "affiliation": null
        },
        {
          "name": "Yue Yu",
          "affiliation": null
        }
      ],
      "abstract": "Code adaptation is a fundamental but challenging task in software development, requiring developers to modify existing code for new contexts. A key challenge is to resolve Context Adaptation Bugs (CtxBugs), which occurs when code correct in its original context violates constraints in the target environment. Unlike isolated bugs, CtxBugs cannot be resolved through local fixes and require cross-context reasoning to identify semantic mismatches. Overlooking them may lead to critical failures in adaptation. Although Large Language Models (LLMs) show great potential in automating code-related tasks, their ability to resolve CtxBugs remains a significant and unexplored obstacle to their practical use in code adaptation. To bridge this gap, we propose CtxBugGen, a novel framework for generating CtxBugs to evaluate LLMs. Its core idea is to leverage LLMs' tendency to generate plausible but context-free code when contextual constraints are absent. The framework generates CtxBugs through a four-step process to ensure their relevance and validity: (1) Adaptation Task Selection, (2) Task-specific Perturbation,(3) LLM-based Variant Generation and (4) CtxBugs Identification. Based on the benchmark constructed by CtxBugGen, we conduct an empirical study with four state-of-the-art LLMs. Our results reveal their unsatisfactory performance in CtxBug resolution. The best performing LLM, Kimi-K2, achieves 55.93% on Pass@1 and resolves just 52.47% of CtxBugs. The presence of CtxBugs degrades LLMs' adaptation performance by up to 30%. Failure analysis indicates that LLMs often overlook CtxBugs and replicate them in their outputs. Our study highlights a critical weakness in LLMs' cross-context reasoning and emphasize the need for new methods to enhance their context awareness for reliable code adaptation.",
      "publishedDate": "2026-01-10T09:14:00Z",
      "updatedDate": "2026-01-10T09:14:00Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06497v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06497",
      "comment": "24 pages, 11 figures, accepted by FSE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06474",
      "title": "SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning",
      "authors": [
        {
          "name": "Chenxu Dang",
          "affiliation": null
        },
        {
          "name": "Jie Wang",
          "affiliation": null
        },
        {
          "name": "Guang Li",
          "affiliation": null
        },
        {
          "name": "Zhiwen Hou",
          "affiliation": null
        },
        {
          "name": "Zihan You",
          "affiliation": null
        },
        {
          "name": "Hangjun Ye",
          "affiliation": null
        },
        {
          "name": "Jie Ma",
          "affiliation": null
        },
        {
          "name": "Long Chen",
          "affiliation": null
        },
        {
          "name": "Yan Wang",
          "affiliation": null
        }
      ],
      "abstract": "In autonomous driving, Vision Language Models (VLMs) excel at high-level reasoning , whereas semantic occupancy provides fine-grained details. Despite significant progress in individual fields, there is still no method that can effectively integrate both paradigms. Conventional VLMs struggle with token explosion and limited spatiotemporal reasoning, while semantic occupancy provides a unified, explicit spatial representation but is too dense to integrate efficiently with VLMs. To address these challenges and bridge the gap between VLMs and occupancy, we propose SparseOccVLA, a novel vision-language-action model that unifies scene understanding, occupancy forecasting, and trajectory planning powered by sparse occupancy queries. Starting with a lightweight Sparse Occupancy Encoder, SparseOccVLA generates compact yet highly informative sparse occupancy queries that serve as the single bridge between vision and language. These queries are aligned into the language space and reasoned by the LLM for unified scene understanding and future occupancy forecasting. Furthermore, we introduce an LLM-guided Anchor-Diffusion Planner featuring decoupled anchor scoring and denoising, as well as cross-model trajectory-condition fusion. SparseOccVLA achieves a 7% relative improvement in CIDEr over the state-of-the-art on OmniDrive-nuScenes, a 0.5 increase in mIoU score on Occ3D-nuScenes, and sets state-of-the-art open-loop planning metric on nuScenes benchmark, demonstrating its strong holistic capability.",
      "publishedDate": "2026-01-10T07:54:20Z",
      "updatedDate": "2026-01-10T07:54:20Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06474v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06474",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "planning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "planning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06464",
      "title": "On the Adversarial Robustness of 3D Large Vision-Language Models",
      "authors": [
        {
          "name": "Chao Liu",
          "affiliation": null
        },
        {
          "name": "Ngai-Man Cheung",
          "affiliation": null
        }
      ],
      "abstract": "3D Vision-Language Models (VLMs), such as PointLLM and GPT4Point, have shown strong reasoning and generalization abilities in 3D understanding tasks. However, their adversarial robustness remains largely unexplored. Prior work in 2D VLMs has shown that the integration of visual inputs significantly increases vulnerability to adversarial attacks, making these models easier to manipulate into generating toxic or misleading outputs. In this paper, we investigate whether incorporating 3D vision similarly compromises the robustness of 3D VLMs. To this end, we present the first systematic study of adversarial robustness in point-based 3D VLMs. We propose two complementary attack strategies: \\textit{Vision Attack}, which perturbs the visual token features produced by the 3D encoder and projector to assess the robustness of vision-language alignment; and \\textit{Caption Attack}, which directly manipulates output token sequences to evaluate end-to-end system robustness. Each attack includes both untargeted and targeted variants to measure general vulnerability and susceptibility to controlled manipulation. Our experiments reveal that 3D VLMs exhibit significant adversarial vulnerabilities under untargeted attacks, while demonstrating greater resilience against targeted attacks aimed at forcing specific harmful outputs, compared to their 2D counterparts. These findings highlight the importance of improving the adversarial robustness of 3D VLMs, especially as they are deployed in safety-critical applications.",
      "publishedDate": "2026-01-10T07:17:29Z",
      "updatedDate": "2026-01-10T07:17:29Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06464v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06464",
      "comment": "Under Review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06453",
      "title": "ConSensus: Multi-Agent Collaboration for Multimodal Sensing",
      "authors": [
        {
          "name": "Hyungjun Yoon",
          "affiliation": null
        },
        {
          "name": "Mohammad Malekzadeh",
          "affiliation": null
        },
        {
          "name": "Sung-Ju Lee",
          "affiliation": null
        },
        {
          "name": "Fahim Kawsar",
          "affiliation": null
        },
        {
          "name": "Lorena Qendro",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly grounded in sensor data to perceive and reason about human physiology and the physical world. However, accurately interpreting heterogeneous multimodal sensor data remains a fundamental challenge. We show that a single monolithic LLM often fails to reason coherently across modalities, leading to incomplete interpretations and prior-knowledge bias. We introduce ConSensus, a training-free multi-agent collaboration framework that decomposes multimodal sensing tasks into specialized, modality-aware agents. To aggregate agent-level interpretations, we propose a hybrid fusion mechanism that balances semantic aggregation, which enables cross-modal reasoning and contextual understanding, with statistical consensus, which provides robustness through agreement across modalities. While each approach has complementary failure modes, their combination enables reliable inference under sensor noise and missing data. We evaluate ConSensus on five diverse multimodal sensing benchmarks, demonstrating an average accuracy improvement of 7.1% over the single-agent baseline. Furthermore, ConSensus matches or exceeds the performance of iterative multi-agent debate methods while achieving a 12.7 times reduction in average fusion token cost through a single-round hybrid fusion protocol, yielding a robust and efficient solution for real-world multimodal sensing tasks.",
      "publishedDate": "2026-01-10T06:41:01Z",
      "updatedDate": "2026-01-10T06:41:01Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06453v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06453",
      "comment": "17 pages, 6 figures, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06437",
      "title": "Time Travel Engine: A Shared Latent Chronological Manifold Enables Historical Navigation in Large Language Models",
      "authors": [
        {
          "name": "Jingmin An",
          "affiliation": null
        },
        {
          "name": "Wei Liu",
          "affiliation": null
        },
        {
          "name": "Qian Wang",
          "affiliation": null
        },
        {
          "name": "Fang Fang",
          "affiliation": null
        }
      ],
      "abstract": "Time functions as a fundamental dimension of human cognition, yet the mechanisms by which Large Language Models (LLMs) encode chronological progression remain opaque. We demonstrate that temporal information in their latent space is organized not as discrete clusters but as a continuous, traversable geometry. We introduce the Time Travel Engine (TTE), an interpretability-driven framework that projects diachronic linguistic patterns onto a shared chronological manifold. Unlike surface-level prompting, TTE directly modulates latent representations to induce coherent stylistic, lexical, and conceptual shifts aligned with target eras. By parameterizing diachronic evolution as a continuous manifold within the residual stream, TTE enables fluid navigation through period-specific \"zeitgeists\" while restricting access to future knowledge. Furthermore, experiments across diverse architectures reveal topological isomorphism between the temporal subspaces of Chinese and English-indicating that distinct languages share a universal geometric logic of historical evolution. These findings bridge historical linguistics with mechanistic interpretability, offering a novel paradigm for controlling temporal reasoning in neural networks.",
      "publishedDate": "2026-01-10T05:39:54Z",
      "updatedDate": "2026-01-10T05:39:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06437v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06437",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06431",
      "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
      "authors": [
        {
          "name": "Qingyu Ren",
          "affiliation": null
        },
        {
          "name": "Qianyu He",
          "affiliation": null
        },
        {
          "name": "Jingwen Chang",
          "affiliation": null
        },
        {
          "name": "Jie Zeng",
          "affiliation": null
        },
        {
          "name": "Jiaqing Liang",
          "affiliation": null
        },
        {
          "name": "Yanghua Xiao",
          "affiliation": null
        },
        {
          "name": "Han Xia",
          "affiliation": null
        },
        {
          "name": "Zeye Sun",
          "affiliation": null
        },
        {
          "name": "Fei Yu",
          "affiliation": null
        }
      ],
      "abstract": "Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.",
      "publishedDate": "2026-01-10T05:11:38Z",
      "updatedDate": "2026-01-10T05:11:38Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06431v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06431",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06428",
      "title": "Teach Diffusion Language Models to Learn from Their Own Mistakes",
      "authors": [
        {
          "name": "Liming Liu",
          "affiliation": null
        },
        {
          "name": "Binxuan Huang",
          "affiliation": null
        },
        {
          "name": "Xin Liu",
          "affiliation": null
        },
        {
          "name": "Bing Yin",
          "affiliation": null
        },
        {
          "name": "Tuo Zhao",
          "affiliation": null
        }
      ],
      "abstract": "Masked Diffusion Language Models (DLMs) achieve significant speed by generating multiple tokens in parallel. However, this parallel sampling approach, especially when using fewer inference steps, will introduce strong dependency errors and cause quality to deteriorate rapidly as the generation step size grows. As a result, reliable self-correction becomes essential for maintaining high-quality multi-token generation. To address this, we propose Decoupled Self-Correction (DSC), a novel two-stage methodology. DSC first fully optimizes the DLM's generative ability before freezing the model and training a specialized correction head. This decoupling preserves the model's peak SFT performance and ensures the generated errors used for correction head training are of higher quality. Additionally, we introduce Future-Context Augmentation (FCA) to maximize the correction head's accuracy. FCA generalizes the error training distribution by augmenting samples with ground-truth tokens, effectively training the head to utilize a richer, future-looking context. This mechanism is used for reliably detecting the subtle errors of the high-fidelity base model. Our DSC framework enables the model, at inference time, to jointly generate and revise tokens, thereby correcting errors introduced by multi-token generation and mitigating error accumulation across steps. Experiments on mathematical reasoning and code generation benchmarks demonstrate that our approach substantially reduces the quality degradation associated with larger generation steps, allowing DLMs to achieve both high generation speed and strong output fidelity.",
      "publishedDate": "2026-01-10T05:04:33Z",
      "updatedDate": "2026-01-10T05:04:33Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06428v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06428",
      "comment": "18 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "tool-use",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06424",
      "title": "Can a Unimodal Language Agent Provide Preferences to Tune a Multimodal Vision-Language Model?",
      "authors": [
        {
          "name": "Sazia Tabasum Mim",
          "affiliation": null
        },
        {
          "name": "Jack Morris",
          "affiliation": null
        },
        {
          "name": "Manish Dhakal",
          "affiliation": null
        },
        {
          "name": "Yanming Xiu",
          "affiliation": null
        },
        {
          "name": "Maria Gorlatova",
          "affiliation": null
        },
        {
          "name": "Yi Ding",
          "affiliation": null
        }
      ],
      "abstract": "To explore a more scalable path for adding multimodal capabilities to existing LLMs, this paper addresses a fundamental question: Can a unimodal LLM, relying solely on text, reason about its own informational needs and provide effective feedback to optimize a multimodal model? To answer this, we propose a method that enables a language agent to give feedback to a vision-language model (VLM) to adapt text generation to the agent's preferences. Our results from different experiments affirm this hypothesis, showing that LLM preference feedback significantly enhances VLM descriptions. Using our proposed method, we find that the VLM can generate multimodal scene descriptions to help the LLM better understand multimodal context, leading to improvements of maximum 13% in absolute accuracy compared to the baseline multimodal approach. Furthermore, a human study validated our AI-driven feedback, showing a 64.6% preference alignment rate between the LLM's choices and human judgments. Extensive experiments provide insights on how and why the method works and its limitations.",
      "publishedDate": "2026-01-10T04:28:53Z",
      "updatedDate": "2026-01-10T04:28:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06424v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06424",
      "comment": "Accepted to IJCNLP-AACL 2025 Findings",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents"
      ],
      "tags": {
        "auto": [
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06423",
      "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
      "authors": [
        {
          "name": "Deep Mehta",
          "affiliation": null
        }
      ],
      "abstract": "Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness? We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency. GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212). Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs.",
      "publishedDate": "2026-01-10T04:20:00Z",
      "updatedDate": "2026-01-10T04:20:00Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06423v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06423",
      "comment": "24 pages, 3 figures, 9 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06419",
      "title": "Lightweight Yet Secure: Secure Scripting Language Generation via Lightweight LLMs",
      "authors": [
        {
          "name": "Keyang Zhang",
          "affiliation": null
        },
        {
          "name": "Zeyu Chen",
          "affiliation": null
        },
        {
          "name": "Xuan Feng",
          "affiliation": null
        },
        {
          "name": "Dongliang Fang",
          "affiliation": null
        },
        {
          "name": "Yaowen Zheng",
          "affiliation": null
        },
        {
          "name": "Zhi Li",
          "affiliation": null
        },
        {
          "name": "Limin Sun",
          "affiliation": null
        }
      ],
      "abstract": "The security of scripting languages such as PowerShell is critical given their powerful automation and administration capabilities, often exercised with elevated privileges. Today, securing these languages still demands substantial human effort to craft and enforce rules, imposing heavy burdens on typical administrators and creating critical production risks (e.g., misoperations that shut down servers).Large language models (LLMs) have demonstrated strong capabilities in code generation, vulnerability detection, and automated repair for languages like Python and JavaScript. However, their ability to assist with generating secure scripting-language code remains largely underexplored. In this paper, we present SecGenEval-PS, a benchmark designed to systematically evaluate LLMs on secure scripting generation, security analysis, and automated repair. Our results show that both proprietary and open-source models fall short in these areas. For instance, over 60% of PowerShell scripts produced by GPT-4o and o3-mini are insecure without structured guidance.To bridge this gap, we propose PSSec, a framework that combines data synthesis with fine-tuning to enhance model security capabilities. We develop a self-debugging agent that integrates static analyzers with the reasoning abilities of advanced LLMs to synthesize large-scale structured triplets of insecure scripts, violation analyses, and corresponding repairs. We then fine-tune lightweight LLMs (as small as 1.7B parameters) using supervised fine-tuning (SFT) and reinforcement learning (RL), enabling security-aware reasoning and the generation of secure PowerShell code.Across multiple LLM families, including GPT and Qwen, \\textit{PSSec}-trained models match or surpass general-purpose large models on PowerShell security tasks while reducing inference cost by more than an order of magnitude.",
      "publishedDate": "2026-01-10T04:00:56Z",
      "updatedDate": "2026-01-10T04:00:56Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.PL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06419v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06419",
      "comment": "19 pages,8 figures,conference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06415",
      "title": "Semantic Enrichment of CAD-Based Industrial Environments via Scene Graphs for Simulation and Reasoning",
      "authors": [
        {
          "name": "Nathan Pascal Walus",
          "affiliation": null
        },
        {
          "name": "Ranulfo Bezerra",
          "affiliation": null
        },
        {
          "name": "Shotaro Kojima",
          "affiliation": null
        },
        {
          "name": "Tsige Tadesse Alemayoh",
          "affiliation": null
        },
        {
          "name": "Satoshi Tadokoro",
          "affiliation": null
        },
        {
          "name": "Kazunori Ohno",
          "affiliation": null
        }
      ],
      "abstract": "Utilizing functional elements in an industrial environment, such as displays and interactive valves, provide effective possibilities for robot training. When preparing simulations for robots or applications that involve high-level scene understanding, the simulation environment must be equally detailed. Although CAD files for such environments deliver an exact description of the geometry and visuals, they usually lack semantic, relational and functional information, thus limiting the simulation and training possibilities. A 3D scene graph can organize semantic, spatial and functional information by enriching the environment through a Large Vision-Language Model (LVLM). In this paper we present an offline approach to creating detailed 3D scene graphs from CAD environments. This will serve as a foundation to include the relations of functional and actionable elements, which then can be used for dynamic simulation and reasoning. Key results of this research include both quantitative results of the generated semantic labels as well as qualitative results of the scene graph, especially in hindsight of pipe structures and identified functional relations. All code, results and the environment will be made available at https://cad-scenegraph.github.io",
      "publishedDate": "2026-01-10T03:22:29Z",
      "updatedDate": "2026-01-10T03:22:29Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06415v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06415",
      "comment": "Accepted to IEEE SSRR 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06395",
      "title": "AfriqueLLM: How Data Mixing and Model Architecture Impact Continued Pre-training for African Languages",
      "authors": [
        {
          "name": "Hao Yu",
          "affiliation": null
        },
        {
          "name": "Tianyi Xu",
          "affiliation": null
        },
        {
          "name": "Michael A. Hedderich",
          "affiliation": null
        },
        {
          "name": "Wassim Hamidouche",
          "affiliation": null
        },
        {
          "name": "Syed Waqas Zamir",
          "affiliation": null
        },
        {
          "name": "David Ifeoluwa Adelani",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly multilingual, yet open models continue to underperform relative to proprietary systems, with the gap most pronounced for African languages. Continued pre-training (CPT) offers a practical route to language adaptation, but improvements on demanding capabilities such as mathematical reasoning often remain limited. This limitation is driven in part by the uneven domain coverage and missing task-relevant knowledge that characterize many low-resource language corpora. We present \\texttt{AfriqueLLM}, a suite of open LLMs adapted to 20 African languages through CPT on 26B tokens. We perform a comprehensive empirical study across five base models spanning sizes and architectures, including Llama 3.1, Gemma 3, and Qwen 3, and systematically analyze how CPT data composition shapes downstream performance. In particular, we vary mixtures that include math, code, and synthetic translated data, and evaluate the resulting models on a range of multilingual benchmarks. Our results identify data composition as the primary driver of CPT gains. Adding math, code, and synthetic translated data yields consistent improvements, including on reasoning-oriented evaluations. Within a fixed architecture, larger models typically improve performance, but architectural choices dominate scale when comparing across model families. Moreover, strong multilingual performance in the base model does not reliably predict post-CPT outcomes; robust architectures coupled with task-aligned data provide a more dependable recipe. Finally, our best models improve long-context performance, including document-level translation. Models have been released on [Huggingface](https://huggingface.co/collections/McGill-NLP/afriquellm).",
      "publishedDate": "2026-01-10T02:39:31Z",
      "updatedDate": "2026-01-10T02:39:31Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06395v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06395",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06373",
      "title": "DemMA: Dementia Multi-Turn Dialogue Agent with Expert-Guided Reasoning and Action Simulation",
      "authors": [
        {
          "name": "Yutong Song",
          "affiliation": null
        },
        {
          "name": "Jiang Wu",
          "affiliation": null
        },
        {
          "name": "Kazi Sharif",
          "affiliation": null
        },
        {
          "name": "Honghui Xu",
          "affiliation": null
        },
        {
          "name": "Nikil Dutt",
          "affiliation": null
        },
        {
          "name": "Amir Rahmani",
          "affiliation": null
        }
      ],
      "abstract": "Simulating dementia patients with large language models (LLMs) is challenging due to the need to jointly model cognitive impairment, emotional dynamics, and nonverbal behaviors over long conversations. We present DemMA, an expert-guided dementia dialogue agent for high-fidelity multi-turn patient simulation. DemMA constructs clinically grounded dementia personas by integrating pathology information, personality traits, and subtype-specific memory-status personas informed by clinical experts. To move beyond text-only simulation, DemMA explicitly models nonverbal behaviors, including motion, facial expressions, and vocal cues. We further introduce a Chain-of-Thought distillation framework that trains a single LLM to jointly generate reasoning traces, patient utterances, and aligned behavioral actions within one forward pass, enabling efficient deployment without multi-agent inference. Extensive evaluations with experts, medical students, and LLM judges demonstrate that DemMA significantly outperforms strong baselines across multiple metrics.",
      "publishedDate": "2026-01-10T01:10:52Z",
      "updatedDate": "2026-01-10T01:10:52Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06373v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06373",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06289",
      "title": "How well can off-the-shelf LLMs elucidate molecular structures from mass spectra using chain-of-thought reasoning?",
      "authors": [
        {
          "name": "Yufeng Wang",
          "affiliation": null
        },
        {
          "name": "Lu Wei",
          "affiliation": null
        },
        {
          "name": "Lin Liu",
          "affiliation": null
        },
        {
          "name": "Hao Xu",
          "affiliation": null
        },
        {
          "name": "Haibin Ling",
          "affiliation": null
        }
      ],
      "abstract": "Mass spectrometry (MS) is a powerful analytical technique for identifying small molecules, yet determining complete molecular structures directly from tandem mass spectra (MS/MS) remains a long-standing challenge due to complex fragmentation patterns and the vast diversity of chemical space. Recent progress in large language models (LLMs) has shown promise for reasoning-intensive scientific tasks, but their capability for chemical interpretation is still unclear. In this work, we introduce a Chain-of-Thought (CoT) prompting framework and benchmark that evaluate how LLMs reason about mass spectral data to predict molecular structures. We formalize expert chemists' reasoning steps-such as double bond equivalent (DBE) analysis, neutral loss identification, and fragment assembly-into structured prompts and assess multiple state-of-the-art LLMs (Claude-3.5-Sonnet, GPT-4o-mini, and Llama-3 series) in a zero-shot setting using the MassSpecGym dataset. Our evaluation across metrics of SMILES validity, formula consistency, and structural similarity reveals that while LLMs can produce syntactically valid and partially plausible structures, they fail to achieve chemical accuracy or link reasoning to correct molecular predictions. These findings highlight both the interpretive potential and the current limitations of LLM-based reasoning for molecular elucidation, providing a foundation for future work that combines domain knowledge and reinforcement learning to achieve chemically grounded AI reasoning.",
      "publishedDate": "2026-01-09T20:08:42Z",
      "updatedDate": "2026-01-09T20:08:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06289v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06289",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06234",
      "title": "PCoKG: Personality-aware Commonsense Reasoning with Debate",
      "authors": [
        {
          "name": "Weijie Li",
          "affiliation": null
        },
        {
          "name": "Zhongqing Wang",
          "affiliation": null
        },
        {
          "name": "Guodong Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Most commonsense reasoning models overlook the influence of personality traits, limiting their effectiveness in personalized systems such as dialogue generation. To address this limitation, we introduce the Personality-aware Commonsense Knowledge Graph (PCoKG), a structured dataset comprising 521,316 quadruples. We begin by employing three evaluators to score and filter events from the ATOMIC dataset, selecting those that are likely to elicit diverse reasoning patterns across different personality types. For knowledge graph construction, we leverage the role-playing capabilities of large language models (LLMs) to perform reasoning tasks. To enhance the quality of the generated knowledge, we incorporate a debate mechanism consisting of a proponent, an opponent, and a judge, which iteratively refines the outputs through feedback loops. We evaluate the dataset from multiple perspectives and conduct fine-tuning and ablation experiments using multiple LLM backbones to assess PCoKG's robustness and the effectiveness of its construction pipeline. Our LoRA-based fine-tuning results indicate a positive correlation between model performance and the parameter scale of the base models. Finally, we apply PCoKG to persona-based dialogue generation, where it demonstrates improved consistency between generated responses and reference outputs. This work bridges the gap between commonsense reasoning and individual cognitive differences, enabling the development of more personalized and context-aware AI systems.",
      "publishedDate": "2026-01-09T15:05:01Z",
      "updatedDate": "2026-01-09T15:05:01Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06234v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06234",
      "comment": "Accept by AAAI-2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06224",
      "title": "Ground What You See: Hallucination-Resistant MLLMs via Caption Feedback, Diversity-Aware Sampling, and Conflict Regularization",
      "authors": [
        {
          "name": "Miao Pan",
          "affiliation": null
        },
        {
          "name": "Wangjie Gan",
          "affiliation": null
        },
        {
          "name": "Jintao Chen",
          "affiliation": null
        },
        {
          "name": "Wenqi Zhang",
          "affiliation": null
        },
        {
          "name": "Bing Sun",
          "affiliation": null
        },
        {
          "name": "Jianwei Yin",
          "affiliation": null
        },
        {
          "name": "Xuhong Zhang",
          "affiliation": null
        }
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse tasks, their practical deployment is severely hindered by hallucination issues, which become particularly acute during Reinforcement Learning (RL) optimization. This paper systematically analyzes the root causes of hallucinations in MLLMs under RL training, identifying three critical factors: (1) an over-reliance on chained visual reasoning, where inaccurate initial descriptions or redundant information anchor subsequent inferences to incorrect premises; (2) insufficient exploration diversity during policy optimization, leading the model to generate overly confident but erroneous outputs; and (3) destructive conflicts between training samples, where Neural Tangent Kernel (NTK) similarity causes false associations and unstable parameter updates. To address these challenges, we propose a comprehensive framework comprising three core modules. First, we enhance visual localization by introducing dedicated planning and captioning stages before the reasoning phase, employing a quality-based caption reward to ensure accurate initial anchoring. Second, to improve exploration, we categorize samples based on the mean and variance of their reward distributions, prioritizing samples with high variance to focus the model on diverse and informative data. Finally, to mitigate sample interference, we regulate NTK similarity by grouping sample pairs and applying an InfoNCE loss to push overly similar pairs apart and pull dissimilar ones closer, thereby guiding gradient interactions toward a balanced range. Experimental results demonstrate that our proposed method significantly reduces hallucination rates and effectively enhances the inference accuracy of MLLMs.",
      "publishedDate": "2026-01-09T07:59:18Z",
      "updatedDate": "2026-01-13T07:12:55Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06224v2",
      "arxivUrl": "https://arxiv.org/abs/2601.06224",
      "comment": "AAAI-2026 Poster",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06204",
      "title": "Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification",
      "authors": [
        {
          "name": "Tayyab Rehman",
          "affiliation": null
        },
        {
          "name": "Giovanni De Gasperis",
          "affiliation": null
        },
        {
          "name": "Aly Shmahell",
          "affiliation": null
        }
      ],
      "abstract": "Intelligent anomaly detection in dynamic visual environments requires reconciling real-time performance with semantic interpretability. Conventional approaches address only fragments of this challenge. Reconstruction-based models capture low-level deviations without contextual reasoning, object detectors provide speed but limited semantics, and large vision-language systems deliver interpretability at prohibitive computational cost. This work introduces a cascading multi-agent framework that unifies these complementary paradigms into a coherent and interpretable architecture. Early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked to interpret semantically ambiguous events. The system employs adaptive escalation thresholds and a publish-subscribe communication backbone, enabling asynchronous coordination and scalable deployment across heterogeneous hardware. Extensive evaluation on large-scale monitoring data demonstrates that the proposed cascade achieves a threefold reduction in latency compared to direct vision-language inference, while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling. The framework advances beyond conventional detection pipelines by combining early-exit efficiency, adaptive multi-agent reasoning, and explainable anomaly attribution, establishing a reproducible and energy-efficient foundation for scalable intelligent visual monitoring.",
      "publishedDate": "2026-01-08T11:31:47Z",
      "updatedDate": "2026-01-13T14:40:15Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06204v2",
      "arxivUrl": "https://arxiv.org/abs/2601.06204",
      "comment": "Author email changed, Acknowlegement changes",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "multi-agent",
        "evaluation",
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "evaluation",
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06199",
      "title": "FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation",
      "authors": [
        {
          "name": "Junseok Lee",
          "affiliation": null
        },
        {
          "name": "Sangyong Lee",
          "affiliation": null
        },
        {
          "name": "Chang-Jae Chun",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated human-expert-level capabilities, driving significant interest in their potential for achieving artificial general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities, including vision, video, and speech, through the development of multimodal LLMs (MLLMs). However, existing speech-language model (SLM) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In this paper, we propose FastSLM, a lightweight yet efficient SLM designed for effective understanding and reasoning over long-form speech. To address the challenge of aligning high-frame-rate speech features with LLMs, we introduce the Hierarchical Frame Querying Transformer (HFQ-Former), which compresses frame-level speech features while capturing both local and global context. Furthermore, we present a novel three-stage training strategy that enhances generalization across a wide range of speech-related tasks. Experimental results demonstrate that FastSLM achieves competitive performance compared to existing state-of-the-art models, despite operating with significantly lower FLOPs and parameter counts, while representing speech with only 1.67 tokens per second. The source code and model checkpoints are available at https://huggingface.co/okestro-ai-lab/FastSLM.",
      "publishedDate": "2026-01-08T07:46:03Z",
      "updatedDate": "2026-01-08T07:46:03Z",
      "primaryCategory": "eess.AS",
      "arxivCategories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06199v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06199",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06198",
      "title": "How Does India Cook Biryani?",
      "authors": [
        {
          "name": "Shubham Goel",
          "affiliation": null
        },
        {
          "name": "Farzana S",
          "affiliation": null
        },
        {
          "name": "C V Rishi",
          "affiliation": null
        },
        {
          "name": "Aditya Arun",
          "affiliation": null
        },
        {
          "name": "C V Jawahar",
          "affiliation": null
        }
      ],
      "abstract": "Biryani, one of India's most celebrated dishes, exhibits remarkable regional diversity in its preparation, ingredients, and presentation. With the growing availability of online cooking videos, there is unprecedented potential to study such culinary variations using computational tools systematically. However, existing video understanding methods fail to capture the fine-grained, multimodal, and culturally grounded differences in procedural cooking videos. This work presents the first large-scale, curated dataset of biryani preparation videos, comprising 120 high-quality YouTube recordings across 12 distinct regional styles. We propose a multi-stage framework leveraging recent advances in vision-language models (VLMs) to segment videos into fine-grained procedural units and align them with audio transcripts and canonical recipe text. Building on these aligned representations, we introduce a video comparison pipeline that automatically identifies and explains procedural differences between regional variants. We construct a comprehensive question-answer (QA) benchmark spanning multiple reasoning levels to evaluate procedural understanding in VLMs. Our approach employs multiple VLMs in complementary roles, incorporates human-in-the-loop verification for high-precision tasks, and benchmarks several state-of-the-art models under zero-shot and fine-tuned settings. The resulting dataset, comparison methodology, and QA benchmark provide a new testbed for evaluating VLMs on structured, multimodal reasoning tasks and open new directions for computational analysis of cultural heritage through cooking videos. We release all data, code, and the project website at https://farzanashaju.github.io/how-does-india-cook-biryani/.",
      "publishedDate": "2026-01-08T07:23:10Z",
      "updatedDate": "2026-01-08T07:23:10Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06198v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06198",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06193",
      "title": "MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications",
      "authors": [
        {
          "name": "Qing He",
          "affiliation": null
        },
        {
          "name": "Dongsheng Bi",
          "affiliation": null
        },
        {
          "name": "Jianrong Lu",
          "affiliation": null
        },
        {
          "name": "Minghui Yang",
          "affiliation": null
        },
        {
          "name": "Zixiao Chen",
          "affiliation": null
        },
        {
          "name": "Jiacheng Lu",
          "affiliation": null
        },
        {
          "name": "Jing Chen",
          "affiliation": null
        },
        {
          "name": "Nannan Du",
          "affiliation": null
        },
        {
          "name": "Xiao Cu",
          "affiliation": null
        },
        {
          "name": "Sijing Wu",
          "affiliation": null
        },
        {
          "name": "Peng Xiang",
          "affiliation": null
        },
        {
          "name": "Yinyin Hu",
          "affiliation": null
        },
        {
          "name": "Yi Guo",
          "affiliation": null
        },
        {
          "name": "Chunpu Li",
          "affiliation": null
        },
        {
          "name": "Shaoyang Li",
          "affiliation": null
        },
        {
          "name": "Zhuo Dong",
          "affiliation": null
        },
        {
          "name": "Ming Jiang",
          "affiliation": null
        },
        {
          "name": "Shuai Guo",
          "affiliation": null
        },
        {
          "name": "Liyun Feng",
          "affiliation": null
        },
        {
          "name": "Jin Peng",
          "affiliation": null
        },
        {
          "name": "Jian Wang",
          "affiliation": null
        },
        {
          "name": "Jinjie Gu",
          "affiliation": null
        },
        {
          "name": "Junwei Liu",
          "affiliation": null
        }
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) presents transformative potential for healthcare, yet practical deployment is hindered by the absence of frameworks that assess real-world clinical utility. Existing benchmarks test static knowledge, failing to capture the dynamic, application-oriented capabilities required in clinical practice. To bridge this gap, we introduce a Medical LLM Benchmark MLB, a comprehensive benchmark evaluating LLMs on both foundational knowledge and scenario-based reasoning. MLB is structured around five core dimensions: Medical Knowledge (MedKQA), Safety and Ethics (MedSE), Medical Record Understanding (MedRU), Smart Services (SmartServ), and Smart Healthcare (SmartCare). The benchmark integrates 22 datasets (17 newly curated) from diverse Chinese clinical sources, covering 64 clinical specialties. Its design features a rigorous curation pipeline involving 300 licensed physicians. Besides, we provide a scalable evaluation methodology, centered on a specialized judge model trained via Supervised Fine-Tuning (SFT) on expert annotations. Our comprehensive evaluation of 10 leading models reveals a critical translational gap: while the top-ranked model, Kimi-K2-Instruct (77.3% accuracy overall), excels in structured tasks like information extraction (87.8% accuracy in MedRU), performance plummets in patient-facing scenarios (61.3% in SmartServ). Moreover, the exceptional safety score (90.6% in MedSE) of the much smaller Baichuan-M2-32B highlights that targeted training is equally critical. Our specialized judge model, trained via SFT on a 19k expert-annotated medical dataset, achieves 92.1% accuracy, an F1-score of 94.37%, and a Cohen's Kappa of 81.3% for human-AI consistency, validating a reproducible and expert-aligned evaluation protocol. MLB thus provides a rigorous framework to guide the development of clinically viable LLMs.",
      "publishedDate": "2026-01-08T02:41:42Z",
      "updatedDate": "2026-01-08T02:41:42Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06193v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06193",
      "comment": "11 pages, 4 figures, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06181",
      "title": "Neuro-Symbolic Compliance: Integrating LLMs and SMT Solvers for Automated Financial Legal Analysis",
      "authors": [
        {
          "name": "Yung-Shen Hsia",
          "affiliation": null
        },
        {
          "name": "Fang Yu",
          "affiliation": null
        },
        {
          "name": "Jie-Hong Roland Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Financial regulations are increasingly complex, hindering automated compliance-especially the maintenance of logical consistency with minimal human oversight. We introduce a Neuro-Symbolic Compliance Framework that integrates Large Language Models (LLMs) with Satisfiability Modulo Theories (SMT) solvers to enable formal verifiability and optimization-based compliance correction. The LLM interprets statutes and enforcement cases to generate SMT constraints, while the solver enforces consistency and computes the minimal factual modification required to restore legality when penalties arise. Unlike transparency-oriented methods, our approach emphasizes logic-driven optimization, delivering verifiable, legally consistent reasoning rather than post-hoc explanation. Evaluated on 87 enforcement cases from Taiwan's Financial Supervisory Commission (FSC), the system attains 86.2% correctness in SMT code generation, improves reasoning efficiency by over 100x, and consistently corrects violations-establishing a preliminary foundation for optimization-based compliance applications.",
      "publishedDate": "2026-01-07T17:05:37Z",
      "updatedDate": "2026-01-07T17:05:37Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.FL",
        "cs.LG",
        "cs.LO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06181v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06181",
      "comment": "10 pages, 6 tables, 3 figures, accepted by the 2nd ACM AIware Conference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06176",
      "title": "TIR-Flow: Active Video Search and Reasoning with Frozen VLMs",
      "authors": [
        {
          "name": "Hongbo Jin",
          "affiliation": null
        },
        {
          "name": "Siyi Xie",
          "affiliation": null
        },
        {
          "name": "Jiayu Ding",
          "affiliation": null
        },
        {
          "name": "Kuanwei Lin",
          "affiliation": null
        },
        {
          "name": "Ge Li",
          "affiliation": null
        }
      ],
      "abstract": "While Large Video-Language Models (Video-LLMs) have achieved remarkable progress in perception, their reasoning capabilities remain a bottleneck. Existing solutions typically resort to a heavy \"data engineering\" paradigm-synthesizing large-scale Chain-of-Thought (CoT) datasets followed by Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). This pipeline primarily optimizes probability sampling efficiency and aligns output distributions, but fails to activate the intrinsic intelligence required for dynamic visual exploration. In this work, we propose TIR-Flow, a novel framework that shifts the paradigm from passive processing to active video searching and reasoning without additional data or parameter updating. Concretely, our framework operates through three synergistic modules: HDD decomposes complex queries into a set of verifiable sub-tasks; HAP actively directs visual attention to gather high-resolution evidence for hypothesis validation; EBA maintains a persistent workspace to accumulate and update the discovered clues for logical reasoning. Extensive experiments on seven benchmarks demonstrate that TIR-Flow significantly outperforms recent strong baselines, delivering an average performance boost of 5.9%, with gains reaching 10.5% on Egoschema. Our analysis confirms that empowering frozen VLMs with System-2-like active perception is a scalable path toward solving long-horizon video reasoning.",
      "publishedDate": "2026-01-07T13:54:44Z",
      "updatedDate": "2026-01-07T13:54:44Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06176v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06176",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06160",
      "title": "Student Guides Teacher: Weak-to-Strong Inference via Spectral Orthogonal Exploration",
      "authors": [
        {
          "name": "Dayu Wang",
          "affiliation": null
        },
        {
          "name": "Jiaye Yang",
          "affiliation": null
        },
        {
          "name": "Weikang Li",
          "affiliation": null
        },
        {
          "name": "Jiahui Liang",
          "affiliation": null
        },
        {
          "name": "Yang Li",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) demonstrate near-human capabilities, they often suffer from \"Reasoning Collapse\" in complex mathematical proving and long-horizon planning. Models tend to degenerate into low-rank Bias Manifold, where stochastic sampling merely produces lexical variations of erroneous logic rather than semantic exploration. This geometric collapse renders the model \"blind\" to high-value solutions that lie within its Null Space. To address this, we propose Spectral Orthogonal Exploration (SOE), a geometric framework operating on a counter-intuitive \"Student Guides Teacher\" paradigm. Specifically, we utilize a weak auxiliary agent not for imitation, but as an orthogonal probe. By explicitly navigating the Teacher's Null Space, SOE serves as a geometric bridge, effectively ejecting the model from local optima to explore diverse, high-value solution spaces. Experiments on mathematical benchmarks demonstrate that, relative to baseline methods, our approach improves average accuracy by 62.4% and increases average sampling efficiency by 113.7%, indicating a promising path toward overcoming performance plateaus in advanced reasoning tasks.",
      "publishedDate": "2026-01-06T16:39:40Z",
      "updatedDate": "2026-01-06T16:39:40Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06160v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06160",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06940",
      "title": "VISTA: Knowledge-Driven Interpretable Vessel Trajectory Imputation via Large Language Models",
      "authors": [
        {
          "name": "Hengyu Liu",
          "affiliation": null
        },
        {
          "name": "Tianyi Li",
          "affiliation": null
        },
        {
          "name": "Haoyu Wang",
          "affiliation": null
        },
        {
          "name": "Kristian Torp",
          "affiliation": null
        },
        {
          "name": "Tiancheng Zhang",
          "affiliation": null
        },
        {
          "name": "Yushuai Li",
          "affiliation": null
        },
        {
          "name": "Christian S. Jensen",
          "affiliation": null
        }
      ],
      "abstract": "The Automatic Identification System provides critical information for maritime navigation and safety, yet its trajectories are often incomplete due to signal loss or deliberate tampering. Existing imputation methods emphasize trajectory recovery, paying limited attention to interpretability and failing to provide underlying knowledge that benefits downstream tasks such as anomaly detection and route planning. We propose knowledge-driven interpretable vessel trajectory imputation (VISTA), the first trajectory imputation framework that offers interpretability while simultaneously providing underlying knowledge to support downstream analysis. Specifically, we first define underlying knowledge as a combination of Structured Data-derived Knowledge (SDK) distilled from AIS data and Implicit LLM Knowledge acquired from large-scale Internet corpora. Second, to manage and leverage the SDK effectively at scale, we develop a data-knowledge-data loop that employs a Structured Data-derived Knowledge Graph for SDK extraction and knowledge-driven trajectory imputation. Third, to efficiently process large-scale AIS data, we introduce a workflow management layer that coordinates the end-to-end pipeline, enabling parallel knowledge extraction and trajectory imputation with anomaly handling and redundancy elimination. Experiments on two large AIS datasets show that VISTA is capable of state-of-the-art imputation accuracy and computational efficiency, improving over state-of-the-art baselines by 5%-94% and reducing time cost by 51%-93%, while producing interpretable knowledge cues that benefit downstream tasks. The source code and implementation details of VISTA are publicly available.",
      "publishedDate": "2026-01-11T15:02:28Z",
      "updatedDate": "2026-01-11T15:02:28Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06940v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06940",
      "comment": "22 pages, 13 figures, 3 algorithms, 5 tables. Code available at https://github.com/hyLiu1994/VISTA",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "planning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "planning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06703",
      "title": "Mapping and Comparing Climate Equity Policy Practices Using RAG LLM-Based Semantic Analysis and Recommendation Systems",
      "authors": [
        {
          "name": "Seung Jun Choi",
          "affiliation": null
        }
      ],
      "abstract": "This study investigates the use of large language models to enhance the policymaking process. We first analyze planning-related job postings to revisit the evolving roles of planners in the era of AI. We then examine climate equity plans across the U.S. and apply ChatGPT to conduct semantic analysis, extracting policy, strategy, and action items related to transportation and energy. The methodological framework relied on a LangChain-native retrieval-augmented generation pipeline. Based on these extracted elements and their evaluated presence, we develop a content-based recommendation system to support cross-city policy comparison. The results indicate that, despite growing attention to AI, planning jobs largely retain their traditional domain emphases in transportation, environmental planning, housing, and land use. Communicative responsibilities remain central to planning practice. Climate equity plans commonly address transportation, environmental, and energy-related measures aimed at reducing greenhouse gas emissions and predominantly employ affirmative language. The demonstration of the recommendation system illustrates how planners can efficiently identify cities with similar policy practices, revealing patterns of geographic similarity in policy adoption. The study concludes by envisioning localized yet personalized AI-assisted systems that can be adapted within urban systems.",
      "publishedDate": "2026-01-10T22:01:28Z",
      "updatedDate": "2026-01-10T22:01:28Z",
      "primaryCategory": "cs.CY",
      "arxivCategories": [
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06703v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06703",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "planning"
      ],
      "tags": {
        "auto": [
          "rag",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06652",
      "title": "Follow the Signs: Using Textual Cues and LLMs to Guide Efficient Robot Navigation",
      "authors": [
        {
          "name": "Jing Cao",
          "affiliation": null
        },
        {
          "name": "Nishanth Kumar",
          "affiliation": null
        },
        {
          "name": "Aidan Curtis",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous navigation in unfamiliar environments often relies on geometric mapping and planning strategies that overlook rich semantic cues such as signs, room numbers, and textual labels. We propose a novel semantic navigation framework that leverages large language models (LLMs) to infer patterns from partial observations and predict regions where the goal is most likely located. Our method combines local perceptual inputs with frontier-based exploration and periodic LLM queries, which extract symbolic patterns (e.g., room numbering schemes and building layout structures) and update a confidence grid used to guide exploration. This enables robots to move efficiently toward goal locations labeled with textual identifiers (e.g., \"room 8\") even before direct observation. We demonstrate that this approach enables more efficient navigation in sparse, partially observable grid environments by exploiting symbolic patterns. Experiments across environments modeled after real floor plans show that our approach consistently achieves near-optimal paths and outperforms baselines by over 25% in Success weighted by Path Length.",
      "publishedDate": "2026-01-10T18:47:25Z",
      "updatedDate": "2026-01-10T18:47:25Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06652v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06652",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "planning",
        "rag",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "rag",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06562",
      "title": "Mosaic: Unlocking Long-Context Inference for Diffusion LLMs via Global Memory Planning and Dynamic Peak Taming",
      "authors": [
        {
          "name": "Liang Zheng",
          "affiliation": null
        },
        {
          "name": "Bowen Shi",
          "affiliation": null
        },
        {
          "name": "Yitao Hu",
          "affiliation": null
        },
        {
          "name": "Jiawei Zhang",
          "affiliation": null
        },
        {
          "name": "Ruofan Li",
          "affiliation": null
        },
        {
          "name": "Sheng Chen",
          "affiliation": null
        },
        {
          "name": "Wenxin Li",
          "affiliation": null
        },
        {
          "name": "Keqiu Li",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion-based large language models (dLLMs) have emerged as a promising paradigm, utilizing simultaneous denoising to enable global planning and iterative refinement. While these capabilities are particularly advantageous for long-context generation, deploying such models faces a prohibitive memory capacity barrier stemming from severe system inefficiencies. We identify that existing inference systems are ill-suited for this paradigm: unlike autoregressive models constrained by the cumulative KV-cache, dLLMs are bottlenecked by transient activations recomputed at every step. Furthermore, general-purpose memory reuse mechanisms lack the global visibility to adapt to dLLMs' dynamic memory peaks, which toggle between logits and FFNs. To address these mismatches, we propose Mosaic, a memory-efficient inference system that shifts from local, static management to a global, dynamic paradigm. Mosaic integrates a mask-only logits kernel to eliminate redundancy, a lazy chunking optimizer driven by an online heuristic search to adaptively mitigate dynamic peaks, and a global memory manager to resolve fragmentation via virtual addressing. Extensive evaluations demonstrate that Mosaic achieves an average 2.71$\\times$ reduction in the memory peak-to-average ratio and increases the maximum inference sequence length supportable on identical hardware by 15.89-32.98$\\times$. This scalability is achieved without compromising accuracy and speed, and in fact reducing latency by 4.12%-23.26%.",
      "publishedDate": "2026-01-10T13:17:08Z",
      "updatedDate": "2026-01-10T13:17:08Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06562v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06562",
      "comment": "11 pages, 18 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "planning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06505",
      "title": "Neural Nonmyopic Bayesian Optimization in Dynamic Cost Settings",
      "authors": [
        {
          "name": "Sang T. Truong",
          "affiliation": null
        },
        {
          "name": "Duc Q. Nguyen",
          "affiliation": null
        },
        {
          "name": "Willie Neiswanger",
          "affiliation": null
        },
        {
          "name": "Ryan-Rhys Griffiths",
          "affiliation": null
        },
        {
          "name": "Stefano Ermon",
          "affiliation": null
        },
        {
          "name": "Nick Haber",
          "affiliation": null
        },
        {
          "name": "Sanmi Koyejo",
          "affiliation": null
        }
      ],
      "abstract": "Bayesian optimization (BO) is a common framework for optimizing black-box functions, yet most existing methods assume static query costs and rely on myopic acquisition strategies. We introduce LookaHES, a nonmyopic BO framework designed for dynamic, history-dependent cost environments, where evaluation costs vary with prior actions, such as travel distance in spatial tasks or edit distance in sequence design. LookaHES combines a multi-step variant of $H$-Entropy Search with pathwise sampling and neural policy optimization, enabling long-horizon planning beyond twenty steps without the exponential complexity of existing nonmyopic methods. The key innovation is the integration of neural policies, including large language models, to effectively navigate structured, combinatorial action spaces such as protein sequences. These policies amortize lookahead planning and can be integrated with domain-specific constraints during rollout. Empirically, LookaHES outperforms strong myopic and nonmyopic baselines across nine synthetic benchmarks from two to eight dimensions and two real-world tasks: geospatial optimization using NASA night-light imagery and protein sequence design with constrained token-level edits. In short, LookaHES provides a general, scalable, and cost-aware solution for robust long-horizon optimization in complex decision spaces, which makes it a useful tool for researchers in machine learning, statistics, and applied domains. Our implementation is available at https://github.com/sangttruong/nonmyopia.",
      "publishedDate": "2026-01-10T09:49:45Z",
      "updatedDate": "2026-01-10T09:49:45Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06505v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06505",
      "comment": "32 pages, 20 figures, 13 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "planning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07606",
      "title": "Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments",
      "authors": [
        {
          "name": "Bingyang Ye",
          "affiliation": null
        },
        {
          "name": "Shan Chen",
          "affiliation": null
        },
        {
          "name": "Jingxuan Tu",
          "affiliation": null
        },
        {
          "name": "Chen Liu",
          "affiliation": null
        },
        {
          "name": "Zidi Xiong",
          "affiliation": null
        },
        {
          "name": "Samuel Schmidgall",
          "affiliation": null
        },
        {
          "name": "Danielle S. Bitterman",
          "affiliation": null
        }
      ],
      "abstract": "Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models' judgments about these scientific ideas. Towards this goal, we introduce PoT, a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later (e.g., citations and shifts in researchers' agendas). PoT freezes a pre-cutoff snapshot of evidence in an offline sandbox and asks models to forecast post-cutoff outcomes, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human-model misalignment against signals such as peer-review awards. In addition, PoT provides a controlled testbed for agent-based research judgments that evaluate scientific ideas, comparing tool-using agents to non-agent baselines under prompt ablations and budget scaling. Across 30,000+ instances spanning four benchmark domains, we find that, compared with non-agent baselines, higher interaction budgets generally improve agent performance, while the benefit of tool use is strongly task-dependent. By combining time-partitioned, future-verifiable targets with an offline sandbox for tool use, PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.",
      "publishedDate": "2026-01-12T14:55:37Z",
      "updatedDate": "2026-01-12T14:55:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07606v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07606",
      "comment": "under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06910",
      "title": "PenForge: On-the-Fly Expert Agent Construction for Automated Penetration Testing",
      "authors": [
        {
          "name": "Huihui Huang",
          "affiliation": null
        },
        {
          "name": "Jieke Shi",
          "affiliation": null
        },
        {
          "name": "Junkai Chen",
          "affiliation": null
        },
        {
          "name": "Ting Zhang",
          "affiliation": null
        },
        {
          "name": "Yikun Li",
          "affiliation": null
        },
        {
          "name": "Chengran Yang",
          "affiliation": null
        },
        {
          "name": "Eng Lieh Ouh",
          "affiliation": null
        },
        {
          "name": "Lwin Khin Shar",
          "affiliation": null
        },
        {
          "name": "David Lo",
          "affiliation": null
        }
      ],
      "abstract": "Penetration testing is essential for identifying vulnerabilities in web applications before real adversaries can exploit them. Recent work has explored automating this process with Large Language Model (LLM)-powered agents, but existing approaches either rely on a single generic agent that struggles in complex scenarios or narrowly specialized agents that cannot adapt to diverse vulnerability types. We therefore introduce PenForge, a framework that dynamically constructs expert agents during testing rather than relying on those prepared beforehand. By integrating automated reconnaissance of potential attack surfaces with agents instantiated on the fly for context-aware exploitation, PenForge achieves a 30.0% exploit success rate (12/40) on CVE-Bench in the particularly challenging zero-day setting, which is a 3 times improvement over the state-of-the-art. Our analysis also identifies three opportunities for future work: (1) supplying richer tool-usage knowledge to improve exploitation effectiveness; (2) extending benchmarks to include more vulnerabilities and attack types; and (3) fostering developer trust by incorporating explainable mechanisms and human review. As an emerging result with substantial potential impact, PenForge embodies the early-stage yet paradigm-shifting idea of on-the-fly agent construction, marking its promise as a step toward scalable and effective LLM-driven penetration testing.",
      "publishedDate": "2026-01-11T13:29:32Z",
      "updatedDate": "2026-01-11T13:29:32Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06910v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06910",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07252",
      "title": "SwarmFoam: An OpenFOAM Multi-Agent System Based on Multiple Types of Large Language Models",
      "authors": [
        {
          "name": "Chunwei Yang",
          "affiliation": null
        },
        {
          "name": "Yankai Wang",
          "affiliation": null
        },
        {
          "name": "Jianxiang Tang",
          "affiliation": null
        },
        {
          "name": "Haojie Qu",
          "affiliation": null
        },
        {
          "name": "Ziqiang Zou",
          "affiliation": null
        },
        {
          "name": "YuLiu",
          "affiliation": null
        },
        {
          "name": "Chunrui Deng",
          "affiliation": null
        },
        {
          "name": "Zhifang Qiu",
          "affiliation": null
        },
        {
          "name": "Ming Ding",
          "affiliation": null
        }
      ],
      "abstract": "Numerical simulation is one of the mainstream methods in scientific research, typically performed by professional engineers. With the advancement of multi-agent technology, using collaborating agents to replicate human behavior shows immense potential for intelligent Computational Fluid Dynamics (CFD) simulations. Some muti-agent systems based on Large Language Models have been proposed. However, they exhibit significant limitations when dealing with complex geometries. This paper introduces a new multi-agent simulation framework, SwarmFoam. SwarmFoam integrates functionalities such as Multi-modal perception, Intelligent error correction, and Retrieval-Augmented Generation, aiming to achieve more complex simulations through dual parsing of images and high-level instructions. Experimental results demonstrate that SwarmFoam has good adaptability to simulation inputs from different modalities. The overall pass rate for 25 test cases was 84%, with natural language and multi-modal input cases achieving pass rates of 80% and 86.7%, respectively. The work presented by SwarmFoam will further promote the development of intelligent agent methods for CFD.",
      "publishedDate": "2026-01-12T06:37:26Z",
      "updatedDate": "2026-01-12T06:37:26Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07252v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07252",
      "comment": "26 pages, 15 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "rag",
        "multi-agent",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "multi-agent",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07136",
      "title": "A Large-Scale Study on the Development and Issues of Multi-Agent AI Systems",
      "authors": [
        {
          "name": "Daniel Liu",
          "affiliation": null
        },
        {
          "name": "Krishna Upadhyay",
          "affiliation": null
        },
        {
          "name": "Vinaik Chhetri",
          "affiliation": null
        },
        {
          "name": "A. B. Siddique",
          "affiliation": null
        },
        {
          "name": "Umar Farooq",
          "affiliation": null
        }
      ],
      "abstract": "The rapid emergence of multi-agent AI systems (MAS), including LangChain, CrewAI, and AutoGen, has shaped how large language model (LLM) applications are developed and orchestrated. However, little is known about how these systems evolve and are maintained in practice. This paper presents the first large-scale empirical study of open-source MAS, analyzing over 42K unique commits and over 4.7K resolved issues across eight leading systems. Our analysis identifies three distinct development profiles: sustained, steady, and burst-driven. These profiles reflect substantial variation in ecosystem maturity. Perfective commits constitute 40.8% of all changes, suggesting that feature enhancement is prioritized over corrective maintenance (27.4%) and adaptive updates (24.3%). Data about issues shows that the most frequent concerns involve bugs (22%), infrastructure (14%), and agent coordination challenges (10%). Issue reporting also increased sharply across all frameworks starting in 2023. Median resolution times range from under one day to about two weeks, with distributions skewed toward fast responses but a minority of issues requiring extended attention. These results highlight both the momentum and the fragility of the current ecosystem, emphasizing the need for improved testing infrastructure, documentation quality, and maintenance practices to ensure long-term reliability and sustainability.",
      "publishedDate": "2026-01-12T02:07:15Z",
      "updatedDate": "2026-01-12T02:07:15Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07136v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07136",
      "comment": "8 pages, 8 figures, IEEE BigData Workshop on Software Engineering for Agentic AI 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07072",
      "title": "Overcoming the Retrieval Barrier: Indirect Prompt Injection in the Wild for LLM Systems",
      "authors": [
        {
          "name": "Hongyan Chang",
          "affiliation": null
        },
        {
          "name": "Ergute Bao",
          "affiliation": null
        },
        {
          "name": "Xinjian Luo",
          "affiliation": null
        },
        {
          "name": "Ting Yu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) increasingly rely on retrieving information from external corpora. This creates a new attack surface: indirect prompt injection (IPI), where hidden instructions are planted in the corpora and hijack model behavior once retrieved. Previous studies have highlighted this risk but often avoid the hardest step: ensuring that malicious content is actually retrieved. In practice, unoptimized IPI is rarely retrieved under natural queries, which leaves its real-world impact unclear. We address this challenge by decomposing the malicious content into a trigger fragment that guarantees retrieval and an attack fragment that encodes arbitrary attack objectives. Based on this idea, we design an efficient and effective black-box attack algorithm that constructs a compact trigger fragment to guarantee retrieval for any attack fragment. Our attack requires only API access to embedding models, is cost-efficient (as little as $0.21 per target user query on OpenAI's embedding models), and achieves near-100% retrieval across 11 benchmarks and 8 embedding models (including both open-source models and proprietary services). Based on this attack, we present the first end-to-end IPI exploits under natural queries and realistic external corpora, spanning both RAG and agentic systems with diverse attack objectives. These results establish IPI as a practical and severe threat: when a user issued a natural query to summarize emails on frequently asked topics, a single poisoned email was sufficient to coerce GPT-4o into exfiltrating SSH keys with over 80% success in a multi-agent workflow. We further evaluate several defenses and find that they are insufficient to prevent the retrieval of malicious text, highlighting retrieval as a critical open vulnerability.",
      "publishedDate": "2026-01-11T21:33:59Z",
      "updatedDate": "2026-01-11T21:33:59Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07072v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07072",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "prompting",
        "agents",
        "tool-use",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "agents",
          "tool-use",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06966",
      "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
      "authors": [
        {
          "name": "Haonan Bian",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Yao",
          "affiliation": null
        },
        {
          "name": "Sen Hu",
          "affiliation": null
        },
        {
          "name": "Zishan Xu",
          "affiliation": null
        },
        {
          "name": "Shaolei Zhang",
          "affiliation": null
        },
        {
          "name": "Yifu Guo",
          "affiliation": null
        },
        {
          "name": "Ziliang Yang",
          "affiliation": null
        },
        {
          "name": "Xueran Han",
          "affiliation": null
        },
        {
          "name": "Huacan Wang",
          "affiliation": null
        },
        {
          "name": "Ronghao Chen",
          "affiliation": null
        }
      ],
      "abstract": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals. To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation. We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects. Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).",
      "publishedDate": "2026-01-11T15:49:36Z",
      "updatedDate": "2026-01-11T15:49:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06966v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06966",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "multi-agent",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06776",
      "title": "From Text to Simulation: A Multi-Agent LLM Workflow for Automated Chemical Process Design",
      "authors": [
        {
          "name": "Xufei Tian",
          "affiliation": null
        },
        {
          "name": "Wenli Du",
          "affiliation": null
        },
        {
          "name": "Shaoyi Yang",
          "affiliation": null
        },
        {
          "name": "Han Hu",
          "affiliation": null
        },
        {
          "name": "Hui Xin",
          "affiliation": null
        },
        {
          "name": "Shifeng Qu",
          "affiliation": null
        },
        {
          "name": "Ke Ye",
          "affiliation": null
        }
      ],
      "abstract": "Process simulation is a critical cornerstone of chemical engineering design. Current automated chemical design methodologies focus mainly on various representations of process flow diagrams. However, transforming these diagrams into executable simulation flowsheets remains a time-consuming and labor-intensive endeavor, requiring extensive manual parameter configuration within simulation software. In this work, we propose a novel multi-agent workflow that leverages the semantic understanding capabilities of large language models(LLMs) and enables iterative interactions with chemical process simulation software, achieving end-to-end automated simulation from textual process specifications to computationally validated software configurations for design enhancement. Our approach integrates four specialized agents responsible for task understanding, topology generation, parameter configuration, and evaluation analysis, respectively, coupled with Enhanced Monte Carlo Tree Search to accurately interpret semantics and robustly generate configurations. Evaluated on Simona, a large-scale process description dataset, our method achieves a 31.1% improvement in the simulation convergence rate compared to state-of-the-art baselines and reduces the design time by 89. 0% compared to the expert manual design. This work demonstrates the potential of AI-assisted chemical process design, which bridges the gap between conceptual design and practical implementation. Our workflow is applicable to diverse process-oriented industries, including pharmaceuticals, petrochemicals, food processing, and manufacturing, offering a generalizable solution for automated process design.",
      "publishedDate": "2026-01-11T04:41:57Z",
      "updatedDate": "2026-01-11T04:41:57Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06776v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06776",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "rag",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06235",
      "title": "An Intelligent AI glasses System with Multi-Agent Architecture for Real-Time Voice Processing and Task Execution",
      "authors": [
        {
          "name": "Sheng-Kai Chen",
          "affiliation": null
        },
        {
          "name": "Jyh-Horng Wu",
          "affiliation": null
        },
        {
          "name": "Ching-Yao Lin",
          "affiliation": null
        },
        {
          "name": "Yen-Ting Lin",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents an AI glasses system that integrates real-time voice processing, artificial intelligence(AI) agents, and cross-network streaming capabilities. The system employs dual-agent architecture where Agent 01 handles Automatic Speech Recognition (ASR) and Agent 02 manages AI processing through local Large Language Models (LLMs), Model Context Protocol (MCP) tools, and Retrieval-Augmented Generation (RAG). The system supports real-time RTSP streaming for voice and video data transmission, eye tracking data collection, and remote task execution through RabbitMQ messaging. Implementation demonstrates successful voice command processing with multilingual support and cross-platform task execution capabilities.",
      "publishedDate": "2026-01-09T15:13:32Z",
      "updatedDate": "2026-01-09T15:13:32Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06235v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06235",
      "comment": "Published in NCS 2025 (Paper No. N0180)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07711",
      "title": "Is Agentic RAG worth it? An experimental comparison of RAG approaches",
      "authors": [
        {
          "name": "Pietro Ferrazzi",
          "affiliation": null
        },
        {
          "name": "Milica Cvjeticanin",
          "affiliation": null
        },
        {
          "name": "Alessio Piraccini",
          "affiliation": null
        },
        {
          "name": "Davide Giannuzzi",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of \"Enhanced\" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as \"Agentic\" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.",
      "publishedDate": "2026-01-12T16:43:44Z",
      "updatedDate": "2026-01-12T16:43:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07711v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07711",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "agents",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07528",
      "title": "From RAG to Agentic RAG for Faithful Islamic Question Answering",
      "authors": [
        {
          "name": "Gagan Bhatia",
          "affiliation": null
        },
        {
          "name": "Hamdy Mubarak",
          "affiliation": null
        },
        {
          "name": "Mustafa Jarrar",
          "affiliation": null
        },
        {
          "name": "George Mikros",
          "affiliation": null
        },
        {
          "name": "Fadi Zaraket",
          "affiliation": null
        },
        {
          "name": "Mahmoud Alhirthani",
          "affiliation": null
        },
        {
          "name": "Mutaz Al-Khatib",
          "affiliation": null
        },
        {
          "name": "Logan Cochrane",
          "affiliation": null
        },
        {
          "name": "Kareem Darwish",
          "affiliation": null
        },
        {
          "name": "Rashid Yahiaoui",
          "affiliation": null
        },
        {
          "name": "Firoj Alam",
          "affiliation": null
        }
      ],
      "abstract": "LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ/MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur'an retrieval corpus of $\\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.",
      "publishedDate": "2026-01-12T13:28:28Z",
      "updatedDate": "2026-01-12T13:28:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07528v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07528",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07329",
      "title": "BayesRAG: Probabilistic Mutual Evidence Corroboration for Multimodal Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Xuan Li",
          "affiliation": null
        },
        {
          "name": "Yining Wang",
          "affiliation": null
        },
        {
          "name": "Haocai Luo",
          "affiliation": null
        },
        {
          "name": "Shengping Liu",
          "affiliation": null
        },
        {
          "name": "Jerry Liang",
          "affiliation": null
        },
        {
          "name": "Ying Fu",
          "affiliation": null
        },
        {
          "name": "Weihuang",
          "affiliation": null
        },
        {
          "name": "Jun Yu",
          "affiliation": null
        },
        {
          "name": "Junnan Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has become a pivotal paradigm for Large Language Models (LLMs), yet current approaches struggle with visually rich documents by treating text and images as isolated retrieval targets. Existing methods relying solely on cosine similarity often fail to capture the semantic reinforcement provided by cross-modal alignment and layout-induced coherence. To address these limitations, we propose BayesRAG, a novel multimodal retrieval framework grounded in Bayesian inference and Dempster-Shafer evidence theory. Unlike traditional approaches that rank candidates strictly by similarity, BayesRAG models the intrinsic consistency of retrieved candidates across modalities as probabilistic evidence to refine retrieval confidence. Specifically, our method computes the posterior association probability for combinations of multimodal retrieval results, prioritizing text-image pairs that mutually corroborate each other in terms of both semantics and layout. Extensive experiments demonstrate that BayesRAG significantly outperforms state-of-the-art (SOTA) methods on challenging multimodal benchmarks. This study establishes a new paradigm for multimodal retrieval fusion that effectively resolves the isolation of heterogeneous modalities through an evidence fusion mechanism and enhances the robustness of retrieval outcomes. Our code is available at https://github.com/TioeAre/BayesRAG.",
      "publishedDate": "2026-01-12T08:53:14Z",
      "updatedDate": "2026-01-12T08:53:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07329v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07329",
      "comment": "17 pages, 8 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07226",
      "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
      "authors": [
        {
          "name": "Seongyun Lee",
          "affiliation": null
        },
        {
          "name": "Yongrae Jo",
          "affiliation": null
        },
        {
          "name": "Minju Seo",
          "affiliation": null
        },
        {
          "name": "Moontae Lee",
          "affiliation": null
        },
        {
          "name": "Minjoon Seo",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.",
      "publishedDate": "2026-01-12T05:43:51Z",
      "updatedDate": "2026-01-12T05:43:51Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07226v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07226",
      "comment": "Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "evaluation",
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06979",
      "title": "MedTutor: A Retrieval-Augmented LLM System for Case-Based Medical Education",
      "authors": [
        {
          "name": "Dongsuk Jang",
          "affiliation": null
        },
        {
          "name": "Ziyao Shangguan",
          "affiliation": null
        },
        {
          "name": "Kyle Tegtmeyer",
          "affiliation": null
        },
        {
          "name": "Anurag Gupta",
          "affiliation": null
        },
        {
          "name": "Jan Czerminski",
          "affiliation": null
        },
        {
          "name": "Sophie Chheang",
          "affiliation": null
        },
        {
          "name": "Arman Cohan",
          "affiliation": null
        }
      ],
      "abstract": "The learning process for medical residents presents significant challenges, demanding both the ability to interpret complex case reports and the rapid acquisition of accurate medical knowledge from reliable sources. Residents typically study case reports and engage in discussions with peers and mentors, but finding relevant educational materials and evidence to support their learning from these cases is often time-consuming and challenging. To address this, we introduce MedTutor, a novel system designed to augment resident training by automatically generating evidence-based educational content and multiple-choice questions from clinical case reports. MedTutor leverages a Retrieval-Augmented Generation (RAG) pipeline that takes clinical case reports as input and produces targeted educational materials. The system's architecture features a hybrid retrieval mechanism that synergistically queries a local knowledge base of medical textbooks and academic literature (using PubMed, Semantic Scholar APIs) for the latest related research, ensuring the generated content is both foundationally sound and current. The retrieved evidence is filtered and ordered using a state-of-the-art reranking model and then an LLM generates the final long-form output describing the main educational content regarding the case-report. We conduct a rigorous evaluation of the system. First, three radiologists assessed the quality of outputs, finding them to be of high clinical and educational value. Second, we perform a large scale evaluation using an LLM-as-a Judge to understand if LLMs can be used to evaluate the output of the system. Our analysis using correlation between LLMs outputs and human expert judgments reveals a moderate alignment and highlights the continued necessity of expert oversight.",
      "publishedDate": "2026-01-11T16:27:21Z",
      "updatedDate": "2026-01-11T16:27:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06979v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06979",
      "comment": "Accepted to EMNLP 2025 (System Demonstrations)",
      "journalRef": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 319-353",
      "doi": "10.18653/v1/2025.emnlp-demos.24",
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06922",
      "title": "TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG",
      "authors": [
        {
          "name": "Tianhua Zhang",
          "affiliation": null
        },
        {
          "name": "Kun Li",
          "affiliation": null
        },
        {
          "name": "Junan Li",
          "affiliation": null
        },
        {
          "name": "Yunxiang Li",
          "affiliation": null
        },
        {
          "name": "Hongyin Luo",
          "affiliation": null
        },
        {
          "name": "Xixin Wu",
          "affiliation": null
        },
        {
          "name": "James Glass",
          "affiliation": null
        },
        {
          "name": "Helen Meng",
          "affiliation": null
        }
      ],
      "abstract": "Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.",
      "publishedDate": "2026-01-11T14:07:30Z",
      "updatedDate": "2026-01-11T14:07:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06922v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06922",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06842",
      "title": "Seeing through the Conflict: Transparent Knowledge Conflict Handling in Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Hua Ye",
          "affiliation": null
        },
        {
          "name": "Siyuan Chen",
          "affiliation": null
        },
        {
          "name": "Ziqi Zhong",
          "affiliation": null
        },
        {
          "name": "Canran Xiao",
          "affiliation": null
        },
        {
          "name": "Haoliang Zhang",
          "affiliation": null
        },
        {
          "name": "Yuhan Wu",
          "affiliation": null
        },
        {
          "name": "Fei Shen",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) equipped with retrieval--the Retrieval-Augmented Generation (RAG) paradigm--should combine their parametric knowledge with external evidence, yet in practice they often hallucinate, over-trust noisy snippets, or ignore vital context. We introduce TCR (Transparent Conflict Resolution), a plug-and-play framework that makes this decision process observable and controllable. TCR (i) disentangles semantic match and factual consistency via dual contrastive encoders, (ii) estimates self-answerability to gauge confidence in internal memory, and (iii) feeds the three scalar signals to the generator through a lightweight soft-prompt with SNR-based weighting. Across seven benchmarks TCR improves conflict detection (+5-18 F1), raises knowledge-gap recovery by +21.4 pp and cuts misleading-context overrides by -29.3 pp, while adding only 0.3% parameters. The signals align with human judgements and expose temporal decision patterns.",
      "publishedDate": "2026-01-11T10:08:49Z",
      "updatedDate": "2026-01-11T10:08:49Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06842v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06842",
      "comment": "9 pages, 9 figures, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "evaluation",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06799",
      "title": "CIRAG: Construction-Integration Retrieval and Adaptive Generation for Multi-hop Question Answering",
      "authors": [
        {
          "name": "Zili Wei",
          "affiliation": null
        },
        {
          "name": "Xiaocui Yang",
          "affiliation": null
        },
        {
          "name": "Yilin Wang",
          "affiliation": null
        },
        {
          "name": "Zihan Wang",
          "affiliation": null
        },
        {
          "name": "Weidong Bao",
          "affiliation": null
        },
        {
          "name": "Shi Feng",
          "affiliation": null
        },
        {
          "name": "Daling Wang",
          "affiliation": null
        },
        {
          "name": "Yifei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Triple-based Iterative Retrieval-Augmented Generation (iRAG) mitigates document-level noise for multi-hop question answering. However, existing methods still face limitations: (i) greedy single-path expansion, which propagates early errors and fails to capture parallel evidence from different reasoning branches, and (ii) granularity-demand mismatch, where a single evidence representation struggles to balance noise control with contextual sufficiency. In this paper, we propose the Construction-Integration Retrieval and Adaptive Generation model, CIRAG. It introduces an Iterative Construction-Integration module that constructs candidate triples and history-conditionally integrates them to distill core triples and generate the next-hop query. This module mitigates the greedy trap by preserving multiple plausible evidence chains. Besides, we propose an Adaptive Cascaded Multi-Granularity Generation module that progressively expands contextual evidence based on the problem requirements, from triples to supporting sentences and full passages. Moreover, we introduce Trajectory Distillation, which distills the teacher model's integration policy into a lightweight student, enabling efficient and reliable long-horizon reasoning. Extensive experiments demonstrate that CIRAG achieves superior performance compared to existing iRAG methods.",
      "publishedDate": "2026-01-11T07:56:02Z",
      "updatedDate": "2026-01-11T07:56:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06799v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06799",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06727",
      "title": "Vextra: A Unified Middleware Abstraction for Heterogeneous Vector Database Systems",
      "authors": [
        {
          "name": "Chandan Suri",
          "affiliation": null
        },
        {
          "name": "Gursifath Bhasin",
          "affiliation": null
        }
      ],
      "abstract": "The rapid integration of vector search into AI applications, particularly for Retrieval Augmented Generation (RAG), has catalyzed the emergence of a diverse ecosystem of specialized vector databases. While this innovation offers a rich choice of features and performance characteristics, it has simultaneously introduced a significant challenge: severe API fragmentation. Developers face a landscape of disparate, proprietary, and often volatile API contracts, which hinders application portability, increases maintenance overhead, and leads to vendor lock-in. This paper introduces Vextra, a novel middleware abstraction layer designed to address this fragmentation. Vextra presents a unified, high-level API for core database operations, including data upsertion, similarity search, and metadata filtering. It employs a pluggable adapter architecture to translate these unified API calls into the native protocols of various backend databases. We argue that such an abstraction layer is a critical step towards maturing the vector database ecosystem, fostering interoperability, and enabling higher-level query optimization, while imposing minimal performance overhead.",
      "publishedDate": "2026-01-11T00:35:35Z",
      "updatedDate": "2026-01-11T00:35:35Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06727v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06727",
      "comment": "11 pages, 8 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06607",
      "title": "Pragya: An AI-Based Semantic Recommendation System for Sanskrit Subhasitas",
      "authors": [
        {
          "name": "Tanisha Raorane",
          "affiliation": null
        },
        {
          "name": "Prasenjit Kole",
          "affiliation": null
        }
      ],
      "abstract": "Sanskrit Subhasitas encapsulate centuries of cultural and philosophical wisdom, yet remain underutilized in the digital age due to linguistic and contextual barriers. In this work, we present Pragya, a retrieval-augmented generation (RAG) framework for semantic recommendation of Subhasitas. We curate a dataset of 200 verses annotated with thematic tags such as motivation, friendship, and compassion. Using sentence embeddings (IndicBERT), the system retrieves top-k verses relevant to user queries. The retrieved results are then passed to a generative model (Mistral LLM) to produce transliterations, translations, and contextual explanations. Experimental evaluation demonstrates that semantic retrieval significantly outperforms keyword matching in precision and relevance, while user studies highlight improved accessibility through generated summaries. To our knowledge, this is the first attempt at integrating retrieval and generation for Sanskrit Subhasitas, bridging cultural heritage with modern applied AI.",
      "publishedDate": "2026-01-10T16:13:25Z",
      "updatedDate": "2026-01-10T16:13:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06607v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06607",
      "comment": "Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06603",
      "title": "N2N-GQA: Noise-to-Narrative for Graph-Based Table-Text Question Answering Using LLMs",
      "authors": [
        {
          "name": "Mohamed Sharafath",
          "affiliation": null
        },
        {
          "name": "Aravindh Annamalai",
          "affiliation": null
        },
        {
          "name": "Ganesh Murugan",
          "affiliation": null
        },
        {
          "name": "Aravindakumar Venugopalan",
          "affiliation": null
        }
      ],
      "abstract": "Multi-hop question answering over hybrid table-text data requires retrieving and reasoning across multiple evidence pieces from large corpora, but standard Retrieval-Augmented Generation (RAG) pipelines process documents as flat ranked lists, causing retrieval noise to obscure reasoning chains. We introduce N2N-GQA. To our knowledge, it is the first zeroshot framework for open-domain hybrid table-text QA that constructs dynamic evidence graphs from noisy retrieval outputs. Our key insight is that multi-hop reasoning requires understanding relationships between evidence pieces: by modeling documents as graph nodes with semantic relationships as edges, we identify bridge documents connecting reasoning steps, a capability absent in list-based retrieval. On OTT-QA, graph-based evidence curation provides a 19.9-point EM improvement over strong baselines, demonstrating that organizing retrieval results as structured graphs is critical for multihop reasoning. N2N-GQA achieves 48.80 EM, matching finetuned retrieval models (CORE: 49.0 EM) and approaching heavily optimized systems (COS: 56.9 EM) without any task specific training. This establishes graph-structured evidence organization as essential for scalable, zero-shot multi-hop QA systems and demonstrates that simple, interpretable graph construction can rival sophisticated fine-tuned approaches.",
      "publishedDate": "2026-01-10T15:55:15Z",
      "updatedDate": "2026-01-10T15:55:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06603v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06603",
      "comment": "Accepted at an AAAI 2026 Workshop",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06564",
      "title": "CSR-RAG: An Efficient Retrieval System for Text-to-SQL on the Enterprise Scale",
      "authors": [
        {
          "name": "Rajpreet Singh",
          "affiliation": null
        },
        {
          "name": "Novak Boškov",
          "affiliation": null
        },
        {
          "name": "Lawrence Drabeck",
          "affiliation": null
        },
        {
          "name": "Aditya Gudal",
          "affiliation": null
        },
        {
          "name": "Manzoor A. Khan",
          "affiliation": null
        }
      ],
      "abstract": "Natural language to SQL translation (Text-to-SQL) is one of the long-standing problems that has recently benefited from advances in Large Language Models (LLMs). While most academic Text-to-SQL benchmarks request schema description as a part of natural language input, enterprise-scale applications often require table retrieval before SQL query generation. To address this need, we propose a novel hybrid Retrieval Augmented Generation (RAG) system consisting of contextual, structural, and relational retrieval (CSR-RAG) to achieve computationally efficient yet sufficiently accurate retrieval for enterprise-scale databases. Through extensive enterprise benchmarks, we demonstrate that CSR-RAG achieves up to 40% precision and over 80% recall while incurring a negligible average query generation latency of only 30ms on commodity data center hardware, which makes it appropriate for modern LLM-based enterprise-scale systems.",
      "publishedDate": "2026-01-10T13:20:07Z",
      "updatedDate": "2026-01-10T13:20:07Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06564v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06564",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06551",
      "title": "L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading",
      "authors": [
        {
          "name": "Sergii Voloshyn",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as the predominant paradigm for grounding Large Language Model outputs in factual knowledge, effectively mitigating hallucinations. However, conventional RAG systems operate under a \"retrieve-always\" assumption, querying vector databases for every input regardless of query complexity. This static approach incurs substantial computational overhead and inference latency, particularly problematic for high-throughput production deployments. We introduce L-RAG (Lazy Retrieval-Augmented Generation), an adaptive framework that implements hierarchical context management through entropy-based gating. L-RAG employs a two-tier architecture: queries are first processed with a compact document summary, and expensive chunk retrieval is triggered only when the model's predictive entropy exceeds a calibrated threshold, signaling genuine uncertainty. Through experiments on SQuAD 2.0 (N=500) using the Phi-2 model, we demonstrate that L-RAG provides a tunable accuracy-efficiency trade-off: at a conservative threshold (tau=0.5), L-RAG achieves 78.2% accuracy, matching Standard RAG (77.8%), with 8% retrieval reduction; at a balanced threshold (tau=1.0), retrieval reduction increases to 26% with modest accuracy trade-off (76.0%). Latency analysis shows that L-RAG saves 80-210ms per query when retrieval latency exceeds 500ms. Analysis of entropy distributions reveals statistically significant separation (p < 0.001) between correct predictions (H=1.72) and errors (H=2.20), validating entropy as a reliable uncertainty signal. L-RAG offers a practical, training-free approach toward more efficient RAG deployment, providing system architects with a configurable knob to balance accuracy and throughput requirements.",
      "publishedDate": "2026-01-10T12:25:19Z",
      "updatedDate": "2026-01-10T12:25:19Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06551v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06551",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06519",
      "title": "MedRAGChecker: Claim-Level Verification for Biomedical Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Yuelyu Ji",
          "affiliation": null
        },
        {
          "name": "Min Gu Kwak",
          "affiliation": null
        },
        {
          "name": "Hang Zhang",
          "affiliation": null
        },
        {
          "name": "Xizhi Wu",
          "affiliation": null
        },
        {
          "name": "Chenyu Li",
          "affiliation": null
        },
        {
          "name": "Yanshan Wang",
          "affiliation": null
        }
      ],
      "abstract": "Biomedical retrieval-augmented generation (RAG) can ground LLM answers in medical literature, yet long-form outputs often contain isolated unsupported or contradictory claims with safety implications. We introduce MedRAGChecker, a claim-level verification and diagnostic framework for biomedical RAG. Given a question, retrieved evidence, and a generated answer, MedRAGChecker decomposes the answer into atomic claims and estimates claim support by combining evidence-grounded natural language inference (NLI) with biomedical knowledge-graph (KG) consistency signals. Aggregating claim decisions yields answer-level diagnostics that help disentangle retrieval and generation failures, including faithfulness, under-evidence, contradiction, and safety-critical error rates. To enable scalable evaluation, we distill the pipeline into compact biomedical models and use an ensemble verifier with class-specific reliability weighting. Experiments on four biomedical QA benchmarks show that MedRAGChecker reliably flags unsupported and contradicted claims and reveals distinct risk profiles across generators, particularly on safety-critical biomedical relations.",
      "publishedDate": "2026-01-10T10:40:42Z",
      "updatedDate": "2026-01-10T10:40:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06519v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06519",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06426",
      "title": "NC-Bench: An LLM Benchmark for Evaluating Conversational Competence",
      "authors": [
        {
          "name": "Robert J. Moore",
          "affiliation": null
        },
        {
          "name": "Sungeun An",
          "affiliation": null
        },
        {
          "name": "Farhan Ahmed",
          "affiliation": null
        },
        {
          "name": "Jay Pankaj Gala",
          "affiliation": null
        }
      ],
      "abstract": "The Natural Conversation Benchmark (NC-Bench) introduce a new approach to evaluating the general conversational competence of large language models (LLMs). Unlike prior benchmarks that focus on the content of model behavior, NC-Bench focuses on the form and structure of natural conversation. Grounded in the IBM Natural Conversation Framework (NCF), NC-Bench comprises three distinct sets. The Basic Conversation Competence set evaluates fundamental sequence management practices, such as answering inquiries, repairing responses, and closing conversational pairs. The RAG set applies the same sequence management patterns as the first set but incorporates retrieval-augmented generation (RAG). The Complex Request set extends the evaluation to complex requests involving more intricate sequence management patterns. Each benchmark tests a model's ability to produce contextually appropriate conversational actions in response to characteristic interaction patterns. Initial evaluations across 6 open-source models and 14 interaction patterns show that models perform well on basic answering tasks, struggle more with repair tasks (especially repeat), have mixed performance on closing sequences, and find complex multi-turn requests most challenging, with Qwen models excelling on the Basic set and Granite models on the RAG set and the Complex Request set. By operationalizing fundamental principles of human conversation, NC-Bench provides a lightweight, extensible, and theory-grounded framework for assessing and improving the conversational abilities of LLMs beyond topical or task-specific benchmarks.",
      "publishedDate": "2026-01-10T04:57:24Z",
      "updatedDate": "2026-01-10T04:57:24Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06426v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06426",
      "comment": "9 pages, 1 figure, 2 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06282",
      "title": "Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning",
      "authors": [
        {
          "name": "Yue Zhou",
          "affiliation": null
        },
        {
          "name": "Xiaobo Guo",
          "affiliation": null
        },
        {
          "name": "Belhassen Bayar",
          "affiliation": null
        },
        {
          "name": "Srinivasan H. Sengamedu",
          "affiliation": null
        }
      ],
      "abstract": "Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.",
      "publishedDate": "2026-01-09T19:51:11Z",
      "updatedDate": "2026-01-09T19:51:11Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06282v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06282",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06189",
      "title": "Rational Synthesizers or Heuristic Followers? Analyzing LLMs in RAG-based Question-Answering",
      "authors": [
        {
          "name": "Atharv Naphade",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is the prevailing paradigm for grounding Large Language Models (LLMs), yet the mechanisms governing how models integrate groups of conflicting retrieved evidence remain opaque. Does an LLM answer a certain way because the evidence is factually strong, because of a prior belief, or merely because it is repeated frequently? To answer this, we introduce GroupQA, a curated dataset of 1,635 controversial questions paired with 15,058 diversely-sourced evidence documents, annotated for stance and qualitative strength. Through controlled experiments, we characterize group-level evidence aggregation dynamics: Paraphrasing an argument can be more persuasive than providing distinct independent support; Models favor evidence presented first rather than last, and Larger models are increasingly resistant to adapt to presented evidence. Additionally, we find that LLM explanations to group-based answers are unfaithful. Together, we show that LLMs behave consistently as vulnerable heuristic followers, with direct implications for improving RAG system design.",
      "publishedDate": "2026-01-08T01:54:21Z",
      "updatedDate": "2026-01-08T01:54:21Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06189v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06189",
      "comment": "13 pages, 9 figures, ACL ARR submission",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06152",
      "title": "HiMeS: Hippocampus-inspired Memory System for Personalized AI Assistants",
      "authors": [
        {
          "name": "Hailong Li",
          "affiliation": null
        },
        {
          "name": "Feifei Li",
          "affiliation": null
        },
        {
          "name": "Wenhui Que",
          "affiliation": null
        },
        {
          "name": "Xingyu Fan",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) power many interactive systems such as chatbots, customer-service agents, and personal assistants. In knowledge-intensive scenarios requiring user-specific personalization, conventional retrieval-augmented generation (RAG) pipelines exhibit limited memory capacity and insufficient coordination between retrieval mechanisms and user-specific conversational history, leading to redundant clarification, irrelevant documents, and degraded user experience. Inspired by the hippocampus-neocortex memory mechanism, we propose HiMeS, an AI-assistant architecture that fuses short-term and long-term memory. Our contributions are fourfold: (1) A short-term memory extractor is trained end-to-end with reinforcement learning to compress recent dialogue and proactively pre-retrieve documents from the knowledge base, emulating the cooperative interaction between the hippocampus and prefrontal cortex. (2) A partitioned long-term memory network stores user-specific information and re-ranks retrieved documents, simulating distributed cortical storage and memory reactivation. (3) On a real-world industrial dataset, HiMeS significantly outperforms a cascaded RAG baseline on question-answering quality. (4) Ablation studies confirm the necessity of both memory modules and suggest a practical path toward more reliable, context-aware, user-customized LLM-based assistants.",
      "publishedDate": "2026-01-06T05:05:50Z",
      "updatedDate": "2026-01-06T05:05:50Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06152v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06152",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06458",
      "title": "PixRec: Leveraging Visual Context for Next-Item Prediction in Sequential Recommendation",
      "authors": [
        {
          "name": "Sayak Chakrabarty",
          "affiliation": null
        },
        {
          "name": "Souradip Pal",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have recently shown strong potential for usage in sequential recommendation tasks through text-only models, which combine advanced prompt design, contrastive alignment, and fine-tuning on downstream domain-specific data. While effective, these approaches overlook the rich visual information present in many real-world recommendation scenarios, particularly in e-commerce. This paper proposes PixRec - a vision-language framework that incorporates both textual attributes and product images into the recommendation pipeline. Our architecture leverages a vision-language model backbone capable of jointly processing image-text sequences, maintaining a dual-tower structure and mixed training objective while aligning multi-modal feature projections for both item-item and user-item interactions. Using the Amazon Reviews dataset augmented with product images, our experiments demonstrate $3\\times$ and 40% improvements in top-rank and top-10 rank accuracy over text-only recommenders respectively, indicating that visual features can help distinguish items with similar textual descriptions. Our work outlines future directions for scaling multi-modal recommenders training, enhancing visual-text feature fusion, and evaluating inference-time performance. This work takes a step toward building software systems utilizing visual information in sequential recommendation for real-world applications like e-commerce.",
      "publishedDate": "2026-01-10T06:52:58Z",
      "updatedDate": "2026-01-10T06:52:58Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06458v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06458",
      "comment": "9 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06169",
      "title": "Think Bright, Diffuse Nice: Enhancing T2I-ICL via Inductive-Bias Hint Instruction and Query Contrastive Decoding",
      "authors": [
        {
          "name": "Zhiyong Ma",
          "affiliation": null
        },
        {
          "name": "Zhenpeng Li",
          "affiliation": null
        },
        {
          "name": "Yuanjie Shi",
          "affiliation": null
        },
        {
          "name": "Zhengping Li",
          "affiliation": null
        },
        {
          "name": "Jiahao Chen",
          "affiliation": null
        },
        {
          "name": "Qingyuan Chuai",
          "affiliation": null
        }
      ],
      "abstract": "Text-to-Image In-Context Learning (T2I-ICL) enables customized image synthesis via interleaved text-image examples but faces two mutually reinforcing bottlenecks, compliance failure and prior-dominated hallucination, that form a vicious cycle degrading generation quality. Existing methods rely on tailored training, which limits flexibility and raises deployment costs. To address these challenges effectively, we propose TBDN, a training-free framework integrating two complementary closed-loop mechanisms: Hint Instruction (HI) and Query Contrastive Decoding (QCD). HI injects task-aware inductive bias via lightweight prompt engineering to anchor models on contextual mapping rules, thereby mitigating compliance failure. QCD adjusts the decoding distributions of language models by contrasting full-input and query-omitted distributions, suppressing prior-dominated hallucination. TBDN achieves State-of-the-Art performance on CoBSAT and Text-to-Image Fast Mini-ImageNet, with robust generalization across model backbones, prompt designs, and hyperparameters. It also maintains promising performance in concept preservation and prompt following on Dreambench++. By breaking the two bottlenecks, TBDN establishes a simple yet effective framework for efficient and reliable T2I-ICL.",
      "publishedDate": "2026-01-07T06:39:45Z",
      "updatedDate": "2026-01-07T06:39:45Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06169v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06169",
      "comment": "Submitted to ACL 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07235",
      "title": "Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges",
      "authors": [
        {
          "name": "Agnivo Gosai",
          "affiliation": null
        },
        {
          "name": "Shuvodeep De",
          "affiliation": null
        },
        {
          "name": "Karun Thankachan",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.",
      "publishedDate": "2026-01-12T06:13:11Z",
      "updatedDate": "2026-01-12T06:13:11Z",
      "primaryCategory": "cs.IT",
      "arxivCategories": [
        "cs.IT"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07235v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07235",
      "comment": "31 Pages; 1 figure; 108 references; ongoing paper that would be submitted to suitable Wiley journal",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07005",
      "title": "MicLog: Towards Accurate and Efficient LLM-based Log Parsing via Progressive Meta In-Context Learning",
      "authors": [
        {
          "name": "Jianbo Yu",
          "affiliation": null
        },
        {
          "name": "Yixuan Li",
          "affiliation": null
        },
        {
          "name": "Hai Xu",
          "affiliation": null
        },
        {
          "name": "Kang Xu",
          "affiliation": null
        },
        {
          "name": "Junjielong Xu",
          "affiliation": null
        },
        {
          "name": "Zhijing Li",
          "affiliation": null
        },
        {
          "name": "Pinjia He",
          "affiliation": null
        },
        {
          "name": "Wanyuan Wang",
          "affiliation": null
        }
      ],
      "abstract": "Log parsing converts semi-structured logs into structured templates, forming a critical foundation for downstream analysis. Traditional syntax and semantic-based parsers often struggle with semantic variations in evolving logs and data scarcity stemming from their limited domain coverage. Recent large language model (LLM)-based parsers leverage in-context learning (ICL) to extract semantics from examples, demonstrating superior accuracy. However, LLM-based parsers face two main challenges: 1) underutilization of ICL capabilities, particularly in dynamic example selection and cross-domain generalization, leading to inconsistent performance; 2) time-consuming and costly LLM querying. To address these challenges, we present MicLog, the first progressive meta in-context learning (ProgMeta-ICL) log parsing framework that combines meta-learning with ICL on small open-source LLMs (i.e., Qwen-2.5-3B). Specifically, MicLog: i) enhances LLMs' ICL capability through a zero-shot to k-shot ProgMeta-ICL paradigm, employing weighted DBSCAN candidate sampling and enhanced BM25 demonstration selection; ii) accelerates parsing via a multi-level pre-query cache that dynamically matches and refines recently parsed templates. Evaluated on Loghub-2.0, MicLog achieves 10.3% higher parsing accuracy than the state-of-the-art parser while reducing parsing time by 42.4%.",
      "publishedDate": "2026-01-11T17:46:10Z",
      "updatedDate": "2026-01-11T17:46:10Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07005v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07005",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06884",
      "title": "Paraphrasing Adversarial Attack on LLM-as-a-Reviewer",
      "authors": [
        {
          "name": "Masahiro Kaneko",
          "affiliation": null
        }
      ],
      "abstract": "The use of large language models (LLMs) in peer review systems has attracted growing attention, making it essential to examine their potential vulnerabilities. Prior attacks rely on prompt injection, which alters manuscript content and conflates injection susceptibility with evaluation robustness. We propose the Paraphrasing Adversarial Attack (PAA), a black-box optimization method that searches for paraphrased sequences yielding higher review scores while preserving semantic equivalence and linguistic naturalness. PAA leverages in-context learning, using previous paraphrases and their scores to guide candidate generation. Experiments across five ML and NLP conferences with three LLM reviewers and five attacking models show that PAA consistently increases review scores without changing the paper's claims. Human evaluation confirms that generated paraphrases maintain meaning and naturalness. We also find that attacked papers exhibit increased perplexity in reviews, offering a potential detection signal, and that paraphrasing submissions can partially mitigate attacks.",
      "publishedDate": "2026-01-11T12:14:10Z",
      "updatedDate": "2026-01-11T12:14:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06884v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06884",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06196",
      "title": "Manifold-based Sampling for In-Context Hallucination Detection in Large Language Models",
      "authors": [
        {
          "name": "Bodla Krishna Vamshi",
          "affiliation": null
        },
        {
          "name": "Rohan Bhatnagar",
          "affiliation": null
        },
        {
          "name": "Haizhao Yang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) frequently generate factually incorrect or unsupported content, commonly referred to as hallucinations. Prior work has explored decoding strategies, retrieval augmentation, and supervised fine-tuning for hallucination detection, while recent studies show that in-context learning (ICL) can substantially influence factual reliability. However, existing ICL demonstration selection methods often rely on surface-level similarity heuristics and exhibit limited robustness across tasks and models. We propose MB-ICL, a manifold-based demonstration sampling framework for selecting in-context demonstrations that leverages latent representations extracted from frozen LLMs. By jointly modeling local manifold structure and class-aware prototype geometry, MB-ICL selects demonstrations based on their proximity to learned prototypes rather than lexical or embedding similarity alone. Across factual verification (FEVER) and hallucination detection (HaluEval) benchmarks, MB-ICL outperforms standard ICL selection baselines in the majority of evaluated settings, with particularly strong gains on dialogue and summarization tasks. The method remains robust under temperature perturbations and model variation, indicating improved stability compared to heuristic retrieval strategies. While lexical retrieval can remain competitive in certain question-answering regimes, our results demonstrate that manifold-based prototype selection provides a reliable and training light approach for hallucination detection without modifying LLM parameters, offering a principled direction for improved ICL demonstration selection.",
      "publishedDate": "2026-01-08T06:17:18Z",
      "updatedDate": "2026-01-08T06:17:18Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06196v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06196",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07786",
      "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
      "authors": [
        {
          "name": "Abdullah Al Mujahid",
          "affiliation": null
        },
        {
          "name": "Mia Mohammad Imran",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
      "publishedDate": "2026-01-12T17:59:34Z",
      "updatedDate": "2026-01-12T17:59:34Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07786v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07786",
      "comment": "9th International Conference on Technical Debt (TechDebt 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07602",
      "title": "OODEval: Evaluating Large Language Models on Object-Oriented Design",
      "authors": [
        {
          "name": "Bingxu Xiao",
          "affiliation": null
        },
        {
          "name": "Yunwei Dong",
          "affiliation": null
        },
        {
          "name": "Yiqi Tang",
          "affiliation": null
        },
        {
          "name": "Manqing Zhang",
          "affiliation": null
        },
        {
          "name": "Yifan Zhou",
          "affiliation": null
        },
        {
          "name": "Chunyan Ma",
          "affiliation": null
        },
        {
          "name": "Yepang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.",
      "publishedDate": "2026-01-12T14:51:31Z",
      "updatedDate": "2026-01-12T14:51:31Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07602v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07602",
      "comment": "31 pages,8 figures,9 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "code-generation",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07568",
      "title": "d3LLM: Ultra-Fast Diffusion LLM using Pseudo-Trajectory Distillation",
      "authors": [
        {
          "name": "Yu-Yang Qian",
          "affiliation": null
        },
        {
          "name": "Junda Su",
          "affiliation": null
        },
        {
          "name": "Lanxiang Hu",
          "affiliation": null
        },
        {
          "name": "Peiyuan Zhang",
          "affiliation": null
        },
        {
          "name": "Zhijie Deng",
          "affiliation": null
        },
        {
          "name": "Peng Zhao",
          "affiliation": null
        },
        {
          "name": "Hao Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion large language models (dLLMs) offer capabilities beyond those of autoregressive (AR) LLMs, such as parallel decoding and random-order generation. However, realizing these benefits in practice is non-trivial, as dLLMs inherently face an accuracy-parallelism trade-off. Despite increasing interest, existing methods typically focus on only one-side of the coin, targeting either efficiency or performance. To address this limitation, we propose d3LLM (Pseudo-Distilled Diffusion Large Language Model), striking a balance between accuracy and parallelism: (i) during training, we introduce pseudo-trajectory distillation to teach the model which tokens can be decoded confidently at early steps, thereby improving parallelism; (ii) during inference, we employ entropy-based multi-block decoding with a KV-cache refresh mechanism to achieve high parallelism while maintaining accuracy. To better evaluate dLLMs, we also introduce AUP (Accuracy Under Parallelism), a new metric that jointly measures accuracy and parallelism. Experiments demonstrate that our d3LLM achieves up to 10$\\times$ speedup over vanilla LLaDA/Dream and 5$\\times$ speedup over AR models without much accuracy drop. Our code is available at https://github.com/hao-ai-lab/d3LLM.",
      "publishedDate": "2026-01-12T14:25:36Z",
      "updatedDate": "2026-01-12T14:25:36Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07568v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07568",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07565",
      "title": "A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models",
      "authors": [
        {
          "name": "Jiaqi Qiao",
          "affiliation": null
        },
        {
          "name": "Xiujuan Xu",
          "affiliation": null
        },
        {
          "name": "Xinran Li",
          "affiliation": null
        },
        {
          "name": "Yu Liu",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal emotion understanding requires effective integration of text, audio, and visual modalities for both discrete emotion recognition and continuous sentiment analysis. We present EGMF, a unified framework combining expert-guided multimodal fusion with large language models. Our approach features three specialized expert networks--a fine-grained local expert for subtle emotional nuances, a semantic correlation expert for cross-modal relationships, and a global context expert for long-range dependencies--adaptively integrated through hierarchical dynamic gating for context-aware feature selection. Enhanced multimodal representations are integrated with LLMs via pseudo token injection and prompt-based conditioning, enabling a single generative framework to handle both classification and regression through natural language generation. We employ LoRA fine-tuning for computational efficiency. Experiments on bilingual benchmarks (MELD, CHERMA, MOSEI, SIMS-V2) demonstrate consistent improvements over state-of-the-art methods, with superior cross-lingual robustness revealing universal patterns in multimodal emotional expressions across English and Chinese. We will release the source code publicly.",
      "publishedDate": "2026-01-12T14:21:32Z",
      "updatedDate": "2026-01-12T14:21:32Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07565v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07565",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07475",
      "title": "ARCQuant: Boosting NVFP4 Quantization with Augmented Residual Channels for LLMs",
      "authors": [
        {
          "name": "Haoqian Meng",
          "affiliation": null
        },
        {
          "name": "Yilun Luo",
          "affiliation": null
        },
        {
          "name": "Yafei Zhao",
          "affiliation": null
        },
        {
          "name": "Wenyuan Liu",
          "affiliation": null
        },
        {
          "name": "Peng Zhang",
          "affiliation": null
        },
        {
          "name": "Xindian Ma",
          "affiliation": null
        }
      ],
      "abstract": "The emergence of fine-grained numerical formats like NVFP4 presents new opportunities for efficient Large Language Model (LLM) inference. However, it is difficult to adapt existing Post-Training Quantization (PTQ) strategies to these formats: rotation-based methods compromise fine-grained block isolation; smoothing techniques struggle with significant 4-bit quantization errors; and mixed-precision approaches often conflict with hardware constraints on unified-precision computation. To address these challenges, we propose ARCQuant, a framework that boosts NVFP4 performance via Augmented Residual Channels. Distinct from methods that compromise block isolation or hardware uniformity, ARCQuant maintains a strictly unified NVFP4 format by augmenting the activation matrix with quantized residual channels. This design integrates the error compensation process directly into the matrix reduction dimension, enabling the use of standard, highly optimized GEMM kernels with minimal overhead. Theoretical analysis confirms that the worst-case error bound of our dual-stage NVFP4 quantization is comparable to that of standard 8-bit formats such as MXFP8. Extensive experiments on LLaMA and Qwen models demonstrate that ARCQuant achieves state-of-the-art accuracy, comparable to full-precision baselines in perplexity and downstream tasks. Furthermore, deployment on RTX 5090 and RTX PRO 6000 GPUs confirms practical benefits, achieving up to 3x speedup over FP16. Our code is available at https://github.com/actypedef/ARCQuant .",
      "publishedDate": "2026-01-12T12:27:22Z",
      "updatedDate": "2026-01-12T12:27:22Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07475v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07475",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07338",
      "title": "Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation",
      "authors": [
        {
          "name": "Yanzhi Tian",
          "affiliation": null
        },
        {
          "name": "Cunxiang Wang",
          "affiliation": null
        },
        {
          "name": "Zeming Liu",
          "affiliation": null
        },
        {
          "name": "Heyan Huang",
          "affiliation": null
        },
        {
          "name": "Wenbo Yu",
          "affiliation": null
        },
        {
          "name": "Dawei Song",
          "affiliation": null
        },
        {
          "name": "Jie Tang",
          "affiliation": null
        },
        {
          "name": "Yuhang Guo",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have significantly advanced Machine Translation (MT), applying them to linguistically complex domains-such as Social Network Services, literature etc. In these scenarios, translations often require handling non-literal expressions, leading to the inaccuracy of MT metrics. To systematically investigate the reliability of MT metrics, we first curate a meta-evaluation dataset focused on non-literal translations, namely MENT. MENT encompasses four non-literal translation domains and features source sentences paired with translations from diverse MT systems, with 7,530 human-annotated scores on translation quality. Experimental results reveal the inaccuracies of traditional MT metrics and the limitations of LLM-as-a-Judge, particularly the knowledge cutoff and score inconsistency problem. To mitigate these limitations, we propose RATE, a novel agentic translation evaluation framework, centered by a reflective Core Agent that dynamically invokes specialized sub-agents. Experimental results indicate the efficacy of RATE, achieving an improvement of at least 3.2 meta score compared with current metrics. Further experiments demonstrate the robustness of RATE to general-domain MT evaluation. Code and dataset are available at: https://github.com/BITHLP/RATE.",
      "publishedDate": "2026-01-12T09:03:42Z",
      "updatedDate": "2026-01-12T09:03:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07338v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07338",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07206",
      "title": "LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing",
      "authors": [
        {
          "name": "Hao Li",
          "affiliation": null
        },
        {
          "name": "Yiqun Zhang",
          "affiliation": null
        },
        {
          "name": "Zhaoyan Guo",
          "affiliation": null
        },
        {
          "name": "Chenxu Wang",
          "affiliation": null
        },
        {
          "name": "Shengji Tang",
          "affiliation": null
        },
        {
          "name": "Qiaosheng Zhang",
          "affiliation": null
        },
        {
          "name": "Yang Chen",
          "affiliation": null
        },
        {
          "name": "Biqing Qi",
          "affiliation": null
        },
        {
          "name": "Peng Ye",
          "affiliation": null
        },
        {
          "name": "Lei Bai",
          "affiliation": null
        },
        {
          "name": "Zhen Wang",
          "affiliation": null
        },
        {
          "name": "Shuyue Hu",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM) routing assigns each query to the most suitable model from an ensemble. We introduce LLMRouterBench, a large-scale benchmark and unified framework for LLM routing. It comprises over 400K instances from 21 datasets and 33 models. Moreover, it provides comprehensive metrics for both performance-oriented routing and performance-cost trade-off routing, and integrates 10 representative routing baselines. Using LLMRouterBench, we systematically re-evaluate the field. While confirming strong model complementarity-the central premise of LLM routing-we find that many routing methods exhibit similar performance under unified evaluation, and several recent approaches, including commercial routers, fail to reliably outperform a simple baseline. Meanwhile, a substantial gap remains to the Oracle, driven primarily by persistent model-recall failures. We further show that backbone embedding models have limited impact, that larger ensembles exhibit diminishing returns compared to careful model curation, and that the benchmark also enables latency-aware analysis. All code and data are available at https://github.com/ynulihao/LLMRouterBench.",
      "publishedDate": "2026-01-12T05:01:15Z",
      "updatedDate": "2026-01-12T05:01:15Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07206v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07206",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07177",
      "title": "Safe-FedLLM: Delving into the Safety of Federated Large Language Models",
      "authors": [
        {
          "name": "Mingxiang Tao",
          "affiliation": null
        },
        {
          "name": "Yu Tian",
          "affiliation": null
        },
        {
          "name": "Wenxuan Tu",
          "affiliation": null
        },
        {
          "name": "Yue Yang",
          "affiliation": null
        },
        {
          "name": "Xue Yang",
          "affiliation": null
        },
        {
          "name": "Xiangyan Tang",
          "affiliation": null
        }
      ],
      "abstract": "Federated learning (FL) addresses data privacy and silo issues in large language models (LLMs). Most prior work focuses on improving the training efficiency of federated LLMs. However, security in open environments is overlooked, particularly defenses against malicious clients. To investigate the safety of LLMs during FL, we conduct preliminary experiments to analyze potential attack surfaces and defensible characteristics from the perspective of Low-Rank Adaptation (LoRA) weights. We find two key properties of FL: 1) LLMs are vulnerable to attacks from malicious clients in FL, and 2) LoRA weights exhibit distinct behavioral patterns that can be filtered through simple classifiers. Based on these properties, we propose Safe-FedLLM, a probe-based defense framework for federated LLMs, constructing defenses across three dimensions: Step-Level, Client-Level, and Shadow-Level. The core concept of Safe-FedLLM is to perform probe-based discrimination on the LoRA weights locally trained by each client during FL, treating them as high-dimensional behavioral features and using lightweight classification models to determine whether they possess malicious attributes. Extensive experiments demonstrate that Safe-FedLLM effectively enhances the defense capability of federated LLMs without compromising performance on benign data. Notably, our method effectively suppresses malicious data impact without significant impact on training speed, and remains effective even with many malicious clients. Our code is available at: https://github.com/dmqx/Safe-FedLLM.",
      "publishedDate": "2026-01-12T04:01:03Z",
      "updatedDate": "2026-01-12T04:01:03Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07177v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07177",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06959",
      "title": "HAS-VQ: Hessian-Adaptive Sparse Vector Quantization for High-Fidelity LLM Compression",
      "authors": [
        {
          "name": "Vladimer Khasia",
          "affiliation": null
        }
      ],
      "abstract": "Post-training quantization is essential for deploying Large Language Models (LLMs) on resource- constrained devices. However, standard integer quantization (e.g., INT4) fundamentally degrades per- formance by imposing a uniform grid on the heavy-tailed distribution of weight parameters, particularly in smaller-scale models (e.g., <2B parameters). We introduce HAS-VQ (Hessian-Adaptive Sparse Vec- tor Quantization), a compression framework that strictly decouples high-sensitivity outliers from the bulk weight distribution using second-order sensitivity analysis. HAS-VQ employs a Hessian-Masked Decoupling strategy to isolate sensitive parameters, followed by robust Vector Quantization (VQ) of the remaining dense body. Crucially, we introduce a residual sparse feedback mechanism that corrects quan- tization errors in the most sensitive dimensions, ensuring exact reconstruction of outliers. We evaluate HAS-VQ on SmolLM2-1.7B, demonstrating two distinct regimes of superiority: (1) Pareto Dominance over Integer Baselines: At 4.23 effective bits-per-parameter (BPP), we achieve a perplexity of 14.23, significantly outperforming the standard INT4 baseline (20.03 PPL at 4.71 BPP). (2) High-Fidelity Compression: Relative to the FP16 baseline, HAS-VQ achieves a 2.3x reduction in model size (7.03 BPP) while maintaining statistically indistinguishable perplexity (10.12 vs. 10.04), effectively offering a lossless compression alternative for bandwidth-constrained environments. The code is available at https://github.com/VladimerKhasia/HASVQ",
      "publishedDate": "2026-01-11T15:35:10Z",
      "updatedDate": "2026-01-11T15:35:10Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06959v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06959",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06847",
      "title": "MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data",
      "authors": [
        {
          "name": "Mengmeng Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaoping Wu",
          "affiliation": null
        },
        {
          "name": "Hao Luo",
          "affiliation": null
        },
        {
          "name": "Fan Wang",
          "affiliation": null
        },
        {
          "name": "Yisheng Lv",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.",
      "publishedDate": "2026-01-11T10:34:18Z",
      "updatedDate": "2026-01-11T10:34:18Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06847v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06847",
      "comment": "18 pages, 10 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06843",
      "title": "Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models",
      "authors": [
        {
          "name": "Junyan Lin",
          "affiliation": null
        },
        {
          "name": "Junlong Tong",
          "affiliation": null
        },
        {
          "name": "Hao Wu",
          "affiliation": null
        },
        {
          "name": "Jialiang Zhang",
          "affiliation": null
        },
        {
          "name": "Jinming Liu",
          "affiliation": null
        },
        {
          "name": "Xin Jin",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Shen",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved strong performance across many tasks, yet most systems remain limited to offline inference, requiring complete inputs before generating outputs. Recent streaming methods reduce latency by interleaving perception and generation, but still enforce a sequential perception-generation cycle, limiting real-time interaction. In this work, we target a fundamental bottleneck that arises when extending MLLMs to real-time video understanding: the global positional continuity constraint imposed by standard positional encoding schemes. While natural in offline inference, this constraint tightly couples perception and generation, preventing effective input-output parallelism. To address this limitation, we propose a parallel streaming framework that relaxes positional continuity through three designs: Overlapped, Group-Decoupled, and Gap-Isolated. These designs enable simultaneous perception and generation, allowing the model to process incoming inputs while producing responses in real time. Extensive experiments reveal that Group-Decoupled achieves the best efficiency-performance balance, maintaining high fluency and accuracy while significantly reducing latency. We further show that the proposed framework yields up to 2x acceleration under balanced perception-generation workloads, establishing a principled pathway toward speak-while-watching real-time systems. We make all our code publicly available: https://github.com/EIT-NLP/Speak-While-Watching.",
      "publishedDate": "2026-01-11T10:12:11Z",
      "updatedDate": "2026-01-11T10:12:11Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06843v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06843",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06633",
      "title": "KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks",
      "authors": [
        {
          "name": "Zhangqi Duan",
          "affiliation": null
        },
        {
          "name": "Nigel Fernandez",
          "affiliation": null
        },
        {
          "name": "Andrew Lan",
          "affiliation": null
        }
      ],
      "abstract": "Open-ended tasks, such as coding problems that are common in computer science education, provide detailed insights into student knowledge. However, training large language models (LLMs) to simulate and predict possible student errors in their responses to these problems can be challenging: they often suffer from mode collapse and fail to fully capture the diversity in syntax, style, and solution approach in student responses. In this work, we present KASER (Knowledge-Aligned Student Error Simulator), a novel approach that aligns errors with student knowledge. We propose a training method based on reinforcement learning using a hybrid reward that reflects three aspects of student code prediction: i) code similarity to the ground-truth, ii) error matching, and iii) code prediction diversity. On two real-world datasets, we perform two levels of evaluation and show that: At the per-student-problem pair level, our method outperforms baselines on code and error prediction; at the per-problem level, our method outperforms baselines on error coverage and simulated code diversity.",
      "publishedDate": "2026-01-10T17:36:48Z",
      "updatedDate": "2026-01-10T17:36:48Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06633v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06633",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06588",
      "title": "TCLNet: A Hybrid Transformer-CNN Framework Leveraging Language Models as Lossless Compressors for CSI Feedback",
      "authors": [
        {
          "name": "Zijiu Yang",
          "affiliation": null
        },
        {
          "name": "Qianqian Yang",
          "affiliation": null
        },
        {
          "name": "Shunpu Tang",
          "affiliation": null
        },
        {
          "name": "Tingting Yang",
          "affiliation": null
        },
        {
          "name": "Zhiguo Shi",
          "affiliation": null
        }
      ],
      "abstract": "In frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems, downlink channel state information (CSI) plays a crucial role in achieving high spectrum and energy efficiency. However, the CSI feedback overhead becomes a major bottleneck as the number of antennas increases. Although existing deep learning-based CSI compression methods have shown great potential, they still face limitations in capturing both local and global features of CSI, thereby limiting achievable compression efficiency. To address these issues, we propose TCLNet, a unified CSI compression framework that integrates a hybrid Transformer-CNN architecture for lossy compression with a hybrid language model (LM) and factorized model (FM) design for lossless compression. The lossy module jointly exploits local features and global context, while the lossless module adaptively switches between context-aware coding and parallel coding to optimize the rate-distortion-complexity (RDC) trade-off. Extensive experiments on both real-world and simulated datasets demonstrate that the proposed TCLNet outperforms existing approaches in terms of reconstruction accuracy and transmission efficiency, achieving up to a 5 dB performance gain across diverse scenarios. Moreover, we show that large language models (LLMs) can be leveraged as zero-shot CSI lossless compressors via carefully designed prompts.",
      "publishedDate": "2026-01-10T14:54:40Z",
      "updatedDate": "2026-01-10T14:54:40Z",
      "primaryCategory": "cs.IT",
      "arxivCategories": [
        "cs.IT",
        "physics.comp-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06588v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06588",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06586",
      "title": "Detecting LLM-Generated Text with Performance Guarantees",
      "authors": [
        {
          "name": "Hongyi Zhou",
          "affiliation": null
        },
        {
          "name": "Jin Zhu",
          "affiliation": null
        },
        {
          "name": "Ying Yang",
          "affiliation": null
        },
        {
          "name": "Chengchun Shi",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) such as GPT, Claude, Gemini, and Grok have been deeply integrated into our daily life. They now support a wide range of tasks -- from dialogue and email drafting to assisting with teaching and coding, serving as search engines, and much more. However, their ability to produce highly human-like text raises serious concerns, including the spread of fake news, the generation of misleading governmental reports, and academic misconduct. To address this practical problem, we train a classifier to determine whether a piece of text is authored by an LLM or a human. Our detector is deployed on an online CPU-based platform https://huggingface.co/spaces/stats-powered-ai/StatDetectLLM, and contains three novelties over existing detectors: (i) it does not rely on auxiliary information, such as watermarks or knowledge of the specific LLM used to generate the text; (ii) it more effectively distinguishes between human- and LLM-authored text; and (iii) it enables statistical inference, which is largely absent in the current literature. Empirically, our classifier achieves higher classification accuracy compared to existing detectors, while maintaining type-I error control, high statistical power, and computational efficiency.",
      "publishedDate": "2026-01-10T14:52:45Z",
      "updatedDate": "2026-01-10T14:52:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06586v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06586",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06565",
      "title": "EVM-QuestBench: An Execution-Grounded Benchmark for Natural-Language Transaction Code Generation",
      "authors": [
        {
          "name": "Pei Yang",
          "affiliation": null
        },
        {
          "name": "Wanyi Chen",
          "affiliation": null
        },
        {
          "name": "Ke Wang",
          "affiliation": null
        },
        {
          "name": "Lynn Ai",
          "affiliation": null
        },
        {
          "name": "Eric Yang",
          "affiliation": null
        },
        {
          "name": "Tianyu Shi",
          "affiliation": null
        }
      ],
      "abstract": "Large language models are increasingly applied to various development scenarios. However, in on-chain transaction scenarios, even a minor error can cause irreversible loss for users. Existing evaluations often overlook execution accuracy and safety. We introduce EVM-QuestBench, an execution-grounded benchmark for natural-language transaction-script generation on EVM-compatible chains. The benchmark employs dynamic evaluation: instructions are sampled from template pools, numeric parameters are drawn from predefined intervals, and validators verify outcomes against these instantiated values. EVM-QuestBench contains 107 tasks (62 atomic, 45 composite). Its modular architecture enables rapid task development. The runner executes scripts on a forked EVM chain with snapshot isolation; composite tasks apply step-efficiency decay. We evaluate 20 models and find large performance gaps, with split scores revealing persistent asymmetry between single-action precision and multi-step workflow completion. Code: https://anonymous.4open.science/r/bsc_quest_bench-A9CF/.",
      "publishedDate": "2026-01-10T13:25:27Z",
      "updatedDate": "2026-01-10T13:25:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06565v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06565",
      "comment": "10 pages, 13 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "evaluation",
        "tool-use",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "tool-use",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06543",
      "title": "SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation",
      "authors": [
        {
          "name": "Jun-Qi Chen",
          "affiliation": null
        },
        {
          "name": "Kun Zhang",
          "affiliation": null
        },
        {
          "name": "Rui Zheng",
          "affiliation": null
        },
        {
          "name": "Ying Zhong",
          "affiliation": null
        }
      ],
      "abstract": "The Python package SimPy is widely used for modeling queueing systems due to its flexibility, simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent advances in large language models (LLMs) have shown strong ability in generating clear and executable code, making them powerful and suitable tools for writing SimPy queueing simulation code. However, directly employing closed-source models like GPT-4o to generate such code may lead to high computational costs and raise data privacy concerns. To address this, we fine-tune two open-source LLMs, Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their code-generating performance in executability, output-format compliance, and instruction-code consistency. Particularly, we proposed a multi-stage fine-tuning framework comprising two stages of supervised fine-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing the model's ability in SimPy-based queueing simulation code generation. Extensive evaluations demonstrate that both fine-tuned models achieve substantial improvements in executability, output-format compliance, and instruct consistency. These results confirm that domain-specific fine-tuning can effectively transform compact open-source code models into reliable SimPy simulation generators which provide a practical alternative to closed-source LLMs for education, research, and operational decision support.",
      "publishedDate": "2026-01-10T11:53:39Z",
      "updatedDate": "2026-01-10T11:53:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06543v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06543",
      "comment": "33 pages, 10 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06477",
      "title": "IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed Social Media Comments",
      "authors": [
        {
          "name": "Debasmita Panda",
          "affiliation": null
        },
        {
          "name": "Akash Anil",
          "affiliation": null
        },
        {
          "name": "Neelesh Kumar Shukla",
          "affiliation": null
        }
      ],
      "abstract": "Warning: This paper consists of examples representing regional biases in Indian regions that might be offensive towards a particular region. While social biases corresponding to gender, race, socio-economic conditions, etc., have been extensively studied in the major applications of Natural Language Processing (NLP), biases corresponding to regions have garnered less attention. This is mainly because of (i) difficulty in the extraction of regional bias datasets, (ii) disagreements in annotation due to inherent human biases, and (iii) regional biases being studied in combination with other types of social biases and often being under-represented. This paper focuses on creating a dataset IndRegBias, consisting of regional biases in an Indian context reflected in users' comments on popular social media platforms, namely Reddit and YouTube. We carefully selected 25,000 comments appearing on various threads in Reddit and videos on YouTube discussing trending topics on regional issues in India. Furthermore, we propose a multilevel annotation strategy to annotate the comments describing the severity of regional biased statements. To detect the presence of regional bias and its severity in IndRegBias, we evaluate open-source Large Language Models (LLMs) and Indic Language Models (ILMs) using zero-shot, few-shot, and fine-tuning strategies. We observe that zero-shot and few-shot approaches show lower accuracy in detecting regional biases and severity in the majority of the LLMs and ILMs. However, the fine-tuning approach significantly enhances the performance of the LLM in detecting Indian regional bias along with its severity.",
      "publishedDate": "2026-01-10T08:13:03Z",
      "updatedDate": "2026-01-13T06:53:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CY",
        "cs.SI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06477v2",
      "arxivUrl": "https://arxiv.org/abs/2601.06477",
      "comment": "Preprint. Under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06401",
      "title": "BizFinBench.v2: A Unified Dual-Mode Bilingual Benchmark for Expert-Level Financial Capability Alignment",
      "authors": [
        {
          "name": "Xin Guo",
          "affiliation": null
        },
        {
          "name": "Rongjunchen Zhang",
          "affiliation": null
        },
        {
          "name": "Guilong Lu",
          "affiliation": null
        },
        {
          "name": "Xuntao Guo",
          "affiliation": null
        },
        {
          "name": "Shuai Jia",
          "affiliation": null
        },
        {
          "name": "Zhi Yang",
          "affiliation": null
        },
        {
          "name": "Liwen Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models have undergone rapid evolution, emerging as a pivotal technology for intelligence in financial operations. However, existing benchmarks are often constrained by pitfalls such as reliance on simulated or general-purpose samples and a focus on singular, offline static scenarios. Consequently, they fail to align with the requirements for authenticity and real-time responsiveness in financial services, leading to a significant discrepancy between benchmark performance and actual operational efficacy. To address this, we introduce BizFinBench.v2, the first large-scale evaluation benchmark grounded in authentic business data from both Chinese and U.S. equity markets, integrating online assessment. We performed clustering analysis on authentic user queries from financial platforms, resulting in eight fundamental tasks and two online tasks across four core business scenarios, totaling 29,578 expert-level Q&A pairs. Experimental results demonstrate that ChatGPT-5 achieves a prominent 61.5% accuracy in main tasks, though a substantial gap relative to financial experts persists; in online tasks, DeepSeek-R1 outperforms all other commercial LLMs. Error analysis further identifies the specific capability deficiencies of existing models within practical financial business contexts. BizFinBench.v2 transcends the limitations of current benchmarks, achieving a business-level deconstruction of LLM financial capabilities and providing a precise basis for evaluating efficacy in the widespread deployment of LLMs within the financial domain. The data and code are available at https://github.com/HiThink-Research/BizFinBench.v2.",
      "publishedDate": "2026-01-10T02:51:53Z",
      "updatedDate": "2026-01-10T02:51:53Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06401v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06401",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "tool-use",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06309",
      "title": "VideoWeave: A Data-Centric Approach for Efficient Video Understanding",
      "authors": [
        {
          "name": "Zane Durante",
          "affiliation": null
        },
        {
          "name": "Silky Singh",
          "affiliation": null
        },
        {
          "name": "Arpandeep Khatua",
          "affiliation": null
        },
        {
          "name": "Shobhit Agarwal",
          "affiliation": null
        },
        {
          "name": "Reuben Tan",
          "affiliation": null
        },
        {
          "name": "Yong Jae Lee",
          "affiliation": null
        },
        {
          "name": "Jianfeng Gao",
          "affiliation": null
        },
        {
          "name": "Ehsan Adeli",
          "affiliation": null
        },
        {
          "name": "Li Fei-Fei",
          "affiliation": null
        }
      ],
      "abstract": "Training video-language models is often prohibitively expensive due to the high cost of processing long frame sequences and the limited availability of annotated long videos. We present VideoWeave, a simple yet effective approach to improve data efficiency by constructing synthetic long-context training samples that splice together short, captioned videos from existing datasets. Rather than modifying model architectures or optimization objectives, VideoWeave reorganizes available video-text pairs to expand temporal diversity within fixed compute. We systematically study how different data composition strategies like random versus visually clustered splicing and caption enrichment affect downstream performance on downstream video question answering. Under identical compute constraints, models trained with VideoWeave achieve higher accuracy than conventional video finetuning. Our results highlight that reorganizing training data, rather than altering architectures, may offer a simple and scalable path for training video-language models. We link our code for all experiments here.",
      "publishedDate": "2026-01-09T20:55:26Z",
      "updatedDate": "2026-01-09T20:55:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06309v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06309",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06276",
      "title": "Automated Generation of Accurate Privacy Captions From Android Source Code Using Large Language Models",
      "authors": [
        {
          "name": "Vijayanta Jain",
          "affiliation": null
        },
        {
          "name": "Sepideh Ghanavati",
          "affiliation": null
        },
        {
          "name": "Sai Teja Peddinti",
          "affiliation": null
        },
        {
          "name": "Collin McMillan",
          "affiliation": null
        }
      ],
      "abstract": "Privacy captions are short sentences that succinctly describe what personal information is used, how it is used, and why, within an app. These captions can be utilized in various notice formats, such as privacy policies, app rationales, and app store descriptions. However, inaccurate captions may mislead users and expose developers to regulatory fines. Existing approaches to generating privacy notices or just privacy captions include using questionnaires, templates, static analysis, or machine learning. However, these approaches either rely heavily on developers' inputs and thus strain their efforts, use limited source code context, leading to the incomplete capture of app privacy behaviors, or depend on potentially inaccurate privacy policies as a source for creating notices. In this work, we address these limitations by developing Privacy Caption Generator (PCapGen), an approach that - i) automatically identifies and extracts large and precise source code context that implements privacy behaviors in an app, ii) uses a Large Language Model (LLM) to describe coarse- and fine-grained privacy behaviors, and iii) generates accurate, concise, and complete privacy captions to describe the privacy behaviors of the app. Our evaluation shows PCapGen generates concise, complete, and accurate privacy captions as compared to the baseline approach. Furthermore, privacy experts choose PCapGen captions at least 71\\% of the time, whereas LLMs-as-judge prefer PCapGen captions at least 76\\% of the time, indicating strong performance of our approach.",
      "publishedDate": "2026-01-09T19:41:28Z",
      "updatedDate": "2026-01-09T19:41:28Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06276v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06276",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06266",
      "title": "Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with ML and Non-ML Software",
      "authors": [
        {
          "name": "Niruthiha Selvanayagam",
          "affiliation": null
        },
        {
          "name": "Taher A. Ghaleb",
          "affiliation": null
        },
        {
          "name": "Manel Abdellatif",
          "affiliation": null
        }
      ],
      "abstract": "Self-admitted technical debt (SATD), referring to comments flagged by developers that explicitly acknowledge suboptimal code or incomplete functionality, has received extensive attention in machine learning (ML) and traditional (Non-ML) software. However, little is known about how SATD manifests and evolves in contemporary Large Language Model (LLM)-based systems, whose architectures, workflows, and dependencies differ fundamentally from both traditional and pre-LLM ML software. In this paper, we conduct the first empirical study of SATD in the LLM era, replicating and extending prior work on ML technical debt to modern LLM-based systems. We compare SATD prevalence across LLM, ML, and non-ML repositories across a total of 477 repositories (159 per category). We perform survival analysis of SATD introduction and removal to understand the dynamics of technical debt across different development paradigms. Surprisingly, despite their architectural complexity, our results reveal that LLM repositories accumulate SATD at similar rates to ML systems (3.95% vs. 4.10%). However, we observe that LLM repositories remain debt-free 2.4x longer than ML repositories (a median of 492 days vs. 204 days), and then start to accumulate technical debt rapidly. Moreover, our qualitative analysis of 377 SATD instances reveals three new forms of technical debt unique to LLM-based development that have not been reported in prior research: Model-Stack Workaround Debt, Model Dependency Debt, and Performance Optimization Debt. Finally, by mapping SATD to stages of the LLM development pipeline, we observe that debt concentrates",
      "publishedDate": "2026-01-09T19:25:48Z",
      "updatedDate": "2026-01-13T02:51:00Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06266v2",
      "arxivUrl": "https://arxiv.org/abs/2601.06266",
      "comment": "Accepted to SANER 2026 (IEEE International Conference on Software Analysis, Evolution and Reengineering)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06211",
      "title": "Large Multimodal Model-Aided Scheduling for 6G Autonomous Communications",
      "authors": [
        {
          "name": "Sunwoo Kim",
          "affiliation": null
        },
        {
          "name": "Byonghyo Shim",
          "affiliation": null
        }
      ],
      "abstract": "Recently, large language models (LLMs) have gained significant attention for their ability to generate fast and accurate answer to the given query. These models have evolved into large multimodal models (LMMs), which can interpret and analyze multimodal inputs such as images and text. With the exponential growth of AI functionalities in autonomous devices, the central unit (CU), a digital processing unit performing AI inference, needs to handle LMMs to effectively control these devices. To ensure seamless command delivery to devices, the CU must perform the scheduling, which involves resource block (RB) allocation for data transmission and modulation and coding scheme (MCS) index selection based on the channel conditions. This task is challenging in many practical environments in 6G, where even small user movement can cause abrupt channel changes. In this paper, we propose a novel LMM-based scheduling technique to address this challenge. Our key idea is to leverage LMM to predict future channel parameters (e.g., distance, angles, and path gain) by analyzing the visual sensing information as well as pilot signals. By exploiting LMMs to predict the presence of reliable path and geometric information of users from the visual sensing information, and then combining these with past channel states from pilot signals, we can accurately predict future channel parameters. Using these predictions, we can preemptively make channel-aware scheduling decisions. From the numerical evaluations, we show that the proposed technique achieves more than 30% throughput gain over the conventional scheduling techniques.",
      "publishedDate": "2026-01-08T15:16:08Z",
      "updatedDate": "2026-01-08T15:16:08Z",
      "primaryCategory": "cs.IT",
      "arxivCategories": [
        "cs.IT"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06211v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06211",
      "comment": "16 pages",
      "journalRef": "IEEE Transactions on Cognitive Communications and Networking, vol. 12, pp. 3732-3747, 2026",
      "doi": "10.1109/TCCN.2025.3633741",
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "agents",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06194",
      "title": "Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias",
      "authors": [
        {
          "name": "Adib Sakhawat",
          "affiliation": null
        },
        {
          "name": "Tahsin Islam",
          "affiliation": null
        },
        {
          "name": "Takia Farhin",
          "affiliation": null
        },
        {
          "name": "Syed Rifat Raiyan",
          "affiliation": null
        },
        {
          "name": "Hasan Mahmud",
          "affiliation": null
        },
        {
          "name": "Md Kamrul Hasan",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness. This study presents a sociotechnical audit of 26 prominent LLMs, triangulating their positions across three psychometric inventories (Political Compass, SapplyValues, 8 Values) and evaluating their performance on a large-scale news labeling task ($N \\approx 27{,}000$). Our results reveal a strong clustering of models in the Libertarian-Left region of the ideological space, encompassing 96.3% of the cohort. Alignment signals appear to be consistent architectural traits rather than stochastic noise ($η^2 > 0.90$); however, we identify substantial discrepancies in measurement validity. In particular, the Political Compass exhibits a strong negative correlation with cultural progressivism ($r=-0.64$) when compared against multi-axial instruments, suggesting a conflation of social conservatism with authoritarianism in this context. We further observe a significant divergence between open-weights and closed-source models, with the latter displaying markedly higher cultural progressivism scores ($p<10^{-25}$). In downstream media analysis, models exhibit a systematic \"center-shift,\" frequently categorizing neutral articles as left-leaning, alongside an asymmetric detection capability in which \"Far Left\" content is identified with greater accuracy (19.2%) than \"Far Right\" content (2.0%). These findings suggest that single-axis evaluations are insufficient and that multidimensional auditing frameworks are necessary to characterize alignment behavior in deployed LLMs. Our code and data will be made public.",
      "publishedDate": "2026-01-08T03:26:15Z",
      "updatedDate": "2026-01-08T03:26:15Z",
      "primaryCategory": "cs.CY",
      "arxivCategories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06194v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06194",
      "comment": "Under review, 16 pages, 3 figures, 16 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06180",
      "title": "MixDPO: Modeling Preference Strength for Pluralistic Alignment",
      "authors": [
        {
          "name": "Saki Imai",
          "affiliation": null
        },
        {
          "name": "Pedram Heydari",
          "affiliation": null
        },
        {
          "name": "Anthony Sicilia",
          "affiliation": null
        },
        {
          "name": "Asteria Kaeberlein",
          "affiliation": null
        },
        {
          "name": "Katherine Atwell",
          "affiliation": null
        },
        {
          "name": "Malihe Alikhani",
          "affiliation": null
        }
      ],
      "abstract": "Preference based alignment objectives implicitly assume that all human preferences are expressed with equal strength. In practice, however, preference strength varies across individuals and contexts -- a phenomenon established in behavioral economics and discrete choice theory. This mismatch limits the ability of existing objectives to faithfully capture heterogeneous human judgments. Inspired by this literature, we introduce Mixed Logit Direct Preference Optimization (MixDPO), a generalization of Direct Preference Optimization that models variation in preference strength. MixDPO enables alignment objectives to capture heterogeneity in how strongly preferences are expressed across training examples. We evaluate MixDPO on three preference datasets using two open-weight language models. Across datasets, MixDPO improves aggregate alignment performance (+11.2 points on Pythia-2.8B) while preserving subgroup level preferences, with the largest gains appearing in settings with higher inferred preference heterogeneity. MixDPO makes preference heterogeneity explicit through learned strength distributions. We release our code for reproducibility.",
      "publishedDate": "2026-01-07T16:57:43Z",
      "updatedDate": "2026-01-07T16:57:43Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06180v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06180",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06552",
      "title": "Model Reconciliation through Explainability and Collaborative Recovery in Assistive Robotics",
      "authors": [
        {
          "name": "Britt Besch",
          "affiliation": null
        },
        {
          "name": "Tai Mai",
          "affiliation": null
        },
        {
          "name": "Jeremias Thun",
          "affiliation": null
        },
        {
          "name": "Markus Huff",
          "affiliation": null
        },
        {
          "name": "Jörn Vogel",
          "affiliation": null
        },
        {
          "name": "Freek Stulp",
          "affiliation": null
        },
        {
          "name": "Samuel Bustamante",
          "affiliation": null
        }
      ],
      "abstract": "Whenever humans and robots work together, it is essential that unexpected robot behavior can be explained to the user. Especially in applications such as shared control the user and the robot must share the same model of the objects in the world, and the actions that can be performed on these objects. In this paper, we achieve this with a so-called model reconciliation framework. We leverage a Large Language Model to predict and explain the difference between the robot's and the human's mental models, without the need of a formal mental model of the user. Furthermore, our framework aims to solve the model divergence after the explanation by allowing the human to correct the robot. We provide an implementation in an assistive robotics domain, where we conduct a set of experiments with a real wheelchair-based mobile manipulator and its digital twin.",
      "publishedDate": "2026-01-10T12:38:08Z",
      "updatedDate": "2026-01-10T12:38:08Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06552v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06552",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "robotics",
        "rag"
      ],
      "tags": {
        "auto": [
          "robotics",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07779",
      "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
      "authors": [
        {
          "name": "Bowen Yang",
          "affiliation": null
        },
        {
          "name": "Kaiming Jin",
          "affiliation": null
        },
        {
          "name": "Zhenyu Wu",
          "affiliation": null
        },
        {
          "name": "Zhaoyang Liu",
          "affiliation": null
        },
        {
          "name": "Qiushi Sun",
          "affiliation": null
        },
        {
          "name": "Zehao Li",
          "affiliation": null
        },
        {
          "name": "JingJing Xie",
          "affiliation": null
        },
        {
          "name": "Zhoumianze Liu",
          "affiliation": null
        },
        {
          "name": "Fangzhi Xu",
          "affiliation": null
        },
        {
          "name": "Kanzhi Cheng",
          "affiliation": null
        },
        {
          "name": "Qingyun Li",
          "affiliation": null
        },
        {
          "name": "Yian Wang",
          "affiliation": null
        },
        {
          "name": "Yu Qiao",
          "affiliation": null
        },
        {
          "name": "Zun Wang",
          "affiliation": null
        },
        {
          "name": "Zichen Ding",
          "affiliation": null
        }
      ],
      "abstract": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.",
      "publishedDate": "2026-01-12T17:55:51Z",
      "updatedDate": "2026-01-12T17:55:51Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07779v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07779",
      "comment": "31 pages, 11 figures, 12 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07143",
      "title": "EZBlender: Efficient 3D Editing with Plan-and-ReAct Agent",
      "authors": [
        {
          "name": "Hao Wang",
          "affiliation": null
        },
        {
          "name": "Wenhui Zhu",
          "affiliation": null
        },
        {
          "name": "Shao Tang",
          "affiliation": null
        },
        {
          "name": "Zhipeng Wang",
          "affiliation": null
        },
        {
          "name": "Xuanzhao Dong",
          "affiliation": null
        },
        {
          "name": "Xin Li",
          "affiliation": null
        },
        {
          "name": "Xiwen Chen",
          "affiliation": null
        },
        {
          "name": "Ashish Bastola",
          "affiliation": null
        },
        {
          "name": "Xinhao Huang",
          "affiliation": null
        },
        {
          "name": "Yalin Wang",
          "affiliation": null
        },
        {
          "name": "Abolfazl Razi",
          "affiliation": null
        }
      ],
      "abstract": "As a cornerstone of the modern digital economy, 3D modeling and rendering demand substantial resources and manual effort when scene editing is performed in the traditional manner. Despite recent progress in VLM-based agents for 3D editing, the fundamental trade-off between editing precision and agent responsiveness remains unresolved. To overcome these limitations, we present EZBlender, a Blender agent with a hybrid framework that combines planning-based task decomposition and reactive local autonomy for efficient human AI collaboration and semantically faithful 3D editing. Specifically, this unexplored Plan-and-ReAct design not only preserves editing quality but also significantly reduces latency and computational cost. To further validate the efficiency and effectiveness of the proposed edge-autonomy architecture, we construct a dedicated multi-tasking benchmark that has not been systematically investigated in prior research. In addition, we provide a comprehensive analysis of language model preference, system responsiveness, and economic efficiency.",
      "publishedDate": "2026-01-12T02:19:34Z",
      "updatedDate": "2026-01-12T02:19:34Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07143v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07143",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "planning",
        "agents",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "agents",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06875",
      "title": "An Ubuntu-Guided Large Language Model Framework for Cognitive Behavioral Mental Health Dialogue",
      "authors": [
        {
          "name": "Sontaga G. Forane",
          "affiliation": null
        },
        {
          "name": "Absalom E. Ezugwu",
          "affiliation": null
        },
        {
          "name": "Kevin Igwe",
          "affiliation": null
        },
        {
          "name": "Karen van den Berg",
          "affiliation": null
        }
      ],
      "abstract": "South Africa's escalating mental health crisis, compounded by limited access to culturally responsive care, calls for innovative and contextually grounded interventions. While large language models show considerable promise for mental health support, their predominantly Western-centric training data limit cultural and linguistic applicability in African contexts. This study introduces a proof-of-concept framework that integrates cognitive behavioral therapy with the African philosophy of Ubuntu to create a culturally sensitive, emotionally intelligent, AI-driven mental health dialogue system. Guided by a design science research methodology, the framework applies both deep theoretical and therapeutic adaptations as well as surface-level linguistic and communicative cultural adaptations. Key CBT techniques, including behavioral activation and cognitive restructuring, were reinterpreted through Ubuntu principles that emphasize communal well-being, spiritual grounding, and interconnectedness. A culturally adapted dataset was developed through iterative processes of language simplification, spiritual contextualization, and Ubuntu-based reframing. The fine-tuned model was evaluated through expert-informed case studies, employing UniEval for conversational quality assessment alongside additional measures of CBT reliability and cultural linguistic alignment. Results demonstrate that the model effectively engages in empathetic, context-aware dialogue aligned with both therapeutic and cultural objectives. Although real-time end-user testing has not yet been conducted, the model underwent rigorous review and supervision by domain specialist clinical psychologists. The findings highlight the potential of culturally embedded emotional intelligence to enhance the contextual relevance, inclusivity, and effectiveness of AI-driven mental health interventions across African settings.",
      "publishedDate": "2026-01-11T11:50:18Z",
      "updatedDate": "2026-01-11T11:50:18Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06875v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06875",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06663",
      "title": "SafePro: Evaluating the Safety of Professional-Level AI Agents",
      "authors": [
        {
          "name": "Kaiwen Zhou",
          "affiliation": null
        },
        {
          "name": "Shreedhar Jangam",
          "affiliation": null
        },
        {
          "name": "Ashwin Nagarajan",
          "affiliation": null
        },
        {
          "name": "Tejas Polu",
          "affiliation": null
        },
        {
          "name": "Suhas Oruganti",
          "affiliation": null
        },
        {
          "name": "Chengzhi Liu",
          "affiliation": null
        },
        {
          "name": "Ching-Chen Kuo",
          "affiliation": null
        },
        {
          "name": "Yuting Zheng",
          "affiliation": null
        },
        {
          "name": "Sravana Narayanaraju",
          "affiliation": null
        },
        {
          "name": "Xin Eric Wang",
          "affiliation": null
        }
      ],
      "abstract": "Large language model-based agents are rapidly evolving from simple conversational assistants into autonomous systems capable of performing complex, professional-level tasks in various domains. While these advancements promise significant productivity gains, they also introduce critical safety risks that remain under-explored. Existing safety evaluations primarily focus on simple, daily assistance tasks, failing to capture the intricate decision-making processes and potential consequences of misaligned behaviors in professional settings. To address this gap, we introduce \\textbf{SafePro}, a comprehensive benchmark designed to evaluate the safety alignment of AI agents performing professional activities. SafePro features a dataset of high-complexity tasks across diverse professional domains with safety risks, developed through a rigorous iterative creation and review process. Our evaluation of state-of-the-art AI models reveals significant safety vulnerabilities and uncovers new unsafe behaviors in professional contexts. We further show that these models exhibit both insufficient safety judgment and weak safety alignment when executing complex professional tasks. In addition, we investigate safety mitigation strategies for improving agent safety in these scenarios and observe encouraging improvements. Together, our findings highlight the urgent need for robust safety mechanisms tailored to the next generation of professional AI agents.",
      "publishedDate": "2026-01-10T19:53:09Z",
      "updatedDate": "2026-01-13T18:20:33Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06663v2",
      "arxivUrl": "https://arxiv.org/abs/2601.06663",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06460",
      "title": "Tone Matters: The Impact of Linguistic Tone on Hallucination in VLMs",
      "authors": [
        {
          "name": "Weihao Hong",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Jiang",
          "affiliation": null
        },
        {
          "name": "Bingyu Shen",
          "affiliation": null
        },
        {
          "name": "Xinlei Guan",
          "affiliation": null
        },
        {
          "name": "Yangyi Feng",
          "affiliation": null
        },
        {
          "name": "Meng Xu",
          "affiliation": null
        },
        {
          "name": "Boyang Li",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) are increasingly used in safety-critical applications that require reliable visual grounding. However, these models often hallucinate details that are not present in the image to satisfy user prompts. While recent datasets and benchmarks have been introduced to evaluate systematic hallucinations in VLMs, many hallucination behaviors remain insufficiently characterized. In particular, prior work primarily focuses on object presence or absence, leaving it unclear how prompt phrasing and structural constraints can systematically induce hallucinations. In this paper, we investigate how different forms of prompt pressure influence hallucination behavior. We introduce Ghost-100, a procedurally generated dataset of synthetic scenes in which key visual details are deliberately removed, enabling controlled analysis of absence-based hallucinations. Using a structured 5-Level Prompt Intensity Framework, we vary prompts from neutral queries to toxic demands and rigid formatting constraints. We evaluate three representative open-weight VLMs: MiniCPM-V 2.6-8B, Qwen2-VL-7B, and Qwen3-VL-8B. Across all three models, hallucination rates do not increase monotonically with prompt intensity. All models exhibit reductions at higher intensity levels at different thresholds, though not all show sustained reduction under maximum coercion. These results suggest that current safety alignment is more effective at detecting semantic hostility than structural coercion, revealing model-specific limitations in handling compliance pressure. Our dataset is available at: https://github.com/bli1/tone-matters",
      "publishedDate": "2026-01-10T07:00:22Z",
      "updatedDate": "2026-01-10T07:00:22Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06460v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06460",
      "comment": "10 pages, 6 figures, WACV Workshop",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06341",
      "title": "Evaluating Robustness of Large Language Models in Enterprise Applications: Benchmarks for Perturbation Consistency Across Formats and Languages",
      "authors": [
        {
          "name": "Tara Bogavelli",
          "affiliation": null
        },
        {
          "name": "Oluwanifemi Bamgbose",
          "affiliation": null
        },
        {
          "name": "Gabrielle Gauthier Melançon",
          "affiliation": null
        },
        {
          "name": "Fanny Riols",
          "affiliation": null
        },
        {
          "name": "Roshnee Sharma",
          "affiliation": null
        }
      ],
      "abstract": "Enterprise LLM applications require consistently high quality and reliable performance across diverse scenarios, demanding robustness to minor variations. Existing research shows that even small prompt changes can lead to substantial differences in output, but has mainly focused on a narrow set of perturbations with small academic datasets, limiting their relevance to real-world applications. To address this, we present a comprehensive benchmark suite that evaluates robustness across multiple perturbation types, including general text edits (e.g., punctuation, whitespace), formatting changes (e.g., JSON, YAML), multilingual and cross-lingual inputs, and positional variations in instructions. Evaluating 11 models ranging from 4B to 120B+ parameters, we find that minor perturbations reduce performance by up to 40 percentage points on key enterprise metrics. Critically, we demonstrate that the relationship between model size and robustness is more nuanced than conventional assumptions suggest: an 8B parameter model (Ministral 3 8B) outperforms most larger models, while another 8B model (Llama 3.1 8B) performs worst overall.",
      "publishedDate": "2026-01-09T22:26:31Z",
      "updatedDate": "2026-01-09T22:26:31Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06341v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06341",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-13T03:23:47.378Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08829",
      "title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System",
      "authors": [
        {
          "name": "Hsiang-Wei Huang",
          "affiliation": null
        },
        {
          "name": "Junbin Lu",
          "affiliation": null
        },
        {
          "name": "Kuang-Ming Chen",
          "affiliation": null
        },
        {
          "name": "Jenq-Neng Hwang",
          "affiliation": null
        }
      ],
      "abstract": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.",
      "publishedDate": "2026-01-13T18:59:17Z",
      "updatedDate": "2026-01-13T18:59:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08829v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08829",
      "comment": "In submission. The first two authors contributed equally",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08742",
      "title": "Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents",
      "authors": [
        {
          "name": "Xin Quan",
          "affiliation": null
        },
        {
          "name": "Jiafeng Xiong",
          "affiliation": null
        },
        {
          "name": "Marco Valentino",
          "affiliation": null
        },
        {
          "name": "André Freitas",
          "affiliation": null
        }
      ],
      "abstract": "Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.",
      "publishedDate": "2026-01-13T17:18:38Z",
      "updatedDate": "2026-01-13T17:18:38Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08742v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08742",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08462",
      "title": "M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games",
      "authors": [
        {
          "name": "Sixiong Xie",
          "affiliation": null
        },
        {
          "name": "Zhuofan Shi",
          "affiliation": null
        },
        {
          "name": "Haiyang Shen",
          "affiliation": null
        },
        {
          "name": "Gang Huang",
          "affiliation": null
        },
        {
          "name": "Yun Ma",
          "affiliation": null
        },
        {
          "name": "Xiang Jing",
          "affiliation": null
        }
      ],
      "abstract": "As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.",
      "publishedDate": "2026-01-13T11:38:51Z",
      "updatedDate": "2026-01-13T11:38:51Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08462v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08462",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08435",
      "title": "Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management",
      "authors": [
        {
          "name": "Weitao Ma",
          "affiliation": null
        },
        {
          "name": "Xiaocheng Feng",
          "affiliation": null
        },
        {
          "name": "Lei Huang",
          "affiliation": null
        },
        {
          "name": "Xiachong Feng",
          "affiliation": null
        },
        {
          "name": "Zhanyu Ma",
          "affiliation": null
        },
        {
          "name": "Jun Xu",
          "affiliation": null
        },
        {
          "name": "Jiuchong Gao",
          "affiliation": null
        },
        {
          "name": "Jinghua Hao",
          "affiliation": null
        },
        {
          "name": "Renqing He",
          "affiliation": null
        },
        {
          "name": "Bing Qin",
          "affiliation": null
        }
      ],
      "abstract": "Effective memory management is essential for large language model agents to navigate long-horizon tasks. Recent research has explored using Reinforcement Learning to develop specialized memory manager agents. However, existing approaches rely on final task performance as the primary reward, which results in severe reward sparsity and ineffective credit assignment, providing insufficient guidance for individual memory operations. To this end, we propose Fine-Mem, a unified framework designed for fine-grained feedback alignment. First, we introduce a Chunk-level Step Reward to provide immediate step-level supervision via auxiliary chunk-specific question answering tasks. Second, we devise Evidence-Anchored Reward Attribution to redistribute global rewards by anchoring credit to key memory operations, based on the specific memory items utilized as evidence in reasoning. Together, these components enable stable policy optimization and align local memory operations with the long-term utility of memory. Experiments on Memalpha and MemoryAgentBench demonstrate that Fine-Mem consistently outperforms strong baselines, achieving superior success rates across various sub-tasks. Further analysis reveals its adaptability and strong generalization capabilities across diverse model configurations and backbones.",
      "publishedDate": "2026-01-13T11:06:17Z",
      "updatedDate": "2026-01-13T11:06:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08435v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08435",
      "comment": "18 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08235",
      "title": "MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents",
      "authors": [
        {
          "name": "Shouju Wang",
          "affiliation": null
        },
        {
          "name": "Haopeng Zhang",
          "affiliation": null
        }
      ],
      "abstract": "As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.",
      "publishedDate": "2026-01-13T05:39:43Z",
      "updatedDate": "2026-01-13T05:39:43Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08235v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08235",
      "comment": "Submitted to ACL 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08160",
      "title": "SwiftMem: Fast Agentic Memory via Query-aware Indexing",
      "authors": [
        {
          "name": "Anxin Tian",
          "affiliation": null
        },
        {
          "name": "Yiming Li",
          "affiliation": null
        },
        {
          "name": "Xing Li",
          "affiliation": null
        },
        {
          "name": "Hui-Ling Zhen",
          "affiliation": null
        },
        {
          "name": "Lei Chen",
          "affiliation": null
        },
        {
          "name": "Xianzhi Yu",
          "affiliation": null
        },
        {
          "name": "Zhenhua Dong",
          "affiliation": null
        },
        {
          "name": "Mingxuan Yuan",
          "affiliation": null
        }
      ],
      "abstract": "Agentic memory systems have become critical for enabling LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. We propose SwiftMem, a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. Our temporal index enables logarithmic-time range queries for time-sensitive retrieval, while the semantic DAG-Tag index maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, we introduce an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47$\\times$ faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.",
      "publishedDate": "2026-01-13T02:51:04Z",
      "updatedDate": "2026-01-13T02:51:04Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08160v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08160",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08012",
      "title": "Towards Verifiably Safe Tool Use for LLM Agents",
      "authors": [
        {
          "name": "Aarya Doshi",
          "affiliation": null
        },
        {
          "name": "Yining Hong",
          "affiliation": null
        },
        {
          "name": "Congying Xu",
          "affiliation": null
        },
        {
          "name": "Eunsuk Kang",
          "affiliation": null
        },
        {
          "name": "Alexandros Kapravelos",
          "affiliation": null
        },
        {
          "name": "Christian Kästner",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. While this empowers agents to perform complex tasks, LLMs may invoke unintended tool interactions and introduce risks, such as leaking sensitive data or overwriting critical records, which are unacceptable in enterprise contexts. Current approaches to mitigate these risks, such as model-based safeguards, enhance agents' reliability but cannot guarantee system safety. Methods like information flow control (IFC) and temporal constraints aim to provide guarantees but often require extensive human annotation. We propose a process that starts with applying System-Theoretic Process Analysis (STPA) to identify hazards in agent workflows, derive safety requirements, and formalize them as enforceable specifications on data flows and tool sequences. To enable this, we introduce a capability-enhanced Model Context Protocol (MCP) framework that requires structured labels on capabilities, confidentiality, and trust level. Together, these contributions aim to shift LLM-based agent safety from ad hoc reliability fixes to proactive guardrails with formal guarantees, while reducing dependence on user confirmation and making autonomy a deliberate design choice.",
      "publishedDate": "2026-01-12T21:31:38Z",
      "updatedDate": "2026-01-12T21:31:38Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08012v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08012",
      "comment": "4 pages, 1 figure; accepted to ICSE NIER 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "tool-use",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08196",
      "title": "Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis",
      "authors": [
        {
          "name": "Da Song",
          "affiliation": null
        },
        {
          "name": "Yuheng Huang",
          "affiliation": null
        },
        {
          "name": "Boqi Chen",
          "affiliation": null
        },
        {
          "name": "Tianshuo Cong",
          "affiliation": null
        },
        {
          "name": "Randy Goebel",
          "affiliation": null
        },
        {
          "name": "Lei Ma",
          "affiliation": null
        },
        {
          "name": "Foutse Khomh",
          "affiliation": null
        }
      ],
      "abstract": "The integration of large language models (LLMs) into autonomous agents has enabled complex tool use, yet in high-stakes domains, these systems must strictly adhere to regulatory standards beyond simple functional correctness. However, existing benchmarks often overlook implicit regulatory compliance, thus failing to evaluate whether LLMs can autonomously enforce mandatory safety constraints. To fill this gap, we introduce LogiSafetyGen, a framework that converts unstructured regulations into Linear Temporal Logic oracles and employs logic-guided fuzzing to synthesize valid, safety-critical traces. Building on this framework, we construct LogiSafetyBench, a benchmark comprising 240 human-verified tasks that require LLMs to generate Python programs that satisfy both functional objectives and latent compliance rules. Evaluations of 13 state-of-the-art (SOTA) LLMs reveal that larger models, despite achieving better functional correctness, frequently prioritize task completion over safety, which results in non-compliant behavior.",
      "publishedDate": "2026-01-13T03:55:18Z",
      "updatedDate": "2026-01-13T03:55:18Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LO",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08196v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08196",
      "comment": "11 pages, 3 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08811",
      "title": "Reasoning Matters for 3D Visual Grounding",
      "authors": [
        {
          "name": "Hsiang-Wei Huang",
          "affiliation": null
        },
        {
          "name": "Kuang-Ming Chen",
          "affiliation": null
        },
        {
          "name": "Wenhao Chai",
          "affiliation": null
        },
        {
          "name": "Cheng-Yen Yang",
          "affiliation": null
        },
        {
          "name": "Jen-Hao Cheng",
          "affiliation": null
        },
        {
          "name": "Jenq-Neng Hwang",
          "affiliation": null
        }
      ],
      "abstract": "The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.",
      "publishedDate": "2026-01-13T18:48:41Z",
      "updatedDate": "2026-01-13T18:48:41Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08811v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08811",
      "comment": "2025 CVPR Workshop on 3D-LLM/VLA: Bridging Language, Vision and Action in 3D Environments",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08808",
      "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
      "authors": [
        {
          "name": "Yao Tang",
          "affiliation": null
        },
        {
          "name": "Li Dong",
          "affiliation": null
        },
        {
          "name": "Yaru Hao",
          "affiliation": null
        },
        {
          "name": "Qingxiu Dong",
          "affiliation": null
        },
        {
          "name": "Furu Wei",
          "affiliation": null
        },
        {
          "name": "Jiatao Gu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.",
      "publishedDate": "2026-01-13T18:48:00Z",
      "updatedDate": "2026-01-13T18:48:00Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08808v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08808",
      "comment": "21 pages. Code available at https://github.com/GMLR-Penn/Multiplex-Thinking",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08763",
      "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
      "authors": [
        {
          "name": "Zhiyuan Hu",
          "affiliation": null
        },
        {
          "name": "Yucheng Wang",
          "affiliation": null
        },
        {
          "name": "Yufei He",
          "affiliation": null
        },
        {
          "name": "Jiaying Wu",
          "affiliation": null
        },
        {
          "name": "Yilun Zhao",
          "affiliation": null
        },
        {
          "name": "See-Kiong Ng",
          "affiliation": null
        },
        {
          "name": "Cynthia Breazeal",
          "affiliation": null
        },
        {
          "name": "Anh Tuan Luu",
          "affiliation": null
        },
        {
          "name": "Hae Won Park",
          "affiliation": null
        },
        {
          "name": "Bryan Hooi",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.",
      "publishedDate": "2026-01-13T17:48:43Z",
      "updatedDate": "2026-01-13T17:48:43Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08763v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08763",
      "comment": "Work in Progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08758",
      "title": "M3CoTBench: Benchmark Chain-of-Thought of MLLMs in Medical Image Understanding",
      "authors": [
        {
          "name": "Juntao Jiang",
          "affiliation": null
        },
        {
          "name": "Jiangning Zhang",
          "affiliation": null
        },
        {
          "name": "Yali Bi",
          "affiliation": null
        },
        {
          "name": "Jinsheng Bai",
          "affiliation": null
        },
        {
          "name": "Weixuan Liu",
          "affiliation": null
        },
        {
          "name": "Weiwei Jin",
          "affiliation": null
        },
        {
          "name": "Zhucun Xue",
          "affiliation": null
        },
        {
          "name": "Yong Liu",
          "affiliation": null
        },
        {
          "name": "Xiaobin Hu",
          "affiliation": null
        },
        {
          "name": "Shuicheng Yan",
          "affiliation": null
        }
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models by encouraging step-by-step intermediate reasoning, and recent advances have extended this paradigm to Multimodal Large Language Models (MLLMs). In the medical domain, where diagnostic decisions depend on nuanced visual cues and sequential reasoning, CoT aligns naturally with clinical thinking processes. However, Current benchmarks for medical image understanding generally focus on the final answer while ignoring the reasoning path. An opaque process lacks reliable bases for judgment, making it difficult to assist doctors in diagnosis. To address this gap, we introduce a new M3CoTBench benchmark specifically designed to evaluate the correctness, efficiency, impact, and consistency of CoT reasoning in medical image understanding. M3CoTBench features 1) a diverse, multi-level difficulty dataset covering 24 examination types, 2) 13 varying-difficulty tasks, 3) a suite of CoT-specific evaluation metrics (correctness, efficiency, impact, and consistency) tailored to clinical reasoning, and 4) a performance analysis of multiple MLLMs. M3CoTBench systematically evaluates CoT reasoning across diverse medical imaging tasks, revealing current limitations of MLLMs in generating reliable and clinically interpretable reasoning, and aims to foster the development of transparent, trustworthy, and diagnostically accurate AI systems for healthcare. Project page at https://juntaojianggavin.github.io/projects/M3CoTBench/.",
      "publishedDate": "2026-01-13T17:42:27Z",
      "updatedDate": "2026-01-13T17:42:27Z",
      "primaryCategory": "eess.IV",
      "arxivCategories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08758v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08758",
      "comment": "40 pages, 8 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08741",
      "title": "From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding",
      "authors": [
        {
          "name": "Anmol Gulati",
          "affiliation": null
        },
        {
          "name": "Sahil Sen",
          "affiliation": null
        },
        {
          "name": "Waqar Sarguroh",
          "affiliation": null
        },
        {
          "name": "Kevin Paul",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.",
      "publishedDate": "2026-01-13T17:18:14Z",
      "updatedDate": "2026-01-13T17:18:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08741v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08741",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08739",
      "title": "PrivGemo: Privacy-Preserving Dual-Tower Graph Retrieval for Empowering LLM Reasoning with Memory Augmentation",
      "authors": [
        {
          "name": "Xingyu Tan",
          "affiliation": null
        },
        {
          "name": "Xiaoyang Wang",
          "affiliation": null
        },
        {
          "name": "Qing Liu",
          "affiliation": null
        },
        {
          "name": "Xiwei Xu",
          "affiliation": null
        },
        {
          "name": "Xin Yuan",
          "affiliation": null
        },
        {
          "name": "Liming Zhu",
          "affiliation": null
        },
        {
          "name": "Wenjie Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Knowledge graphs (KGs) provide structured evidence that can ground large language model (LLM) reasoning for knowledge-intensive question answering. However, many practical KGs are private, and sending retrieved triples or exploration traces to closed-source LLM APIs introduces leakage risk. Existing privacy treatments focus on masking entity names, but they still face four limitations: structural leakage under semantic masking, uncontrollable remote interaction, fragile multi-hop and multi-entity reasoning, and limited experience reuse for stability and efficiency. To address these issues, we propose PrivGemo, a privacy-preserving retrieval-augmented framework for KG-grounded reasoning with memory-guided exposure control. PrivGemo uses a dual-tower design to keep raw KG knowledge local while enabling remote reasoning over an anonymized view that goes beyond name masking to limit both semantic and structural exposure. PrivGemo supports multi-hop, multi-entity reasoning by retrieving anonymized long-hop paths that connect all topic entities, while keeping grounding and verification on the local KG. A hierarchical controller and a privacy-aware experience memory further reduce unnecessary exploration and remote interactions. Comprehensive experiments on six benchmarks show that PrivGemo achieves overall state-of-the-art results, outperforming the strongest baseline by up to 17.1%. Furthermore, PrivGemo enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.",
      "publishedDate": "2026-01-13T17:14:23Z",
      "updatedDate": "2026-01-13T17:14:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08739v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08739",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08699",
      "title": "RAGShaper: Eliciting Sophisticated Agentic RAG Skills via Automated Data Synthesis",
      "authors": [
        {
          "name": "Zhengwei Tao",
          "affiliation": null
        },
        {
          "name": "Bo Li",
          "affiliation": null
        },
        {
          "name": "Jialong Wu",
          "affiliation": null
        },
        {
          "name": "Guochen Yan",
          "affiliation": null
        },
        {
          "name": "Huanyao Zhang",
          "affiliation": null
        },
        {
          "name": "Jiahao Xu",
          "affiliation": null
        },
        {
          "name": "Haitao Mi",
          "affiliation": null
        },
        {
          "name": "Wentao Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Agentic Retrieval-Augmented Generation (RAG) empowers large language models to autonomously plan and retrieve information for complex problem-solving. However, the development of robust agents is hindered by the scarcity of high-quality training data that reflects the noise and complexity of real-world retrieval environments. Conventional manual annotation is unscalable and often fails to capture the dynamic reasoning strategies required to handle retrieval failures. To bridge this gap, we introduce RAGShaper, a novel data synthesis framework designed to automate the construction of RAG tasks and robust agent trajectories. RAGShaper incorporates an InfoCurator to build dense information trees enriched with adversarial distractors spanning Perception and Cognition levels. Furthermore, we propose a constrained navigation strategy that forces a teacher agent to confront these distractors, thereby eliciting trajectories that explicitly demonstrate error correction and noise rejection. Comprehensive experiments confirm that models trained on our synthesized corpus significantly outperform existing baselines, exhibiting superior robustness in noise-intensive and complex retrieval tasks.",
      "publishedDate": "2026-01-13T16:25:07Z",
      "updatedDate": "2026-01-13T16:25:07Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08699v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08699",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08689",
      "title": "QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language Models",
      "authors": [
        {
          "name": "Zhaolu Kang",
          "affiliation": null
        },
        {
          "name": "Junhao Gong",
          "affiliation": null
        },
        {
          "name": "Wenqing Hu",
          "affiliation": null
        },
        {
          "name": "Shuo Yin",
          "affiliation": null
        },
        {
          "name": "Kehan Jiang",
          "affiliation": null
        },
        {
          "name": "Zhicheng Fang",
          "affiliation": null
        },
        {
          "name": "Yingjie He",
          "affiliation": null
        },
        {
          "name": "Chunlei Meng",
          "affiliation": null
        },
        {
          "name": "Rong Fu",
          "affiliation": null
        },
        {
          "name": "Dongyang Chen",
          "affiliation": null
        },
        {
          "name": "Leqi Zheng",
          "affiliation": null
        },
        {
          "name": "Eric Hanchen Jiang",
          "affiliation": null
        },
        {
          "name": "Yunfei Feng",
          "affiliation": null
        },
        {
          "name": "Yitong Leng",
          "affiliation": null
        },
        {
          "name": "Junfan Zhu",
          "affiliation": null
        },
        {
          "name": "Xiaoyou Chen",
          "affiliation": null
        },
        {
          "name": "Xi Yang",
          "affiliation": null
        },
        {
          "name": "Richeng Xuan",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have shown strong capabilities across many domains, yet their evaluation in financial quantitative tasks remains fragmented and mostly limited to knowledge-centric question answering. We introduce QuantEval, a benchmark that evaluates LLMs across three essential dimensions of quantitative finance: knowledge-based QA, quantitative mathematical reasoning, and quantitative strategy coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting framework that executes model-generated strategies and evaluates them using financial performance metrics, enabling a more realistic assessment of quantitative coding ability. We evaluate some state-of-the-art open-source and proprietary LLMs and observe substantial gaps to human experts, particularly in reasoning and strategy coding. Finally, we conduct large-scale supervised fine-tuning and reinforcement learning experiments on domain-aligned data, demonstrating consistent improvements. We hope QuantEval will facilitate research on LLMs' quantitative finance capabilities and accelerate their practical adoption in real-world trading workflows. We additionally release the full deterministic backtesting configuration (asset universe, cost model, and metric definitions) to ensure strict reproducibility.",
      "publishedDate": "2026-01-13T16:14:23Z",
      "updatedDate": "2026-01-13T16:14:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08689v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08689",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08673",
      "title": "Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock",
      "authors": [
        {
          "name": "Didier Sornette",
          "affiliation": null
        },
        {
          "name": "Sandro Claudio Lera",
          "affiliation": null
        },
        {
          "name": "Ke Wu",
          "affiliation": null
        }
      ],
      "abstract": "Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.",
      "publishedDate": "2026-01-13T15:53:26Z",
      "updatedDate": "2026-01-13T15:53:26Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08673v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08673",
      "comment": "20 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.08641",
      "title": "Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Yichen Luo",
          "affiliation": null
        },
        {
          "name": "Yebo Feng",
          "affiliation": null
        },
        {
          "name": "Jiahua Xu",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "The launch of \\$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data. To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \\$500,000 across these projects.",
      "publishedDate": "2026-01-13T15:13:41Z",
      "updatedDate": "2026-01-13T15:13:41Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08641v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08641",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "prompting",
        "reasoning",
        "agents",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "agents",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08584",
      "title": "Ministral 3",
      "authors": [
        {
          "name": "Alexander H. Liu",
          "affiliation": null
        },
        {
          "name": "Kartik Khandelwal",
          "affiliation": null
        },
        {
          "name": "Sandeep Subramanian",
          "affiliation": null
        },
        {
          "name": "Victor Jouault",
          "affiliation": null
        },
        {
          "name": "Abhinav Rastogi",
          "affiliation": null
        },
        {
          "name": "Adrien Sadé",
          "affiliation": null
        },
        {
          "name": "Alan Jeffares",
          "affiliation": null
        },
        {
          "name": "Albert Jiang",
          "affiliation": null
        },
        {
          "name": "Alexandre Cahill",
          "affiliation": null
        },
        {
          "name": "Alexandre Gavaudan",
          "affiliation": null
        },
        {
          "name": "Alexandre Sablayrolles",
          "affiliation": null
        },
        {
          "name": "Amélie Héliou",
          "affiliation": null
        },
        {
          "name": "Amos You",
          "affiliation": null
        },
        {
          "name": "Andy Ehrenberg",
          "affiliation": null
        },
        {
          "name": "Andy Lo",
          "affiliation": null
        },
        {
          "name": "Anton Eliseev",
          "affiliation": null
        },
        {
          "name": "Antonia Calvi",
          "affiliation": null
        },
        {
          "name": "Avinash Sooriyarachchi",
          "affiliation": null
        },
        {
          "name": "Baptiste Bout",
          "affiliation": null
        },
        {
          "name": "Baptiste Rozière",
          "affiliation": null
        },
        {
          "name": "Baudouin De Monicault",
          "affiliation": null
        },
        {
          "name": "Clémence Lanfranchi",
          "affiliation": null
        },
        {
          "name": "Corentin Barreau",
          "affiliation": null
        },
        {
          "name": "Cyprien Courtot",
          "affiliation": null
        },
        {
          "name": "Daniele Grattarola",
          "affiliation": null
        },
        {
          "name": "Darius Dabert",
          "affiliation": null
        },
        {
          "name": "Diego de las Casas",
          "affiliation": null
        },
        {
          "name": "Elliot Chane-Sane",
          "affiliation": null
        },
        {
          "name": "Faruk Ahmed",
          "affiliation": null
        },
        {
          "name": "Gabrielle Berrada",
          "affiliation": null
        },
        {
          "name": "Gaëtan Ecrepont",
          "affiliation": null
        },
        {
          "name": "Gauthier Guinet",
          "affiliation": null
        },
        {
          "name": "Georgii Novikov",
          "affiliation": null
        },
        {
          "name": "Guillaume Kunsch",
          "affiliation": null
        },
        {
          "name": "Guillaume Lample",
          "affiliation": null
        },
        {
          "name": "Guillaume Martin",
          "affiliation": null
        },
        {
          "name": "Gunshi Gupta",
          "affiliation": null
        },
        {
          "name": "Jan Ludziejewski",
          "affiliation": null
        },
        {
          "name": "Jason Rute",
          "affiliation": null
        },
        {
          "name": "Joachim Studnia",
          "affiliation": null
        },
        {
          "name": "Jonas Amar",
          "affiliation": null
        },
        {
          "name": "Joséphine Delas",
          "affiliation": null
        },
        {
          "name": "Josselin Somerville Roberts",
          "affiliation": null
        },
        {
          "name": "Karmesh Yadav",
          "affiliation": null
        },
        {
          "name": "Khyathi Chandu",
          "affiliation": null
        },
        {
          "name": "Kush Jain",
          "affiliation": null
        },
        {
          "name": "Laurence Aitchison",
          "affiliation": null
        },
        {
          "name": "Laurent Fainsin",
          "affiliation": null
        },
        {
          "name": "Léonard Blier",
          "affiliation": null
        },
        {
          "name": "Lingxiao Zhao",
          "affiliation": null
        },
        {
          "name": "Louis Martin",
          "affiliation": null
        },
        {
          "name": "Lucile Saulnier",
          "affiliation": null
        },
        {
          "name": "Luyu Gao",
          "affiliation": null
        },
        {
          "name": "Maarten Buyl",
          "affiliation": null
        },
        {
          "name": "Margaret Jennings",
          "affiliation": null
        },
        {
          "name": "Marie Pellat",
          "affiliation": null
        },
        {
          "name": "Mark Prins",
          "affiliation": null
        },
        {
          "name": "Mathieu Poirée",
          "affiliation": null
        },
        {
          "name": "Mathilde Guillaumin",
          "affiliation": null
        },
        {
          "name": "Matthieu Dinot",
          "affiliation": null
        },
        {
          "name": "Matthieu Futeral",
          "affiliation": null
        },
        {
          "name": "Maxime Darrin",
          "affiliation": null
        },
        {
          "name": "Maximilian Augustin",
          "affiliation": null
        },
        {
          "name": "Mia Chiquier",
          "affiliation": null
        },
        {
          "name": "Michel Schimpf",
          "affiliation": null
        },
        {
          "name": "Nathan Grinsztajn",
          "affiliation": null
        },
        {
          "name": "Neha Gupta",
          "affiliation": null
        },
        {
          "name": "Nikhil Raghuraman",
          "affiliation": null
        },
        {
          "name": "Olivier Bousquet",
          "affiliation": null
        },
        {
          "name": "Olivier Duchenne",
          "affiliation": null
        },
        {
          "name": "Patricia Wang",
          "affiliation": null
        },
        {
          "name": "Patrick von Platen",
          "affiliation": null
        },
        {
          "name": "Paul Jacob",
          "affiliation": null
        },
        {
          "name": "Paul Wambergue",
          "affiliation": null
        },
        {
          "name": "Paula Kurylowicz",
          "affiliation": null
        },
        {
          "name": "Pavankumar Reddy Muddireddy",
          "affiliation": null
        },
        {
          "name": "Philomène Chagniot",
          "affiliation": null
        },
        {
          "name": "Pierre Stock",
          "affiliation": null
        },
        {
          "name": "Pravesh Agrawal",
          "affiliation": null
        },
        {
          "name": "Quentin Torroba",
          "affiliation": null
        },
        {
          "name": "Romain Sauvestre",
          "affiliation": null
        },
        {
          "name": "Roman Soletskyi",
          "affiliation": null
        },
        {
          "name": "Rupert Menneer",
          "affiliation": null
        },
        {
          "name": "Sagar Vaze",
          "affiliation": null
        },
        {
          "name": "Samuel Barry",
          "affiliation": null
        },
        {
          "name": "Sanchit Gandhi",
          "affiliation": null
        },
        {
          "name": "Siddhant Waghjale",
          "affiliation": null
        },
        {
          "name": "Siddharth Gandhi",
          "affiliation": null
        },
        {
          "name": "Soham Ghosh",
          "affiliation": null
        },
        {
          "name": "Srijan Mishra",
          "affiliation": null
        },
        {
          "name": "Sumukh Aithal",
          "affiliation": null
        },
        {
          "name": "Szymon Antoniak",
          "affiliation": null
        },
        {
          "name": "Teven Le Scao",
          "affiliation": null
        },
        {
          "name": "Théo Cachet",
          "affiliation": null
        },
        {
          "name": "Theo Simon Sorg",
          "affiliation": null
        },
        {
          "name": "Thibaut Lavril",
          "affiliation": null
        },
        {
          "name": "Thiziri Nait Saada",
          "affiliation": null
        },
        {
          "name": "Thomas Chabal",
          "affiliation": null
        },
        {
          "name": "Thomas Foubert",
          "affiliation": null
        },
        {
          "name": "Thomas Robert",
          "affiliation": null
        },
        {
          "name": "Thomas Wang",
          "affiliation": null
        },
        {
          "name": "Tim Lawson",
          "affiliation": null
        },
        {
          "name": "Tom Bewley",
          "affiliation": null
        },
        {
          "name": "Tom Bewley",
          "affiliation": null
        },
        {
          "name": "Tom Edwards",
          "affiliation": null
        },
        {
          "name": "Umar Jamil",
          "affiliation": null
        },
        {
          "name": "Umberto Tomasini",
          "affiliation": null
        },
        {
          "name": "Valeriia Nemychnikova",
          "affiliation": null
        },
        {
          "name": "Van Phung",
          "affiliation": null
        },
        {
          "name": "Vincent Maladière",
          "affiliation": null
        },
        {
          "name": "Virgile Richard",
          "affiliation": null
        },
        {
          "name": "Wassim Bouaziz",
          "affiliation": null
        },
        {
          "name": "Wen-Ding Li",
          "affiliation": null
        },
        {
          "name": "William Marshall",
          "affiliation": null
        },
        {
          "name": "Xinghui Li",
          "affiliation": null
        },
        {
          "name": "Xinyu Yang",
          "affiliation": null
        },
        {
          "name": "Yassine El Ouahidi",
          "affiliation": null
        },
        {
          "name": "Yihan Wang",
          "affiliation": null
        },
        {
          "name": "Yunhao Tang",
          "affiliation": null
        },
        {
          "name": "Zaccharie Ramzi",
          "affiliation": null
        }
      ],
      "abstract": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",
      "publishedDate": "2026-01-13T14:06:03Z",
      "updatedDate": "2026-01-13T14:06:03Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08584v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08584",
      "comment": "Release page: https://mistral.ai/news/mistral-3 ; Models available at https://huggingface.co/collections/mistralai/ministral-3",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08521",
      "title": "Your Group-Relative Advantage Is Biased",
      "authors": [
        {
          "name": "Fengkai Yang",
          "affiliation": null
        },
        {
          "name": "Zherui Chen",
          "affiliation": null
        },
        {
          "name": "Xiaohan Wang",
          "affiliation": null
        },
        {
          "name": "Xiaodong Lu",
          "affiliation": null
        },
        {
          "name": "Jiajun Chai",
          "affiliation": null
        },
        {
          "name": "Guojun Yin",
          "affiliation": null
        },
        {
          "name": "Wei Lin",
          "affiliation": null
        },
        {
          "name": "Shuai Ma",
          "affiliation": null
        },
        {
          "name": "Fuzhen Zhuang",
          "affiliation": null
        },
        {
          "name": "Deqing Wang",
          "affiliation": null
        },
        {
          "name": "Yaodong Yang",
          "affiliation": null
        },
        {
          "name": "Jianxin Li",
          "affiliation": null
        },
        {
          "name": "Yikun Ban",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood. In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.",
      "publishedDate": "2026-01-13T13:03:15Z",
      "updatedDate": "2026-01-13T13:03:15Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08521v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08521",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08517",
      "title": "Closed-Loop LLM Discovery of Non-Standard Channel Priors in Vision Models",
      "authors": [
        {
          "name": "Tolgay Atinc Uzun",
          "affiliation": null
        },
        {
          "name": "Dmitry Ignatov",
          "affiliation": null
        },
        {
          "name": "Radu Timofte",
          "affiliation": null
        }
      ],
      "abstract": "Channel configuration search the optimization of layer specifications such as layer widths in deep neural networks presents a complex combinatorial challenge constrained by tensor shape compatibility and computational budgets. We posit that Large Language Models (LLMs) offer a transformative approach to Neural Architecture Search (NAS), capable of reasoning about architectural code structure in ways that traditional heuristics cannot. In this paper, we investigate the application of an LLM-driven NAS framework to the problem of channel configuration. We formulate the search as a sequence of conditional code generation tasks, where an LLM refines architectural specifications based on performance telemetry. Crucially, we address the data scarcity problem by generating a vast corpus of valid, shape-consistent architectures via Abstract Syntax Tree (AST) mutations. While these mutated networks are not necessarily high-performing, they provide the critical volume of structural data required for the LLM to learn the latent relationship between channel configurations and model performance. This allows the LLM to internalize complex design patterns and apply them to optimize feature extraction strategies. Experimental results on CIFAR-100 validate the efficacy of this approach, demonstrating that the model yields statistically significant improvements in accuracy. Our analysis confirms that the LLM successfully acquires domain-specific architectural priors, distinguishing this method from random search and highlighting the immense potential of language-driven design in deep learning.",
      "publishedDate": "2026-01-13T13:00:30Z",
      "updatedDate": "2026-01-13T13:00:30Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08517v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08517",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08472",
      "title": "sui-1: Grounded and Verifiable Long-Form Summarization",
      "authors": [
        {
          "name": "Benedikt Droste",
          "affiliation": null
        },
        {
          "name": "Jan Philipp Harries",
          "affiliation": null
        },
        {
          "name": "Maximilian Idahl",
          "affiliation": null
        },
        {
          "name": "Björn Plüster",
          "affiliation": null
        }
      ],
      "abstract": "Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available.",
      "publishedDate": "2026-01-13T11:59:15Z",
      "updatedDate": "2026-01-13T11:59:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08472v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08472",
      "comment": "13 pages, 4 figures, model weights at https://huggingface.co/ellamind/sui-1-24b",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08468",
      "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
      "authors": [
        {
          "name": "Jiangshan Duo",
          "affiliation": null
        },
        {
          "name": "Hanyu Li",
          "affiliation": null
        },
        {
          "name": "Hailin Zhang",
          "affiliation": null
        },
        {
          "name": "Yudong Wang",
          "affiliation": null
        },
        {
          "name": "Sujian Li",
          "affiliation": null
        },
        {
          "name": "Liang Zhao",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.",
      "publishedDate": "2026-01-13T11:47:42Z",
      "updatedDate": "2026-01-13T11:47:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08468v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08468",
      "comment": "16 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "planning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08454",
      "title": "Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees",
      "authors": [
        {
          "name": "Alessandro Adami",
          "affiliation": null
        },
        {
          "name": "Sebastian Zudaire",
          "affiliation": null
        },
        {
          "name": "Ruggero Carli",
          "affiliation": null
        },
        {
          "name": "Pietro Falco",
          "affiliation": null
        }
      ],
      "abstract": "Constructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines. Given a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation. Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.",
      "publishedDate": "2026-01-13T11:28:46Z",
      "updatedDate": "2026-01-13T11:28:46Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08454v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08454",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "reasoning",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08444",
      "title": "Beyond Linearization: Attributed Table Graphs for Table Reasoning",
      "authors": [
        {
          "name": "Yuxiang Wang",
          "affiliation": null
        },
        {
          "name": "Junhao Gan",
          "affiliation": null
        },
        {
          "name": "Shengxiang Gao",
          "affiliation": null
        },
        {
          "name": "Shenghao Ye",
          "affiliation": null
        },
        {
          "name": "Zhengyi Yang",
          "affiliation": null
        },
        {
          "name": "Jianzhong Qi",
          "affiliation": null
        }
      ],
      "abstract": "Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the \"lost-in-the-middle\" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.",
      "publishedDate": "2026-01-13T11:14:43Z",
      "updatedDate": "2026-01-13T11:14:43Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08444v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08444",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08440",
      "title": "Incentivizing Cardiologist-Like Reasoning in MLLMs for Interpretable Echocardiographic Diagnosis",
      "authors": [
        {
          "name": "Yi Qin",
          "affiliation": null
        },
        {
          "name": "Lehan Wang",
          "affiliation": null
        },
        {
          "name": "Chenxu Zhao",
          "affiliation": null
        },
        {
          "name": "Alex P. W. Lee",
          "affiliation": null
        },
        {
          "name": "Xiaomeng Li",
          "affiliation": null
        }
      ],
      "abstract": "Echocardiographic diagnosis is vital for cardiac screening yet remains challenging. Existing echocardiography foundation models do not effectively capture the relationships between quantitative measurements and clinical manifestations, whereas medical reasoning multimodal large language models (MLLMs) require costly construction of detailed reasoning paths and remain ineffective at directly incorporating such echocardiographic priors into their reasoning. To address these limitations, we propose a novel approach comprising Cardiac Reasoning Template (CRT) and CardiacMind to enhance MLLM's echocardiographic reasoning by introducing cardiologist-like mindset. Specifically, CRT provides stepwise canonical diagnostic procedures for complex cardiac diseases to streamline reasoning path construction without the need for costly case-by-case verification. To incentivize reasoning MLLM under CRT, we develop CardiacMind, a new reinforcement learning scheme with three novel rewards: Procedural Quantity Reward (PQtR), Procedural Quality Reward (PQlR), and Echocardiographic Semantic Reward (ESR). PQtR promotes detailed reasoning; PQlR promotes integration of evidence across views and modalities, while ESR grounds stepwise descriptions in visual content. Our methods show a 48% improvement in multiview echocardiographic diagnosis for 15 complex cardiac diseases and a 5% improvement on CardiacNet-PAH over prior methods. The user study on our method's reasoning outputs shows 93.33% clinician agreement with cardiologist-like reasoning logic. Our code will be available.",
      "publishedDate": "2026-01-13T11:09:46Z",
      "updatedDate": "2026-01-13T11:09:46Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08440v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08440",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08427",
      "title": "Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering",
      "authors": [
        {
          "name": "Nonghai Zhang",
          "affiliation": null
        },
        {
          "name": "Weitao Ma",
          "affiliation": null
        },
        {
          "name": "Zhanyu Ma",
          "affiliation": null
        },
        {
          "name": "Jun Xu",
          "affiliation": null
        },
        {
          "name": "Jiuchong Gao",
          "affiliation": null
        },
        {
          "name": "Jinghua Hao",
          "affiliation": null
        },
        {
          "name": "Renqing He",
          "affiliation": null
        },
        {
          "name": "Jingwen Xu",
          "affiliation": null
        }
      ],
      "abstract": "Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.",
      "publishedDate": "2026-01-13T10:55:08Z",
      "updatedDate": "2026-01-13T10:55:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08427v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08427",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08412",
      "title": "Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation",
      "authors": [
        {
          "name": "Yizhan Feng",
          "affiliation": null
        },
        {
          "name": "Hichem Snoussi",
          "affiliation": null
        },
        {
          "name": "Yuhang Wang",
          "affiliation": null
        },
        {
          "name": "Jing Teng",
          "affiliation": null
        },
        {
          "name": "Abel Cherouat",
          "affiliation": null
        },
        {
          "name": "Tian Wang",
          "affiliation": null
        }
      ],
      "abstract": "With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs",
      "publishedDate": "2026-01-13T10:31:09Z",
      "updatedDate": "2026-01-13T10:31:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08412v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08412",
      "comment": "2nd International Conference on Drones and Unmanned Systems (DAUS' 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "prompting",
        "code-generation",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "code-generation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08408",
      "title": "Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2",
      "authors": [
        {
          "name": "Yizhan Feng",
          "affiliation": null
        },
        {
          "name": "Hichem Snoussi",
          "affiliation": null
        },
        {
          "name": "Jing Teng",
          "affiliation": null
        },
        {
          "name": "Jian Liu",
          "affiliation": null
        },
        {
          "name": "Yuyang Wang",
          "affiliation": null
        },
        {
          "name": "Abel Cherouat",
          "affiliation": null
        },
        {
          "name": "Tian Wang",
          "affiliation": null
        }
      ],
      "abstract": "The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.",
      "publishedDate": "2026-01-13T10:26:10Z",
      "updatedDate": "2026-01-13T10:26:10Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08408v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08408",
      "comment": "The Tenth International Conference on Data Mining and Big Data (DMBD'2025)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08403",
      "title": "Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs",
      "authors": [
        {
          "name": "Abhijnan Nath",
          "affiliation": null
        },
        {
          "name": "Alireza Bagheri Garakani",
          "affiliation": null
        },
        {
          "name": "Tianchen Zhou",
          "affiliation": null
        },
        {
          "name": "Fan Yang",
          "affiliation": null
        },
        {
          "name": "Nikhil Krishnaswamy",
          "affiliation": null
        }
      ],
      "abstract": "Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.",
      "publishedDate": "2026-01-13T10:17:46Z",
      "updatedDate": "2026-01-13T10:17:46Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08403v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08403",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08355",
      "title": "Semantic Misalignment in Vision-Language Models under Perceptual Degradation",
      "authors": [
        {
          "name": "Guo Cheng",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.",
      "publishedDate": "2026-01-13T09:13:05Z",
      "updatedDate": "2026-01-13T09:13:05Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08355v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08355",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "robotics",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "robotics",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08342",
      "title": "Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue",
      "authors": [
        {
          "name": "Run Chen",
          "affiliation": null
        },
        {
          "name": "Wen Liang",
          "affiliation": null
        },
        {
          "name": "Ziwei Gong",
          "affiliation": null
        },
        {
          "name": "Lin Ai",
          "affiliation": null
        },
        {
          "name": "Julia Hirschberg",
          "affiliation": null
        }
      ],
      "abstract": "Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.",
      "publishedDate": "2026-01-13T09:02:08Z",
      "updatedDate": "2026-01-13T09:02:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08342v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08342",
      "comment": "Accepted to IWSDS 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08302",
      "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
      "authors": [
        {
          "name": "Marvin Schmitt",
          "affiliation": null
        },
        {
          "name": "Anne Schwerk",
          "affiliation": null
        },
        {
          "name": "Sebastian Lempert",
          "affiliation": null
        }
      ],
      "abstract": "This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.",
      "publishedDate": "2026-01-13T07:45:36Z",
      "updatedDate": "2026-01-13T07:45:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08302v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08302",
      "comment": "21 pages, 4 figures, 13 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08292",
      "title": "KidVis: Do Multimodal Large Language Models Possess the Visual Perceptual Capabilities of a 6-Year-Old?",
      "authors": [
        {
          "name": "Xianfeng Wang",
          "affiliation": null
        },
        {
          "name": "Kaiwei Zhang",
          "affiliation": null
        },
        {
          "name": "Qi Jia",
          "affiliation": null
        },
        {
          "name": "Zijian Chen",
          "affiliation": null
        },
        {
          "name": "Guangtao Zhai",
          "affiliation": null
        },
        {
          "name": "Xiongkuo Min",
          "affiliation": null
        }
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have demonstrated impressive proficiency in high-level reasoning tasks, such as complex diagrammatic interpretation, it remains an open question whether they possess the fundamental visual primitives comparable to human intuition. To investigate this, we introduce KidVis, a novel benchmark grounded in the theory of human visual development. KidVis deconstructs visual intelligence into six atomic capabilities - Concentration, Tracking, Discrimination, Memory, Spatial, and Closure - already possessed by 6-7 year old children, comprising 10 categories of low-semantic-dependent visual tasks. Evaluating 20 state-of-the-art MLLMs against a human physiological baseline reveals a stark performance disparity. Results indicate that while human children achieve a near-perfect average score of 95.32, the state-of-the-art GPT-5 attains only 67.33. Crucially, we observe a \"Scaling Law Paradox\": simply increasing model parameters fails to yield linear improvements in these foundational visual capabilities. This study confirms that current MLLMs, despite their reasoning prowess, lack the essential physiological perceptual primitives required for generalized visual intelligence.",
      "publishedDate": "2026-01-13T07:32:50Z",
      "updatedDate": "2026-01-13T07:32:50Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08292v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08292",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08274",
      "title": "Discovery and Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees",
      "authors": [
        {
          "name": "Kun Li",
          "affiliation": null
        },
        {
          "name": "Zenan Xu",
          "affiliation": null
        },
        {
          "name": "Junan Li",
          "affiliation": null
        },
        {
          "name": "Zengrui Jin",
          "affiliation": null
        },
        {
          "name": "Jinghao Deng",
          "affiliation": null
        },
        {
          "name": "Zexuan Qiu",
          "affiliation": null
        },
        {
          "name": "Bo Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Tool-Integrated Reasoning has emerged as a key paradigm to augment Large Language Models (LLMs) with computational capabilities, yet integrating tool-use into long Chain-of-Thought (long CoT) remains underexplored, largely due to the scarcity of training data and the challenge of integrating tool-use without compromising the model's intrinsic long-chain reasoning. In this paper, we introduce DART (Discovery And Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees), a reinforcement learning framework that enables spontaneous tool-use during long CoT reasoning without human annotation. DART operates by constructing dynamic rollout trees during training to discover valid tool-use opportunities, branching out at promising positions to explore diverse tool-integrated trajectories. Subsequently, a tree-based process advantage estimation identifies and credits specific sub-trajectories where tool invocation positively contributes to the solution, effectively reinforcing these beneficial behaviors. Extensive experiments on challenging benchmarks like AIME and GPQA-Diamond demonstrate that DART significantly outperforms existing methods, successfully harmonizing tool execution with long CoT reasoning.",
      "publishedDate": "2026-01-13T07:06:21Z",
      "updatedDate": "2026-01-13T07:06:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08274v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08274",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08267",
      "title": "Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning",
      "authors": [
        {
          "name": "Fan Gao",
          "affiliation": null
        },
        {
          "name": "Sherry T. Tong",
          "affiliation": null
        },
        {
          "name": "Jiwoong Sohn",
          "affiliation": null
        },
        {
          "name": "Jiahao Huang",
          "affiliation": null
        },
        {
          "name": "Junfeng Jiang",
          "affiliation": null
        },
        {
          "name": "Ding Xia",
          "affiliation": null
        },
        {
          "name": "Piyalitt Ittichaiwong",
          "affiliation": null
        },
        {
          "name": "Kanyakorn Veerakanjana",
          "affiliation": null
        },
        {
          "name": "Hyunjae Kim",
          "affiliation": null
        },
        {
          "name": "Qingyu Chen",
          "affiliation": null
        },
        {
          "name": "Edison Marrese Taylor",
          "affiliation": null
        },
        {
          "name": "Kazuma Kobayashi",
          "affiliation": null
        },
        {
          "name": "Akkiko Aizawa",
          "affiliation": null
        },
        {
          "name": "Irene Li",
          "affiliation": null
        }
      ],
      "abstract": "While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.",
      "publishedDate": "2026-01-13T06:51:40Z",
      "updatedDate": "2026-01-13T06:51:40Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08267v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08267",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08259",
      "title": "Unleashing Tool Engineering and Intelligence for Agentic AI in Next-Generation Communication Networks",
      "authors": [
        {
          "name": "Yinqiu Liu",
          "affiliation": null
        },
        {
          "name": "Ruichen Zhang",
          "affiliation": null
        },
        {
          "name": "Dusit Niyato",
          "affiliation": null
        },
        {
          "name": "Abbas Jamalipour",
          "affiliation": null
        },
        {
          "name": "Trung Q. Duong",
          "affiliation": null
        },
        {
          "name": "Dong In Kim",
          "affiliation": null
        }
      ],
      "abstract": "Nowadays, agentic AI is emerging as a transformative paradigm for next-generation communication networks, promising to evolve large language models (LLMs) from passive chatbots into autonomous operators. However, unleashing this potential requires bridging the critical gap between abstract reasoning and physical actuation, a capability we term tool intelligence. In this article, we explore the landscape of tool engineering to empower agentic AI in communications. We first analyze the functionalities of tool intelligence and its effects on communications. We then propose a systematic review for tool engineering, covering the entire lifecycle from tool creation and discovery to selection, learning, and benchmarking. Furthermore, we present a case study on tool-assisted uncrewed aerial vehicles (UAV) trajectory planning to demonstrate the realization of tool intelligence in communications. By introducing a teacher-guided reinforcement learning approach with a feasibility shield, we enable agents to intelligently operate tools. They utilize external tools to eliminate navigational uncertainty while mastering cost-aware scheduling under strict energy constraints. This article aims to provide a roadmap for building the tool-augmented intelligent agents of the 6G era.",
      "publishedDate": "2026-01-13T06:32:12Z",
      "updatedDate": "2026-01-13T06:32:12Z",
      "primaryCategory": "cs.NI",
      "arxivCategories": [
        "cs.NI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08259v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08259",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "tool-use",
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08241",
      "title": "Improving Zero-shot ADL Recognition with Large Language Models through Event-based Context and Confidence",
      "authors": [
        {
          "name": "Michele Fiori",
          "affiliation": null
        },
        {
          "name": "Gabriele Civitarese",
          "affiliation": null
        },
        {
          "name": "Marco Colussi",
          "affiliation": null
        },
        {
          "name": "Claudio Bettini",
          "affiliation": null
        }
      ],
      "abstract": "Unobtrusive sensor-based recognition of Activities of Daily Living (ADLs) in smart homes by processing data collected from IoT sensing devices supports applications such as healthcare, safety, and energy management. Recent zero-shot methods based on Large Language Models (LLMs) have the advantage of removing the reliance on labeled ADL sensor data. However, existing approaches rely on time-based segmentation, which is poorly aligned with the contextual reasoning capabilities of LLMs. Moreover, existing approaches lack methods for estimating prediction confidence. This paper proposes to improve zero-shot ADL recognition with event-based segmentation and a novel method for estimating prediction confidence. Our experimental evaluation shows that event-based segmentation consistently outperforms time-based LLM approaches on complex, realistic datasets and surpasses supervised data-driven methods, even with relatively small LLMs (e.g., Gemma 3 27B). The proposed confidence measure effectively distinguishes correct from incorrect predictions.",
      "publishedDate": "2026-01-13T05:58:24Z",
      "updatedDate": "2026-01-13T05:58:24Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.DC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08241v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08241",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08192",
      "title": "Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging",
      "authors": [
        {
          "name": "Md. Faiyaz Abdullah Sayeedi",
          "affiliation": null
        },
        {
          "name": "Rashedur Rahman",
          "affiliation": null
        },
        {
          "name": "Siam Tahsin Bhuiyan",
          "affiliation": null
        },
        {
          "name": "Sefatul Wasi",
          "affiliation": null
        },
        {
          "name": "Ashraful Islam",
          "affiliation": null
        },
        {
          "name": "Saadia Binte Alam",
          "affiliation": null
        },
        {
          "name": "AKM Mahbubur Rahman",
          "affiliation": null
        }
      ],
      "abstract": "Medical image analysis increasingly relies on large vision-language models (VLMs), yet most systems remain single-pass black boxes that offer limited control over reasoning, safety, and spatial grounding. We propose R^4, an agentic framework that decomposes medical imaging workflows into four coordinated agents: a Router that configures task- and specialization-aware prompts from the image, patient history, and metadata; a Retriever that uses exemplar memory and pass@k sampling to jointly generate free-text reports and bounding boxes; a Reflector that critiques each draft-box pair for key clinical error modes (negation, laterality, unsupported claims, contradictions, missing findings, and localization errors); and a Repairer that iteratively revises both narrative and spatial outputs under targeted constraints while curating high-quality exemplars for future cases. Instantiated on chest X-ray analysis with multiple modern VLM backbones and evaluated on report generation and weakly supervised detection, R^4 consistently boosts LLM-as-a-Judge scores by roughly +1.7-+2.5 points and mAP50 by +2.5-+3.5 absolute points over strong single-VLM baselines, without any gradient-based fine-tuning. These results show that agentic routing, reflection, and repair can turn strong but brittle VLMs into more reliable and better grounded tools for clinical image interpretation. Our code can be found at: https://github.com/faiyazabdullah/MultimodalMedAgent",
      "publishedDate": "2026-01-13T03:44:06Z",
      "updatedDate": "2026-01-13T03:44:06Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08192v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08192",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08187",
      "title": "Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression",
      "authors": [
        {
          "name": "Zijun Di",
          "affiliation": null
        },
        {
          "name": "Bin Lu",
          "affiliation": null
        },
        {
          "name": "Huquan Kang",
          "affiliation": null
        },
        {
          "name": "Luoyi Fu",
          "affiliation": null
        },
        {
          "name": "Jiaxin Ding",
          "affiliation": null
        },
        {
          "name": "Xiaoying Gan",
          "affiliation": null
        },
        {
          "name": "Lei Zhou",
          "affiliation": null
        },
        {
          "name": "Xinbing Wang",
          "affiliation": null
        },
        {
          "name": "Chenghu Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.",
      "publishedDate": "2026-01-13T03:35:18Z",
      "updatedDate": "2026-01-13T03:35:18Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08187v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08187",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08183",
      "title": "GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards",
      "authors": [
        {
          "name": "Yan Zhu",
          "affiliation": null
        },
        {
          "name": "Te Luo",
          "affiliation": null
        },
        {
          "name": "Pei-Yao Fu",
          "affiliation": null
        },
        {
          "name": "Zhen Zhang",
          "affiliation": null
        },
        {
          "name": "Zi-Long Wang",
          "affiliation": null
        },
        {
          "name": "Yi-Fan Qu",
          "affiliation": null
        },
        {
          "name": "Zi-Han Geng",
          "affiliation": null
        },
        {
          "name": "Jia-Qi Xu",
          "affiliation": null
        },
        {
          "name": "Lu Yao",
          "affiliation": null
        },
        {
          "name": "Li-Yun Ma",
          "affiliation": null
        },
        {
          "name": "Wei Su",
          "affiliation": null
        },
        {
          "name": "Wei-Feng Chen",
          "affiliation": null
        },
        {
          "name": "Quan-Lin Li",
          "affiliation": null
        },
        {
          "name": "Shuo Wang",
          "affiliation": null
        },
        {
          "name": "Ping-Hong Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) show promise in gastroenterology, yet their performance against comprehensive clinical workflows and human benchmarks remains unverified. To systematically evaluate state-of-the-art MLLMs across a panoramic gastrointestinal endoscopy workflow and determine their clinical utility compared with human endoscopists. We constructed GI-Bench, a benchmark encompassing 20 fine-grained lesion categories. Twelve MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance was benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. Gemini-3-Pro achieved state-of-the-art performance. In diagnostic reasoning, top-tier models (Macro-F1 0.641) outperformed trainees (0.492) and rivaled junior endoscopists (0.727; p>0.05). However, a critical \"spatial grounding bottleneck\" persisted; human lesion localization (mIoU >0.506) significantly outperformed the best model (0.345; p<0.05). Furthermore, qualitative analysis revealed a \"fluency-accuracy paradox\": models generated reports with superior linguistic readability compared with humans (p<0.05) but exhibited significantly lower factual correctness (p<0.05) due to \"over-interpretation\" and hallucination of visual features.GI-Bench maintains a dynamic leaderboard that tracks the evolving performance of MLLMs in clinical endoscopy. The current rankings and benchmark results are available at https://roterdl.github.io/GIBench/.",
      "publishedDate": "2026-01-13T03:23:11Z",
      "updatedDate": "2026-01-13T03:23:11Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08183v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08183",
      "comment": "45 pages, 17 figures, 6 tables. Leaderboard available at: https://roterdl.github.io/GIBench/ . Includes supplementary material",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08176",
      "title": "Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering",
      "authors": [
        {
          "name": "Lavanya Prahallad",
          "affiliation": null
        },
        {
          "name": "Sai Utkarsh Choudarypally",
          "affiliation": null
        },
        {
          "name": "Pragna Prahallad",
          "affiliation": null
        },
        {
          "name": "Pranathi Prahallad",
          "affiliation": null
        }
      ],
      "abstract": "Automatic evaluation of large language model (LLM) responses requires not only factual correctness but also clarity, particularly in political question-answering. While recent datasets provide human annotations for clarity and evasion, the impact of prompt design on automatic clarity evaluation remains underexplored. In this paper, we study prompt-based clarity evaluation using the CLARITY dataset from the SemEval 2026 shared task. We compare a GPT-3.5 baseline provided with the dataset against GPT-5.2 evaluated under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. Model predictions are evaluated against human annotations using accuracy and class-wise metrics for clarity and evasion, along with hierarchical exact match. Results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56 percent to 63 percent under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy at 34 percent, though improvements are less stable across fine-grained evasion categories. We further evaluate topic identification and find that reasoning-based prompting improves accuracy from 60 percent to 74 percent relative to human annotations. Overall, our findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.",
      "publishedDate": "2026-01-13T03:10:58Z",
      "updatedDate": "2026-01-13T03:10:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08176v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08176",
      "comment": "6 pages, 6 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08169",
      "title": "Relational Knowledge Distillation Using Fine-tuned Function Vectors",
      "authors": [
        {
          "name": "Andrea Kang",
          "affiliation": null
        },
        {
          "name": "Yingnian Wu",
          "affiliation": null
        },
        {
          "name": "Hongjing Lu",
          "affiliation": null
        }
      ],
      "abstract": "Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known as the function vector. We show that fine-tuning function vectors with only a small set of examples (about 20 word pairs) yields better performance on relation-based word-completion tasks than using the original vectors derived from causal mediation analysis. These improvements hold for both small and large language models. Moreover, the fine-tuned function vectors yield improved decoding performance for relation words and show stronger alignment with human similarity judgments of semantic relations. Next, we introduce the composite function vector - a weighted combination of fine-tuned function vectors - to extract relational knowledge and support analogical reasoning. At inference time, inserting this composite vector into LLM activations markedly enhances performance on challenging analogy problems drawn from cognitive science and SAT benchmarks. Our results highlight the potential of activation patching as a controllable mechanism for encoding and manipulating relational knowledge, advancing both the interpretability and reasoning capabilities of large language models.",
      "publishedDate": "2026-01-13T03:02:18Z",
      "updatedDate": "2026-01-13T03:02:18Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08169v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08169",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "code-generation",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08158",
      "title": "WISE-Flow: Workflow-Induced Structured Experience for Self-Evolving Conversational Service Agents",
      "authors": [
        {
          "name": "Yuqing Zhou",
          "affiliation": null
        },
        {
          "name": "Zhuoer Wang",
          "affiliation": null
        },
        {
          "name": "Jie Yuan",
          "affiliation": null
        },
        {
          "name": "Hong Wang",
          "affiliation": null
        },
        {
          "name": "Samson Koelle",
          "affiliation": null
        },
        {
          "name": "Ziwei Zhu",
          "affiliation": null
        },
        {
          "name": "Wei Niu",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM)-based agents are widely deployed in user-facing services but remain error-prone in new tasks, tend to repeat the same failure patterns, and show substantial run-to-run variability. Fixing failures via environment-specific training or manual patching is costly and hard to scale. To enable self-evolving agents in user-facing service environments, we propose WISE-Flow, a workflow-centric framework that converts historical service interactions into reusable procedural experience by inducing workflows with prerequisite-augmented action blocks. At deployment, WISE-Flow aligns the agent's execution trajectory to retrieved workflows and performs prerequisite-aware feasibility reasoning to achieve state-grounded next actions. Experiments on ToolSandbox and $τ^2$-bench show consistent improvement across base models.",
      "publishedDate": "2026-01-13T02:43:41Z",
      "updatedDate": "2026-01-13T02:43:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08158v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08158",
      "comment": "19 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08151",
      "title": "Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention",
      "authors": [
        {
          "name": "Shezheng Song",
          "affiliation": null
        },
        {
          "name": "Shasha Li",
          "affiliation": null
        },
        {
          "name": "Jie Yu",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage \"review\" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.",
      "publishedDate": "2026-01-13T02:26:21Z",
      "updatedDate": "2026-01-13T02:26:21Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.MM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08151v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08151",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08141",
      "title": "Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training",
      "authors": [
        {
          "name": "Muhammad Taimoor Hassan",
          "affiliation": null
        },
        {
          "name": "Jawad Ahmed",
          "affiliation": null
        },
        {
          "name": "Muhammad Awais",
          "affiliation": null
        }
      ],
      "abstract": "Despite remarkable progress in large language models, Urdu-a language spoken by over 230 million people-remains critically underrepresented in modern NLP systems. Existing multilingual models demonstrate poor performance on Urdu-specific tasks, struggling with the language's complex morphology, right-to-left Nastaliq script, and rich literary traditions. Even the base LLaMA-3.1 8B-Instruct model shows limited capability in generating fluent, contextually appropriate Urdu text. We introduce Qalb, an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Starting from LLaMA 3.1 8B, we perform continued pre-training on a dataset of 1.97 billion tokens. This corpus comprises 1.84 billion tokens of diverse Urdu text-spanning news archives, classical and contemporary literature, government documents, and social media-combined with 140 million tokens of English Wikipedia data to prevent catastrophic forgetting. We then fine-tune the resulting model on the Alif Urdu-instruct dataset. Through extensive evaluation on Urdu-specific benchmarks, Qalb demonstrates substantial improvements, achieving a weighted average score of 90.34 and outperforming the previous state-of-the-art Alif-1.0-Instruct model (87.1) by 3.24 points, while also surpassing the base LLaMA-3.1 8B-Instruct model by 44.64 points. Qalb achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning. Our results demonstrate that continued pre-training on diverse, high-quality language data, combined with targeted instruction fine-tuning, effectively adapts foundation models to low-resource languages.",
      "publishedDate": "2026-01-13T02:05:05Z",
      "updatedDate": "2026-01-13T02:05:05Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08141v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08141",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08118",
      "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
      "authors": [
        {
          "name": "Ashutosh Hathidara",
          "affiliation": null
        },
        {
          "name": "Julien Yu",
          "affiliation": null
        },
        {
          "name": "Vaishali Senthil",
          "affiliation": null
        },
        {
          "name": "Sebastian Schreiber",
          "affiliation": null
        },
        {
          "name": "Anil Babu Ankisettipalli",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive \"act-as-a-user\" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench.",
      "publishedDate": "2026-01-13T01:16:13Z",
      "updatedDate": "2026-01-13T01:16:13Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08118v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08118",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "prompting",
        "agents"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08108",
      "title": "Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought",
      "authors": [
        {
          "name": "Bowen Li",
          "affiliation": null
        },
        {
          "name": "Ziqi Xu",
          "affiliation": null
        },
        {
          "name": "Jing Ren",
          "affiliation": null
        },
        {
          "name": "Renqiang Luo",
          "affiliation": null
        },
        {
          "name": "Xikun Zhang",
          "affiliation": null
        },
        {
          "name": "Xiuzhen Zhang",
          "affiliation": null
        },
        {
          "name": "Yongli Ren",
          "affiliation": null
        },
        {
          "name": "Feng Xia",
          "affiliation": null
        }
      ],
      "abstract": "Despite notable advancements in prompting methods for Large Language Models (LLMs), such as Chain-of-Thought (CoT), existing strategies still suffer from excessive token usage and limited generalisability across diverse reasoning tasks. To address these limitations, we propose an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework, which leverages structural causal models to infer the causal effect of a query on its answer and adaptively select an appropriate intervention (i.e., standard front-door and conditional front-door adjustments). This design enables generalisable causal reasoning across heterogeneous tasks without task-specific retraining. By replacing verbose CoT with concise Sketch-of-Thought, ACPS enables efficient reasoning that significantly reduces token usage and inference cost. Extensive experiments on multiple reasoning benchmarks and LLMs demonstrate that ACPS consistently outperforms existing prompting baselines in terms of accuracy, robustness, and computational efficiency.",
      "publishedDate": "2026-01-13T00:58:43Z",
      "updatedDate": "2026-01-13T00:58:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08108v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08108",
      "comment": "Accepted by Findings of EACL 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08079",
      "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
      "authors": [
        {
          "name": "Hongjin Qian",
          "affiliation": null
        },
        {
          "name": "Zhao Cao",
          "affiliation": null
        },
        {
          "name": "Zheng Liu",
          "affiliation": null
        }
      ],
      "abstract": "Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons. We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation. We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.",
      "publishedDate": "2026-01-12T23:44:59Z",
      "updatedDate": "2026-01-12T23:44:59Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08079v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08079",
      "comment": "Our codes are in https://github.com/qhjqhj00/MemoBrain",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08058",
      "title": "Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models",
      "authors": [
        {
          "name": "Zhenghao He",
          "affiliation": null
        },
        {
          "name": "Guangzhi Xiong",
          "affiliation": null
        },
        {
          "name": "Bohan Liu",
          "affiliation": null
        },
        {
          "name": "Sanchit Sinha",
          "affiliation": null
        },
        {
          "name": "Aidong Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple model families and reasoning benchmarks, we find that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting. For large models, latent steering achieves performance comparable to standard CoT prompting while producing more efficient outputs. We further observe that this reasoning-oriented internal state is triggered early in generation and can override prompt-level instructions that discourage explicit reasoning. Overall, our results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, while CoT prompting is one effective, but not unique, way of activating this mechanism rather than its necessary cause.",
      "publishedDate": "2026-01-12T23:01:21Z",
      "updatedDate": "2026-01-12T23:01:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08058v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08058",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08045",
      "title": "Cognitive Biases in LLM-Assisted Software Development",
      "authors": [
        {
          "name": "Xinyi Zhou",
          "affiliation": null
        },
        {
          "name": "Zeinadsadat Saghi",
          "affiliation": null
        },
        {
          "name": "Sadra Sabouri",
          "affiliation": null
        },
        {
          "name": "Rahul Pandita",
          "affiliation": null
        },
        {
          "name": "Mollie McGuire",
          "affiliation": null
        },
        {
          "name": "Souti Chattopadhyay",
          "affiliation": null
        }
      ],
      "abstract": "The widespread adoption of Large Language Models (LLMs) in software development is transforming programming from a solution-generative to a solution-evaluative activity. This shift opens a pathway for new cognitive challenges that amplify existing decision-making biases or create entirely novel ones. One such type of challenge stems from cognitive biases, which are thinking patterns that lead people away from logical reasoning and result in sub-optimal decisions. How do cognitive biases manifest and impact decision-making in emerging AI-collaborative development? This paper presents the first comprehensive study of cognitive biases in LLM-assisted development. We employ a mixed-methods approach, combining observational studies with 14 student and professional developers, followed by surveys with 22 additional developers. We qualitatively compare categories of biases affecting developers against the traditional non-LLM workflows. Our findings suggest that LLM-related actions are more likely to be associated with novel biases. Through a systematic analysis of 90 cognitive biases specific to developer-LLM interactions, we develop a taxonomy of 15 bias categories validated by cognitive psychologists. We found that 48.8% of total programmer actions are biased, and developer-LLM interactions account for 56.4% of these biased actions. We discuss how these bias categories manifest, present tools and practices for developers, and recommendations for LLM tool builders to help mitigate cognitive biases in human-AI programming.",
      "publishedDate": "2026-01-12T22:32:21Z",
      "updatedDate": "2026-01-12T22:32:21Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08045v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08045",
      "comment": "13 pages, 6 figures, 7 tables",
      "journalRef": null,
      "doi": "10.1145/3744916.3773104",
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08010",
      "title": "CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation",
      "authors": [
        {
          "name": "Chaoyu Li",
          "affiliation": null
        },
        {
          "name": "Deeparghya Dutta Barua",
          "affiliation": null
        },
        {
          "name": "Fei Tao",
          "affiliation": null
        },
        {
          "name": "Pooyan Fazli",
          "affiliation": null
        }
      ],
      "abstract": "Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.",
      "publishedDate": "2026-01-12T21:24:45Z",
      "updatedDate": "2026-01-12T21:24:45Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08010v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08010",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08003",
      "title": "LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback",
      "authors": [
        {
          "name": "Weiyue Li",
          "affiliation": null
        },
        {
          "name": "Mingxiao Song",
          "affiliation": null
        },
        {
          "name": "Zhenda Shen",
          "affiliation": null
        },
        {
          "name": "Dachuan Zhao",
          "affiliation": null
        },
        {
          "name": "Yunfan Long",
          "affiliation": null
        },
        {
          "name": "Yi Li",
          "affiliation": null
        },
        {
          "name": "Yongce Li",
          "affiliation": null
        },
        {
          "name": "Ruyi Yang",
          "affiliation": null
        },
        {
          "name": "Mengyu Wang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) often struggle with creative generation, and multi-agent frameworks that improve reasoning through interaction can paradoxically hinder creativity by inducing content homogenization. We introduce LLM Review, a peer-review-inspired framework implementing Blind Peer Review: agents exchange targeted feedback while revising independently, preserving divergent creative trajectories. To enable rigorous evaluation, we propose SciFi-100, a science fiction writing dataset with a unified framework combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments demonstrate that LLM Review consistently outperforms multi-agent baselines, and smaller models with our framework can surpass larger single-agent models, suggesting interaction structure may substitute for model scale.",
      "publishedDate": "2026-01-12T21:10:28Z",
      "updatedDate": "2026-01-12T21:10:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08003v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08003",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08000",
      "title": "Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety",
      "authors": [
        {
          "name": "Can Jin",
          "affiliation": null
        },
        {
          "name": "Rui Wu",
          "affiliation": null
        },
        {
          "name": "Tong Che",
          "affiliation": null
        },
        {
          "name": "Qixin Zhang",
          "affiliation": null
        },
        {
          "name": "Hongwu Peng",
          "affiliation": null
        },
        {
          "name": "Jiahui Zhao",
          "affiliation": null
        },
        {
          "name": "Zhenting Wang",
          "affiliation": null
        },
        {
          "name": "Wenqi Wei",
          "affiliation": null
        },
        {
          "name": "Ligong Han",
          "affiliation": null
        },
        {
          "name": "Zhao Zhang",
          "affiliation": null
        },
        {
          "name": "Yuan Cao",
          "affiliation": null
        },
        {
          "name": "Ruixiang Tang",
          "affiliation": null
        },
        {
          "name": "Dimitris N. Metaxas",
          "affiliation": null
        }
      ],
      "abstract": "Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.",
      "publishedDate": "2026-01-12T21:08:46Z",
      "updatedDate": "2026-01-12T21:08:46Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08000v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08000",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07986",
      "title": "VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding",
      "authors": [
        {
          "name": "Haorui Yu",
          "affiliation": null
        },
        {
          "name": "Ramon Ruiz-Dolz",
          "affiliation": null
        },
        {
          "name": "Diji Yang",
          "affiliation": null
        },
        {
          "name": "Hang He",
          "affiliation": null
        },
        {
          "name": "Fengrui Zhang",
          "affiliation": null
        },
        {
          "name": "Qiufeng Yi",
          "affiliation": null
        }
      ],
      "abstract": "We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.",
      "publishedDate": "2026-01-12T20:36:30Z",
      "updatedDate": "2026-01-12T20:36:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07986v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07986",
      "comment": "8 pages, 4 figures, submitted to ACL 2026 Dataset Track",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07935",
      "title": "Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation",
      "authors": [
        {
          "name": "Yuxin Yang",
          "affiliation": null
        },
        {
          "name": "Aoxiong Zeng",
          "affiliation": null
        },
        {
          "name": "Xiangquan Yang",
          "affiliation": null
        }
      ],
      "abstract": "The rapid evolution of Large Language Models (LLMs) has shifted focus from general-purpose capabilities to domain-specific expertise. However, adapting LLMs to specialized fields such as medicine presents two challenge: (1) the \"Stability-Plasticity Dilemma\", where the model must acquire complex clinical knowledge without suffering from catastrophic forgetting of general world knowledge; and (2) \"Task Interference\", where disparate sub-tasks, such as medical diagnosis, report summarization, and drug-drug interaction prediction, compete for limited low-rank parameter space. In this paper, we propose Med-MoE-LoRA, a novel framework that integrates Mixture-of-Experts (MoE) with Low-Rank Adaptation (LoRA) to enable efficient multi-task domain adaptation, especially for medical scenarios. Drawing inspiration from recent advances, our framework employs an asymmetric expert distribution where deeper layers are equipped with a higher density of LoRA experts to capture complex semantic abstractions. We further introduce a \"Knowledge-Preservation Plugin\", inspired by LoRA MoE, to isolate and protect general-purpose reasoning. By utilizing soft merging with adaptive routing and rank-wise decoupling, Med-MoE-LoRA achieves superior performance in medical benchmarks while reducing interference. Experimental results demonstrate that our approach consistently outperforms standard LoRA and conventional MoE architectures across multiple clinical NLP tasks while retaining the model's general cognitive capabilities.",
      "publishedDate": "2026-01-12T19:04:58Z",
      "updatedDate": "2026-01-12T19:04:58Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07935v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07935",
      "comment": "Work in Progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07898",
      "title": "Large Language Models and Algorithm Execution: Application to an Arithmetic Function",
      "authors": [
        {
          "name": "Farah Ben Slama",
          "affiliation": null
        },
        {
          "name": "Frédéric Armetta",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have recently developed new advanced functionalities. Their effectiveness relies on statistical learning and generalization capabilities. However, they face limitations in internalizing the data they process and struggle, for instance, to autonomously execute algorithms. In this paper, we investigate the possibility of extending these models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. We introduce a training model called LLM-DAL (Large Language Model - Decompositional Algorithmic Learning), through which we demonstrate that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.",
      "publishedDate": "2026-01-12T12:27:59Z",
      "updatedDate": "2026-01-12T12:27:59Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07898v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07898",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07891",
      "title": "KVzap: Fast, Adaptive, and Faithful KV Cache Pruning",
      "authors": [
        {
          "name": "Simon Jegou",
          "affiliation": null
        },
        {
          "name": "Maximilian Jeblick",
          "affiliation": null
        }
      ],
      "abstract": "Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. While many KV cache pruning methods have been proposed, they have not yet been adopted in major inference engines due to speed--accuracy trade-offs. We introduce KVzap, a fast, input-adaptive approximation of KVzip that works in both prefilling and decoding. On Qwen3-8B, Llama-3.1-8B-Instruct, and Qwen3-32B across long-context and reasoning tasks, KVzap achieves $2$--$4\\times$ KV cache compression with negligible accuracy loss and achieves state-of-the-art performance on the KVpress leaderboard. Code and models are available at https://github.com/NVIDIA/kvpress.",
      "publishedDate": "2026-01-12T08:27:47Z",
      "updatedDate": "2026-01-12T08:27:47Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07891v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07891",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07877",
      "title": "E^2-LLM: Bridging Neural Signals and Interpretable Affective Analysis",
      "authors": [
        {
          "name": "Fei Ma",
          "affiliation": null
        },
        {
          "name": "Han Lin",
          "affiliation": null
        },
        {
          "name": "Yifan Xie",
          "affiliation": null
        },
        {
          "name": "Hongwei Ren",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Shen",
          "affiliation": null
        },
        {
          "name": "Wenbo Ding",
          "affiliation": null
        },
        {
          "name": "Qi Tian",
          "affiliation": null
        }
      ],
      "abstract": "Emotion recognition from electroencephalography (EEG) signals remains challenging due to high inter-subject variability, limited labeled data, and the lack of interpretable reasoning in existing approaches. While recent multimodal large language models (MLLMs) have advanced emotion analysis, they have not been adapted to handle the unique spatiotemporal characteristics of neural signals. We present E^2-LLM (EEG-to-Emotion Large Language Model), the first MLLM framework for interpretable emotion analysis from EEG. E^2-LLM integrates a pretrained EEG encoder with Qwen-based LLMs through learnable projection layers, employing a multi-stage training pipeline that encompasses emotion-discriminative pretraining, cross-modal alignment, and instruction tuning with chain-of-thought reasoning. We design a comprehensive evaluation protocol covering basic emotion prediction, multi-task reasoning, and zero-shot scenario understanding. Experiments on the dataset across seven emotion categories demonstrate that E^2-LLM achieves excellent performance on emotion classification, with larger variants showing enhanced reliability and superior zero-shot generalization to complex reasoning scenarios. Our work establishes a new paradigm combining physiological signals with LLM reasoning capabilities, showing that model scaling improves both recognition accuracy and interpretable emotional understanding in affective computing.",
      "publishedDate": "2026-01-11T13:21:20Z",
      "updatedDate": "2026-01-11T13:21:20Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07877v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07877",
      "comment": "11 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08626",
      "title": "How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction",
      "authors": [
        {
          "name": "Yingjie He",
          "affiliation": null
        },
        {
          "name": "Zhaolu Kang",
          "affiliation": null
        },
        {
          "name": "Kehan Jiang",
          "affiliation": null
        },
        {
          "name": "Qianyuan Zhang",
          "affiliation": null
        },
        {
          "name": "Jiachen Qian",
          "affiliation": null
        },
        {
          "name": "Chunlei Meng",
          "affiliation": null
        },
        {
          "name": "Yujie Feng",
          "affiliation": null
        },
        {
          "name": "Yuan Wang",
          "affiliation": null
        },
        {
          "name": "Jiabao Dou",
          "affiliation": null
        },
        {
          "name": "Aming Wu",
          "affiliation": null
        },
        {
          "name": "Leqi Zheng",
          "affiliation": null
        },
        {
          "name": "Pengxiang Zhao",
          "affiliation": null
        },
        {
          "name": "Jiaxin Liu",
          "affiliation": null
        },
        {
          "name": "Zeyu Zhang",
          "affiliation": null
        },
        {
          "name": "Lei Wang",
          "affiliation": null
        },
        {
          "name": "Guansu Wang",
          "affiliation": null
        },
        {
          "name": "Qishi Zhan",
          "affiliation": null
        },
        {
          "name": "Xiaomin He",
          "affiliation": null
        },
        {
          "name": "Meisheng Zhang",
          "affiliation": null
        },
        {
          "name": "Jianyuan Ni",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) excel at semantic understanding, yet their ability to reconstruct internal structure from scrambled inputs remains underexplored. Sentence-level restoration is ill-posed for automated evaluation because multiple valid word orders often exist. We introduce OrderProbe, a deterministic benchmark for structural reconstruction using fixed four-character expressions in Chinese, Japanese, and Korean, which have a unique canonical order and thus support exact-match scoring. We further propose a diagnostic framework that evaluates models beyond recovery accuracy, including semantic fidelity, logical validity, consistency, robustness sensitivity, and information density. Experiments on twelve widely used LLMs show that structural reconstruction remains difficult even for frontier systems: zero-shot recovery frequently falls below 35%. We also observe a consistent dissociation between semantic recall and structural planning, suggesting that structural robustness is not an automatic byproduct of semantic competence.",
      "publishedDate": "2026-01-13T15:03:38Z",
      "updatedDate": "2026-01-13T15:03:38Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08626v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08626",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "planning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "planning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08405",
      "title": "Large Language Models to Enhance Multi-task Drone Operations in Simulated Environments",
      "authors": [
        {
          "name": "Yizhan Feng",
          "affiliation": null
        },
        {
          "name": "Hichem Snoussi",
          "affiliation": null
        },
        {
          "name": "Jing Teng",
          "affiliation": null
        },
        {
          "name": "Abel Cherouat",
          "affiliation": null
        },
        {
          "name": "Tian Wang",
          "affiliation": null
        }
      ],
      "abstract": "Benefiting from the rapid advancements in large language models (LLMs), human-drone interaction has reached unprecedented opportunities. In this paper, we propose a method that integrates a fine-tuned CodeT5 model with the Unreal Engine-based AirSim drone simulator to efficiently execute multi-task operations using natural language commands. This approach enables users to interact with simulated drones through prompts or command descriptions, allowing them to easily access and control the drone's status, significantly lowering the operational threshold. In the AirSim simulator, we can flexibly construct visually realistic dynamic environments to simulate drone applications in complex scenarios. By combining a large dataset of (natural language, program code) command-execution pairs generated by ChatGPT with developer-written drone code as training data, we fine-tune the CodeT5 to achieve automated translation from natural language to executable code for drone tasks. Experimental results demonstrate that the proposed method exhibits superior task execution efficiency and command understanding capabilities in simulated environments. In the future, we plan to extend the model functionality in a modular manner, enhancing its adaptability to complex scenarios and driving the application of drone technologies in real-world environments.",
      "publishedDate": "2026-01-13T10:21:17Z",
      "updatedDate": "2026-01-13T10:21:17Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08405v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08405",
      "comment": "1st International Conference on Drones and Unmanned Systems (DAUS' 2025)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "tool-use",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07853",
      "title": "FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments",
      "authors": [
        {
          "name": "Zhi Yang",
          "affiliation": null
        },
        {
          "name": "Runguo Li",
          "affiliation": null
        },
        {
          "name": "Qiqi Qiang",
          "affiliation": null
        },
        {
          "name": "Jiashun Wang",
          "affiliation": null
        },
        {
          "name": "Fangqi Lou",
          "affiliation": null
        },
        {
          "name": "Mengping Li",
          "affiliation": null
        },
        {
          "name": "Dongpo Cheng",
          "affiliation": null
        },
        {
          "name": "Rui Xu",
          "affiliation": null
        },
        {
          "name": "Heng Lian",
          "affiliation": null
        },
        {
          "name": "Shuo Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaolong Liang",
          "affiliation": null
        },
        {
          "name": "Xiaoming Huang",
          "affiliation": null
        },
        {
          "name": "Zheng Wei",
          "affiliation": null
        },
        {
          "name": "Zhaowei Liu",
          "affiliation": null
        },
        {
          "name": "Xin Guo",
          "affiliation": null
        },
        {
          "name": "Huacan Wang",
          "affiliation": null
        },
        {
          "name": "Ronghao Chen",
          "affiliation": null
        },
        {
          "name": "Liwen Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.",
      "publishedDate": "2026-01-09T03:25:45Z",
      "updatedDate": "2026-01-09T03:25:45Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07853v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07853",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "agents",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08676",
      "title": "Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance",
      "authors": [
        {
          "name": "Yilei Zhao",
          "affiliation": null
        },
        {
          "name": "Wentao Zhang",
          "affiliation": null
        },
        {
          "name": "Xiao Lei",
          "affiliation": null
        },
        {
          "name": "Yandan Zheng",
          "affiliation": null
        },
        {
          "name": "Mengpu Liu",
          "affiliation": null
        },
        {
          "name": "Wei Yang Bryan Lim",
          "affiliation": null
        }
      ],
      "abstract": "Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.",
      "publishedDate": "2026-01-13T15:58:29Z",
      "updatedDate": "2026-01-13T15:58:29Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08676v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08676",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "evaluation",
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08237",
      "title": "The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination",
      "authors": [
        {
          "name": "Haoran Su",
          "affiliation": null
        },
        {
          "name": "Yandong Sun",
          "affiliation": null
        },
        {
          "name": "Congjia Yu",
          "affiliation": null
        }
      ],
      "abstract": "Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.",
      "publishedDate": "2026-01-13T05:47:18Z",
      "updatedDate": "2026-01-13T05:47:18Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08237v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08237",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "multi-agent",
        "agents"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07978",
      "title": "Cost and accuracy of long-term graph memory in distributed LLM-based multi-agent systems",
      "authors": [
        {
          "name": "Benedict Wolff",
          "affiliation": null
        },
        {
          "name": "Jacopo Bennati",
          "affiliation": null
        }
      ],
      "abstract": "Distributed multi-agent systems use large language models to enable collaborative intelligence while preserving privacy, yet systematic evaluations of long-term memory under network constraints remain limited. This study presents a flexible testbed comparing mem0, a vector-based memory framework, and Graphiti, a graph-based knowledge graph, using the LOCOMO long-context benchmark. Experiments were conducted under unconstrained and constrained network conditions, measuring computational, financial, and accuracy metrics. Results indicate that mem0 significantly outperforms Graphiti in efficiency, with faster loading times, lower resource consumption, and minimal network overhead, while accuracy differences are not statistically significant. Applying a statistical pareto efficiency framework, mem0 is identified as the optimal choice that balances cost and accuracy in DMAS.",
      "publishedDate": "2026-01-12T20:20:35Z",
      "updatedDate": "2026-01-12T20:20:35Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07978v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07978",
      "comment": "23 pages, 4 figures, 7 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08773",
      "title": "Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs",
      "authors": [
        {
          "name": "Manideep Reddy Chinthareddy",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal. Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.",
      "publishedDate": "2026-01-13T18:03:41Z",
      "updatedDate": "2026-01-13T18:03:41Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08773v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08773",
      "comment": "46 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08747",
      "title": "To Retrieve or To Think? An Agentic Approach for Context Evolution",
      "authors": [
        {
          "name": "Rubing Chen",
          "affiliation": null
        },
        {
          "name": "Jian Wang",
          "affiliation": null
        },
        {
          "name": "Wenjie Li",
          "affiliation": null
        },
        {
          "name": "Xiao-Yong Wei",
          "affiliation": null
        },
        {
          "name": "Qing Li",
          "affiliation": null
        }
      ],
      "abstract": "Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.",
      "publishedDate": "2026-01-13T17:25:57Z",
      "updatedDate": "2026-01-13T17:25:57Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08747v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08747",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08706",
      "title": "\"Where is My Troubleshooting Procedure?\": Studying the Potential of RAG in Assisting Failure Resolution of Large Cyber-Physical System",
      "authors": [
        {
          "name": "Maria Teresa Rossi",
          "affiliation": null
        },
        {
          "name": "Leonardo Mariani",
          "affiliation": null
        },
        {
          "name": "Oliviero Riganelli",
          "affiliation": null
        },
        {
          "name": "Giuseppe Filomento",
          "affiliation": null
        },
        {
          "name": "Danilo Giannone",
          "affiliation": null
        },
        {
          "name": "Paolo Gavazzo",
          "affiliation": null
        }
      ],
      "abstract": "In today's complex industrial environments, operators must often navigate through extensive technical manuals to identify troubleshooting procedures that may help react to some observed failure symptoms. These manuals, written in natural language, describe many steps in detail. Unfortunately, the number, magnitude, and articulation of these descriptions can significantly slow down and complicate the retrieval of the correct procedure during critical incidents. Interestingly, Retrieval Augmented Generation (RAG) enables the development of tools based on conversational interfaces that can assist operators in their retrieval tasks, improving their capability to respond to incidents. This paper presents the results of a set of experiments that derive from the analysis of the troubleshooting procedures available in Fincantieri, a large international company developing complex naval cyber-physical systems. Results show that RAG can assist operators in reacting promptly to failure symptoms, although specific measures have to be taken into consideration to cross-validate recommendations before actuating them.",
      "publishedDate": "2026-01-13T16:34:43Z",
      "updatedDate": "2026-01-13T16:34:43Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08706v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08706",
      "comment": "This paper has been accepted at the Software Engineering in Practice track of the 48th International Conference on Software Engineering (ICSE 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08670",
      "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
      "authors": [
        {
          "name": "Giulio Corallo",
          "affiliation": null
        },
        {
          "name": "Paolo Papotti",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.",
      "publishedDate": "2026-01-13T15:46:59Z",
      "updatedDate": "2026-01-13T15:46:59Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08670v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08670",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08620",
      "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
      "authors": [
        {
          "name": "António Loison",
          "affiliation": null
        },
        {
          "name": "Quentin Macé",
          "affiliation": null
        },
        {
          "name": "Antoine Edy",
          "affiliation": null
        },
        {
          "name": "Victor Xing",
          "affiliation": null
        },
        {
          "name": "Tom Balough",
          "affiliation": null
        },
        {
          "name": "Gabriel Moreira",
          "affiliation": null
        },
        {
          "name": "Bo Liu",
          "affiliation": null
        },
        {
          "name": "Manuel Faysse",
          "affiliation": null
        },
        {
          "name": "Céline Hudelot",
          "affiliation": null
        },
        {
          "name": "Gautier Viaud",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.",
      "publishedDate": "2026-01-13T15:00:33Z",
      "updatedDate": "2026-01-13T15:00:33Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08620v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08620",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08559",
      "title": "WaterCopilot: An AI-Driven Virtual Assistant for Water Management",
      "authors": [
        {
          "name": "Keerththanan Vickneswaran",
          "affiliation": null
        },
        {
          "name": "Mariangel Garcia Andarcia",
          "affiliation": null
        },
        {
          "name": "Hugo Retief",
          "affiliation": null
        },
        {
          "name": "Chris Dickens",
          "affiliation": null
        },
        {
          "name": "Paulo Silva",
          "affiliation": null
        }
      ],
      "abstract": "Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.",
      "publishedDate": "2026-01-13T13:44:00Z",
      "updatedDate": "2026-01-13T13:44:00Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08559v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08559",
      "comment": "15 pages, 12 figures. This work was developed in collaboration between the International Water Management Institute (IWMI) and Microsoft Research. The supplementary user guide for WaterCopilot is available via this https://cgspace.cgiar.org/server/api/core/bitstreams/a5818d12-629d-4f70-a3b0-ee6c25e82cd7/content",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "tool-use",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08554",
      "title": "Efficient Maintenance of Leiden Communities in Large Dynamic Graphs",
      "authors": [
        {
          "name": "Chunxu Lin",
          "affiliation": null
        },
        {
          "name": "Yumao Xie",
          "affiliation": null
        },
        {
          "name": "Yixiang Fang",
          "affiliation": null
        },
        {
          "name": "Yongmin Hu",
          "affiliation": null
        },
        {
          "name": "Yingqian Hu",
          "affiliation": null
        },
        {
          "name": "Chen Cheng",
          "affiliation": null
        }
      ],
      "abstract": "As a well-known community detection algorithm, Leiden has been widely used in various scenarios such as large language model generation (e.g., Graph-RAG), anomaly detection, and biological analysis. In these scenarios, the graphs are often large and dynamic, where vertices and edges are inserted and deleted frequently, so it is costly to obtain the updated communities by Leiden from scratch when the graph has changed. Recently, one work has attempted to study how to maintain Leiden communities in the dynamic graph, but it lacks a detailed theoretical analysis, and its algorithms are inefficient for large graphs. To address these issues, in this paper, we first theoretically show that the existing algorithms are relatively unbounded via the boundedness analysis (a powerful tool for analyzing incremental algorithms on dynamic graphs), and also analyze the memberships of vertices in communities when the graph changes. Based on theoretical analysis, we develop a novel efficient maintenance algorithm, called Hierarchical Incremental Tree Leiden (HIT-Leiden), which effectively reduces the range of affected vertices by maintaining the connected components and hierarchical community structures. Comprehensive experiments in various datasets demonstrate the superior performance of HIT-Leiden. In particular, it achieves speedups of up to five orders of magnitude over existing methods.",
      "publishedDate": "2026-01-13T13:39:22Z",
      "updatedDate": "2026-01-13T13:39:22Z",
      "primaryCategory": "cs.SI",
      "arxivCategories": [
        "cs.SI",
        "cs.DB",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08554v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08554",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08311",
      "title": "Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Kang Fu",
          "affiliation": null
        },
        {
          "name": "Huiyu Duan",
          "affiliation": null
        },
        {
          "name": "Zicheng Zhang",
          "affiliation": null
        },
        {
          "name": "Yucheng Zhu",
          "affiliation": null
        },
        {
          "name": "Jun Zhao",
          "affiliation": null
        },
        {
          "name": "Xiongkuo Min",
          "affiliation": null
        },
        {
          "name": "Jia Wang",
          "affiliation": null
        },
        {
          "name": "Guangtao Zhai",
          "affiliation": null
        }
      ],
      "abstract": "Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.",
      "publishedDate": "2026-01-13T08:00:02Z",
      "updatedDate": "2026-01-13T08:00:02Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08311v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08311",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08288",
      "title": "OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System",
      "authors": [
        {
          "name": "Yuyang Wu",
          "affiliation": null
        },
        {
          "name": "Hanzhong Cao",
          "affiliation": null
        },
        {
          "name": "Jianhao Chen",
          "affiliation": null
        },
        {
          "name": "Yufei Li",
          "affiliation": null
        }
      ],
      "abstract": "Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.",
      "publishedDate": "2026-01-13T07:26:23Z",
      "updatedDate": "2026-01-13T07:26:23Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08288v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08288",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "reasoning",
        "rag",
        "agents",
        "planning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "agents",
          "planning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08226",
      "title": "Knowledge-based learning in Text-RAG and Image-RAG",
      "authors": [
        {
          "name": "Alexander Shim",
          "affiliation": null
        },
        {
          "name": "Khalil Saieh",
          "affiliation": null
        },
        {
          "name": "Samuel Clarke",
          "affiliation": null
        }
      ],
      "abstract": "This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con\"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.",
      "publishedDate": "2026-01-13T05:14:18Z",
      "updatedDate": "2026-01-13T05:14:18Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08226v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08226",
      "comment": "9 pages, 10 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08209",
      "title": "Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models",
      "authors": [
        {
          "name": "Rongji Li",
          "affiliation": null
        },
        {
          "name": "Jian Xu",
          "affiliation": null
        },
        {
          "name": "Xueqing Chen",
          "affiliation": null
        },
        {
          "name": "Yisheng Yang",
          "affiliation": null
        },
        {
          "name": "Jiayi Wang",
          "affiliation": null
        },
        {
          "name": "Xingyu Chen",
          "affiliation": null
        },
        {
          "name": "Chunyu Xie",
          "affiliation": null
        },
        {
          "name": "Dawei Leng",
          "affiliation": null
        },
        {
          "name": "Xu-Yao Zhang",
          "affiliation": null
        }
      ],
      "abstract": "In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.",
      "publishedDate": "2026-01-13T04:23:36Z",
      "updatedDate": "2026-01-13T04:23:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08209v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08209",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "evaluation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08109",
      "title": "CSQL: Mapping Documents into Causal Databases",
      "authors": [
        {
          "name": "Sridhar Mahadevan",
          "affiliation": null
        }
      ],
      "abstract": "We describe a novel system, CSQL, which automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). A CDB differs from a traditional DB: it is designed to answer \"why'' questions via causal interventions and structured causal queries. CSQL builds on our earlier system, DEMOCRITUS, which converts documents into thousands of local causal models derived from causal discourse. Unlike RAG-based systems or knowledge-graph based approaches, CSQL supports causal analysis over document collections rather than purely associative retrieval. For example, given an article on the origins of human bipedal walking, CSQL enables queries such as: \"What are the strongest causal influences on bipedalism?'' or \"Which variables act as causal hubs with the largest downstream influence?'' Beyond single-document case studies, we show that CSQL can also ingest RAG/IE-compiled causal corpora at scale by compiling the Testing Causal Claims (TCC) dataset of economics papers into a causal database containing 265,656 claim instances spanning 45,319 papers, 44 years, and 1,575 reported method strings, thereby enabling corpus-level causal queries and longitudinal analyses in CSQL. Viewed abstractly, CSQL functions as a compiler from unstructured documents into a causal database equipped with a principled algebra of queries, and can be applied broadly across many domains ranging from business, humanities, and science.",
      "publishedDate": "2026-01-13T01:00:38Z",
      "updatedDate": "2026-01-13T01:00:38Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08109v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08109",
      "comment": "26 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08105",
      "title": "Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning",
      "authors": [
        {
          "name": "Fabian Spaeh",
          "affiliation": null
        },
        {
          "name": "Tianyi Chen",
          "affiliation": null
        },
        {
          "name": "Chen-Hao Chiang",
          "affiliation": null
        },
        {
          "name": "Bin Shen",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction. In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.",
      "publishedDate": "2026-01-13T00:56:38Z",
      "updatedDate": "2026-01-13T00:56:38Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08105v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08105",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "prompting",
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07861",
      "title": "EmbeddingRWKV: State-Centric Retrieval with Reusable States",
      "authors": [
        {
          "name": "Haowen Hou",
          "affiliation": null
        },
        {
          "name": "Jie Yang",
          "affiliation": null
        }
      ],
      "abstract": "Current Retrieval-Augmented Generation (RAG) systems typically employ a traditional two-stage pipeline: an embedding model for initial retrieval followed by a reranker for refinement. However, this paradigm suffers from significant inefficiency due to the lack of shared information between stages, leading to substantial redundant computation. To address this limitation, we propose \\textbf{State-Centric Retrieval}, a unified retrieval paradigm that utilizes \"states\" as a bridge to connect embedding models and rerankers. First, we perform state representation learning by fine-tuning an RWKV-based LLM, transforming it into \\textbf{EmbeddingRWKV}, a unified model that serves as both an embedding model and a state backbone for extracting compact, reusable states. Building upon these reusable states, we further design a state-based reranker to fully leverage precomputed information. During reranking, the model processes only query tokens, decoupling inference cost from document length and yielding a 5.4$\\times$--44.8$\\times$ speedup. Furthermore, we observe that retaining all intermediate layer states is unnecessary; with a uniform layer selection strategy, our model maintains 98.62\\% of full-model performance using only 25\\% of the layers. Extensive experiments demonstrate that State-Centric Retrieval achieves high-quality retrieval and reranking results while significantly enhancing overall system efficiency. Code is available at \\href{https://github.com/howard-hou/EmbeddingRWKV}{our GitHub repository}.",
      "publishedDate": "2026-01-10T03:29:43Z",
      "updatedDate": "2026-01-10T03:29:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07861v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07861",
      "comment": "23 pages, 3 figures, 6 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07903",
      "title": "Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning",
      "authors": [
        {
          "name": "Jianqi Zhang",
          "affiliation": null
        },
        {
          "name": "Jingyao Wang",
          "affiliation": null
        },
        {
          "name": "Wenwen Qiang",
          "affiliation": null
        },
        {
          "name": "Fanjiang Xu",
          "affiliation": null
        },
        {
          "name": "Changwen Zheng",
          "affiliation": null
        }
      ],
      "abstract": "The World Wide Web needs reliable predictive capabilities to respond to changes in user behavior and usage patterns. Time series forecasting (TSF) is a key means to achieve this goal. In recent years, the large language models (LLMs) for TSF (LLM4TSF) have achieved good performance. However, there is a significant difference between pretraining corpora and time series data, making it hard to guarantee forecasting quality when directly applying LLMs to TSF; fine-tuning LLMs can mitigate this issue, but often incurs substantial computational overhead. Thus, LLM4TSF faces a dual challenge of prediction performance and compute overhead. To address this, we aim to explore a method for improving the forecasting performance of LLM4TSF while freezing all LLM parameters to reduce computational overhead. Inspired by in-context learning (ICL), we propose LVICL. LVICL uses our vector-injected ICL to inject example information into a frozen LLM, eliciting its in-context learning ability and thereby enhancing its performance on the example-related task (i.e., TSF). Specifically, we first use the LLM together with a learnable context vector adapter to extract a context vector from multiple examples adaptively. This vector contains compressed, example-related information. Subsequently, during the forward pass, we inject this vector into every layer of the LLM to improve forecasting performance. Compared with conventional ICL that adds examples into the prompt, our vector-injected ICL does not increase prompt length; moreover, adaptively deriving a context vector from examples suppresses components harmful to forecasting, thereby improving model performance. Extensive experiments demonstrate the effectiveness of our approach.",
      "publishedDate": "2026-01-12T14:55:05Z",
      "updatedDate": "2026-01-12T14:55:05Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07903v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07903",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08734",
      "title": "TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback",
      "authors": [
        {
          "name": "Prithwish Jana",
          "affiliation": null
        },
        {
          "name": "Sam Davidson",
          "affiliation": null
        },
        {
          "name": "Bhavana Bhasker",
          "affiliation": null
        },
        {
          "name": "Andrey Kan",
          "affiliation": null
        },
        {
          "name": "Anoop Deoras",
          "affiliation": null
        },
        {
          "name": "Laurent Callot",
          "affiliation": null
        }
      ],
      "abstract": "Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.",
      "publishedDate": "2026-01-13T17:08:30Z",
      "updatedDate": "2026-01-13T17:08:30Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08734v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08734",
      "comment": "The paper has been published at the 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE 2026), Rio de Janeiro, Brazil, April 12-18, 2026",
      "journalRef": null,
      "doi": "10.1145/3786583.3786898",
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08691",
      "title": "LLMs in Code Vulnerability Analysis: A Proof of Concept",
      "authors": [
        {
          "name": "Shaznin Sultana",
          "affiliation": null
        },
        {
          "name": "Sadia Afreen",
          "affiliation": null
        },
        {
          "name": "Nasir U. Eisty",
          "affiliation": null
        }
      ],
      "abstract": "Context: Traditional software security analysis methods struggle to keep pace with the scale and complexity of modern codebases, requiring intelligent automation to detect, assess, and remediate vulnerabilities more efficiently and accurately. Objective: This paper explores the incorporation of code-specific and general-purpose Large Language Models (LLMs) to automate critical software security tasks, such as identifying vulnerabilities, predicting severity and access complexity, and generating fixes as a proof of concept. Method: We evaluate five pairs of recent LLMs, including both code-based and general-purpose open-source models, on two recognized C/C++ vulnerability datasets, namely Big-Vul and Vul-Repair. Additionally, we compare fine-tuning and prompt-based approaches. Results: The results show that fine-tuning uniformly outperforms both zero-shot and few-shot approaches across all tasks and models. Notably, code-specialized models excel in zero-shot and few-shot settings on complex tasks, while general-purpose models remain nearly as effective. Discrepancies among CodeBLEU, CodeBERTScore, BLEU, and ChrF highlight the inadequacy of current metrics for measuring repair quality. Conclusions: This study contributes to the software security community by investigating the potential of advanced LLMs to improve vulnerability analysis and remediation.",
      "publishedDate": "2026-01-13T16:16:11Z",
      "updatedDate": "2026-01-13T16:16:11Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08691v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08691",
      "comment": "Accepted for publication at the Fourth International Workshop on Software Vulnerability Management (SVM 2026) co-located with Intenational Conference in Software Engineering (ICSE 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08653",
      "title": "Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding",
      "authors": [
        {
          "name": "Zenghua Liao",
          "affiliation": null
        },
        {
          "name": "Jinzhi Liao",
          "affiliation": null
        },
        {
          "name": "Xiang Zhao",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.",
      "publishedDate": "2026-01-13T15:30:48Z",
      "updatedDate": "2026-01-13T15:30:48Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08653v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08653",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "tool-use",
        "rag",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "rag",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08557",
      "title": "VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations",
      "authors": [
        {
          "name": "Sushant Gautam",
          "affiliation": null
        },
        {
          "name": "Cise Midoglu",
          "affiliation": null
        },
        {
          "name": "Vajira Thambawita",
          "affiliation": null
        },
        {
          "name": "Michael A. Riegler",
          "affiliation": null
        },
        {
          "name": "Pål Halvorsen",
          "affiliation": null
        }
      ],
      "abstract": "Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .",
      "publishedDate": "2026-01-13T13:42:05Z",
      "updatedDate": "2026-01-13T13:42:05Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08557v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08557",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08545",
      "title": "Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement",
      "authors": [
        {
          "name": "Zhenlong Dai",
          "affiliation": null
        },
        {
          "name": "Zhuoluo Zhao",
          "affiliation": null
        },
        {
          "name": "Hengning Wang",
          "affiliation": null
        },
        {
          "name": "Xiu Tang",
          "affiliation": null
        },
        {
          "name": "Sai Wu",
          "affiliation": null
        },
        {
          "name": "Chang Yao",
          "affiliation": null
        },
        {
          "name": "Zhipeng Gao",
          "affiliation": null
        },
        {
          "name": "Jingyuan Chen",
          "affiliation": null
        }
      ],
      "abstract": "With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \\textbf{LPR} (\\textbf{L}earner-Tailored \\textbf{P}rogram \\textbf{R}epair). We then propose a novel and effective framework, \\textbf{\\textsc{\\MethodName{}}} (\\textbf{L}earner-Tailored \\textbf{S}olution \\textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.",
      "publishedDate": "2026-01-13T13:31:11Z",
      "updatedDate": "2026-01-13T13:31:11Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08545v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08545",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08489",
      "title": "Surgical Refusal Ablation: Disentangling Safety from Intelligence via Concept-Guided Spectral Cleaning",
      "authors": [
        {
          "name": "Tony Cristofano",
          "affiliation": null
        }
      ],
      "abstract": "Safety-aligned language models systematically refuse harmful requests. While activation steering can modulate refusal, ablating the raw \"refusal vector\" calculated from contrastive harmful and harmless prompts often causes collateral damage and distribution drift. We argue this degradation occurs because the raw vector is polysemantic, entangling the refusal signal with core capability circuits and linguistic style. We introduce Surgical Refusal Ablation (SRA) to distill these steering directions. SRA constructs a registry of independent Concept Atoms representing protected capabilities and stylistic confounds, then uses ridge-regularized spectral residualization to orthogonalize the refusal vector against these directions. This yields a clean refusal direction that targets refusal-relevant structure while minimizing disruption to the model's semantic geometry. Across five models (Qwen3-VL and Ministral series), SRA achieves deep refusal reduction (0-2%) with negligible perplexity impact on Wikitext-2 (mean delta PPL approx. 0.02) and minimal distribution drift. Notably, standard ablation on Qwen3-VL-4B induces severe drift (first-token KL = 2.088), whereas SRA maintains the original distribution (KL = 0.044) while achieving the same 0% refusal rate. Using teacher-forced perplexity on GSM8K and MBPP as a high-resolution capability proxy, we show SRA preserves math and code distributions. These results suggest that common \"model damage\" is often \"Ghost Noise,\" defined as the spectral bleeding of the dirty refusal direction into capability subspaces.",
      "publishedDate": "2026-01-13T12:21:44Z",
      "updatedDate": "2026-01-13T12:21:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08489v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08489",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08441",
      "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation",
      "authors": [
        {
          "name": "Abdelaziz Bounhar",
          "affiliation": null
        },
        {
          "name": "Rania Hossam Elmohamady Elbadry",
          "affiliation": null
        },
        {
          "name": "Hadi Abdine",
          "affiliation": null
        },
        {
          "name": "Preslav Nakov",
          "affiliation": null
        },
        {
          "name": "Michalis Vazirgiannis",
          "affiliation": null
        },
        {
          "name": "Guokan Shang",
          "affiliation": null
        }
      ],
      "abstract": "Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \\textit{reference-free} method that learns \\textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\\footnote{https://github.com/MBZUAI-Paris/YaPO}.",
      "publishedDate": "2026-01-13T11:10:13Z",
      "updatedDate": "2026-01-13T11:10:13Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08441v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08441",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08420",
      "title": "MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP",
      "authors": [
        {
          "name": "Aditya Chaudhary",
          "affiliation": null
        },
        {
          "name": "Sneha Barman",
          "affiliation": null
        },
        {
          "name": "Mainak Singha",
          "affiliation": null
        },
        {
          "name": "Ankit Jha",
          "affiliation": null
        },
        {
          "name": "Girish Mishra",
          "affiliation": null
        },
        {
          "name": "Biplab Banerjee",
          "affiliation": null
        }
      ],
      "abstract": "In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.",
      "publishedDate": "2026-01-13T10:44:37Z",
      "updatedDate": "2026-01-13T10:44:37Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08420v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08420",
      "comment": "Accepted at InGARSS 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08418",
      "title": "Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance",
      "authors": [
        {
          "name": "Jihang Li",
          "affiliation": null
        },
        {
          "name": "Qing Liu",
          "affiliation": null
        },
        {
          "name": "Zulong Chen",
          "affiliation": null
        },
        {
          "name": "Jing Wang",
          "affiliation": null
        },
        {
          "name": "Wei Wang",
          "affiliation": null
        },
        {
          "name": "Chuanfei Xu",
          "affiliation": null
        },
        {
          "name": "Zeyi Wen",
          "affiliation": null
        }
      ],
      "abstract": "Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.",
      "publishedDate": "2026-01-13T10:41:23Z",
      "updatedDate": "2026-01-13T10:41:23Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08418v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08418",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08189",
      "title": "ForgetMark: Stealthy Fingerprint Embedding via Targeted Unlearning in Language Models",
      "authors": [
        {
          "name": "Zhenhua Xu",
          "affiliation": null
        },
        {
          "name": "Haobo Zhang",
          "affiliation": null
        },
        {
          "name": "Zhebo Wang",
          "affiliation": null
        },
        {
          "name": "Qichen Liu",
          "affiliation": null
        },
        {
          "name": "Haitao Xu",
          "affiliation": null
        },
        {
          "name": "Wenpeng Xing",
          "affiliation": null
        },
        {
          "name": "Meng Han",
          "affiliation": null
        }
      ],
      "abstract": "Existing invasive (backdoor) fingerprints suffer from high-perplexity triggers that are easily filtered, fixed response patterns exposed by heuristic detectors, and spurious activations on benign inputs. We introduce \\textsc{ForgetMark}, a stealthy fingerprinting framework that encodes provenance via targeted unlearning. It builds a compact, human-readable key--value set with an assistant model and predictive-entropy ranking, then trains lightweight LoRA adapters to suppress the original values on their keys while preserving general capabilities. Ownership is verified under black/gray-box access by aggregating likelihood and semantic evidence into a fingerprint success rate. By relying on probabilistic forgetting traces rather than fixed trigger--response patterns, \\textsc{ForgetMark} avoids high-perplexity triggers, reduces detectability, and lowers false triggers. Across diverse architectures and settings, it achieves 100\\% ownership verification on fingerprinted models while maintaining standard performance, surpasses backdoor baselines in stealthiness and robustness to model merging, and remains effective under moderate incremental fine-tuning. Our code and data are available at \\href{https://github.com/Xuzhenhua55/ForgetMark}{https://github.com/Xuzhenhua55/ForgetMark}.",
      "publishedDate": "2026-01-13T03:41:28Z",
      "updatedDate": "2026-01-13T03:41:28Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08189v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08189",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08173",
      "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
      "authors": [
        {
          "name": "Daocheng Fu",
          "affiliation": null
        },
        {
          "name": "Jianbiao Mei",
          "affiliation": null
        },
        {
          "name": "Rong Wu",
          "affiliation": null
        },
        {
          "name": "Xuemeng Yang",
          "affiliation": null
        },
        {
          "name": "Jia Xu",
          "affiliation": null
        },
        {
          "name": "Ding Wang",
          "affiliation": null
        },
        {
          "name": "Pinlong Cai",
          "affiliation": null
        },
        {
          "name": "Yong Liu",
          "affiliation": null
        },
        {
          "name": "Licheng Wen",
          "affiliation": null
        },
        {
          "name": "Botian Shi",
          "affiliation": null
        }
      ],
      "abstract": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv",
      "publishedDate": "2026-01-13T03:09:18Z",
      "updatedDate": "2026-01-13T03:09:18Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08173v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08173",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07984",
      "title": "Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models",
      "authors": [
        {
          "name": "Haorui Yu",
          "affiliation": null
        },
        {
          "name": "Ramon Ruiz-Dolz",
          "affiliation": null
        },
        {
          "name": "Xuehang Wen",
          "affiliation": null
        },
        {
          "name": "Fengrui Zhang",
          "affiliation": null
        },
        {
          "name": "Qiufeng Yi",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) excel at visual perception, yet their ability to interpret cultural meaning in art remains under-validated. We present a tri-tier evaluation framework for cross-cultural art-critique assessment: Tier I computes automated coverage and risk indicators offline; Tier II applies rubric-based scoring using a single primary judge across five dimensions; and Tier III calibrates the Tier II aggregate score to human ratings via isotonic regression, yielding a 5.2% reduction in MAE on a 152-sample held-out set. The framework outputs a calibrated cultural-understanding score for model selection and cultural-gap diagnosis, together with dimension-level diagnostics and risk indicators. We evaluate 15 VLMs on 294 expert anchors spanning six cultural traditions. Key findings are that (i) automated metrics are unreliable proxies for cultural depth, (ii) Western samples score higher than non-Western samples under our sampling and rubric, and (iii) cross-judge scale mismatch makes naive score averaging unreliable, motivating a single primary judge with explicit calibration. Dataset and code are available in the supplementary materials.",
      "publishedDate": "2026-01-12T20:33:35Z",
      "updatedDate": "2026-01-12T20:33:35Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07984v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07984",
      "comment": "16 pages, 7 figures, submitted to ACL 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07892",
      "title": "Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification",
      "authors": [
        {
          "name": "Hong Huang",
          "affiliation": null
        },
        {
          "name": "Decheng Wu",
          "affiliation": null
        },
        {
          "name": "Qiangqiang Hu",
          "affiliation": null
        },
        {
          "name": "Guanghua Yu",
          "affiliation": null
        },
        {
          "name": "Jinhai Yang",
          "affiliation": null
        },
        {
          "name": "Jianchen Zhu",
          "affiliation": null
        },
        {
          "name": "Xue Liu",
          "affiliation": null
        },
        {
          "name": "Dapeng Wu",
          "affiliation": null
        }
      ],
      "abstract": "The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .",
      "publishedDate": "2026-01-12T08:49:34Z",
      "updatedDate": "2026-01-12T08:49:34Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07892v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07892",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07885",
      "title": "Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in Large Language Models",
      "authors": [
        {
          "name": "Weipeng Jiang",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Zhang",
          "affiliation": null
        },
        {
          "name": "Juan Zhai",
          "affiliation": null
        },
        {
          "name": "Shiqing Ma",
          "affiliation": null
        },
        {
          "name": "Chao Shen",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Emoticons are widely used in digital communication to convey affective intent, yet their safety implications for Large Language Models (LLMs) remain largely unexplored. In this paper, we identify emoticon semantic confusion, a vulnerability where LLMs misinterpret ASCII-based emoticons to perform unintended and even destructive actions. To systematically study this phenomenon, we develop an automated data generation pipeline and construct a dataset containing 3,757 code-oriented test cases spanning 21 meta-scenarios, four programming languages, and varying contextual complexities. Our study on six LLMs reveals that emoticon semantic confusion is pervasive, with an average confusion ratio exceeding 38%. More critically, over 90% of confused responses yield 'silent failures', which are syntactically valid outputs but deviate from user intent, potentially leading to destructive security consequences. Furthermore, we observe that this vulnerability readily transfers to popular agent frameworks, while existing prompt-based mitigations remain largely ineffective. We call on the community to recognize this emerging vulnerability and develop effective mitigation methods to uphold the safety and reliability of the LLM system.",
      "publishedDate": "2026-01-12T05:34:18Z",
      "updatedDate": "2026-01-12T05:34:18Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07885v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07885",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation",
        "agents",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07873",
      "title": "Multiplicative Orthogonal Sequential Editing for Language Models",
      "authors": [
        {
          "name": "Hao-Xiang Xu",
          "affiliation": null
        },
        {
          "name": "Jun-Yu Ma",
          "affiliation": null
        },
        {
          "name": "Ziqi Peng",
          "affiliation": null
        },
        {
          "name": "Yuhao Sun",
          "affiliation": null
        },
        {
          "name": "Zhen-Hua Ling",
          "affiliation": null
        },
        {
          "name": "Jia-Chen Gu",
          "affiliation": null
        }
      ],
      "abstract": "Knowledge editing aims to efficiently modify the internal knowledge of large language models (LLMs) without compromising their other capabilities. The prevailing editing paradigm, which appends an update matrix to the original parameter matrix, has been shown by some studies to damage key numerical stability indicators (such as condition number and norm), thereby reducing editing performance and general abilities, especially in sequential editing scenario. Although subsequent methods have made some improvements, they remain within the additive framework and have not fundamentally addressed this limitation. To solve this problem, we analyze it from both statistical and mathematical perspectives and conclude that multiplying the original matrix by an orthogonal matrix does not change the numerical stability of the matrix. Inspired by this, different from the previous additive editing paradigm, a multiplicative editing paradigm termed Multiplicative Orthogonal Sequential Editing (MOSE) is proposed. Specifically, we first derive the matrix update in the multiplicative form, the new knowledge is then incorporated into an orthogonal matrix, which is multiplied by the original parameter matrix. In this way, the numerical stability of the edited matrix is unchanged, thereby maintaining editing performance and general abilities. We compared MOSE with several current knowledge editing methods, systematically evaluating their impact on both editing performance and the general abilities across three different LLMs. Experimental results show that MOSE effectively limits deviations in the edited parameter matrix and maintains its numerical stability. Compared to current methods, MOSE achieves a 12.08% improvement in sequential editing performance, while retaining 95.73% of general abilities across downstream tasks. The code is available at https://github.com/famoustourist/MOSE.",
      "publishedDate": "2026-01-11T04:09:32Z",
      "updatedDate": "2026-01-11T04:09:32Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07873v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07873",
      "comment": "Accepted by AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08325",
      "title": "ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation",
      "authors": [
        {
          "name": "Zhenyang Liu",
          "affiliation": null
        },
        {
          "name": "Yongchong Gu",
          "affiliation": null
        },
        {
          "name": "Yikai Wang",
          "affiliation": null
        },
        {
          "name": "Xiangyang Xue",
          "affiliation": null
        },
        {
          "name": "Yanwei Fu",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.",
      "publishedDate": "2026-01-13T08:29:07Z",
      "updatedDate": "2026-01-13T08:29:07Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08325v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08325",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "rag",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08470",
      "title": "Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs",
      "authors": [
        {
          "name": "Takara Taniguchi",
          "affiliation": null
        },
        {
          "name": "Kuniaki Saito",
          "affiliation": null
        },
        {
          "name": "Atsushi Hashimoto",
          "affiliation": null
        }
      ],
      "abstract": "Vision Language Models (VLMs) are increasingly deployed in autonomous vehicles and mobile systems, making it crucial to evaluate their ability to support safer decision-making in complex environments. However, existing benchmarks inadequately cover diverse hazardous situations, especially anomalous scenarios with spatio-temporal dynamics. While image editing models are a promising means to synthesize such hazards, it remains challenging to generate well-formulated scenarios that include moving, intrusive, and distant objects frequently observed in the real world. To address this gap, we introduce \\textbf{HazardForge}, a scalable pipeline that leverages image editing models to generate these scenarios with layout decision algorithms, and validation modules. Using HazardForge, we construct \\textbf{MovSafeBench}, a multiple-choice question (MCQ) benchmark comprising 7,254 images and corresponding QA pairs across 13 object categories, covering both normal and anomalous objects. Experiments using MovSafeBench show that VLM performance degrades notably under conditions including anomalous objects, with the largest drop in scenarios requiring nuanced motion understanding.",
      "publishedDate": "2026-01-13T11:55:31Z",
      "updatedDate": "2026-01-13T11:55:31Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08470v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08470",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "agents",
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.08402",
      "title": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors",
      "authors": [
        {
          "name": "Donya Rooein",
          "affiliation": null
        },
        {
          "name": "Sankalan Pal Chowdhury",
          "affiliation": null
        },
        {
          "name": "Mariia Eremeeva",
          "affiliation": null
        },
        {
          "name": "Yuan Qin",
          "affiliation": null
        },
        {
          "name": "Debora Nozza",
          "affiliation": null
        },
        {
          "name": "Mrinmaya Sachan",
          "affiliation": null
        },
        {
          "name": "Dirk Hovy",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.",
      "publishedDate": "2026-01-13T10:17:26Z",
      "updatedDate": "2026-01-13T10:17:26Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08402v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08402",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.08024",
      "title": "A Highly Efficient Diversity-based Input Selection for DNN Improvement Using VLMs",
      "authors": [
        {
          "name": "Amin Abbasishahkoo",
          "affiliation": null
        },
        {
          "name": "Mahboubeh Dadkhah",
          "affiliation": null
        },
        {
          "name": "Lionel Briand",
          "affiliation": null
        }
      ],
      "abstract": "Maintaining or improving the performance of Deep Neural Networks (DNNs) through fine-tuning requires labeling newly collected inputs, a process that is often costly and time-consuming. To alleviate this problem, input selection approaches have been developed in recent years to identify small, yet highly informative subsets for labeling. Diversity-based selection is one of the most effective approaches for this purpose. However, they are often computationally intensive and lack scalability for large input sets, limiting their practical applicability. To address this challenge, we introduce Concept-Based Diversity (CBD), a highly efficient metric for image inputs that leverages Vision-Language Models (VLM). Our results show that CBD exhibits a strong correlation with Geometric Diversity (GD), an established diversity metric, while requiring only a fraction of its computation time. Building on this finding, we propose a hybrid input selection approach that combines CBD with Margin, a simple uncertainty metric. We conduct a comprehensive evaluation across a diverse set of DNN models, input sets, selection budgets, and five most effective state-of-the-art selection baselines. The results demonstrate that the CBD-based selection consistently outperforms all baselines at guiding input selection to improve the DNN model. Furthermore, the CBD-based selection approach remains highly efficient, requiring selection times close to those of simple uncertainty-based methods such as Margin, even on larger input sets like ImageNet. These results confirm not only the effectiveness and computational advantage of the CBD-based approach, particularly compared to hybrid baselines, but also its scalability in repetitive and extensive input selection scenarios.",
      "publishedDate": "2026-01-12T21:57:33Z",
      "updatedDate": "2026-01-12T21:57:33Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.08024v1",
      "arxivUrl": "https://arxiv.org/abs/2601.08024",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.07954",
      "title": "A Human-Centric Pipeline for Aligning Large Language Models with Chinese Medical Ethics",
      "authors": [
        {
          "name": "Haoan Jin",
          "affiliation": null
        },
        {
          "name": "Han Ying",
          "affiliation": null
        },
        {
          "name": "Jiacheng Ji",
          "affiliation": null
        },
        {
          "name": "Hanhui Xu",
          "affiliation": null
        },
        {
          "name": "Mengyue Wu",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models have enabled their application to a range of healthcare tasks. However, aligning LLMs with the nuanced demands of medical ethics, especially under complex real world scenarios, remains underexplored. In this work, we present MedES, a dynamic, scenario-centric benchmark specifically constructed from 260 authoritative Chinese medical, ethical, and legal sources to reflect the challenges in clinical decision-making. To facilitate model alignment, we introduce a guardian-in-the-loop framework that leverages a dedicated automated evaluator (trained on expert-labeled data and achieving over 97% accuracy within our domain) to generate targeted prompts and provide structured ethical feedback. Using this pipeline, we align a 7B-parameter LLM through supervised fine-tuning and domain-specific preference optimization. Experimental results, conducted entirely within the Chinese medical ethics context, demonstrate that our aligned model outperforms notably larger baselines on core ethical tasks, with observed improvements in both quality and composite evaluation metrics. Our work offers a practical and adaptable framework for aligning LLMs with medical ethics in the Chinese healthcare domain, and suggests that similar alignment pipelines may be instantiated in other legal and cultural environments through modular replacement of the underlying normative corpus.",
      "publishedDate": "2026-01-12T19:40:13Z",
      "updatedDate": "2026-01-12T19:40:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.07954v1",
      "arxivUrl": "https://arxiv.org/abs/2601.07954",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "evaluation",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.06573",
      "title": "QMAVIS: Long Video-Audio Understanding using Fusion of Large Multimodal Models",
      "authors": [
        {
          "name": "Zixing Lin",
          "affiliation": null
        },
        {
          "name": "Jiale Wang",
          "affiliation": null
        },
        {
          "name": "Gee Wah Ng",
          "affiliation": null
        },
        {
          "name": "Lee Onn Mak",
          "affiliation": null
        },
        {
          "name": "Chan Zhi Yang Jeriel",
          "affiliation": null
        },
        {
          "name": "Jun Yang Lee",
          "affiliation": null
        },
        {
          "name": "Yaohao Li",
          "affiliation": null
        }
      ],
      "abstract": "Large Multimodal Models (LMMs) for video-audio understanding have traditionally been evaluated only on shorter videos of a few minutes long. In this paper, we introduce QMAVIS (Q Team-Multimodal Audio Video Intelligent Sensemaking), a novel long video-audio understanding pipeline built through a late fusion of LMMs, Large Language Models, and speech recognition models. QMAVIS addresses the gap in long-form video analytics, particularly for longer videos of a few minutes to beyond an hour long, opening up new potential applications in sensemaking, video content analysis, embodied AI, etc. Quantitative experiments using QMAVIS demonstrated a 38.75% improvement over state-of-the-art video-audio LMMs like VideoLlaMA2 and InternVL2 on the VideoMME (with subtitles) dataset, which comprises long videos with audio information. Evaluations on other challenging video understanding datasets like PerceptionTest and EgoSchema saw up to 2% improvement, indicating competitive performance. Qualitative experiments also showed that QMAVIS is able to extract the nuances of different scenes in a long video audio content while understanding the overarching narrative. Ablation studies were also conducted to ascertain the impact of each component in the fusion pipeline.",
      "publishedDate": "2026-01-10T13:42:15Z",
      "updatedDate": "2026-01-10T13:42:15Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.MM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.06573v1",
      "arxivUrl": "https://arxiv.org/abs/2601.06573",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-14T03:42:25.537Z",
      "categories": [
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    }
  ]
}