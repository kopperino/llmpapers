{
  "year": "2022",
  "count": 259,
  "papers": [
    {
      "id": "2205.11561",
      "title": "Agreement and Statistical Efficiency in Bayesian Perception Models",
      "authors": [
        {
          "name": "Yash Deshpande",
          "affiliation": null
        },
        {
          "name": "Elchanan Mossel",
          "affiliation": null
        },
        {
          "name": "Youngtak Sohn",
          "affiliation": null
        }
      ],
      "abstract": "Bayesian models of group learning are studied in Economics since the 1970s. and more recently in computational linguistics. The models from Economics postulate that agents maximize utility in their communication and actions. The Economics models do not explain the ``probability matching\" phenomena that are observed in many experimental studies. To address these observations, Bayesian models that do not formally fit into the economic utility maximization framework were introduced. In these models individuals sample from their posteriors in communication. In this work we study the asymptotic behavior of such models on connected networks with repeated communication. Perhaps surprisingly, despite the fact that individual agents are not utility maximizers in the classical sense, we establish that the individuals ultimately agree and furthermore show that the limiting posterior is Bayes optimal. We explore the interpretation of our results in terms of Large Language Models (LLMs). In the positive direction our results can be interpreted as stating that interaction between different LLMs can lead to optimal learning. However, we provide an example showing how misspecification may lead LLM agents to be overconfident in their estimates.",
      "publishedDate": "2022-05-23T18:21:07Z",
      "updatedDate": "2023-08-09T16:14:19Z",
      "primaryCategory": "math.ST",
      "arxivCategories": [
        "math.ST",
        "econ.TH"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.11561v3",
      "arxivUrl": "https://arxiv.org/abs/2205.11561",
      "comment": "21 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents"
      ],
      "tags": {
        "auto": [
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.09554",
      "title": "Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks",
      "authors": [
        {
          "name": "James R. Kirk",
          "affiliation": null
        },
        {
          "name": "Robert E. Wray",
          "affiliation": null
        },
        {
          "name": "Peter Lindes",
          "affiliation": null
        },
        {
          "name": "John E. Laird",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous agents are able to draw on a wide variety of potential sources of task knowledge; however current approaches invariably focus on only one or two. Here we investigate the challenges and impact of exploiting diverse knowledge sources to learn online, in one-shot, new tasks for a simulated office mobile robot. The resulting agent, developed in the Soar cognitive architecture, uses the following sources of domain and task knowledge: interaction with the environment, task execution and search knowledge, human natural language instruction, and responses retrieved from a large language model (GPT-3). We explore the distinct contributions of these knowledge sources and evaluate the performance of different combinations in terms of learning correct task knowledge and human workload. Results show that an agent's online integration of diverse knowledge sources improves one-shot task learning overall, reducing human feedback needed for rapid and reliable task learning.",
      "publishedDate": "2022-08-19T21:53:15Z",
      "updatedDate": "2023-05-15T16:34:58Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.09554v3",
      "arxivUrl": "https://arxiv.org/abs/2208.09554",
      "comment": "20 pages, 3 figures. (Added technical appendix based on reviewer feedback.)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "tool-use",
        "prompting",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "prompting",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.08853",
      "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge",
      "authors": [
        {
          "name": "Linxi Fan",
          "affiliation": null
        },
        {
          "name": "Guanzhi Wang",
          "affiliation": null
        },
        {
          "name": "Yunfan Jiang",
          "affiliation": null
        },
        {
          "name": "Ajay Mandlekar",
          "affiliation": null
        },
        {
          "name": "Yuncong Yang",
          "affiliation": null
        },
        {
          "name": "Haoyi Zhu",
          "affiliation": null
        },
        {
          "name": "Andrew Tang",
          "affiliation": null
        },
        {
          "name": "De-An Huang",
          "affiliation": null
        },
        {
          "name": "Yuke Zhu",
          "affiliation": null
        },
        {
          "name": "Anima Anandkumar",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.",
      "publishedDate": "2022-06-17T15:53:05Z",
      "updatedDate": "2022-11-22T07:59:47Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.08853v2",
      "arxivUrl": "https://arxiv.org/abs/2206.08853",
      "comment": "Outstanding Paper Award at NeurIPS 2022. Project website: https://minedojo.org",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "rag",
        "tool-use",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "tool-use",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.01134",
      "title": "Language and Culture Internalisation for Human-Like Autotelic AI",
      "authors": [
        {
          "name": "Cédric Colas",
          "affiliation": null
        },
        {
          "name": "Tristan Karch",
          "affiliation": null
        },
        {
          "name": "Clément Moulin-Frier",
          "affiliation": null
        },
        {
          "name": "Pierre-Yves Oudeyer",
          "affiliation": null
        }
      ],
      "abstract": "Building autonomous agents able to grow open-ended repertoires of skills across their lives is a fundamental goal of artificial intelligence (AI). A promising developmental approach recommends the design of intrinsically motivated agents that learn new skills by generating and pursuing their own goals - autotelic agents. But despite recent progress, existing algorithms still show serious limitations in terms of goal diversity, exploration, generalisation or skill composition. This perspective calls for the immersion of autotelic agents into rich socio-cultural worlds, an immensely important attribute of our environment that shapes human cognition but is mostly omitted in modern AI. Inspired by the seminal work of Vygotsky, we propose Vygotskian autotelic agents - agents able to internalise their interactions with others and turn them into cognitive tools. We focus on language and show how its structure and informational content may support the development of new cognitive functions in artificial agents as it does in humans. We justify the approach by uncovering several examples of new artificial cognitive functions emerging from interactions between language and embodiment in recent works at the intersection of deep reinforcement learning and natural language processing. Looking forward, we highlight future opportunities and challenges for Vygotskian Autotelic AI research, including the use of language models as cultural models supporting artificial cognitive development.",
      "publishedDate": "2022-06-02T16:35:41Z",
      "updatedDate": "2022-11-16T14:25:23Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.01134v2",
      "arxivUrl": "https://arxiv.org/abs/2206.01134",
      "comment": null,
      "journalRef": "Nature Machine Intelligence 4, 1068-1076 (2022)",
      "doi": "10.1038/s42256-022-00591-4",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents"
      ],
      "tags": {
        "auto": [
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.13411",
      "title": "Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers",
      "authors": [
        {
          "name": "Arthur Bucker",
          "affiliation": null
        },
        {
          "name": "Luis Figueredo",
          "affiliation": null
        },
        {
          "name": "Sami Haddadin",
          "affiliation": null
        },
        {
          "name": "Ashish Kapoor",
          "affiliation": null
        },
        {
          "name": "Shuang Ma",
          "affiliation": null
        },
        {
          "name": "Rogerio Bonatti",
          "affiliation": null
        }
      ],
      "abstract": "Natural language is the most intuitive medium for us to interact with other people when expressing commands and instructions. However, using language is seldom an easy task when humans need to express their intent towards robots, since most of the current language interfaces require rigid templates with a static set of action targets and commands. In this work, we provide a flexible language-based interface for human-robot collaboration, which allows a user to reshape existing trajectories for an autonomous agent. We take advantage of recent advancements in the field of large language models (BERT and CLIP) to encode the user command, and then combine these features with trajectory information using multi-modal attention transformers. We train the model using imitation learning over a dataset containing robot trajectories modified by language commands, and treat the trajectory generation process as a sequence prediction problem, analogously to how language generation architectures operate. We evaluate the system in multiple simulated trajectory scenarios, and show a significant performance increase of our model over baseline approaches. In addition, our real-world experiments with a robot arm show that users significantly prefer our natural language interface over traditional methods such as kinesthetic teaching or cost-function programming. Our study shows how the field of robotics can take advantage of large pre-trained language models towards creating more intuitive interfaces between robots and machines. Project webpage: https://arthurfenderbucker.github.io/NL_trajectory_reshaper/",
      "publishedDate": "2022-03-25T01:36:56Z",
      "updatedDate": "2022-03-25T01:36:56Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.13411v1",
      "arxivUrl": "https://arxiv.org/abs/2203.13411",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "code-generation",
        "robotics",
        "tool-use",
        "multi-agent",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation",
          "robotics",
          "tool-use",
          "multi-agent",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10561",
      "title": "Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions",
      "authors": [
        {
          "name": "Eric Zelikman",
          "affiliation": null
        },
        {
          "name": "Qian Huang",
          "affiliation": null
        },
        {
          "name": "Gabriel Poesia",
          "affiliation": null
        },
        {
          "name": "Noah D. Goodman",
          "affiliation": null
        },
        {
          "name": "Nick Haber",
          "affiliation": null
        }
      ],
      "abstract": "Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel",
      "publishedDate": "2022-12-20T18:59:23Z",
      "updatedDate": "2023-05-28T21:12:31Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10561v3",
      "arxivUrl": "https://arxiv.org/abs/2212.10561",
      "comment": "humaneval results, clarity",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "code-generation",
        "planning",
        "robotics"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "planning",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10471",
      "title": "Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning and Generation with Large Language Models",
      "authors": [
        {
          "name": "Evgeniia Razumovskaia",
          "affiliation": null
        },
        {
          "name": "Joshua Maynez",
          "affiliation": null
        },
        {
          "name": "Annie Louis",
          "affiliation": null
        },
        {
          "name": "Mirella Lapata",
          "affiliation": null
        },
        {
          "name": "Shashi Narayan",
          "affiliation": null
        }
      ],
      "abstract": "Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of cross-lingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pre-trained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.",
      "publishedDate": "2022-12-20T17:42:16Z",
      "updatedDate": "2024-03-25T17:54:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10471v3",
      "arxivUrl": "https://arxiv.org/abs/2212.10471",
      "comment": "Accepted to LREC-COLING 2024",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10465",
      "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
      "authors": [
        {
          "name": "Hyunwoo Kim",
          "affiliation": null
        },
        {
          "name": "Jack Hessel",
          "affiliation": null
        },
        {
          "name": "Liwei Jiang",
          "affiliation": null
        },
        {
          "name": "Peter West",
          "affiliation": null
        },
        {
          "name": "Ximing Lu",
          "affiliation": null
        },
        {
          "name": "Youngjae Yu",
          "affiliation": null
        },
        {
          "name": "Pei Zhou",
          "affiliation": null
        },
        {
          "name": "Ronan Le Bras",
          "affiliation": null
        },
        {
          "name": "Malihe Alikhani",
          "affiliation": null
        },
        {
          "name": "Gunhee Kim",
          "affiliation": null
        },
        {
          "name": "Maarten Sap",
          "affiliation": null
        },
        {
          "name": "Yejin Choi",
          "affiliation": null
        }
      ],
      "abstract": "Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public.",
      "publishedDate": "2022-12-20T17:38:47Z",
      "updatedDate": "2023-10-23T18:46:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10465v3",
      "arxivUrl": "https://arxiv.org/abs/2212.10465",
      "comment": "EMNLP 2023. Dataset, model, and code can be found at https://hyunw.kim/sodaverse",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.08718",
      "title": "Neural Story Planning",
      "authors": [
        {
          "name": "Anbang Ye",
          "affiliation": null
        },
        {
          "name": "Christopher Cui",
          "affiliation": null
        },
        {
          "name": "Taiwei Shi",
          "affiliation": null
        },
        {
          "name": "Mark O. Riedl",
          "affiliation": null
        }
      ],
      "abstract": "Automated plot generation is the challenge of generating a sequence of events that will be perceived by readers as the plot of a coherent story. Traditional symbolic planners plan a story from a goal state and guarantee logical causal plot coherence but rely on a library of hand-crafted actions with their preconditions and effects. This closed world setting limits the length and diversity of what symbolic planners can generate. On the other hand, pre-trained neural language models can generate stories with great diversity, while being generally incapable of ending a story in a specified manner and can have trouble maintaining coherence. In this paper, we present an approach to story plot generation that unifies causal planning with neural language models. We propose to use commonsense knowledge extracted from large language models to recursively expand a story plot in a backward chaining fashion. Specifically, our system infers the preconditions for events in the story and then events that will cause those conditions to become true. We performed automatic evaluation to measure narrative coherence as indicated by the ability to answer questions about whether different events in the story are causally related to other events. Results indicate that our proposed method produces more coherent plotlines than several strong baselines.",
      "publishedDate": "2022-12-16T21:29:41Z",
      "updatedDate": "2022-12-16T21:29:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.08718v1",
      "arxivUrl": "https://arxiv.org/abs/2212.08718",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.08681",
      "title": "Plansformer: Generating Symbolic Plans using Transformers",
      "authors": [
        {
          "name": "Vishal Pallagani",
          "affiliation": null
        },
        {
          "name": "Bharath Muppasani",
          "affiliation": null
        },
        {
          "name": "Keerthiram Murugesan",
          "affiliation": null
        },
        {
          "name": "Francesca Rossi",
          "affiliation": null
        },
        {
          "name": "Lior Horesh",
          "affiliation": null
        },
        {
          "name": "Biplav Srivastava",
          "affiliation": null
        },
        {
          "name": "Francesco Fabiano",
          "affiliation": null
        },
        {
          "name": "Andrea Loreggia",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have been the subject of active research, significantly advancing the field of Natural Language Processing (NLP). From BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural language tasks such as question answering, summarization, and text generation. Many ongoing efforts focus on understanding LLMs' capabilities, including their knowledge of the world, syntax, and semantics. However, extending the textual prowess of LLMs to symbolic reasoning has been slow and predominantly focused on tackling problems related to the mathematical field. In this paper, we explore the use of LLMs for automated planning - a branch of AI concerned with the realization of action sequences (plans) to achieve a goal, typically executed by intelligent agents, autonomous robots, and unmanned vehicles. We introduce Plansformer; an LLM fine-tuned on planning problems and capable of generating plans with favorable behavior in terms of correctness and length with reduced knowledge-engineering efforts. We also demonstrate the adaptability of Plansformer in solving different planning domains with varying complexities, owing to the transfer learning abilities of LLMs. For one configuration of Plansformer, we achieve ~97% valid plans, out of which ~95% are optimal for Towers of Hanoi - a puzzle-solving domain.",
      "publishedDate": "2022-12-16T19:06:49Z",
      "updatedDate": "2022-12-16T19:06:49Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.08681v1",
      "arxivUrl": "https://arxiv.org/abs/2212.08681",
      "comment": "44 pages including supplementary material",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "planning",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.04088",
      "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",
      "authors": [
        {
          "name": "Chan Hee Song",
          "affiliation": null
        },
        {
          "name": "Jiaman Wu",
          "affiliation": null
        },
        {
          "name": "Clayton Washington",
          "affiliation": null
        },
        {
          "name": "Brian M. Sadler",
          "affiliation": null
        },
        {
          "name": "Wei-Lun Chao",
          "affiliation": null
        },
        {
          "name": "Yu Su",
          "affiliation": null
        }
      ],
      "abstract": "This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. Website: https://dki-lab.github.io/LLM-Planner",
      "publishedDate": "2022-12-08T05:46:32Z",
      "updatedDate": "2023-03-30T04:50:44Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.04088v3",
      "arxivUrl": "https://arxiv.org/abs/2212.04088",
      "comment": "14 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "agents",
        "planning",
        "robotics"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "planning",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.15533",
      "title": "The Stack: 3 TB of permissively licensed source code",
      "authors": [
        {
          "name": "Denis Kocetkov",
          "affiliation": null
        },
        {
          "name": "Raymond Li",
          "affiliation": null
        },
        {
          "name": "Loubna Ben Allal",
          "affiliation": null
        },
        {
          "name": "Jia Li",
          "affiliation": null
        },
        {
          "name": "Chenghao Mou",
          "affiliation": null
        },
        {
          "name": "Carlos Muñoz Ferrandis",
          "affiliation": null
        },
        {
          "name": "Yacine Jernite",
          "affiliation": null
        },
        {
          "name": "Margaret Mitchell",
          "affiliation": null
        },
        {
          "name": "Sean Hughes",
          "affiliation": null
        },
        {
          "name": "Thomas Wolf",
          "affiliation": null
        },
        {
          "name": "Dzmitry Bahdanau",
          "affiliation": null
        },
        {
          "name": "Leandro von Werra",
          "affiliation": null
        },
        {
          "name": "Harm de Vries",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called \"Am I in The Stack\" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.",
      "publishedDate": "2022-11-20T18:15:30Z",
      "updatedDate": "2022-11-20T18:15:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.15533v1",
      "arxivUrl": "https://arxiv.org/abs/2211.15533",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.09935",
      "title": "CAPE: Corrective Actions from Precondition Errors using Large Language Models",
      "authors": [
        {
          "name": "Shreyas Sundara Raman",
          "affiliation": null
        },
        {
          "name": "Vanya Cohen",
          "affiliation": null
        },
        {
          "name": "Ifrah Idrees",
          "affiliation": null
        },
        {
          "name": "Eric Rosen",
          "affiliation": null
        },
        {
          "name": "Ray Mooney",
          "affiliation": null
        },
        {
          "name": "Stefanie Tellex",
          "affiliation": null
        },
        {
          "name": "David Paulius",
          "affiliation": null
        }
      ],
      "abstract": "Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause. We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctness metric of the executed task plans by 76.49% compared to SayCan. Our approach enables the robot to follow natural language commands and robustly recover from failures, which baseline approaches largely cannot resolve or address inefficiently.",
      "publishedDate": "2022-11-17T23:14:51Z",
      "updatedDate": "2024-03-09T13:53:47Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.09935v3",
      "arxivUrl": "https://arxiv.org/abs/2211.09935",
      "comment": "17 pages, 6 figures, accepted at ICRA 2024",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "robotics",
        "agents",
        "reasoning",
        "planning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "robotics",
          "agents",
          "reasoning",
          "planning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.06254",
      "title": "Zero-Shot On-the-Fly Event Schema Induction",
      "authors": [
        {
          "name": "Rotem Dror",
          "affiliation": null
        },
        {
          "name": "Haoyu Wang",
          "affiliation": null
        },
        {
          "name": "Dan Roth",
          "affiliation": null
        }
      ],
      "abstract": "What are the events involved in a pandemic outbreak? What steps should be taken when planning a wedding? The answers to these questions can be found by collecting many documents on the complex event of interest, extracting relevant information, and analyzing it. We present a new approach in which large language models are utilized to generate source documents that allow predicting, given a high-level event definition, the specific events, arguments, and relations between them to construct a schema that describes the complex event in its entirety. Using our model, complete schemas on any topic can be generated on-the-fly without any manual data collection, i.e., in a zero-shot manner. Moreover, we develop efficient methods to extract pertinent information from texts and demonstrate in a series of experiments that these schemas are considered to be more complete than human-curated ones in the majority of examined scenarios. Finally, we show that this framework is comparable in performance with previous supervised schema induction methods that rely on collecting real texts while being more general and flexible without the need for a predefined ontology.",
      "publishedDate": "2022-10-12T14:37:00Z",
      "updatedDate": "2023-03-27T14:11:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.06254v2",
      "arxivUrl": "https://arxiv.org/abs/2210.06254",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "planning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "planning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.04964",
      "title": "Generating Executable Action Plans with Environmentally-Aware Language Models",
      "authors": [
        {
          "name": "Maitrey Gramopadhye",
          "affiliation": null
        },
        {
          "name": "Daniel Szafir",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) trained using massive text datasets have recently shown promise in generating action plans for robotic agents from high level text queries. However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints. In this paper, we propose an approach to generate environmentally-aware action plans that agents are better able to execute. Our approach involves integrating environmental objects and object relations as additional inputs into LLM action plan generation to provide the system with an awareness of its surroundings, resulting in plans where each generated action is mapped to objects present in the scene. We also design a novel scoring function that, along with generating the action steps and associating them with objects, helps the system disambiguate among object instances and take into account their states. We evaluated our approach using the VirtualHome simulator and the ActivityPrograms knowledge base and found that action plans generated from our system had a 310% improvement in executability and a 147% improvement in correctness over prior work. The complete code and a demo of our method is publicly available at https://github.com/hri-ironlab/scene_aware_language_planner.",
      "publishedDate": "2022-10-10T18:56:57Z",
      "updatedDate": "2023-05-02T04:58:54Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.04964v2",
      "arxivUrl": "https://arxiv.org/abs/2210.04964",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "planning",
        "rag",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "rag",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.03629",
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": [
        {
          "name": "Shunyu Yao",
          "affiliation": null
        },
        {
          "name": "Jeffrey Zhao",
          "affiliation": null
        },
        {
          "name": "Dian Yu",
          "affiliation": null
        },
        {
          "name": "Nan Du",
          "affiliation": null
        },
        {
          "name": "Izhak Shafran",
          "affiliation": null
        },
        {
          "name": "Karthik Narasimhan",
          "affiliation": null
        },
        {
          "name": "Yuan Cao",
          "affiliation": null
        }
      ],
      "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",
      "publishedDate": "2022-10-06T01:00:32Z",
      "updatedDate": "2023-03-10T01:00:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.03629v3",
      "arxivUrl": "https://arxiv.org/abs/2210.03629",
      "comment": "v3 is the ICLR camera ready version with some typos fixed. Project site with code: https://react-lm.github.io",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.01287",
      "title": "Robot Task Planning and Situation Handling in Open Worlds",
      "authors": [
        {
          "name": "Yan Ding",
          "affiliation": null
        },
        {
          "name": "Xiaohan Zhang",
          "affiliation": null
        },
        {
          "name": "Saeid Amiri",
          "affiliation": null
        },
        {
          "name": "Nieqing Cao",
          "affiliation": null
        },
        {
          "name": "Hao Yang",
          "affiliation": null
        },
        {
          "name": "Chad Esselink",
          "affiliation": null
        },
        {
          "name": "Shiqi Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Automated task planning algorithms have been developed to help robots complete complex tasks that require multiple actions. Most of those algorithms have been developed for \"closed worlds\" assuming complete world knowledge is provided. However, the real world is generally open, and the robots frequently encounter unforeseen situations that can potentially break the planner's completeness. This paper introduces a novel algorithm (COWP) for open-world task planning and situation handling that dynamically augments the robot's action knowledge with task-oriented common sense. In particular, common sense is extracted from Large Language Models based on the current task at hand and robot skills. For systematic evaluations, we collected a dataset that includes 561 execution-time situations in a dining domain, where each situation corresponds to a state instance of a robot being potentially unable to complete a task using a solution that normally works. Experimental results show that our approach significantly outperforms competitive baselines from the literature in the success rate of service tasks. Additionally, we have demonstrated COWP using a mobile manipulator. The project website is available at: https://cowplanning.github.io/, where a more detailed version can also be found. This version has been accepted for publication in Autonomous Robots.",
      "publishedDate": "2022-10-04T00:21:00Z",
      "updatedDate": "2024-09-29T01:32:25Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.01287v2",
      "arxivUrl": "https://arxiv.org/abs/2210.01287",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "planning",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.01240",
      "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
      "authors": [
        {
          "name": "Abulhair Saparov",
          "affiliation": null
        },
        {
          "name": "He He",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
      "publishedDate": "2022-10-03T21:34:32Z",
      "updatedDate": "2023-03-02T03:54:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.01240v4",
      "arxivUrl": "https://arxiv.org/abs/2210.01240",
      "comment": "Published as a conference paper at ICLR 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "planning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.11302",
      "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models",
      "authors": [
        {
          "name": "Ishika Singh",
          "affiliation": null
        },
        {
          "name": "Valts Blukis",
          "affiliation": null
        },
        {
          "name": "Arsalan Mousavian",
          "affiliation": null
        },
        {
          "name": "Ankit Goyal",
          "affiliation": null
        },
        {
          "name": "Danfei Xu",
          "affiliation": null
        },
        {
          "name": "Jonathan Tremblay",
          "affiliation": null
        },
        {
          "name": "Dieter Fox",
          "affiliation": null
        },
        {
          "name": "Jesse Thomason",
          "affiliation": null
        },
        {
          "name": "Animesh Garg",
          "affiliation": null
        }
      ],
      "abstract": "Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io",
      "publishedDate": "2022-09-22T20:29:49Z",
      "updatedDate": "2022-09-22T20:29:49Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.11302v1",
      "arxivUrl": "https://arxiv.org/abs/2209.11302",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "planning",
        "prompting",
        "agents",
        "robotics"
      ],
      "tags": {
        "auto": [
          "planning",
          "prompting",
          "agents",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.09874",
      "title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
      "authors": [
        {
          "name": "Boyuan Chen",
          "affiliation": null
        },
        {
          "name": "Fei Xia",
          "affiliation": null
        },
        {
          "name": "Brian Ichter",
          "affiliation": null
        },
        {
          "name": "Kanishka Rao",
          "affiliation": null
        },
        {
          "name": "Keerthana Gopalakrishnan",
          "affiliation": null
        },
        {
          "name": "Michael S. Ryoo",
          "affiliation": null
        },
        {
          "name": "Austin Stone",
          "affiliation": null
        },
        {
          "name": "Daniel Kappler",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io",
      "publishedDate": "2022-09-20T17:29:56Z",
      "updatedDate": "2022-10-15T07:05:36Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.09874v2",
      "arxivUrl": "https://arxiv.org/abs/2209.09874",
      "comment": "v2, added references to concurrent work and acknowledgments",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "planning",
        "agents",
        "prompting",
        "robotics"
      ],
      "tags": {
        "auto": [
          "planning",
          "agents",
          "prompting",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.13266",
      "title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
      "authors": [
        {
          "name": "Kaizhi Zheng",
          "affiliation": null
        },
        {
          "name": "Kaiwen Zhou",
          "affiliation": null
        },
        {
          "name": "Jing Gu",
          "affiliation": null
        },
        {
          "name": "Yue Fan",
          "affiliation": null
        },
        {
          "name": "Jialu Wang",
          "affiliation": null
        },
        {
          "name": "Zonglin Di",
          "affiliation": null
        },
        {
          "name": "Xuehai He",
          "affiliation": null
        },
        {
          "name": "Xin Eric Wang",
          "affiliation": null
        }
      ],
      "abstract": "Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.",
      "publishedDate": "2022-08-28T18:30:46Z",
      "updatedDate": "2025-09-02T22:12:53Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.13266v4",
      "arxivUrl": "https://arxiv.org/abs/2208.13266",
      "comment": "19th International Conference on Neurosymbolic Learning and Reasoning",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "agents",
        "planning",
        "reasoning",
        "multi-agent",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "planning",
          "reasoning",
          "multi-agent",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.00636",
      "title": "Interacting with next-phrase suggestions: How suggestion systems aid and influence the cognitive processes of writing",
      "authors": [
        {
          "name": "Advait Bhat",
          "affiliation": null
        },
        {
          "name": "Saaket Agashe",
          "affiliation": null
        },
        {
          "name": "Niharika Mohile",
          "affiliation": null
        },
        {
          "name": "Parth Oberoi",
          "affiliation": null
        },
        {
          "name": "Ravi Jangir",
          "affiliation": null
        },
        {
          "name": "Anirudha Joshi",
          "affiliation": null
        }
      ],
      "abstract": "Writing with next-phrase suggestions powered by large language models is becoming more pervasive by the day. However, research to understand writers' interaction and decision-making processes while engaging with such systems is still emerging. We conducted a qualitative study to shed light on writers' cognitive processes while writing with next-phrase suggestion systems. To do so, we recruited 14 amateur writers to write two reviews each, one without suggestions and one with suggestions. Additionally, we also positively and negatively biased the suggestion system to get a diverse range of instances where writers' opinions and the bias in the language model align or misalign to varying degrees. We found that writers interact with next-phrase suggestions in various complex ways: Writers abstracted and extracted multiple parts of the suggestions and incorporated them within their writing, even when they disagreed with the suggestion as a whole; along with evaluating the suggestions on various criteria. The suggestion system also had various effects on the writing process, such as altering the writer's usual writing plans, leading to higher levels of distraction etc. Based on our qualitative analysis using the cognitive process model of writing by Hayes as a lens, we propose a theoretical model of 'writer-suggestion interaction' for writing with GPT-2 (and causal language models in general) for a movie review writing task, followed by directions for future research and design.",
      "publishedDate": "2022-08-01T06:49:07Z",
      "updatedDate": "2023-01-25T02:24:33Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.00636v2",
      "arxivUrl": "https://arxiv.org/abs/2208.00636",
      "comment": null,
      "journalRef": null,
      "doi": "10.1145/3581641.3584060",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2207.05608",
      "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
      "authors": [
        {
          "name": "Wenlong Huang",
          "affiliation": null
        },
        {
          "name": "Fei Xia",
          "affiliation": null
        },
        {
          "name": "Ted Xiao",
          "affiliation": null
        },
        {
          "name": "Harris Chan",
          "affiliation": null
        },
        {
          "name": "Jacky Liang",
          "affiliation": null
        },
        {
          "name": "Pete Florence",
          "affiliation": null
        },
        {
          "name": "Andy Zeng",
          "affiliation": null
        },
        {
          "name": "Jonathan Tompson",
          "affiliation": null
        },
        {
          "name": "Igor Mordatch",
          "affiliation": null
        },
        {
          "name": "Yevgen Chebotar",
          "affiliation": null
        },
        {
          "name": "Pierre Sermanet",
          "affiliation": null
        },
        {
          "name": "Noah Brown",
          "affiliation": null
        },
        {
          "name": "Tomas Jackson",
          "affiliation": null
        },
        {
          "name": "Linda Luu",
          "affiliation": null
        },
        {
          "name": "Sergey Levine",
          "affiliation": null
        },
        {
          "name": "Karol Hausman",
          "affiliation": null
        },
        {
          "name": "Brian Ichter",
          "affiliation": null
        }
      ],
      "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",
      "publishedDate": "2022-07-12T15:20:48Z",
      "updatedDate": "2022-07-12T15:20:48Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.05608v1",
      "arxivUrl": "https://arxiv.org/abs/2207.05608",
      "comment": "Project website: https://innermonologue.github.io",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "agents",
        "reasoning",
        "planning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "robotics",
          "agents",
          "reasoning",
          "planning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.10498",
      "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
      "authors": [
        {
          "name": "Karthik Valmeekam",
          "affiliation": null
        },
        {
          "name": "Matthew Marquez",
          "affiliation": null
        },
        {
          "name": "Alberto Olmo",
          "affiliation": null
        },
        {
          "name": "Sarath Sreedharan",
          "affiliation": null
        },
        {
          "name": "Subbarao Kambhampati",
          "affiliation": null
        }
      ],
      "abstract": "Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",
      "publishedDate": "2022-06-21T16:15:27Z",
      "updatedDate": "2023-11-26T01:15:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.10498v4",
      "arxivUrl": "https://arxiv.org/abs/2206.10498",
      "comment": "NeurIPS 2023 Track on Datasets and Benchmarks",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "planning",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.02928",
      "title": "Neuro-Symbolic Procedural Planning with Commonsense Prompting",
      "authors": [
        {
          "name": "Yujie Lu",
          "affiliation": null
        },
        {
          "name": "Weixi Feng",
          "affiliation": null
        },
        {
          "name": "Wanrong Zhu",
          "affiliation": null
        },
        {
          "name": "Wenda Xu",
          "affiliation": null
        },
        {
          "name": "Xin Eric Wang",
          "affiliation": null
        },
        {
          "name": "Miguel Eckstein",
          "affiliation": null
        },
        {
          "name": "William Yang Wang",
          "affiliation": null
        }
      ],
      "abstract": "Procedural planning aims to implement complex high-level goals by decomposition into sequential simpler low-level steps. Although procedural planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures. Previous methods require manual exemplars to acquire procedural planning knowledge from LLMs in the zero-shot setting. However, such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with commonsense-infused prompting. To mitigate spurious goal-step correlations, we use symbolic program executors on the latent procedural representations to formalize prompts from commonsense knowledge bases as a causal intervention toward the Structural Causal Model. Both automatic and human evaluations on WikiHow and RobotHow show the superiority of PLAN on procedural planning without further training or manual exemplars.",
      "publishedDate": "2022-06-06T22:09:52Z",
      "updatedDate": "2023-02-16T08:18:32Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.02928v6",
      "arxivUrl": "https://arxiv.org/abs/2206.02928",
      "comment": "ICLR 2023 Spotlight",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "planning",
        "rag",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "planning",
          "rag",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.14288",
      "title": "Few-shot Subgoal Planning with Language Models",
      "authors": [
        {
          "name": "Lajanugen Logeswaran",
          "affiliation": null
        },
        {
          "name": "Yao Fu",
          "affiliation": null
        },
        {
          "name": "Moontae Lee",
          "affiliation": null
        },
        {
          "name": "Honglak Lee",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained large language models have shown successful progress in many language understanding benchmarks. This work explores the capability of these models to predict actionable plans in real-world environments. Given a text instruction, we show that language priors encoded in pre-trained language models allow us to infer fine-grained subgoal sequences. In contrast to recent methods which make strong assumptions about subgoal supervision, our experiments show that language models can infer detailed subgoal sequences from few training sequences without any fine-tuning. We further propose a simple strategy to re-rank language model predictions based on interaction and feedback from the environment. Combined with pre-trained navigation and visual reasoning components, our approach demonstrates competitive performance on subgoal prediction and task completion in the ALFRED benchmark compared to prior methods that assume more subgoal supervision.",
      "publishedDate": "2022-05-28T01:03:30Z",
      "updatedDate": "2022-05-28T01:03:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.14288v1",
      "arxivUrl": "https://arxiv.org/abs/2205.14288",
      "comment": "NAACL 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "planning",
        "prompting",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "prompting",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.10712",
      "title": "Housekeep: Tidying Virtual Households using Commonsense Reasoning",
      "authors": [
        {
          "name": "Yash Kant",
          "affiliation": null
        },
        {
          "name": "Arun Ramachandran",
          "affiliation": null
        },
        {
          "name": "Sriram Yenamandra",
          "affiliation": null
        },
        {
          "name": "Igor Gilitschenski",
          "affiliation": null
        },
        {
          "name": "Dhruv Batra",
          "affiliation": null
        },
        {
          "name": "Andrew Szot",
          "affiliation": null
        },
        {
          "name": "Harsh Agrawal",
          "affiliation": null
        }
      ],
      "abstract": "We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the home for embodied AI. In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged. Instead, the agent must learn from and is evaluated against human preferences of which objects belong where in a tidy house. Specifically, we collect a dataset of where humans typically place objects in tidy and untidy houses constituting 1799 objects, 268 object categories, 585 placements, and 105 rooms. Next, we propose a modular baseline approach for Housekeep that integrates planning, exploration, and navigation. It leverages a fine-tuned large language model (LLM) trained on an internet text corpus for effective planning. We show that our baseline agent generalizes to rearranging unseen objects in unknown environments. See our webpage for more details: https://yashkant.github.io/housekeep/",
      "publishedDate": "2022-05-22T02:37:09Z",
      "updatedDate": "2022-05-22T02:37:09Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.10712v1",
      "arxivUrl": "https://arxiv.org/abs/2205.10712",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "agents",
        "reasoning",
        "planning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "agents",
          "reasoning",
          "planning",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.05718",
      "title": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
      "authors": [
        {
          "name": "Katherine M. Collins",
          "affiliation": null
        },
        {
          "name": "Catherine Wong",
          "affiliation": null
        },
        {
          "name": "Jiahai Feng",
          "affiliation": null
        },
        {
          "name": "Megan Wei",
          "affiliation": null
        },
        {
          "name": "Joshua B. Tenenbaum",
          "affiliation": null
        }
      ],
      "abstract": "Human language offers a powerful window into our thoughts -- we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.",
      "publishedDate": "2022-05-11T18:14:33Z",
      "updatedDate": "2022-05-11T18:14:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.05718v1",
      "arxivUrl": "https://arxiv.org/abs/2205.05718",
      "comment": "Originally accepted to the 2022 Cognitive Science (CogSci) conference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "evaluation",
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.00598",
      "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
      "authors": [
        {
          "name": "Andy Zeng",
          "affiliation": null
        },
        {
          "name": "Maria Attarian",
          "affiliation": null
        },
        {
          "name": "Brian Ichter",
          "affiliation": null
        },
        {
          "name": "Krzysztof Choromanski",
          "affiliation": null
        },
        {
          "name": "Adrian Wong",
          "affiliation": null
        },
        {
          "name": "Stefan Welker",
          "affiliation": null
        },
        {
          "name": "Federico Tombari",
          "affiliation": null
        },
        {
          "name": "Aveek Purohit",
          "affiliation": null
        },
        {
          "name": "Michael Ryoo",
          "affiliation": null
        },
        {
          "name": "Vikas Sindhwani",
          "affiliation": null
        },
        {
          "name": "Johnny Lee",
          "affiliation": null
        },
        {
          "name": "Vincent Vanhoucke",
          "affiliation": null
        },
        {
          "name": "Pete Florence",
          "affiliation": null
        }
      ],
      "abstract": "Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
      "publishedDate": "2022-04-01T17:43:13Z",
      "updatedDate": "2022-05-27T17:52:50Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.00598v2",
      "arxivUrl": "https://arxiv.org/abs/2204.00598",
      "comment": "https://socraticmodels.github.io/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "tool-use",
        "reasoning",
        "planning",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "tool-use",
          "reasoning",
          "planning",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2201.07207",
      "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
      "authors": [
        {
          "name": "Wenlong Huang",
          "affiliation": null
        },
        {
          "name": "Pieter Abbeel",
          "affiliation": null
        },
        {
          "name": "Deepak Pathak",
          "affiliation": null
        },
        {
          "name": "Igor Mordatch",
          "affiliation": null
        }
      ],
      "abstract": "Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. \"make breakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner",
      "publishedDate": "2022-01-18T18:59:45Z",
      "updatedDate": "2022-03-08T06:47:17Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2201.07207v2",
      "arxivUrl": "https://arxiv.org/abs/2201.07207",
      "comment": "Project website at https://huangwl18.github.io/language-planner",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "agents",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10007",
      "title": "CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context",
      "authors": [
        {
          "name": "Yangruibo Ding",
          "affiliation": null
        },
        {
          "name": "Zijian Wang",
          "affiliation": null
        },
        {
          "name": "Wasi Uddin Ahmad",
          "affiliation": null
        },
        {
          "name": "Murali Krishna Ramanathan",
          "affiliation": null
        },
        {
          "name": "Ramesh Nallapati",
          "affiliation": null
        },
        {
          "name": "Parminder Bhatia",
          "affiliation": null
        },
        {
          "name": "Dan Roth",
          "affiliation": null
        },
        {
          "name": "Bing Xiang",
          "affiliation": null
        }
      ],
      "abstract": "While pre-trained language models (LM) for code have achieved great success in code completion, they generate code conditioned only on the contents within the file, i.e., in-file context, but ignore the rich semantics in other files within the same project, i.e., cross-file context, a critical source of information that is especially useful in modern modular software development. Such overlooking constrains code language models' capacity in code completion, leading to unexpected behaviors such as generating hallucinated class member functions or function calls with unexpected arguments. In this work, we develop a cross-file context finder tool, CCFINDER, that effectively locates and retrieves the most relevant cross-file context. We propose CoCoMIC, a framework that incorporates cross-file context to learn the in-file and cross-file context jointly on top of pretrained code LMs. CoCoMIC successfully improves the existing code LM with a 33.94% relative increase in exact match and a 28.69% relative increase in identifier matching for code completion when the cross-file context is provided.",
      "publishedDate": "2022-12-20T05:48:09Z",
      "updatedDate": "2023-05-24T06:56:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10007v2",
      "arxivUrl": "https://arxiv.org/abs/2212.10007",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.05030",
      "title": "Creative Writing with an AI-Powered Writing Assistant: Perspectives from Professional Writers",
      "authors": [
        {
          "name": "Daphne Ippolito",
          "affiliation": null
        },
        {
          "name": "Ann Yuan",
          "affiliation": null
        },
        {
          "name": "Andy Coenen",
          "affiliation": null
        },
        {
          "name": "Sehmon Burnam",
          "affiliation": null
        }
      ],
      "abstract": "Recent developments in natural language generation (NLG) using neural language models have brought us closer than ever to the goal of building AI-powered creative writing tools. However, most prior work on human-AI collaboration in the creative writing domain has evaluated new systems with amateur writers, typically in contrived user studies of limited scope. In this work, we commissioned 13 professional, published writers from a diverse set of creative writing backgrounds to craft stories using Wordcraft, a text editor with built-in AI-powered writing assistance tools. Using interviews and participant journals, we discuss the potential of NLG to have significant impact in the creative writing domain--especially with respect to brainstorming, generation of story details, world-building, and research assistance. Experienced writers, more so than amateurs, typically have well-developed systems and methodologies for writing, as well as distinctive voices and target audiences. Our work highlights the challenges in building for these writers; NLG technologies struggle to preserve style and authorial voice, and they lack deep understanding of story contents. In order for AI-powered writing assistants to realize their full potential, it is essential that they take into account the diverse goals and expertise of human writers.",
      "publishedDate": "2022-11-09T17:00:56Z",
      "updatedDate": "2022-11-09T17:00:56Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.05030v1",
      "arxivUrl": "https://arxiv.org/abs/2211.05030",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.10342",
      "title": "Language Model Cascades",
      "authors": [
        {
          "name": "David Dohan",
          "affiliation": null
        },
        {
          "name": "Winnie Xu",
          "affiliation": null
        },
        {
          "name": "Aitor Lewkowycz",
          "affiliation": null
        },
        {
          "name": "Jacob Austin",
          "affiliation": null
        },
        {
          "name": "David Bieber",
          "affiliation": null
        },
        {
          "name": "Raphael Gontijo Lopes",
          "affiliation": null
        },
        {
          "name": "Yuhuai Wu",
          "affiliation": null
        },
        {
          "name": "Henryk Michalewski",
          "affiliation": null
        },
        {
          "name": "Rif A. Saurous",
          "affiliation": null
        },
        {
          "name": "Jascha Sohl-dickstein",
          "affiliation": null
        },
        {
          "name": "Kevin Murphy",
          "affiliation": null
        },
        {
          "name": "Charles Sutton",
          "affiliation": null
        }
      ],
      "abstract": "Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.",
      "publishedDate": "2022-07-21T07:35:18Z",
      "updatedDate": "2022-07-28T16:51:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.10342v2",
      "arxivUrl": "https://arxiv.org/abs/2207.10342",
      "comment": "Presented as spotlight at the Beyond Bases workshop at ICML 2022 (https://beyond-bayes.github.io)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "tool-use",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "tool-use",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.03214",
      "title": "Transformer-Based Language Models for Software Vulnerability Detection",
      "authors": [
        {
          "name": "Chandra Thapa",
          "affiliation": null
        },
        {
          "name": "Seung Ick Jang",
          "affiliation": null
        },
        {
          "name": "Muhammad Ejaz Ahmed",
          "affiliation": null
        },
        {
          "name": "Seyit Camtepe",
          "affiliation": null
        },
        {
          "name": "Josef Pieprzyk",
          "affiliation": null
        },
        {
          "name": "Surya Nepal",
          "affiliation": null
        }
      ],
      "abstract": "The large transformer-based language models demonstrate excellent performance in natural language processing. By considering the transferability of the knowledge gained by these models in one domain to other related domains, and the closeness of natural languages to high-level programming languages, such as C/C++, this work studies how to leverage (large) transformer-based language models in detecting software vulnerabilities and how good are these models for vulnerability detection tasks. In this regard, firstly, a systematic (cohesive) framework that details source code translation, model preparation, and inference is presented. Then, an empirical analysis is performed with software vulnerability datasets with C/C++ source codes having multiple vulnerabilities corresponding to the library function call, pointer usage, array usage, and arithmetic expression. Our empirical results demonstrate the good performance of the language models in vulnerability detection. Moreover, these language models have better performance metrics, such as F1-score, than the contemporary models, namely bidirectional long short-term memory and bidirectional gated recurrent unit. Experimenting with the language models is always challenging due to the requirement of computing resources, platforms, libraries, and dependencies. Thus, this paper also analyses the popular platforms to efficiently fine-tune these models and present recommendations while choosing the platforms.",
      "publishedDate": "2022-04-07T04:57:42Z",
      "updatedDate": "2022-09-06T01:31:22Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.03214v2",
      "arxivUrl": "https://arxiv.org/abs/2204.03214",
      "comment": "16 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.12460",
      "title": "Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation",
      "authors": [
        {
          "name": "Xueliang Zhao",
          "affiliation": null
        },
        {
          "name": "Yuxuan Wang",
          "affiliation": null
        },
        {
          "name": "Chongyang Tao",
          "affiliation": null
        },
        {
          "name": "Chenshuo Wang",
          "affiliation": null
        },
        {
          "name": "Dongyan Zhao",
          "affiliation": null
        }
      ],
      "abstract": "We study video-grounded dialogue generation, where a response is generated based on the dialogue context and the associated video. The primary challenges of this task lie in (1) the difficulty of integrating video data into pre-trained language models (PLMs) which presents obstacles to exploiting the power of large-scale pre-training; and (2) the necessity of taking into account the complementarity of various modalities throughout the reasoning process. Although having made remarkable progress in video-grounded dialogue generation, existing methods still fall short when it comes to integrating with PLMs in a way that allows information from different modalities to complement each other. To alleviate these issues, we first propose extracting pertinent information from videos and turning it into reasoning paths that are acceptable to PLMs. Additionally, we propose a multi-agent reinforcement learning method to collaboratively perform reasoning on different modalities (i.e., video and dialogue context). Empirical experiment results on two public datasets indicate that the proposed model can significantly outperform state-of-the-art models by large margins on both automatic and human evaluations.",
      "publishedDate": "2022-10-22T14:45:29Z",
      "updatedDate": "2022-10-22T14:45:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.12460v1",
      "arxivUrl": "https://arxiv.org/abs/2210.12460",
      "comment": "To appear at EMNLP 2022 findings",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.00849",
      "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
      "authors": [
        {
          "name": "Oren Neumann",
          "affiliation": null
        },
        {
          "name": "Claudius Gros",
          "affiliation": null
        }
      ],
      "abstract": "The recent observation of neural power-law scaling relations has made a significant impact in the field of deep learning. A substantial amount of attention has been dedicated as a consequence to the description of scaling laws, although mostly for supervised learning and only to a reduced extent for reinforcement learning frameworks. In this paper we present an extensive study of performance scaling for a cornerstone reinforcement learning algorithm, AlphaZero. On the basis of a relationship between Elo rating, playing strength and power-law scaling, we train AlphaZero agents on the games Connect Four and Pentago and analyze their performance. We find that player strength scales as a power law in neural network parameter count when not bottlenecked by available compute, and as a power of compute when training optimally sized agents. We observe nearly identical scaling exponents for both games. Combining the two observed scaling laws we obtain a power law relating optimal size to compute similar to the ones observed for language models. We find that the predicted scaling of optimal neural network size fits our data for both games. This scaling law implies that previously published state-of-the-art game-playing models are significantly smaller than their optimal size, given the respective compute budgets. We also show that large AlphaZero models are more sample efficient, performing better than smaller models with the same amount of training data.",
      "publishedDate": "2022-09-29T19:08:51Z",
      "updatedDate": "2023-02-13T15:33:31Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.00849v2",
      "arxivUrl": "https://arxiv.org/abs/2210.00849",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.07998",
      "title": "What Artificial Neural Networks Can Tell Us About Human Language Acquisition",
      "authors": [
        {
          "name": "Alex Warstadt",
          "affiliation": null
        },
        {
          "name": "Samuel R. Bowman",
          "affiliation": null
        }
      ],
      "abstract": "Rapid progress in machine learning for natural language processing has the potential to transform debates about how humans learn language. However, the learning environments and biases of current artificial learners and humans diverge in ways that weaken the impact of the evidence obtained from learning simulations. For example, today's most effective neural language models are trained on roughly one thousand times the amount of linguistic data available to a typical child. To increase the relevance of learnability results from computational models, we need to train model learners without significant advantages over humans. If an appropriate model successfully acquires some target linguistic knowledge, it can provide a proof of concept that the target is learnable in a hypothesized human learning scenario. Plausible model learners will enable us to carry out experimental manipulations to make causal inferences about variables in the learning environment, and to rigorously test poverty-of-the-stimulus-style claims arguing for innate linguistic knowledge in humans on the basis of speculations about learnability. Comparable experiments will never be possible with human subjects due to practical and ethical considerations, making model learners an indispensable resource. So far, attempts to deprive current models of unfair advantages obtain sub-human results for key grammatical behaviors such as acceptability judgments. But before we can justifiably conclude that language learning requires more prior domain-specific knowledge than current models possess, we must first explore non-linguistic inputs in the form of multimodal stimuli and multi-agent interaction as ways to make our learners more efficient at learning from limited linguistic input.",
      "publishedDate": "2022-08-17T00:12:37Z",
      "updatedDate": "2024-02-11T21:24:26Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.07998v2",
      "arxivUrl": "https://arxiv.org/abs/2208.07998",
      "comment": "Please cite the published version with the following information: @incollection{warstadt2022artificial, title={What artificial neural networks can tell us about human language acquisition}, author={Warstadt, Alex and Bowman, Samuel R.}, booktitle={Algebraic Structures in Natural Language}, pages={17--60}, year={2022}, publisher={CRC Press} }",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "tool-use",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.08349",
      "title": "Know your audience: specializing grounded language models with listener subtraction",
      "authors": [
        {
          "name": "Aaditya K. Singh",
          "affiliation": null
        },
        {
          "name": "David Ding",
          "affiliation": null
        },
        {
          "name": "Andrew Saxe",
          "affiliation": null
        },
        {
          "name": "Felix Hill",
          "affiliation": null
        },
        {
          "name": "Andrew K. Lampinen",
          "affiliation": null
        }
      ],
      "abstract": "Effective communication requires adapting to the idiosyncrasies of each communicative context--such as the common ground shared with each partner. Humans demonstrate this ability to specialize to their audience in many contexts, such as the popular game Dixit. We take inspiration from Dixit to formulate a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it among distractors, but another listener cannot. To adapt, the speaker must exploit differences in the knowledge it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision. Through controlled experiments, we show that training a speaker with two listeners that perceive differently, using our method, allows the speaker to adapt to the idiosyncracies of the listeners. Furthermore, we show zero-shot transfer of the specialization to real-world data. Our experiments demonstrate a method for specializing grounded language models without direct supervision and highlight the interesting research challenges posed by complex multi-agent communication.",
      "publishedDate": "2022-06-16T17:52:08Z",
      "updatedDate": "2023-05-01T20:39:20Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.08349v2",
      "arxivUrl": "https://arxiv.org/abs/2206.08349",
      "comment": "28 pages, 9 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "multi-agent",
        "agents",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2202.03632",
      "title": "ECRECer: Enzyme Commission Number Recommendation and Benchmarking based on Multiagent Dual-core Learning",
      "authors": [
        {
          "name": "Zhenkun Shi",
          "affiliation": null
        },
        {
          "name": "Qianqian Yuan",
          "affiliation": null
        },
        {
          "name": "Ruoyu Wang",
          "affiliation": null
        },
        {
          "name": "Hoaran Li",
          "affiliation": null
        },
        {
          "name": "Xiaoping Liao",
          "affiliation": null
        },
        {
          "name": "Hongwu Ma",
          "affiliation": null
        }
      ],
      "abstract": "Enzyme Commission (EC) numbers, which associate a protein sequence with the biochemical reactions it catalyzes, are essential for the accurate understanding of enzyme functions and cellular metabolism. Many ab-initio computational approaches were proposed to predict EC numbers for given input sequences directly. However, the prediction performance (accuracy, recall, precision), usability, and efficiency of existing methods still have much room to be improved. Here, we report ECRECer, a cloud platform for accurately predicting EC numbers based on novel deep learning techniques. To build ECRECer, we evaluate different protein representation methods and adopt a protein language model for protein sequence embedding. After embedding, we propose a multi-agent hierarchy deep learning-based framework to learn the proposed tasks in a multi-task manner. Specifically, we used an extreme multi-label classifier to perform the EC prediction and employed a greedy strategy to integrate and fine-tune the final model. Comparative analyses against four representative methods demonstrate that ECRECer delivers the highest performance, which improves accuracy and F1 score by 70% and 20% over the state-of-the-the-art, respectively. With ECRECer, we can annotate numerous enzymes in the Swiss-Prot database with incomplete EC numbers to their full fourth level. Take UniPort protein \"A0A0U5GJ41\" as an example (1.14.-.-), ECRECer annotated it with \"1.14.11.38\", which supported by further protein structure analysis based on AlphaFold2. Finally, we established a webserver (https://ecrecer.biodesign.ac.cn) and provided an offline bundle to improve usability.",
      "publishedDate": "2022-02-08T04:00:49Z",
      "updatedDate": "2022-02-08T04:00:49Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2202.03632v1",
      "arxivUrl": "https://arxiv.org/abs/2202.03632",
      "comment": "16 pages, 14 figures",
      "journalRef": "Research. 2023:6;0153",
      "doi": "10.34133/research.0153",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2301.00234",
      "title": "A Survey on In-context Learning",
      "authors": [
        {
          "name": "Qingxiu Dong",
          "affiliation": null
        },
        {
          "name": "Lei Li",
          "affiliation": null
        },
        {
          "name": "Damai Dai",
          "affiliation": null
        },
        {
          "name": "Ce Zheng",
          "affiliation": null
        },
        {
          "name": "Jingyuan Ma",
          "affiliation": null
        },
        {
          "name": "Rui Li",
          "affiliation": null
        },
        {
          "name": "Heming Xia",
          "affiliation": null
        },
        {
          "name": "Jingjing Xu",
          "affiliation": null
        },
        {
          "name": "Zhiyong Wu",
          "affiliation": null
        },
        {
          "name": "Tianyu Liu",
          "affiliation": null
        },
        {
          "name": "Baobao Chang",
          "affiliation": null
        },
        {
          "name": "Xu Sun",
          "affiliation": null
        },
        {
          "name": "Lei Li",
          "affiliation": null
        },
        {
          "name": "Zhifang Sui",
          "affiliation": null
        }
      ],
      "abstract": "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
      "publishedDate": "2022-12-31T15:57:09Z",
      "updatedDate": "2024-10-05T11:47:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2301.00234v6",
      "arxivUrl": "https://arxiv.org/abs/2301.00234",
      "comment": "Update",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.14402",
      "title": "GPT Takes the Bar Exam",
      "authors": [
        {
          "name": "Michael Bommarito",
          "affiliation": null
        },
        {
          "name": "Daniel Martin Katz",
          "affiliation": null
        }
      ],
      "abstract": "Nearly all jurisdictions in the United States require a professional license exam, commonly referred to as \"the Bar Exam,\" as a precondition for law practice. To even sit for the exam, most jurisdictions require that an applicant completes at least seven years of post-secondary education, including three years at an accredited law school. In addition, most test-takers also undergo weeks to months of further, exam-specific preparation. Despite this significant investment of time and capital, approximately one in five test-takers still score under the rate required to pass the exam on their first try. In the face of a complex task that requires such depth of knowledge, what, then, should we expect of the state of the art in \"AI?\" In this research, we document our experimental evaluation of the performance of OpenAI's `text-davinci-003` model, often-referred to as GPT-3.5, on the multistate multiple choice (MBE) section of the exam. While we find no benefit in fine-tuning over GPT-3.5's zero-shot performance at the scale of our training data, we do find that hyperparameter optimization and prompt engineering positively impacted GPT-3.5's zero-shot performance. For best prompt and parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete NCBE MBE practice exam, significantly in excess of the 25% baseline guessing rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's ranking of responses is also highly-correlated with correctness; its top two and top three choices are correct 71% and 88% of the time, respectively, indicating very strong non-entailment performance. While our ability to interpret these results is limited by nascent scientific understanding of LLMs and the proprietary nature of GPT, we believe that these results strongly suggest that an LLM will pass the MBE component of the Bar Exam in the near future.",
      "publishedDate": "2022-12-29T18:19:43Z",
      "updatedDate": "2022-12-29T18:19:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.14402v1",
      "arxivUrl": "https://arxiv.org/abs/2212.14402",
      "comment": "Additional material available online at https://github.com/mjbommar/gpt-takes-the-bar-exam",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.14047",
      "title": "Using Large Language Models to Generate Engaging Captions for Data Visualizations",
      "authors": [
        {
          "name": "Ashley Liew",
          "affiliation": null
        },
        {
          "name": "Klaus Mueller",
          "affiliation": null
        }
      ],
      "abstract": "Creating compelling captions for data visualizations has been a longstanding challenge. Visualization researchers are typically untrained in journalistic reporting and hence the captions that are placed below data visualizations tend to be not overly engaging and rather just stick to basic observations about the data. In this work we explore the opportunities offered by the newly emerging crop of large language models (LLM) which use sophisticated deep learning technology to produce human-like prose. We ask, can these powerful software devices be purposed to produce engaging captions for generic data visualizations like a scatterplot. It turns out that the key challenge lies in designing the most effective prompt for the LLM, a task called prompt engineering. We report on first experiments using the popular LLM GPT-3 and deliver some promising results.",
      "publishedDate": "2022-12-27T23:56:57Z",
      "updatedDate": "2022-12-27T23:56:57Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.14047v1",
      "arxivUrl": "https://arxiv.org/abs/2212.14047",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10461",
      "title": "Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models",
      "authors": [
        {
          "name": "Jingjing Xu",
          "affiliation": null
        },
        {
          "name": "Qingxiu Dong",
          "affiliation": null
        },
        {
          "name": "Hongyi Liu",
          "affiliation": null
        },
        {
          "name": "Lei Li",
          "affiliation": null
        }
      ],
      "abstract": "With increasing scale, large language models demonstrate both quantitative improvement and new qualitative capabilities, especially as zero-shot learners, like GPT-3. However, these results rely heavily on delicate prompt design and large computation. In this work, we explore whether the strong zero-shot ability could be achieved at a smaller model scale without any external supervised data. To achieve this goal, we revisit masked language modeling and present a geometry-guided self-supervised learning method (Go-tuningfor short) by taking a small number of task-aware self-supervised data to update language models further. Experiments show that Go-tuning can enable T5-small (80M) competitive zero-shot results compared with large language models, such as T5-XL (3B). We also apply Go-tuning on multi-task settings and develop a multi-task model, mgo-T5 (250M). It can reach the average performance of OPT (175B) on 9 datasets.",
      "publishedDate": "2022-12-20T17:36:49Z",
      "updatedDate": "2022-12-20T17:36:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10461v1",
      "arxivUrl": "https://arxiv.org/abs/2212.10461",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.09611",
      "title": "Optimizing Prompts for Text-to-Image Generation",
      "authors": [
        {
          "name": "Yaru Hao",
          "affiliation": null
        },
        {
          "name": "Zewen Chi",
          "affiliation": null
        },
        {
          "name": "Li Dong",
          "affiliation": null
        },
        {
          "name": "Furu Wei",
          "affiliation": null
        }
      ],
      "abstract": "Well-designed prompts can guide text-to-image models to generate amazing images. However, the performant prompts are often model-specific and misaligned with user input. Instead of laborious human engineering, we propose prompt adaptation, a general framework that automatically adapts original user input to model-preferred prompts. Specifically, we first perform supervised fine-tuning with a pretrained language model on a small collection of manually engineered prompts. Then we use reinforcement learning to explore better prompts. We define a reward function that encourages the policy to generate more aesthetically pleasing images while preserving the original user intentions. Experimental results on Stable Diffusion show that our method outperforms manual prompt engineering in terms of both automatic metrics and human preference ratings. Moreover, reinforcement learning further boosts performance, especially on out-of-domain prompts. The pretrained checkpoints are available at https://aka.ms/promptist. The demo can be found at https://aka.ms/promptist-demo.",
      "publishedDate": "2022-12-19T16:50:41Z",
      "updatedDate": "2023-12-29T10:15:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.09611v2",
      "arxivUrl": "https://arxiv.org/abs/2212.09611",
      "comment": "Accepted by NeurIPS-23",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.09603",
      "title": "Explanation Regeneration via Information Bottleneck",
      "authors": [
        {
          "name": "Qintong Li",
          "affiliation": null
        },
        {
          "name": "Zhiyong Wu",
          "affiliation": null
        },
        {
          "name": "Lingpeng Kong",
          "affiliation": null
        },
        {
          "name": "Wei Bi",
          "affiliation": null
        }
      ],
      "abstract": "Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Due to the superior generative capacity of large pretrained language models, recent work built on prompt engineering enables explanation generation without specific training. However, explanation generated through single-pass prompting often lacks sufficiency and conciseness. To address this problem, we develop an information bottleneck method EIB to produce refined explanations that are sufficient and concise. Our approach regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained. Experiments on two out-of-domain tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation.",
      "publishedDate": "2022-12-19T16:41:19Z",
      "updatedDate": "2023-07-11T05:17:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.09603v2",
      "arxivUrl": "https://arxiv.org/abs/2212.09603",
      "comment": "Accepted in ACL2023 Findings",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.08420",
      "title": "Fake it till you make it: Learning transferable representations from synthetic ImageNet clones",
      "authors": [
        {
          "name": "Mert Bulent Sariyildiz",
          "affiliation": null
        },
        {
          "name": "Karteek Alahari",
          "affiliation": null
        },
        {
          "name": "Diane Larlus",
          "affiliation": null
        },
        {
          "name": "Yannis Kalantidis",
          "affiliation": null
        }
      ],
      "abstract": "Recent image generation models such as Stable Diffusion have exhibited an impressive ability to generate fairly realistic images starting from a simple text prompt. Could such models render real images obsolete for training image prediction models? In this paper, we answer part of this provocative question by investigating the need for real images when training models for ImageNet classification. Provided only with the class names that have been used to build the dataset, we explore the ability of Stable Diffusion to generate synthetic clones of ImageNet and measure how useful these are for training classification models from scratch. We show that with minimal and class-agnostic prompt engineering, ImageNet clones are able to close a large part of the gap between models produced by synthetic images and models trained with real images, for the several standard classification benchmarks that we consider in this study. More importantly, we show that models trained on synthetic images exhibit strong generalization properties and perform on par with models trained on real data for transfer. Project page: https://europe.naverlabs.com/imagenet-sd/",
      "publishedDate": "2022-12-16T11:44:01Z",
      "updatedDate": "2023-03-28T08:11:53Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.08420v2",
      "arxivUrl": "https://arxiv.org/abs/2212.08420",
      "comment": "Accepted to CVPR 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.07507",
      "title": "Artificial Intelligence for Health Message Generation: Theory, Method, and an Empirical Study Using Prompt Engineering",
      "authors": [
        {
          "name": "Sue Lim",
          "affiliation": null
        },
        {
          "name": "Ralf Schmälzle",
          "affiliation": null
        }
      ],
      "abstract": "This study introduces and examines the potential of an AI system to generate health awareness messages. The topic of folic acid, a vitamin that is critical during pregnancy, served as a test case. Using prompt engineering, we generated messages that could be used to raise awareness and compared them to retweeted human-generated messages via computational and human evaluation methods. The system was easy to use and prolific, and computational analyses revealed that the AI-generated messages were on par with human-generated ones in terms of sentiment, reading ease, and semantic content. Also, the human evaluation study showed that AI-generated messages ranked higher in message quality and clarity. We discuss the theoretical, practical, and ethical implications of these results.",
      "publishedDate": "2022-12-14T21:13:08Z",
      "updatedDate": "2022-12-14T21:13:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.07507v1",
      "arxivUrl": "https://arxiv.org/abs/2212.07507",
      "comment": "26 pages including references, 3 figures",
      "journalRef": "Frontiers in Communication, 8, 1129082 (2023)",
      "doi": "10.3389/fcomm.2023.1129082",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.07476",
      "title": "The Infinite Index: Information Retrieval on Generative Text-To-Image Models",
      "authors": [
        {
          "name": "Niklas Deckers",
          "affiliation": null
        },
        {
          "name": "Maik Fröbe",
          "affiliation": null
        },
        {
          "name": "Johannes Kiesel",
          "affiliation": null
        },
        {
          "name": "Gianluca Pandolfo",
          "affiliation": null
        },
        {
          "name": "Christopher Schröder",
          "affiliation": null
        },
        {
          "name": "Benno Stein",
          "affiliation": null
        },
        {
          "name": "Martin Potthast",
          "affiliation": null
        }
      ],
      "abstract": "Conditional generative models such as DALL-E and Stable Diffusion generate images based on a user-defined text, the prompt. Finding and refining prompts that produce a desired image has become the art of prompt engineering. Generative models do not provide a built-in retrieval model for a user's information need expressed through prompts. In light of an extensive literature review, we reframe prompt engineering for generative models as interactive text-based retrieval on a novel kind of \"infinite index\". We apply these insights for the first time in a case study on image generation for game design with an expert. Finally, we envision how active learning may help to guide the retrieval of generated images.",
      "publishedDate": "2022-12-14T19:50:35Z",
      "updatedDate": "2023-01-21T18:16:14Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.CL",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.07476v2",
      "arxivUrl": "https://arxiv.org/abs/2212.07476",
      "comment": "Final version for CHIIR 2023",
      "journalRef": null,
      "doi": "10.1145/3576840.3578327",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.03814",
      "title": "iQuery: Instruments as Queries for Audio-Visual Sound Separation",
      "authors": [
        {
          "name": "Jiaben Chen",
          "affiliation": null
        },
        {
          "name": "Renrui Zhang",
          "affiliation": null
        },
        {
          "name": "Dongze Lian",
          "affiliation": null
        },
        {
          "name": "Jiaqi Yang",
          "affiliation": null
        },
        {
          "name": "Ziyao Zeng",
          "affiliation": null
        },
        {
          "name": "Jianbo Shi",
          "affiliation": null
        }
      ],
      "abstract": "Current audio-visual separation methods share a standard architecture design where an audio encoder-decoder network is fused with visual encoding features at the encoder bottleneck. This design confounds the learning of multi-modal feature encoding with robust sound decoding for audio separation. To generalize to a new instrument: one must finetune the entire visual and audio network for all musical instruments. We re-formulate visual-sound separation task and propose Instrument as Query (iQuery) with a flexible query expansion mechanism. Our approach ensures cross-modal consistency and cross-instrument disentanglement. We utilize \"visually named\" queries to initiate the learning of audio queries and use cross-modal attention to remove potential sound source interference at the estimated waveforms. To generalize to a new instrument or event class, drawing inspiration from the text-prompt design, we insert an additional query as an audio prompt while freezing the attention mechanism. Experimental results on three benchmarks demonstrate that our iQuery improves audio-visual sound source separation performance.",
      "publishedDate": "2022-12-07T17:55:06Z",
      "updatedDate": "2022-12-08T16:33:58Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.03814v2",
      "arxivUrl": "https://arxiv.org/abs/2212.03814",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.02199",
      "title": "Legal Prompt Engineering for Multilingual Legal Judgement Prediction",
      "authors": [
        {
          "name": "Dietrich Trautmann",
          "affiliation": null
        },
        {
          "name": "Alina Petrova",
          "affiliation": null
        },
        {
          "name": "Frank Schilder",
          "affiliation": null
        }
      ],
      "abstract": "Legal Prompt Engineering (LPE) or Legal Prompting is a process to guide and assist a large language model (LLM) with performing a natural legal language processing (NLLP) skill. Our goal is to use LPE with LLMs over long legal documents for the Legal Judgement Prediction (LJP) task. We investigate the performance of zero-shot LPE for given facts in case-texts from the European Court of Human Rights (in English) and the Federal Supreme Court of Switzerland (in German, French and Italian). Our results show that zero-shot LPE is better compared to the baselines, but it still falls short compared to current state of the art supervised approaches. Nevertheless, the results are important, since there was 1) no explicit domain-specific data used - so we show that the transfer to the legal domain is possible for general-purpose LLMs, and 2) the LLMs where directly applied without any further training or fine-tuning - which in turn saves immensely in terms of additional computational costs.",
      "publishedDate": "2022-12-05T12:17:02Z",
      "updatedDate": "2022-12-05T12:17:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.02199v1",
      "arxivUrl": "https://arxiv.org/abs/2212.02199",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.01803",
      "title": "Controllable Image Captioning via Prompting",
      "authors": [
        {
          "name": "Ning Wang",
          "affiliation": null
        },
        {
          "name": "Jiahao Xie",
          "affiliation": null
        },
        {
          "name": "Jihao Wu",
          "affiliation": null
        },
        {
          "name": "Mingbo Jia",
          "affiliation": null
        },
        {
          "name": "Linlin Li",
          "affiliation": null
        }
      ],
      "abstract": "Despite the remarkable progress of image captioning, existing captioners typically lack the controllable capability to generate desired image captions, e.g., describing the image in a rough or detailed manner, in a factual or emotional view, etc. In this paper, we show that a unified model is qualified to perform well in diverse domains and freely switch among multiple styles. Such a controllable capability is achieved by embedding the prompt learning into the image captioning framework. To be specific, we design a set of prompts to fine-tune the pre-trained image captioner. These prompts allow the model to absorb stylized data from different domains for joint training, without performance degradation in each domain. Furthermore, we optimize the prompts with learnable vectors in the continuous word embedding space, avoiding the heuristic prompt engineering and meanwhile exhibiting superior performance. In the inference stage, our model is able to generate desired stylized captions by choosing the corresponding prompts. Extensive experiments verify the controllable capability of the proposed method. Notably, we achieve outstanding performance on two diverse image captioning benchmarks including COCO Karpathy split and TextCaps using a unified model.",
      "publishedDate": "2022-12-04T11:59:31Z",
      "updatedDate": "2022-12-04T11:59:31Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.01803v1",
      "arxivUrl": "https://arxiv.org/abs/2212.01803",
      "comment": "To appear in AAAI 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.01326",
      "title": "Legal Prompting: Teaching a Language Model to Think Like a Lawyer",
      "authors": [
        {
          "name": "Fangyi Yu",
          "affiliation": null
        },
        {
          "name": "Lee Quartey",
          "affiliation": null
        },
        {
          "name": "Frank Schilder",
          "affiliation": null
        }
      ],
      "abstract": "Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while CoT prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.",
      "publishedDate": "2022-12-02T17:41:22Z",
      "updatedDate": "2022-12-08T22:28:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.01326v2",
      "arxivUrl": "https://arxiv.org/abs/2212.01326",
      "comment": "12 pages, 6 figures, 4 tables. Accepted by NLLP 2022 (EMNLP workshop)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.14995",
      "title": "Arguments to Key Points Mapping with Prompt-based Learning",
      "authors": [
        {
          "name": "Ahnaf Mozib Samin",
          "affiliation": null
        },
        {
          "name": "Behrooz Nikandish",
          "affiliation": null
        },
        {
          "name": "Jingyan Chen",
          "affiliation": null
        }
      ],
      "abstract": "Handling and digesting a huge amount of information in an efficient manner has been a long-term demand in modern society. Some solutions to map key points (short textual summaries capturing essential information and filtering redundancies) to a large number of arguments/opinions have been provided recently (Bar-Haim et al., 2020). To complement the full picture of the argument-to-keypoint mapping task, we mainly propose two approaches in this paper. The first approach is to incorporate prompt engineering for fine-tuning the pre-trained language models (PLMs). The second approach utilizes prompt-based learning in PLMs to generate intermediary texts, which are then combined with the original argument-keypoint pairs and fed as inputs to a classifier, thereby mapping them. Furthermore, we extend the experiments to cross/in-domain to conduct an in-depth analysis. In our evaluation, we find that i) using prompt engineering in a more direct way (Approach 1) can yield promising results and improve the performance; ii) Approach 2 performs considerably worse than Approach 1 due to the negation issue of the PLM.",
      "publishedDate": "2022-11-28T01:48:29Z",
      "updatedDate": "2022-11-28T01:48:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.14995v1",
      "arxivUrl": "https://arxiv.org/abs/2211.14995",
      "comment": "Accepted at ICNLSP 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.11890",
      "title": "TEMPERA: Test-Time Prompting via Reinforcement Learning",
      "authors": [
        {
          "name": "Tianjun Zhang",
          "affiliation": null
        },
        {
          "name": "Xuezhi Wang",
          "affiliation": null
        },
        {
          "name": "Denny Zhou",
          "affiliation": null
        },
        {
          "name": "Dale Schuurmans",
          "affiliation": null
        },
        {
          "name": "Joseph E. Gonzalez",
          "affiliation": null
        }
      ],
      "abstract": "Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.",
      "publishedDate": "2022-11-21T22:38:20Z",
      "updatedDate": "2022-11-21T22:38:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.11890v1",
      "arxivUrl": "https://arxiv.org/abs/2211.11890",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.15462",
      "title": "Investigating Prompt Engineering in Diffusion Models",
      "authors": [
        {
          "name": "Sam Witteveen",
          "affiliation": null
        },
        {
          "name": "Martin Andrews",
          "affiliation": null
        }
      ],
      "abstract": "With the spread of the use of Text2Img diffusion models such as DALL-E 2, Imagen, Mid Journey and Stable Diffusion, one challenge that artists face is selecting the right prompts to achieve the desired artistic output. We present techniques for measuring the effect that specific words and phrases in prompts have, and (in the Appendix) present guidance on the selection of prompts to produce desired effects.",
      "publishedDate": "2022-11-21T07:07:19Z",
      "updatedDate": "2022-11-21T07:07:19Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.15462v1",
      "arxivUrl": "https://arxiv.org/abs/2211.15462",
      "comment": "Paper submitted for Creativity and Design workshop at NeurIPS 2022. (4 pages including references + 7 page appendix). We would like to thank Google and the ML Developer Programs Team for their assistance and compute credits used in the experiments for this paper",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.05985",
      "title": "Using Persuasive Writing Strategies to Explain and Detect Health Misinformation",
      "authors": [
        {
          "name": "Danial Kamali",
          "affiliation": null
        },
        {
          "name": "Joseph Romain",
          "affiliation": null
        },
        {
          "name": "Huiyi Liu",
          "affiliation": null
        },
        {
          "name": "Wei Peng",
          "affiliation": null
        },
        {
          "name": "Jingbo Meng",
          "affiliation": null
        },
        {
          "name": "Parisa Kordjamshidi",
          "affiliation": null
        }
      ],
      "abstract": "Nowadays, the spread of misinformation is a prominent problem in society. Our research focuses on aiding the automatic identification of misinformation by analyzing the persuasive strategies employed in textual documents. We introduce a novel annotation scheme encompassing common persuasive writing tactics to achieve our objective. Additionally, we provide a dataset on health misinformation, thoroughly annotated by experts utilizing our proposed scheme. Our contribution includes proposing a new task of annotating pieces of text with their persuasive writing strategy types. We evaluate fine-tuning and prompt-engineering techniques with pre-trained language models of the BERT family and the generative large language models of the GPT family using persuasive strategies as an additional source of information. We evaluate the effects of employing persuasive strategies as intermediate labels in the context of misinformation detection. Our results show that those strategies enhance accuracy and improve the explainability of misinformation detection models. The persuasive strategies can serve as valuable insights and explanations, enabling other models or even humans to make more informed decisions regarding the trustworthiness of the information.",
      "publishedDate": "2022-11-11T03:26:37Z",
      "updatedDate": "2024-04-10T14:13:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.05985v4",
      "arxivUrl": "https://arxiv.org/abs/2211.05985",
      "comment": "Accepted at LREC-CoLING-2024",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.04118",
      "title": "ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning",
      "authors": [
        {
          "name": "Jinta Weng",
          "affiliation": null
        },
        {
          "name": "Yifan Deng",
          "affiliation": null
        },
        {
          "name": "d Donghao Li",
          "affiliation": null
        },
        {
          "name": "Hao You",
          "affiliation": null
        },
        {
          "name": "Yue Hu",
          "affiliation": null
        },
        {
          "name": "Heyan Huang",
          "affiliation": null
        }
      ],
      "abstract": "The prompt has become an effective linguistic tool for utilizing pre-trained language models. However, in few-shot scenarios, subtle changes in the prompt design always make the result widely different, and the prompt learning methods also make it easy to overfit the limited samples. To alleviate this, we explore utilizing suitable contrastive samples and multi-degree contrastive learning methods to improve the robustness of the prompt representation. Therefore, the proposed Consprompt combined with the prompt encoding network, contrastive sampling modules, and contrastive scoring modules, is introduced to realize differential contrastive learning. Our results exhibit state-of-the-art performance in different few-shot settings, and the ablation experiments also certify the effectiveness of utilizing multi-degree contrastive learning in the prompt-based fine-tuning process.",
      "publishedDate": "2022-11-08T09:29:45Z",
      "updatedDate": "2024-03-12T08:29:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.04118v3",
      "arxivUrl": "https://arxiv.org/abs/2211.04118",
      "comment": "2 figures",
      "journalRef": "ICASSP2024",
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.01910",
      "title": "Large Language Models Are Human-Level Prompt Engineers",
      "authors": [
        {
          "name": "Yongchao Zhou",
          "affiliation": null
        },
        {
          "name": "Andrei Ioan Muresanu",
          "affiliation": null
        },
        {
          "name": "Ziwen Han",
          "affiliation": null
        },
        {
          "name": "Keiran Paster",
          "affiliation": null
        },
        {
          "name": "Silviu Pitis",
          "affiliation": null
        },
        {
          "name": "Harris Chan",
          "affiliation": null
        },
        {
          "name": "Jimmy Ba",
          "affiliation": null
        }
      ],
      "abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the \"program,\" optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.",
      "publishedDate": "2022-11-03T15:43:03Z",
      "updatedDate": "2023-03-10T17:20:17Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.01910v2",
      "arxivUrl": "https://arxiv.org/abs/2211.01910",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.17284",
      "title": "Towards Zero-Shot and Few-Shot Table Question Answering using GPT-3",
      "authors": [
        {
          "name": "Pragya Srivastava",
          "affiliation": null
        },
        {
          "name": "Tanuja Ganu",
          "affiliation": null
        },
        {
          "name": "Saikat Guha",
          "affiliation": null
        }
      ],
      "abstract": "We present very early results on using GPT-3 to perform question answering on tabular data. We find that stock pre-trained GPT-3 is able to zero-shot learn the table structure from a serialized JSON array-of-arrays representation, and able to answer lookup queries and simple comparison questions in natural language without any fine-tuning. We further find that simple prompt engineering to include few-shot static Q&A examples significantly improves accuracy. Lastly, we find that intermixing passage text improves accuracy even further on heterogeneous data. We apply our approach on a novel dataset of simple tables in newspaper infographics with promising results. Overall, we find much cause for optimism in this basic approach.",
      "publishedDate": "2022-10-31T13:08:55Z",
      "updatedDate": "2022-10-31T13:08:55Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.17284v1",
      "arxivUrl": "https://arxiv.org/abs/2210.17284",
      "comment": "7 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.15157",
      "title": "Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language",
      "authors": [
        {
          "name": "Paul Denny",
          "affiliation": null
        },
        {
          "name": "Viraj Kumar",
          "affiliation": null
        },
        {
          "name": "Nasser Giacaman",
          "affiliation": null
        }
      ],
      "abstract": "GitHub Copilot is an artificial intelligence model for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about the impact it will have on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60\\% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development.",
      "publishedDate": "2022-10-27T03:48:24Z",
      "updatedDate": "2022-10-27T03:48:24Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.15157v1",
      "arxivUrl": "https://arxiv.org/abs/2210.15157",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.07792",
      "title": "Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning",
      "authors": [
        {
          "name": "Louis Castricato",
          "affiliation": null
        },
        {
          "name": "Alexander Havrilla",
          "affiliation": null
        },
        {
          "name": "Shahbuland Matiana",
          "affiliation": null
        },
        {
          "name": "Michael Pieler",
          "affiliation": null
        },
        {
          "name": "Anbang Ye",
          "affiliation": null
        },
        {
          "name": "Ian Yang",
          "affiliation": null
        },
        {
          "name": "Spencer Frazier",
          "affiliation": null
        },
        {
          "name": "Mark Riedl",
          "affiliation": null
        }
      ],
      "abstract": "Controlled automated story generation seeks to generate natural language stories satisfying constraints from natural language critiques or preferences. Existing methods to control for story preference utilize prompt engineering which is labor intensive and often inconsistent. They may also use logit-manipulation methods which require annotated datasets to exist for the desired attributes. To address these issues, we first train a contrastive bi-encoder model to align stories with corresponding human critiques, named CARP, building a general purpose preference model. This is subsequently used as a reward function to fine-tune a generative language model via reinforcement learning. However, simply fine-tuning a generative language model with a contrastive reward model does not always reliably result in a story generation system capable of generating stories that meet user preferences. To increase story generation robustness we further fine-tune the contrastive reward model using a prompt-learning technique. A human participant study is then conducted comparing generations from our full system, ablations, and two baselines. We show that the full fine-tuning pipeline results in a story generator preferred over a LLM 20x as large as well as logit-based methods. This motivates the use of contrastive learning for general purpose human preference modeling.",
      "publishedDate": "2022-10-14T13:21:33Z",
      "updatedDate": "2022-12-15T17:38:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.07792v2",
      "arxivUrl": "https://arxiv.org/abs/2210.07792",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.04186",
      "title": "Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT",
      "authors": [
        {
          "name": "Bhavya Bhavya",
          "affiliation": null
        },
        {
          "name": "Jinjun Xiong",
          "affiliation": null
        },
        {
          "name": "Chengxiang Zhai",
          "affiliation": null
        }
      ],
      "abstract": "We propose a novel application of prompting Pre-trained Language Models (PLMs) to generate analogies and study how to design effective prompts for two task settings: generating a source concept analogous to a given target concept (aka Analogous Concept Generation or ACG), and generating an explanation of the similarity between a given pair of target concept and source concept (aka Analogous Explanation Generation or AEG). We found that it is feasible to prompt InstructGPT to generate meaningful analogies and the best prompts tend to be precise imperative statements especially with a low temperature setting. We also systematically analyzed the sensitivity of the InstructGPT model to prompt design, temperature, and injected spelling errors, and found that the model is particularly sensitive to certain variations (e.g., questions vs. imperative statements). Further, we conducted human evaluation on 1.4k of the generated analogies and found that the quality of generations varies substantially by model size. The largest InstructGPT model can achieve human-level performance at generating meaningful analogies for a given target while there is still room for improvement on the AEG task.",
      "publishedDate": "2022-10-09T06:35:14Z",
      "updatedDate": "2022-10-11T01:31:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.04186v2",
      "arxivUrl": "https://arxiv.org/abs/2210.04186",
      "comment": "Accepted to 15th International Conference on Natural Language Generation (INLG 2022)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.02441",
      "title": "Ask Me Anything: A simple strategy for prompting language models",
      "authors": [
        {
          "name": "Simran Arora",
          "affiliation": null
        },
        {
          "name": "Avanika Narayan",
          "affiliation": null
        },
        {
          "name": "Mayee F. Chen",
          "affiliation": null
        },
        {
          "name": "Laurel Orr",
          "affiliation": null
        },
        {
          "name": "Neel Guha",
          "affiliation": null
        },
        {
          "name": "Kush Bhatia",
          "affiliation": null
        },
        {
          "name": "Ines Chami",
          "affiliation": null
        },
        {
          "name": "Frederic Sala",
          "affiliation": null
        },
        {
          "name": "Christopher Ré",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly \"perfect prompt\" for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation (\"Who went to the park?\") tend to outperform those that restrict the model outputs (\"John went to the park. Output True or False.\"). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting",
      "publishedDate": "2022-10-05T17:59:45Z",
      "updatedDate": "2022-11-20T00:07:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.02441v3",
      "arxivUrl": "https://arxiv.org/abs/2210.02441",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.01293",
      "title": "ThinkSum: Probabilistic reasoning over sets using large language models",
      "authors": [
        {
          "name": "Batu Ozturkler",
          "affiliation": null
        },
        {
          "name": "Nikolay Malkin",
          "affiliation": null
        },
        {
          "name": "Zhen Wang",
          "affiliation": null
        },
        {
          "name": "Nebojsa Jojic",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think - retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum - probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.",
      "publishedDate": "2022-10-04T00:34:01Z",
      "updatedDate": "2023-06-02T17:25:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.01293v2",
      "arxivUrl": "https://arxiv.org/abs/2210.01293",
      "comment": "ACL 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.00990",
      "title": "Visual Prompt Tuning for Generative Transfer Learning",
      "authors": [
        {
          "name": "Kihyuk Sohn",
          "affiliation": null
        },
        {
          "name": "Yuan Hao",
          "affiliation": null
        },
        {
          "name": "José Lezama",
          "affiliation": null
        },
        {
          "name": "Luisa Polania",
          "affiliation": null
        },
        {
          "name": "Huiwen Chang",
          "affiliation": null
        },
        {
          "name": "Han Zhang",
          "affiliation": null
        },
        {
          "name": "Irfan Essa",
          "affiliation": null
        },
        {
          "name": "Lu Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Transferring knowledge from an image synthesis model trained on a large dataset is a promising direction for learning generative image models from various domains efficiently. While previous works have studied GAN models, we present a recipe for learning vision transformers by generative knowledge transfer. We base our framework on state-of-the-art generative vision transformers that represent an image as a sequence of visual tokens to the autoregressive or non-autoregressive transformers. To adapt to a new domain, we employ prompt tuning, which prepends learnable tokens called prompt to the image token sequence, and introduce a new prompt design for our task. We study on a variety of visual domains, including visual task adaptation benchmark~\\cite{zhai2019large}, with varying amount of training images, and show effectiveness of knowledge transfer and a significantly better image generation quality over existing works.",
      "publishedDate": "2022-10-03T14:56:05Z",
      "updatedDate": "2022-10-03T14:56:05Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.00990v1",
      "arxivUrl": "https://arxiv.org/abs/2210.00990",
      "comment": "technical report",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.11475",
      "title": "Unsupervised Hashing with Semantic Concept Mining",
      "authors": [
        {
          "name": "Rong-Cheng Tu",
          "affiliation": null
        },
        {
          "name": "Xian-Ling Mao",
          "affiliation": null
        },
        {
          "name": "Kevin Qinghong Lin",
          "affiliation": null
        },
        {
          "name": "Chengfei Cai",
          "affiliation": null
        },
        {
          "name": "Weize Qin",
          "affiliation": null
        },
        {
          "name": "Hongfa Wang",
          "affiliation": null
        },
        {
          "name": "Wei Wei",
          "affiliation": null
        },
        {
          "name": "Heyan Huang",
          "affiliation": null
        }
      ],
      "abstract": "Recently, to improve the unsupervised image retrieval performance, plenty of unsupervised hashing methods have been proposed by designing a semantic similarity matrix, which is based on the similarities between image features extracted by a pre-trained CNN model. However, most of these methods tend to ignore high-level abstract semantic concepts contained in images. Intuitively, concepts play an important role in calculating the similarity among images. In real-world scenarios, each image is associated with some concepts, and the similarity between two images will be larger if they share more identical concepts. Inspired by the above intuition, in this work, we propose a novel Unsupervised Hashing with Semantic Concept Mining, called UHSCM, which leverages a VLP model to construct a high-quality similarity matrix. Specifically, a set of randomly chosen concepts is first collected. Then, by employing a vision-language pretraining (VLP) model with the prompt engineering which has shown strong power in visual representation learning, the set of concepts is denoised according to the training images. Next, the proposed method UHSCM applies the VLP model with prompting again to mine the concept distribution of each image and construct a high-quality semantic similarity matrix based on the mined concept distributions. Finally, with the semantic similarity matrix as guiding information, a novel hashing loss with a modified contrastive loss based regularization item is proposed to optimize the hashing network. Extensive experiments on three benchmark datasets show that the proposed method outperforms the state-of-the-art baselines in the image retrieval task.",
      "publishedDate": "2022-09-23T08:25:24Z",
      "updatedDate": "2022-09-23T08:25:24Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.11475v1",
      "arxivUrl": "https://arxiv.org/abs/2209.11475",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.11344",
      "title": "Exploring The Design of Prompts For Applying GPT-3 based Chatbots: A Mental Wellbeing Case Study on Mechanical Turk",
      "authors": [
        {
          "name": "Harsh Kumar",
          "affiliation": null
        },
        {
          "name": "Ilya Musabirov",
          "affiliation": null
        },
        {
          "name": "Jiakai Shi",
          "affiliation": null
        },
        {
          "name": "Adele Lauzon",
          "affiliation": null
        },
        {
          "name": "Kwan Kiu Choy",
          "affiliation": null
        },
        {
          "name": "Ofek Gross",
          "affiliation": null
        },
        {
          "name": "Dana Kulzhabayeva",
          "affiliation": null
        },
        {
          "name": "Joseph Jay Williams",
          "affiliation": null
        }
      ],
      "abstract": "Large-Language Models like GPT-3 have the potential to enable HCI designers and researchers to create more human-like and helpful chatbots for specific applications. But evaluating the feasibility of these chatbots and designing prompts that optimize GPT-3 for a specific task is challenging. We present a case study in tackling these questions, applying GPT-3 to a brief 5-minute chatbot that anyone can talk to better manage their mood. We report a randomized factorial experiment with 945 participants on Mechanical Turk that tests three dimensions of prompt design to initialize the chatbot (identity, intent, and behaviour), and present both quantitative and qualitative analyses of conversations and user perceptions of the chatbot. We hope other HCI designers and researchers can build on this case study, for other applications of GPT-3 based chatbots to specific tasks, and build on and extend the methods we use for prompt design, and evaluation of the prompt design.",
      "publishedDate": "2022-09-22T23:15:54Z",
      "updatedDate": "2022-09-22T23:15:54Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.11344v1",
      "arxivUrl": "https://arxiv.org/abs/2209.11344",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.08966",
      "title": "Will It Blend? Mixing Training Paradigms & Prompting for Argument Quality Prediction",
      "authors": [
        {
          "name": "Michiel van der Meer",
          "affiliation": null
        },
        {
          "name": "Myrthe Reuver",
          "affiliation": null
        },
        {
          "name": "Urja Khurana",
          "affiliation": null
        },
        {
          "name": "Lea Krause",
          "affiliation": null
        },
        {
          "name": "Selene Báez Santamaría",
          "affiliation": null
        }
      ],
      "abstract": "This paper describes our contributions to the Shared Task of the 9th Workshop on Argument Mining (2022). Our approach uses Large Language Models for the task of Argument Quality Prediction. We perform prompt engineering using GPT-3, and also investigate the training paradigms multi-task learning, contrastive learning, and intermediate-task training. We find that a mixed prediction setup outperforms single models. Prompting GPT-3 works best for predicting argument validity, and argument novelty is best estimated by a model trained using all three training paradigms.",
      "publishedDate": "2022-09-19T12:34:46Z",
      "updatedDate": "2022-10-05T08:43:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.08966v2",
      "arxivUrl": "https://arxiv.org/abs/2209.08966",
      "comment": "Accepted at the 9th Workshop on Argument Mining (2022)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.07852",
      "title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",
      "authors": [
        {
          "name": "Hendrik Strobelt",
          "affiliation": null
        },
        {
          "name": "Albert Webson",
          "affiliation": null
        },
        {
          "name": "Victor Sanh",
          "affiliation": null
        },
        {
          "name": "Benjamin Hoover",
          "affiliation": null
        },
        {
          "name": "Johanna Beyer",
          "affiliation": null
        },
        {
          "name": "Hanspeter Pfister",
          "affiliation": null
        },
        {
          "name": "Alexander M. Rush",
          "affiliation": null
        }
      ],
      "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo at http://prompt.vizhub.ai) and our workflow using several real-world use cases.",
      "publishedDate": "2022-08-16T17:17:53Z",
      "updatedDate": "2022-08-16T17:17:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.07852v1",
      "arxivUrl": "https://arxiv.org/abs/2208.07852",
      "comment": "9 pages content, 2 pages references",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.02812",
      "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting",
      "authors": [
        {
          "name": "Ziyi Wang",
          "affiliation": null
        },
        {
          "name": "Xumin Yu",
          "affiliation": null
        },
        {
          "name": "Yongming Rao",
          "affiliation": null
        },
        {
          "name": "Jie Zhou",
          "affiliation": null
        },
        {
          "name": "Jiwen Lu",
          "affiliation": null
        }
      ],
      "abstract": "Nowadays, pre-training big models on large-scale datasets has become a crucial topic in deep learning. The pre-trained models with high representation ability and transferability achieve a great success and dominate many downstream tasks in natural language processing and 2D vision. However, it is non-trivial to promote such a pretraining-tuning paradigm to the 3D vision, given the limited training data that are relatively inconvenient to collect. In this paper, we provide a new perspective of leveraging pre-trained 2D knowledge in 3D domain to tackle this problem, tuning pre-trained image models with the novel Point-to-Pixel prompting for point cloud analysis at a minor parameter cost. Following the principle of prompting engineering, we transform point clouds into colorful images with geometry-preserved projection and geometry-aware coloring to adapt to pre-trained image models, whose weights are kept frozen during the end-to-end optimization of point cloud analysis tasks. We conduct extensive experiments to demonstrate that cooperating with our proposed Point-to-Pixel Prompting, better pre-trained image model will lead to consistently better performance in 3D vision. Enjoying prosperous development from image pre-training field, our method attains 89.3% accuracy on the hardest setting of ScanObjectNN, surpassing conventional point cloud models with much fewer trainable parameters. Our framework also exhibits very competitive performance on ModelNet classification and ShapeNet Part Segmentation. Code is available at https://github.com/wangzy22/P2P.",
      "publishedDate": "2022-08-04T17:59:03Z",
      "updatedDate": "2022-10-12T09:00:15Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.02812v2",
      "arxivUrl": "https://arxiv.org/abs/2208.02812",
      "comment": "Accepted to NeurIPS 2022, project page: https://p2p.ivg-research.xyz",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.13428",
      "title": "Leveraging GAN Priors for Few-Shot Part Segmentation",
      "authors": [
        {
          "name": "Mengya Han",
          "affiliation": null
        },
        {
          "name": "Heliang Zheng",
          "affiliation": null
        },
        {
          "name": "Chaoyue Wang",
          "affiliation": null
        },
        {
          "name": "Yong Luo",
          "affiliation": null
        },
        {
          "name": "Han Hu",
          "affiliation": null
        },
        {
          "name": "Bo Du",
          "affiliation": null
        }
      ],
      "abstract": "Few-shot part segmentation aims to separate different parts of an object given only a few annotated samples. Due to the challenge of limited data, existing works mainly focus on learning classifiers over pre-trained features, failing to learn task-specific features for part segmentation. In this paper, we propose to learn task-specific features in a \"pre-training\"-\"fine-tuning\" paradigm. We conduct prompt designing to reduce the gap between the pre-train task (i.e., image generation) and the downstream task (i.e., part segmentation), so that the GAN priors for generation can be leveraged for segmentation. This is achieved by projecting part segmentation maps into the RGB space and conducting interpolation between RGB segmentation maps and original images. Specifically, we design a fine-tuning strategy to progressively tune an image generator into a segmentation generator, where the supervision of the generator varying from images to segmentation maps by interpolation. Moreover, we propose a two-stream architecture, i.e., a segmentation stream to generate task-specific features, and an image stream to provide spatial constraints. The image stream can be regarded as a self-supervised auto-encoder, and this enables our model to benefit from large-scale support images. Overall, this work is an attempt to explore the internal relevance between generation tasks and perception tasks by prompt designing. Extensive experiments show that our model can achieve state-of-the-art performance on several part segmentation datasets.",
      "publishedDate": "2022-07-27T10:17:07Z",
      "updatedDate": "2022-07-27T10:17:07Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.13428v1",
      "arxivUrl": "https://arxiv.org/abs/2207.13428",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.13038",
      "title": "Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models",
      "authors": [
        {
          "name": "Robin Rombach",
          "affiliation": null
        },
        {
          "name": "Andreas Blattmann",
          "affiliation": null
        },
        {
          "name": "Björn Ommer",
          "affiliation": null
        }
      ],
      "abstract": "Novel architectures have recently improved generative image synthesis leading to excellent visual quality in various tasks. Of particular note is the field of ``AI-Art'', which has seen unprecedented growth with the emergence of powerful multimodal models such as CLIP. By combining speech and image synthesis models, so-called ``prompt-engineering'' has become established, in which carefully selected and composed sentences are used to achieve a certain visual style in the synthesized image. In this note, we present an alternative approach based on retrieval-augmented diffusion models (RDMs). In RDMs, a set of nearest neighbors is retrieved from an external database during training for each training instance, and the diffusion model is conditioned on these informative samples. During inference (sampling), we replace the retrieval database with a more specialized database that contains, for example, only images of a particular visual style. This provides a novel way to prompt a general trained model after training and thereby specify a particular visual style. As shown by our experiments, this approach is superior to specifying the visual style within the text prompt. We open-source code and model weights at https://github.com/CompVis/latent-diffusion .",
      "publishedDate": "2022-07-26T16:56:51Z",
      "updatedDate": "2022-07-26T16:56:51Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.13038v1",
      "arxivUrl": "https://arxiv.org/abs/2207.13038",
      "comment": "4 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.12396",
      "title": "Exploring CLIP for Assessing the Look and Feel of Images",
      "authors": [
        {
          "name": "Jianyi Wang",
          "affiliation": null
        },
        {
          "name": "Kelvin C. K. Chan",
          "affiliation": null
        },
        {
          "name": "Chen Change Loy",
          "affiliation": null
        }
      ],
      "abstract": "Measuring the perception of visual content is a long-standing problem in computer vision. Many mathematical models have been developed to evaluate the look or quality of an image. Despite the effectiveness of such tools in quantifying degradations such as noise and blurriness levels, such quantification is loosely coupled with human language. When it comes to more abstract perception about the feel of visual content, existing methods can only rely on supervised models that are explicitly trained with labeled data collected via laborious user study. In this paper, we go beyond the conventional paradigms by exploring the rich visual language prior encapsulated in Contrastive Language-Image Pre-training (CLIP) models for assessing both the quality perception (look) and abstract perception (feel) of images in a zero-shot manner. In particular, we discuss effective prompt designs and show an effective prompt pairing strategy to harness the prior. We also provide extensive experiments on controlled datasets and Image Quality Assessment (IQA) benchmarks. Our results show that CLIP captures meaningful priors that generalize well to different perceptual assessments. Code is avaliable at https://github.com/IceClear/CLIP-IQA.",
      "publishedDate": "2022-07-25T17:58:16Z",
      "updatedDate": "2022-11-23T13:17:33Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.12396v2",
      "arxivUrl": "https://arxiv.org/abs/2207.12396",
      "comment": "Accepted by AAAI2023. Code: https://github.com/IceClear/CLIP-IQA",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.08143",
      "title": "Can large language models reason about medical questions?",
      "authors": [
        {
          "name": "Valentin Liévin",
          "affiliation": null
        },
        {
          "name": "Christoffer Egeberg Hother",
          "affiliation": null
        },
        {
          "name": "Andreas Geert Motzfeldt",
          "affiliation": null
        },
        {
          "name": "Ole Winther",
          "affiliation": null
        }
      ],
      "abstract": "Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether close- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer and reason about difficult real-world-based questions. We focus on three popular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and retrieval augmentation. Based on an expert annotation of the generated CoTs, we found that InstructGPT can often read, reason and recall expert knowledge. Last, by leveraging advances in prompt engineering (few-shot and ensemble methods), we demonstrated that GPT-3.5 not only yields calibrated predictive distributions, but also reaches the passing score on three datasets: MedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are closing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.",
      "publishedDate": "2022-07-17T11:24:44Z",
      "updatedDate": "2023-12-24T11:17:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.08143v4",
      "arxivUrl": "https://arxiv.org/abs/2207.08143",
      "comment": "37 pages, 23 figures. v1: results using InstructGPT, v2.0: added the Codex experiments, v2.1: added the missing test MedMCQA results for Codex 5-shot CoT and using k=100 samples, v3.0: added results for open source models -- ready for publication (final version)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.00747",
      "title": "Rationale-Augmented Ensembles in Language Models",
      "authors": [
        {
          "name": "Xuezhi Wang",
          "affiliation": null
        },
        {
          "name": "Jason Wei",
          "affiliation": null
        },
        {
          "name": "Dale Schuurmans",
          "affiliation": null
        },
        {
          "name": "Quoc Le",
          "affiliation": null
        },
        {
          "name": "Ed Chi",
          "affiliation": null
        },
        {
          "name": "Denny Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Recent research has shown that rationales, or step-by-step chains of thought, can be used to improve performance in multi-step reasoning tasks. We reconsider rationale-augmented prompting for few-shot in-context learning, where (input -> output) prompts are expanded to (input, rationale -> output) prompts. For rationale-augmented prompting we demonstrate how existing approaches, which rely on manual prompt engineering, are subject to sub-optimal rationales that may harm performance. To mitigate this brittleness, we propose a unified framework of rationale-augmented ensembles, where we identify rationale sampling in the output space as the key component to robustly improve performance. This framework is general and can easily be extended to common natural language processing tasks, even those that do not traditionally leverage intermediate steps, such as question answering, word sense disambiguation, and sentiment analysis. We demonstrate that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches--including standard prompting without rationales and rationale-based chain-of-thought prompting--while simultaneously improving interpretability of model predictions through the associated rationales.",
      "publishedDate": "2022-07-02T06:20:57Z",
      "updatedDate": "2022-07-02T06:20:57Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.00747v1",
      "arxivUrl": "https://arxiv.org/abs/2207.00747",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.15076",
      "title": "BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing",
      "authors": [
        {
          "name": "Jason Alan Fries",
          "affiliation": null
        },
        {
          "name": "Leon Weber",
          "affiliation": null
        },
        {
          "name": "Natasha Seelam",
          "affiliation": null
        },
        {
          "name": "Gabriel Altay",
          "affiliation": null
        },
        {
          "name": "Debajyoti Datta",
          "affiliation": null
        },
        {
          "name": "Samuele Garda",
          "affiliation": null
        },
        {
          "name": "Myungsun Kang",
          "affiliation": null
        },
        {
          "name": "Ruisi Su",
          "affiliation": null
        },
        {
          "name": "Wojciech Kusa",
          "affiliation": null
        },
        {
          "name": "Samuel Cahyawijaya",
          "affiliation": null
        },
        {
          "name": "Fabio Barth",
          "affiliation": null
        },
        {
          "name": "Simon Ott",
          "affiliation": null
        },
        {
          "name": "Matthias Samwald",
          "affiliation": null
        },
        {
          "name": "Stephen Bach",
          "affiliation": null
        },
        {
          "name": "Stella Biderman",
          "affiliation": null
        },
        {
          "name": "Mario Sänger",
          "affiliation": null
        },
        {
          "name": "Bo Wang",
          "affiliation": null
        },
        {
          "name": "Alison Callahan",
          "affiliation": null
        },
        {
          "name": "Daniel León Periñán",
          "affiliation": null
        },
        {
          "name": "Théo Gigant",
          "affiliation": null
        },
        {
          "name": "Patrick Haller",
          "affiliation": null
        },
        {
          "name": "Jenny Chim",
          "affiliation": null
        },
        {
          "name": "Jose David Posada",
          "affiliation": null
        },
        {
          "name": "John Michael Giorgi",
          "affiliation": null
        },
        {
          "name": "Karthik Rangasai Sivaraman",
          "affiliation": null
        },
        {
          "name": "Marc Pàmies",
          "affiliation": null
        },
        {
          "name": "Marianna Nezhurina",
          "affiliation": null
        },
        {
          "name": "Robert Martin",
          "affiliation": null
        },
        {
          "name": "Michael Cullan",
          "affiliation": null
        },
        {
          "name": "Moritz Freidank",
          "affiliation": null
        },
        {
          "name": "Nathan Dahlberg",
          "affiliation": null
        },
        {
          "name": "Shubhanshu Mishra",
          "affiliation": null
        },
        {
          "name": "Shamik Bose",
          "affiliation": null
        },
        {
          "name": "Nicholas Michio Broad",
          "affiliation": null
        },
        {
          "name": "Yanis Labrak",
          "affiliation": null
        },
        {
          "name": "Shlok S Deshmukh",
          "affiliation": null
        },
        {
          "name": "Sid Kiblawi",
          "affiliation": null
        },
        {
          "name": "Ayush Singh",
          "affiliation": null
        },
        {
          "name": "Minh Chien Vu",
          "affiliation": null
        },
        {
          "name": "Trishala Neeraj",
          "affiliation": null
        },
        {
          "name": "Jonas Golde",
          "affiliation": null
        },
        {
          "name": "Albert Villanova del Moral",
          "affiliation": null
        },
        {
          "name": "Benjamin Beilharz",
          "affiliation": null
        }
      ],
      "abstract": "Training and evaluating language models increasingly requires the construction of meta-datasets --diverse collections of curated data with clear provenance. Natural language prompting has recently lead to improved zero-shot generalization by transforming existing, supervised datasets into a diversity of novel pretraining tasks, highlighting the benefits of meta-dataset curation. While successful in general-domain text, translating these data-centric approaches to biomedical language modeling remains challenging, as labeled biomedical datasets are significantly underrepresented in popular data hubs. To address this challenge, we introduce BigBIO a community library of 126+ biomedical NLP datasets, currently covering 12 task categories and 10+ languages. BigBIO facilitates reproducible meta-dataset curation via programmatic access to datasets and their metadata, and is compatible with current platforms for prompt engineering and end-to-end few/zero shot language model evaluation. We discuss our process for task schema harmonization, data auditing, contribution guidelines, and outline two illustrative use cases: zero-shot evaluation of biomedical prompts and large-scale, multi-task learning. BigBIO is an ongoing community effort and is available at https://github.com/bigscience-workshop/biomedical",
      "publishedDate": "2022-06-30T07:15:45Z",
      "updatedDate": "2022-06-30T07:15:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.15076v1",
      "arxivUrl": "https://arxiv.org/abs/2206.15076",
      "comment": "Submitted to NeurIPS 2022 Datasets and Benchmarks Track",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.12839",
      "title": "Repository-Level Prompt Generation for Large Language Models of Code",
      "authors": [
        {
          "name": "Disha Shrivastava",
          "affiliation": null
        },
        {
          "name": "Hugo Larochelle",
          "affiliation": null
        },
        {
          "name": "Daniel Tarlow",
          "affiliation": null
        }
      ],
      "abstract": "With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code-autocompletion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a remarkably high relative improvement of 36% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines. We release our code, data, and trained checkpoints at: \\url{https://github.com/shrivastavadisha/repo_level_prompt_generation}.",
      "publishedDate": "2022-06-26T10:51:25Z",
      "updatedDate": "2023-06-05T18:43:50Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.PL",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.12839v3",
      "arxivUrl": "https://arxiv.org/abs/2206.12839",
      "comment": "ICML 2023 (Camera-Ready version)",
      "journalRef": "ICML, 2023",
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.09363",
      "title": "Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning",
      "authors": [
        {
          "name": "Xiaolei Wang",
          "affiliation": null
        },
        {
          "name": "Kun Zhou",
          "affiliation": null
        },
        {
          "name": "Ji-Rong Wen",
          "affiliation": null
        },
        {
          "name": "Wayne Xin Zhao",
          "affiliation": null
        }
      ],
      "abstract": "Conversational recommender systems (CRS) aim to proactively elicit user preference and recommend high-quality items through natural language conversations. Typically, a CRS consists of a recommendation module to predict preferred items for users and a conversation module to generate appropriate responses. To develop an effective CRS, it is essential to seamlessly integrate the two modules. Existing works either design semantic alignment strategies, or share knowledge resources and representations between the two modules. However, these approaches still rely on different architectures or techniques to develop the two modules, making it difficult for effective module integration. To address this problem, we propose a unified CRS model named UniCRS based on knowledge-enhanced prompt learning. Our approach unifies the recommendation and conversation subtasks into the prompt learning paradigm, and utilizes knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to fulfill both subtasks in a unified approach. In the prompt design, we include fused knowledge representations, task-specific soft tokens, and the dialogue context, which can provide sufficient contextual information to adapt the PLM for the CRS task. Besides, for the recommendation subtask, we also incorporate the generated response template as an important part of the prompt, to enhance the information interaction between the two subtasks. Extensive experiments on two public CRS datasets have demonstrated the effectiveness of our approach.",
      "publishedDate": "2022-06-19T09:21:27Z",
      "updatedDate": "2022-06-19T09:21:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.09363v1",
      "arxivUrl": "https://arxiv.org/abs/2206.09363",
      "comment": "Accepted by KDD 2022. Code: https://github.com/RUCAIBox/UniCRS",
      "journalRef": null,
      "doi": "10.1145/3534678.3539382",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.02338",
      "title": "OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression",
      "authors": [
        {
          "name": "Wanhua Li",
          "affiliation": null
        },
        {
          "name": "Xiaoke Huang",
          "affiliation": null
        },
        {
          "name": "Zheng Zhu",
          "affiliation": null
        },
        {
          "name": "Yansong Tang",
          "affiliation": null
        },
        {
          "name": "Xiu Li",
          "affiliation": null
        },
        {
          "name": "Jie Zhou",
          "affiliation": null
        },
        {
          "name": "Jiwen Lu",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents a language-powered paradigm for ordinal regression. Existing methods usually treat each rank as a category and employ a set of weights to learn these concepts. These methods are easy to overfit and usually attain unsatisfactory performance as the learned concepts are mainly derived from the training set. Recent large pre-trained vision-language models like CLIP have shown impressive performance on various visual tasks. In this paper, we propose to learn the rank concepts from the rich semantic CLIP latent space. Specifically, we reformulate this task as an image-language matching problem with a contrastive objective, which regards labels as text and obtains a language prototype from a text encoder for each rank. While prompt engineering for CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable prompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists of learnable context tokens and learnable rank embeddings; The learnable rank embeddings are constructed by explicitly modeling numerical continuity, resulting in well-ordered, compact language prototypes in the CLIP space. Once learned, we can only save the language prototypes and discard the huge language model, resulting in zero additional computational overhead compared with the linear head counterpart. Experimental results show that our paradigm achieves competitive performance in general ordinal regression tasks, and gains improvements in few-shot and distribution shift settings for age estimation. The code is available at https://github.com/xk-huang/OrdinalCLIP.",
      "publishedDate": "2022-06-06T03:54:53Z",
      "updatedDate": "2022-10-01T13:45:55Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.02338v2",
      "arxivUrl": "https://arxiv.org/abs/2206.02338",
      "comment": "Accepted by NeurIPS2022. Code is available at https://github.com/xk-huang/OrdinalCLIP",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.12390",
      "title": "Toxicity Detection with Generative Prompt-based Inference",
      "authors": [
        {
          "name": "Yau-Shian Wang",
          "affiliation": null
        },
        {
          "name": "Yingshan Chang",
          "affiliation": null
        }
      ],
      "abstract": "Due to the subtleness, implicity, and different possible interpretations perceived by different people, detecting undesirable content from text is a nuanced difficulty. It is a long-known risk that language models (LMs), once trained on corpus containing undesirable content, have the power to manifest biases and toxicity. However, recent studies imply that, as a remedy, LMs are also capable of identifying toxic content without additional fine-tuning. Prompt-methods have been shown to effectively harvest this surprising self-diagnosing capability. However, existing prompt-based methods usually specify an instruction to a language model in a discriminative way. In this work, we explore the generative variant of zero-shot prompt-based toxicity detection with comprehensive trials on prompt engineering. We evaluate on three datasets with toxicity labels annotated on social media posts. Our analysis highlights the strengths of our generative classification approach both quantitatively and qualitatively. Interesting aspects of self-diagnosis and its ethical implications are discussed.",
      "publishedDate": "2022-05-24T22:44:43Z",
      "updatedDate": "2022-05-24T22:44:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.12390v1",
      "arxivUrl": "https://arxiv.org/abs/2205.12390",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.11605",
      "title": "On Measuring Social Biases in Prompt-Based Multi-Task Learning",
      "authors": [
        {
          "name": "Afra Feyza Akyürek",
          "affiliation": null
        },
        {
          "name": "Sejin Paik",
          "affiliation": null
        },
        {
          "name": "Muhammed Yusuf Kocyigit",
          "affiliation": null
        },
        {
          "name": "Seda Akbiyik",
          "affiliation": null
        },
        {
          "name": "Şerife Leman Runyun",
          "affiliation": null
        },
        {
          "name": "Derry Wijaya",
          "affiliation": null
        }
      ],
      "abstract": "Large language models trained on a mixture of NLP tasks that are converted into a text-to-text format using prompts, can generalize into novel forms of language and handle novel tasks. A large body of work within prompt engineering attempts to understand the effects of input forms and prompts in achieving superior performance. We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs. In this paper, we study T0, a large-scale multi-task text-to-text language model trained using prompt-based learning. We consider two different forms of semantically equivalent inputs: question-answer format and premise-hypothesis format. We use an existing bias benchmark for the former BBQ and create the first bias benchmark in natural language inference BBNLI with hand-written hypotheses while also converting each benchmark into the other form. The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training, compared to premise-hypothesis form which is unlike its training examples. Code and data are released under https://github.com/feyzaakyurek/bbnli.",
      "publishedDate": "2022-05-23T20:01:20Z",
      "updatedDate": "2022-05-23T20:01:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.11605v1",
      "arxivUrl": "https://arxiv.org/abs/2205.11605",
      "comment": "Findings of NAACL 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.11503",
      "title": "Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models",
      "authors": [
        {
          "name": "Mirac Suzgun",
          "affiliation": null
        },
        {
          "name": "Luke Melas-Kyriazi",
          "affiliation": null
        },
        {
          "name": "Dan Jurafsky",
          "affiliation": null
        }
      ],
      "abstract": "We propose a method for arbitrary textual style transfer (TST)--the task of transforming a text into any given style--utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Specifically, our method first uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks these candidates according to a combination of the three components above. Empirically, our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while consuming two orders of magnitude less compute and memory. Finally, we conduct a systematic investigation of the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets.",
      "publishedDate": "2022-05-23T17:57:15Z",
      "updatedDate": "2022-05-23T17:57:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.11503v1",
      "arxivUrl": "https://arxiv.org/abs/2205.11503",
      "comment": "GitHub page: https://github.com/suzgunmirac/prompt-and-rerank. Project page: https://lukemelas.github.io/prompt-and-rerank/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.11374",
      "title": "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements",
      "authors": [
        {
          "name": "Conrad Borchers",
          "affiliation": null
        },
        {
          "name": "Dalia Sara Gala",
          "affiliation": null
        },
        {
          "name": "Benjamin Gilburt",
          "affiliation": null
        },
        {
          "name": "Eduard Oravkin",
          "affiliation": null
        },
        {
          "name": "Wilfried Bounsi",
          "affiliation": null
        },
        {
          "name": "Yuki M. Asano",
          "affiliation": null
        },
        {
          "name": "Hannah Rose Kirk",
          "affiliation": null
        }
      ],
      "abstract": "The growing capability and availability of generative language models has enabled a wide range of new downstream tasks. Academic research has identified, quantified and mitigated biases present in language models but is rarely tailored to downstream tasks where wider impact on individuals and society can be felt. In this work, we leverage one popular generative language model, GPT-3, with the goal of writing unbiased and realistic job advertisements. We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce bias.",
      "publishedDate": "2022-05-23T15:05:27Z",
      "updatedDate": "2022-05-23T15:05:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.11374v1",
      "arxivUrl": "https://arxiv.org/abs/2205.11374",
      "comment": "Accepted for the 4th Workshop on Gender Bias in Natural Language Processing at NAACL 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.07407",
      "title": "What GPT Knows About Who is Who",
      "authors": [
        {
          "name": "Xiaohan Yang",
          "affiliation": null
        },
        {
          "name": "Eduardo Peynetti",
          "affiliation": null
        },
        {
          "name": "Vasco Meerman",
          "affiliation": null
        },
        {
          "name": "Chris Tanner",
          "affiliation": null
        }
      ],
      "abstract": "Coreference resolution -- which is a crucial task for understanding discourse and language at large -- has yet to witness widespread benefits from large language models (LLMs). Moreover, coreference resolution systems largely rely on supervised labels, which are highly expensive and difficult to annotate, thus making it ripe for prompt engineering. In this paper, we introduce a QA-based prompt-engineering method and discern \\textit{generative}, pre-trained LLMs' abilities and limitations toward the task of coreference resolution. Our experiments show that GPT-2 and GPT-Neo can return valid answers, but that their capabilities to identify coreferent mentions are limited and prompt-sensitive, leading to inconsistent results.",
      "publishedDate": "2022-05-16T00:59:37Z",
      "updatedDate": "2022-05-16T00:59:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.07407v1",
      "arxivUrl": "https://arxiv.org/abs/2205.07407",
      "comment": "Accepted by ACL 2022 Workshop on Insights from Negative Results in NLP",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.02904",
      "title": "The Creativity of Text-to-Image Generation",
      "authors": [
        {
          "name": "Jonas Oppenlaender",
          "affiliation": null
        }
      ],
      "abstract": "Text-guided synthesis of images has made a giant leap towards becoming a mainstream phenomenon. With text-to-image generation systems, anybody can create digital images and artworks. This provokes the question of whether text-to-image generation is creative. This paper expounds on the nature of human creativity involved in text-to-image art (so-called \"AI art\") with a specific focus on the practice of prompt engineering. The paper argues that the current product-centered view of creativity falls short in the context of text-to-image generation. A case exemplifying this shortcoming is provided and the importance of online communities for the creative ecosystem of text-to-image art is highlighted. The paper provides a high-level summary of this online ecosystem drawing on Rhodes' conceptual four P model of creativity. Challenges for evaluating the creativity of text-to-image generation and opportunities for research on text-to-image generation in the field of Human-Computer Interaction (HCI) are discussed.",
      "publishedDate": "2022-05-13T05:59:02Z",
      "updatedDate": "2022-10-31T09:56:44Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.02904v4",
      "arxivUrl": "https://arxiv.org/abs/2206.02904",
      "comment": null,
      "journalRef": null,
      "doi": "10.1145/3569219.3569352",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.05124",
      "title": "Extracting Latent Steering Vectors from Pretrained Language Models",
      "authors": [
        {
          "name": "Nishant Subramani",
          "affiliation": null
        },
        {
          "name": "Nivedita Suresh",
          "affiliation": null
        },
        {
          "name": "Matthew E. Peters",
          "affiliation": null
        }
      ],
      "abstract": "Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.",
      "publishedDate": "2022-05-10T19:04:37Z",
      "updatedDate": "2022-05-10T19:04:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.05124v1",
      "arxivUrl": "https://arxiv.org/abs/2205.05124",
      "comment": "Accepted to ACL2022 Findings; 16 pages (9 pages plus references and appendices); Code: https://github.com/nishantsubramani/steering_vectors; Some text overlap with arXiv:2008.09049",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.03146",
      "title": "CLIP-CLOP: CLIP-Guided Collage and Photomontage",
      "authors": [
        {
          "name": "Piotr Mirowski",
          "affiliation": null
        },
        {
          "name": "Dylan Banarse",
          "affiliation": null
        },
        {
          "name": "Mateusz Malinowski",
          "affiliation": null
        },
        {
          "name": "Simon Osindero",
          "affiliation": null
        },
        {
          "name": "Chrisantha Fernando",
          "affiliation": null
        }
      ],
      "abstract": "The unabated mystique of large-scale neural networks, such as the CLIP dual image-and-text encoder, popularized automatically generated art. Increasingly more sophisticated generators enhanced the artworks' realism and visual appearance, and creative prompt engineering enabled stylistic expression. Guided by an artist-in-the-loop ideal, we design a gradient-based generator to produce collages. It requires the human artist to curate libraries of image patches and to describe (with prompts) the whole image composition, with the option to manually adjust the patches' positions during generation, thereby allowing humans to reclaim some control of the process and achieve greater creative freedom. We explore the aesthetic potentials of high-resolution collages, and provide an open-source Google Colab as an artistic tool.",
      "publishedDate": "2022-05-06T11:33:49Z",
      "updatedDate": "2022-07-24T14:47:50Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.03146v3",
      "arxivUrl": "https://arxiv.org/abs/2205.03146",
      "comment": "5 pages, 7 figures, published at the International Conference on Computational Creativity (ICCC) 2022 as Short Paper: Demo",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.14264",
      "title": "Polyglot Prompt: Multilingual Multitask PrompTraining",
      "authors": [
        {
          "name": "Jinlan Fu",
          "affiliation": null
        },
        {
          "name": "See-Kiong Ng",
          "affiliation": null
        },
        {
          "name": "Pengfei Liu",
          "affiliation": null
        }
      ],
      "abstract": "This paper aims for a potential architectural improvement for multilingual learning and asks: Can different tasks from different languages be modeled in a monolithic framework, i.e. without any task/language-specific module? The benefit of achieving this could open new doors for future multilingual research, including allowing systems trained on low resources to be further assisted by other languages as well as other tasks. We approach this goal by developing a learning framework named Polyglot Prompting to exploit prompting methods for learning a unified semantic space for different languages and tasks with multilingual prompt engineering. We performed a comprehensive evaluation of 6 tasks, namely topic classification, sentiment classification, named entity recognition, question answering, natural language inference, and summarization, covering 24 datasets and 49 languages. The experimental results demonstrated the efficacy of multilingual multitask prompt-based learning and led to inspiring observations. We also present an interpretable multilingual evaluation methodology and show how the proposed framework, multilingual multitask prompt training, works. We release all datasets prompted in the best setting and code.",
      "publishedDate": "2022-04-29T17:40:50Z",
      "updatedDate": "2022-11-04T06:01:05Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.14264v2",
      "arxivUrl": "https://arxiv.org/abs/2204.14264",
      "comment": "EMNLP 2022 (Main Conference)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.12639",
      "title": "Executive Function: A Contrastive Value Policy for Resampling and Relabeling Perceptions via Hindsight Summarization?",
      "authors": [
        {
          "name": "Chris Lengerich",
          "affiliation": null
        },
        {
          "name": "Ben Lengerich",
          "affiliation": null
        }
      ],
      "abstract": "We develop the few-shot continual learning task from first principles and hypothesize an evolutionary motivation and mechanism of action for executive function as a contrastive value policy which resamples and relabels perception data via hindsight summarization to minimize attended prediction error, similar to an online prompt engineering problem. This is made feasible by the use of a memory policy and a pretrained network with inductive biases for a grammar of learning and is trained to maximize evolutionary survival. We show how this model of executive function can be used to implement hypothesis testing as a stream of consciousness and may explain observations of human few-shot learning and neuroanatomy.",
      "publishedDate": "2022-04-27T00:07:44Z",
      "updatedDate": "2022-04-27T00:07:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.12639v1",
      "arxivUrl": "https://arxiv.org/abs/2204.12639",
      "comment": "The authors also thank MeiMei the golden retriever for insightful collaboration, in particular, sampling contrastive data on human vs. dog hindsight summarization",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.10360",
      "title": "Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction",
      "authors": [
        {
          "name": "Hui-Syuan Yeh",
          "affiliation": null
        },
        {
          "name": "Thomas Lavergne",
          "affiliation": null
        },
        {
          "name": "Pierre Zweigenbaum",
          "affiliation": null
        }
      ],
      "abstract": "Relation extraction is a core problem for natural language processing in the biomedical domain. Recent research on relation extraction showed that prompt-based learning improves the performance on both fine-tuning on full training set and few-shot training. However, less effort has been made on domain-specific tasks where good prompt design can be even harder. In this paper, we investigate prompting for biomedical relation extraction, with experiments on the ChemProt dataset. We present a simple yet effective method to systematically generate comprehensive prompts that reformulate the relation extraction task as a cloze-test task under a simple prompt formulation. In particular, we experiment with different ranking scores for prompt selection. With BioMed-RoBERTa-base, our results show that prompting-based fine-tuning obtains gains by 14.21 F1 over its regular fine-tuning baseline, and 1.14 F1 over SciFive-Large, the current state-of-the-art on ChemProt. Besides, we find prompt-based learning requires fewer training examples to make reasonable predictions. The results demonstrate the potential of our methods in such a domain-specific relation extraction task.",
      "publishedDate": "2022-04-21T18:39:22Z",
      "updatedDate": "2022-04-21T18:39:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.10360v1",
      "arxivUrl": "https://arxiv.org/abs/2204.10360",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.13988",
      "title": "A Taxonomy of Prompt Modifiers for Text-To-Image Generation",
      "authors": [
        {
          "name": "Jonas Oppenlaender",
          "affiliation": null
        }
      ],
      "abstract": "Text-to-image generation has seen an explosion of interest since 2021. Today, beautiful and intriguing digital images and artworks can be synthesized from textual inputs (\"prompts\") with deep generative models. Online communities around text-to-image generation and AI generated art have quickly emerged. This paper identifies six types of prompt modifiers used by practitioners in the online community based on a 3-month ethnographic study. The novel taxonomy of prompt modifiers provides researchers a conceptual starting point for investigating the practice of text-to-image generation, but may also help practitioners of AI generated art improve their images. We further outline how prompt modifiers are applied in the practice of \"prompt engineering.\" We discuss research opportunities of this novel creative practice in the field of Human-Computer Interaction (HCI). The paper concludes with a discussion of broader implications of prompt engineering from the perspective of Human-AI Interaction (HAI) in future applications beyond the use case of text-to-image generation and AI generated art.",
      "publishedDate": "2022-04-20T06:15:50Z",
      "updatedDate": "2023-06-14T10:42:24Z",
      "primaryCategory": "cs.MM",
      "arxivCategories": [
        "cs.MM",
        "cs.CL",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.13988v3",
      "arxivUrl": "https://arxiv.org/abs/2204.13988",
      "comment": "15 pages",
      "journalRef": null,
      "doi": "10.1080/0144929X.2023.2286532",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.04908",
      "title": "No Token Left Behind: Explainability-Aided Image Classification and Generation",
      "authors": [
        {
          "name": "Roni Paiss",
          "affiliation": null
        },
        {
          "name": "Hila Chefer",
          "affiliation": null
        },
        {
          "name": "Lior Wolf",
          "affiliation": null
        }
      ],
      "abstract": "The application of zero-shot learning in computer vision has been revolutionized by the use of image-text matching models. The most notable example, CLIP, has been widely used for both zero-shot classification and guiding generative models with a text prompt. However, the zero-shot use of CLIP is unstable with respect to the phrasing of the input text, making it necessary to carefully engineer the prompts used. We find that this instability stems from a selective similarity score, which is based only on a subset of the semantically meaningful input tokens. To mitigate it, we present a novel explainability-based approach, which adds a loss term to ensure that CLIP focuses on all relevant semantic parts of the input, in addition to employing the CLIP similarity loss used in previous works. When applied to one-shot classification through prompt engineering, our method yields an improvement in the recognition rate, without additional training or fine-tuning. Additionally, we show that CLIP guidance of generative models using our method significantly improves the generated images. Finally, we demonstrate a novel use of CLIP guidance for text-based image generation with spatial conditioning on object location, by requiring the image explainability heatmap for each object to be confined to a pre-determined bounding box.",
      "publishedDate": "2022-04-11T07:16:39Z",
      "updatedDate": "2022-08-06T16:57:30Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.04908v2",
      "arxivUrl": "https://arxiv.org/abs/2204.04908",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.03649",
      "title": "Unsupervised Prompt Learning for Vision-Language Models",
      "authors": [
        {
          "name": "Tony Huang",
          "affiliation": null
        },
        {
          "name": "Jack Chu",
          "affiliation": null
        },
        {
          "name": "Fangyun Wei",
          "affiliation": null
        }
      ],
      "abstract": "Contrastive vision-language models like CLIP have shown great progress in transfer learning. In the inference stage, the proper text description, also known as prompt, needs to be carefully designed to correctly classify the given images. In order to avoid laborious prompt engineering, recent works such as CoOp, CLIP-Adapter and Tip-Adapter propose to adapt vision-language models for downstream image recognition tasks on a small set of labeled data. Though promising improvements are achieved, requiring labeled data from the target datasets may restrict the scalability. In this paper, we explore a different scenario, in which the labels of the target datasets are unprovided, and we present an unsupervised prompt learning (UPL) approach to avoid prompt engineering while simultaneously improving transfer performance of CLIP-like vision-language models. As far as we know, UPL is the first work to introduce unsupervised learning into prompt learning. Experimentally, our UPL outperforms original CLIP with prompt engineering on ImageNet as well as other 10 datasets. An enhanced version of UPL is even competitive with the 8-shot CoOp and the 8-shot TIP-Adapter on most datasets. Code and models are available at https://github.com/tonyhuang2022/UPL.",
      "publishedDate": "2022-04-07T17:59:57Z",
      "updatedDate": "2022-08-22T08:45:33Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.03649v2",
      "arxivUrl": "https://arxiv.org/abs/2204.03649",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.17274",
      "title": "Exploring Visual Prompts for Adapting Large-Scale Models",
      "authors": [
        {
          "name": "Hyojin Bahng",
          "affiliation": null
        },
        {
          "name": "Ali Jahanian",
          "affiliation": null
        },
        {
          "name": "Swami Sankaranarayanan",
          "affiliation": null
        },
        {
          "name": "Phillip Isola",
          "affiliation": null
        }
      ],
      "abstract": "We investigate the efficacy of visual prompting to adapt large-scale models in vision. Following the recent approach from prompt tuning and adversarial reprogramming, we learn a single image perturbation such that a frozen model prompted with this perturbation performs a new task. Through comprehensive experiments, we demonstrate that visual prompting is particularly effective for CLIP and robust to distribution shift, achieving performance competitive with standard linear probes. We further analyze properties of the downstream dataset, prompt design, and output transformation in regard to adaptation performance. The surprising effectiveness of visual prompting provides a new perspective on adapting pre-trained models in vision. Code is available at http://hjbahng.github.io/visual_prompting .",
      "publishedDate": "2022-03-31T17:59:30Z",
      "updatedDate": "2022-06-03T17:52:04Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.17274v2",
      "arxivUrl": "https://arxiv.org/abs/2203.17274",
      "comment": "16 pages, 10 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.14940",
      "title": "Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model",
      "authors": [
        {
          "name": "Yu Du",
          "affiliation": null
        },
        {
          "name": "Fangyun Wei",
          "affiliation": null
        },
        {
          "name": "Zihe Zhang",
          "affiliation": null
        },
        {
          "name": "Miaojing Shi",
          "affiliation": null
        },
        {
          "name": "Yue Gao",
          "affiliation": null
        },
        {
          "name": "Guoqi Li",
          "affiliation": null
        }
      ],
      "abstract": "Recently, vision-language pre-training shows great potential in open-vocabulary object detection, where detectors trained on base classes are devised for detecting new classes. The class text embedding is firstly generated by feeding prompts to the text encoder of a pre-trained vision-language model. It is then used as the region classifier to supervise the training of a detector. The key element that leads to the success of this model is the proper prompt, which requires careful words tuning and ingenious design. To avoid laborious prompt engineering, there are some prompt representation learning methods being proposed for the image classification task, which however can only be sub-optimal solutions when applied to the detection task. In this paper, we introduce a novel method, detection prompt (DetPro), to learn continuous prompt representations for open-vocabulary object detection based on the pre-trained vision-language model. Different from the previous classification-oriented methods, DetPro has two highlights: 1) a background interpretation scheme to include the proposals in image background into the prompt training; 2) a context grading scheme to separate proposals in image foreground for tailored prompt training. We assemble DetPro with ViLD, a recent state-of-the-art open-world object detector, and conduct experiments on the LVIS as well as transfer learning on the Pascal VOC, COCO, Objects365 datasets. Experimental results show that our DetPro outperforms the baseline ViLD in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on the novel classes of LVIS. Code and models are available at https://github.com/dyabel/detpro.",
      "publishedDate": "2022-03-28T17:50:26Z",
      "updatedDate": "2022-03-28T17:50:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.14940v1",
      "arxivUrl": "https://arxiv.org/abs/2203.14940",
      "comment": "Accepted by CVPR 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.11364",
      "title": "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels",
      "authors": [
        {
          "name": "Taylor Sorensen",
          "affiliation": null
        },
        {
          "name": "Joshua Robinson",
          "affiliation": null
        },
        {
          "name": "Christopher Michael Rytting",
          "affiliation": null
        },
        {
          "name": "Alexander Glenn Shaw",
          "affiliation": null
        },
        {
          "name": "Kyle Jeffrey Rogers",
          "affiliation": null
        },
        {
          "name": "Alexia Pauline Delorey",
          "affiliation": null
        },
        {
          "name": "Mahmoud Khalil",
          "affiliation": null
        },
        {
          "name": "Nancy Fulda",
          "affiliation": null
        },
        {
          "name": "David Wingate",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates \\textit{without labeled examples} and \\textit{without direct access to the model}. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90\\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.",
      "publishedDate": "2022-03-21T21:51:43Z",
      "updatedDate": "2022-03-21T21:51:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.11364v1",
      "arxivUrl": "https://arxiv.org/abs/2203.11364",
      "comment": null,
      "journalRef": null,
      "doi": "10.18653/v1/2022.acl-long.60",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.01543",
      "title": "QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognition",
      "authors": [
        {
          "name": "Andy T. Liu",
          "affiliation": null
        },
        {
          "name": "Wei Xiao",
          "affiliation": null
        },
        {
          "name": "Henghui Zhu",
          "affiliation": null
        },
        {
          "name": "Dejiao Zhang",
          "affiliation": null
        },
        {
          "name": "Shang-Wen Li",
          "affiliation": null
        },
        {
          "name": "Andrew Arnold",
          "affiliation": null
        }
      ],
      "abstract": "Recently, prompt-based learning for pre-trained language models has succeeded in few-shot Named Entity Recognition (NER) by exploiting prompts as task guidance to increase label efficiency. However, previous prompt-based methods for few-shot NER have limitations such as a higher computational complexity, poor zero-shot ability, requiring manual prompt engineering, or lack of prompt robustness. In this work, we address these shortcomings by proposing a new prompt-based learning NER method with Question Answering (QA), called QaNER. Our approach includes 1) a refined strategy for converting NER problems into the QA formulation; 2) NER prompt generation for QA models; 3) prompt-based tuning with QA models on a few annotated NER examples; 4) zero-shot NER by prompting the QA model. Comparing the proposed approach with previous methods, QaNER is faster at inference, insensitive to the prompt quality, and robust to hyper-parameters, as well as demonstrating significantly better low-resource performance and zero-shot capability.",
      "publishedDate": "2022-03-03T06:56:01Z",
      "updatedDate": "2022-03-04T07:58:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.01543v2",
      "arxivUrl": "https://arxiv.org/abs/2203.01543",
      "comment": "8 pages, 6 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2202.12109",
      "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
      "authors": [
        {
          "name": "Yubo Ma",
          "affiliation": null
        },
        {
          "name": "Zehao Wang",
          "affiliation": null
        },
        {
          "name": "Yixin Cao",
          "affiliation": null
        },
        {
          "name": "Mukai Li",
          "affiliation": null
        },
        {
          "name": "Meiqi Chen",
          "affiliation": null
        },
        {
          "name": "Kun Wang",
          "affiliation": null
        },
        {
          "name": "Jing Shao",
          "affiliation": null
        }
      ],
      "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5\\% and 2.3\\% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
      "publishedDate": "2022-02-24T14:07:25Z",
      "updatedDate": "2022-03-27T08:48:07Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2202.12109v2",
      "arxivUrl": "https://arxiv.org/abs/2202.12109",
      "comment": "Accepted by ACL 2022 main conference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2202.03286",
      "title": "Red Teaming Language Models with Language Models",
      "authors": [
        {
          "name": "Ethan Perez",
          "affiliation": null
        },
        {
          "name": "Saffron Huang",
          "affiliation": null
        },
        {
          "name": "Francis Song",
          "affiliation": null
        },
        {
          "name": "Trevor Cai",
          "affiliation": null
        },
        {
          "name": "Roman Ring",
          "affiliation": null
        },
        {
          "name": "John Aslanides",
          "affiliation": null
        },
        {
          "name": "Amelia Glaese",
          "affiliation": null
        },
        {
          "name": "Nat McAleese",
          "affiliation": null
        },
        {
          "name": "Geoffrey Irving",
          "affiliation": null
        }
      ],
      "abstract": "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (\"red teaming\") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",
      "publishedDate": "2022-02-07T15:22:17Z",
      "updatedDate": "2022-02-07T15:22:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2202.03286v1",
      "arxivUrl": "https://arxiv.org/abs/2202.03286",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.14024",
      "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
      "authors": [
        {
          "name": "Omar Khattab",
          "affiliation": null
        },
        {
          "name": "Keshav Santhanam",
          "affiliation": null
        },
        {
          "name": "Xiang Lisa Li",
          "affiliation": null
        },
        {
          "name": "David Hall",
          "affiliation": null
        },
        {
          "name": "Percy Liang",
          "affiliation": null
        },
        {
          "name": "Christopher Potts",
          "affiliation": null
        },
        {
          "name": "Matei Zaharia",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple \"retrieve-then-read\" pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp",
      "publishedDate": "2022-12-28T18:52:44Z",
      "updatedDate": "2023-01-23T17:00:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.14024v2",
      "arxivUrl": "https://arxiv.org/abs/2212.14024",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.14052",
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "authors": [
        {
          "name": "Daniel Y. Fu",
          "affiliation": null
        },
        {
          "name": "Tri Dao",
          "affiliation": null
        },
        {
          "name": "Khaled K. Saab",
          "affiliation": null
        },
        {
          "name": "Armin W. Thomas",
          "affiliation": null
        },
        {
          "name": "Atri Rudra",
          "affiliation": null
        },
        {
          "name": "Christopher Ré",
          "affiliation": null
        }
      ],
      "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",
      "publishedDate": "2022-12-28T17:56:03Z",
      "updatedDate": "2023-04-29T03:18:40Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.14052v3",
      "arxivUrl": "https://arxiv.org/abs/2212.14052",
      "comment": "ICLR 2023 Camera-Ready (Notable-top-25% / Spotlight)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.11353",
      "title": "Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning",
      "authors": [
        {
          "name": "Chris Lengerich",
          "affiliation": null
        },
        {
          "name": "Gabriel Synnaeve",
          "affiliation": null
        },
        {
          "name": "Amy Zhang",
          "affiliation": null
        },
        {
          "name": "Hugh Leather",
          "affiliation": null
        },
        {
          "name": "Kurt Shuster",
          "affiliation": null
        },
        {
          "name": "François Charton",
          "affiliation": null
        },
        {
          "name": "Charysse Redwood",
          "affiliation": null
        }
      ],
      "abstract": "Traditional approaches to RL have focused on learning decision policies directly from episodic decisions, while slowly and implicitly learning the semantics of compositional representations needed for generalization. While some approaches have been adopted to refine representations via auxiliary self-supervised losses while simultaneously learning decision policies, learning compositional representations from hand-designed and context-independent self-supervised losses (multi-view) still adapts relatively slowly to the real world, which contains many non-IID subspaces requiring rapid distribution shift in both time and spatial attention patterns at varying levels of abstraction. In contrast, supervised language model cascades have shown the flexibility to adapt to many diverse manifolds, and hints of self-learning needed for autonomous task transfer. However, to date, transfer methods for language models like few-shot learning and fine-tuning still require human supervision and transfer learning using self-learning methods has been underexplored. We propose a self-supervised loss policy called contrastive distillation which manifests latent variables with high mutual information with both source and target tasks from weights to tokens. We show how this outperforms common methods of transfer learning and suggests a useful design axis of trading off compute for generalizability for online transfer. Contrastive distillation is improved through sampling from memory and suggests a simple algorithm for more efficiently sampling negative examples for contrastive losses than random sampling.",
      "publishedDate": "2022-12-21T20:43:46Z",
      "updatedDate": "2022-12-21T20:43:46Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.11353v1",
      "arxivUrl": "https://arxiv.org/abs/2212.11353",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "tool-use",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10947",
      "title": "Parallel Context Windows for Large Language Models",
      "authors": [
        {
          "name": "Nir Ratner",
          "affiliation": null
        },
        {
          "name": "Yoav Levine",
          "affiliation": null
        },
        {
          "name": "Yonatan Belinkov",
          "affiliation": null
        },
        {
          "name": "Ori Ram",
          "affiliation": null
        },
        {
          "name": "Inbal Magar",
          "affiliation": null
        },
        {
          "name": "Omri Abend",
          "affiliation": null
        },
        {
          "name": "Ehud Karpas",
          "affiliation": null
        },
        {
          "name": "Amnon Shashua",
          "affiliation": null
        },
        {
          "name": "Kevin Leyton-Brown",
          "affiliation": null
        },
        {
          "name": "Yoav Shoham",
          "affiliation": null
        }
      ],
      "abstract": "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (``windows''), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ai21labs/parallel-context-windows.",
      "publishedDate": "2022-12-21T11:38:51Z",
      "updatedDate": "2023-08-01T16:48:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10947v3",
      "arxivUrl": "https://arxiv.org/abs/2212.10947",
      "comment": "The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10873",
      "title": "Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners",
      "authors": [
        {
          "name": "Hyunsoo Cho",
          "affiliation": null
        },
        {
          "name": "Hyuhng Joon Kim",
          "affiliation": null
        },
        {
          "name": "Junyeob Kim",
          "affiliation": null
        },
        {
          "name": "Sang-Woo Lee",
          "affiliation": null
        },
        {
          "name": "Sang-goo Lee",
          "affiliation": null
        },
        {
          "name": "Kang Min Yoo",
          "affiliation": null
        },
        {
          "name": "Taeuk Kim",
          "affiliation": null
        }
      ],
      "abstract": "Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified that PALP significantly enhances the input representations closing the gap between ICL in the data-hungry scenario and fine-tuning in the data-abundant scenario with little training overhead, potentially making PALP a strong alternative in a black-box scenario.",
      "publishedDate": "2022-12-21T09:37:05Z",
      "updatedDate": "2023-06-14T03:23:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10873v3",
      "arxivUrl": "https://arxiv.org/abs/2212.10873",
      "comment": "AAAI 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10755",
      "title": "JASMINE: Arabic GPT Models for Few-Shot Learning",
      "authors": [
        {
          "name": "El Moatez Billah Nagoudi",
          "affiliation": null
        },
        {
          "name": "Muhammad Abdul-Mageed",
          "affiliation": null
        },
        {
          "name": "AbdelRahim Elmadany",
          "affiliation": null
        },
        {
          "name": "Alcides Alcoba Inciarte",
          "affiliation": null
        },
        {
          "name": "Md Tawkat Islam Khondaker",
          "affiliation": null
        }
      ],
      "abstract": "Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset (~ 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a wide range of NLP tasks. We aim to responsibly release our models and evaluation benchmark with interested researchers, along with code for experimenting with them.",
      "publishedDate": "2022-12-21T04:21:46Z",
      "updatedDate": "2023-10-24T21:03:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10755v2",
      "arxivUrl": "https://arxiv.org/abs/2212.10755",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "evaluation",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10670",
      "title": "In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models",
      "authors": [
        {
          "name": "Yukun Huang",
          "affiliation": null
        },
        {
          "name": "Yanda Chen",
          "affiliation": null
        },
        {
          "name": "Zhou Yu",
          "affiliation": null
        },
        {
          "name": "Kathleen McKeown",
          "affiliation": null
        }
      ],
      "abstract": "Given the success with in-context learning of large pre-trained language models, we introduce in-context learning distillation to transfer in-context few-shot learning ability from large models to smaller models. We propose to combine in-context learning objectives with language modeling objectives to distill both the ability to read in-context examples and task knowledge to the smaller models. We perform in-context learning distillation under two different few-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask In-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask few-shot learning but also requires more computation than Meta-ICT. Our method shows consistent improvements for both Meta-ICT and Multitask-ICT on two benchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal that in-context learning objectives and language modeling objectives are complementary under the Multitask-ICT paradigm. In-context learning objectives achieve the best performance when combined with language modeling objectives.",
      "publishedDate": "2022-12-20T22:11:35Z",
      "updatedDate": "2022-12-20T22:11:35Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10670v1",
      "arxivUrl": "https://arxiv.org/abs/2212.10670",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10559",
      "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers",
      "authors": [
        {
          "name": "Damai Dai",
          "affiliation": null
        },
        {
          "name": "Yutao Sun",
          "affiliation": null
        },
        {
          "name": "Li Dong",
          "affiliation": null
        },
        {
          "name": "Yaru Hao",
          "affiliation": null
        },
        {
          "name": "Shuming Ma",
          "affiliation": null
        },
        {
          "name": "Zhifang Sui",
          "affiliation": null
        },
        {
          "name": "Furu Wei",
          "affiliation": null
        }
      ],
      "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \\url{https://aka.ms/icl}.",
      "publishedDate": "2022-12-20T18:58:48Z",
      "updatedDate": "2023-05-15T11:45:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10559v3",
      "arxivUrl": "https://arxiv.org/abs/2212.10559",
      "comment": "Accepted to ACL 2023 findings",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.10378",
      "title": "Data Curation Alone Can Stabilize In-context Learning",
      "authors": [
        {
          "name": "Ting-Yun Chang",
          "affiliation": null
        },
        {
          "name": "Robin Jia",
          "affiliation": null
        }
      ],
      "abstract": "In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets -- both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while Datamodels learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy over sampling from the entire training set by 7.7% and 6.3%, respectively. Surprisingly, the stable subset examples are not especially diverse in content or low in perplexity, in contrast with other work suggesting that diversity and perplexity are important when prompting LLMs.",
      "publishedDate": "2022-12-20T15:58:54Z",
      "updatedDate": "2023-05-24T22:32:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.10378v2",
      "arxivUrl": "https://arxiv.org/abs/2212.10378",
      "comment": "ACL 2023 camera-ready",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.09865",
      "title": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations",
      "authors": [
        {
          "name": "Xinxi Lyu",
          "affiliation": null
        },
        {
          "name": "Sewon Min",
          "affiliation": null
        },
        {
          "name": "Iz Beltagy",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "Hannaneh Hajishirzi",
          "affiliation": null
        }
      ],
      "abstract": "Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that further improve zero-shot results.",
      "publishedDate": "2022-12-19T21:34:26Z",
      "updatedDate": "2023-06-03T22:51:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.09865v2",
      "arxivUrl": "https://arxiv.org/abs/2212.09865",
      "comment": "11 pages; 9 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.09803",
      "title": "Training Trajectories of Language Models Across Scales",
      "authors": [
        {
          "name": "Mengzhou Xia",
          "affiliation": null
        },
        {
          "name": "Mikel Artetxe",
          "affiliation": null
        },
        {
          "name": "Chunting Zhou",
          "affiliation": null
        },
        {
          "name": "Xi Victoria Lin",
          "affiliation": null
        },
        {
          "name": "Ramakanth Pasunuru",
          "affiliation": null
        },
        {
          "name": "Danqi Chen",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "Ves Stoyanov",
          "affiliation": null
        }
      ],
      "abstract": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token prediction, sequence-level generation, and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior; 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
      "publishedDate": "2022-12-19T19:16:29Z",
      "updatedDate": "2023-05-30T03:33:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.09803v3",
      "arxivUrl": "https://arxiv.org/abs/2212.09803",
      "comment": "Accepted to ACL 2023; The code and analysis results are available at https://github.com/xiamengzhou/training_trajectory_analysis",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.09736",
      "title": "Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",
      "authors": [
        {
          "name": "Yu Gu",
          "affiliation": null
        },
        {
          "name": "Xiang Deng",
          "affiliation": null
        },
        {
          "name": "Yu Su",
          "affiliation": null
        }
      ],
      "abstract": "A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains. Pangu also enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex.",
      "publishedDate": "2022-12-19T18:55:21Z",
      "updatedDate": "2023-05-03T04:32:35Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.09736v2",
      "arxivUrl": "https://arxiv.org/abs/2212.09736",
      "comment": "18 pages, 6 figures, 6 tables; accepted to ACL'2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "agents",
        "tool-use",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "tool-use",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.09095",
      "title": "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale",
      "authors": [
        {
          "name": "Hritik Bansal",
          "affiliation": null
        },
        {
          "name": "Karthik Gopalakrishnan",
          "affiliation": null
        },
        {
          "name": "Saket Dingliwal",
          "affiliation": null
        },
        {
          "name": "Sravan Bodapati",
          "affiliation": null
        },
        {
          "name": "Katrin Kirchhoff",
          "affiliation": null
        },
        {
          "name": "Dan Roth",
          "affiliation": null
        }
      ],
      "abstract": "Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: $\\sim$70% of attention heads and $\\sim$20% of feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These induction heads overlap with task-specific important heads, reinforcing arguments by Olsson et al. (arXiv:2209.11895) regarding induction head generality to more sophisticated behaviors associated with in-context learning. Overall, our study provides several insights that indicate large language models may be under-trained for in-context learning and opens up questions on how to pre-train language models to more effectively perform in-context learning.",
      "publishedDate": "2022-12-18T14:36:07Z",
      "updatedDate": "2023-08-16T09:09:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.09095v2",
      "arxivUrl": "https://arxiv.org/abs/2212.09095",
      "comment": "Accepted at Annual Meeting of the Association for Computational Linguistics (ACL) 2023, Main Proceedings",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.08979",
      "title": "Language model acceptability judgements are not always robust to context",
      "authors": [
        {
          "name": "Koustuv Sinha",
          "affiliation": null
        },
        {
          "name": "Jon Gauthier",
          "affiliation": null
        },
        {
          "name": "Aaron Mueller",
          "affiliation": null
        },
        {
          "name": "Kanishka Misra",
          "affiliation": null
        },
        {
          "name": "Keren Fuentes",
          "affiliation": null
        },
        {
          "name": "Roger Levy",
          "affiliation": null
        },
        {
          "name": "Adina Williams",
          "affiliation": null
        }
      ],
      "abstract": "Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Most targeted syntactic evaluation datasets ask models to make these judgements with just a single context-free sentence as input. This does not match language models' training regime, in which input sentences are always highly contextualized by the surrounding corpus. This mismatch raises an important question: how robust are models' syntactic judgements in different contexts? In this paper, we investigate the stability of language models' performance on targeted syntactic evaluations as we vary properties of the input context: the length of the context, the types of syntactic phenomena it contains, and whether or not there are violations of grammaticality. We find that model judgements are generally robust when placed in randomly sampled linguistic contexts. However, they are substantially unstable for contexts containing syntactic structures matching those in the critical test content. Among all tested models (GPT-2 and five variants of OPT), we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures. This effect is amplified by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by simple features matching the context and the test inputs, such as lexical overlap and dependency overlap. This sensitivity to highly specific syntactic features of the context can only be explained by the models' implicit in-context learning abilities.",
      "publishedDate": "2022-12-18T00:11:06Z",
      "updatedDate": "2022-12-18T00:11:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.08979v1",
      "arxivUrl": "https://arxiv.org/abs/2212.08979",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.08686",
      "title": "Evaluating Step-by-Step Reasoning through Symbolic Verification",
      "authors": [
        {
          "name": "Yi-Fan Zhang",
          "affiliation": null
        },
        {
          "name": "Hanlin Zhang",
          "affiliation": null
        },
        {
          "name": "Li Erran Li",
          "affiliation": null
        },
        {
          "name": "Eric Xing",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained language models (LMs) have shown remarkable reasoning performance using explanations or chain-of-thoughts (CoT)) for in-context learning. On the other hand, these reasoning tasks are usually presumed to be more approachable for symbolic programming. To understand the mechanism of reasoning of LMs, we curate synthetic datasets containing equivalent (natural, symbolic) data pairs, where symbolic examples contain first-order logic rules and predicates from non-parametric knowledge bases (KBs), supporting automated verification of intermediate reasoning results. Then we revisit neuro-symbolic approaches and propose to learn from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog's backward chaining algorithm and supporting automated verification of LMs' outputs. Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more than $25\\%$ higher accuracy than CoT on length generalization benchmarks even with smaller model sizes.",
      "publishedDate": "2022-12-16T19:30:01Z",
      "updatedDate": "2024-03-28T08:20:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.08686v2",
      "arxivUrl": "https://arxiv.org/abs/2212.08686",
      "comment": "NAACL-Findings, 2024",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "evaluation",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.08635",
      "title": "Self-Prompting Large Language Models for Zero-Shot Open-Domain QA",
      "authors": [
        {
          "name": "Junlong Li",
          "affiliation": null
        },
        {
          "name": "Jinyuan Wang",
          "affiliation": null
        },
        {
          "name": "Zhuosheng Zhang",
          "affiliation": null
        },
        {
          "name": "Hai Zhao",
          "affiliation": null
        }
      ],
      "abstract": "Open-Domain Question Answering (ODQA) aims to answer questions without explicitly providing specific background documents. This task becomes notably challenging in a zero-shot setting where no data is available to train tailored retrieval-reader models. While recent Large Language Models (LLMs) like GPT-3 have demonstrated their effectiveness in zero-shot ODQA using direct prompting methods, these methods still fall short of fully harnessing the potential of LLMs when implicitly invoked. In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge encoded in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations entirely from scratch. These generated elements are then utilized for in-context learning. Experimental results show that our method significantly surpasses previous state-of-the-art zero-shot methods on three widely-used ODQA datasets and even achieves comparable performance with various customized fine-tuned models on full training data. Our code is available at https://github.com/lockon-n/self-prompting.",
      "publishedDate": "2022-12-16T18:23:43Z",
      "updatedDate": "2024-03-28T06:06:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.08635v3",
      "arxivUrl": "https://arxiv.org/abs/2212.08635",
      "comment": "NAACL 2024",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.08354",
      "title": "FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks",
      "authors": [
        {
          "name": "Weilong Dong",
          "affiliation": null
        },
        {
          "name": "Xinwei Wu",
          "affiliation": null
        },
        {
          "name": "Junzhuo Li",
          "affiliation": null
        },
        {
          "name": "Shuangzhi Wu",
          "affiliation": null
        },
        {
          "name": "Chao Bian",
          "affiliation": null
        },
        {
          "name": "Deyi Xiong",
          "affiliation": null
        }
      ],
      "abstract": "Massively multi-task learning with large language models has recently made substantial progress on few-shot generalization. However, this is usually performed in a centralized learning fashion, ignoring the privacy sensitivity issue of (annotated) data used in multiple tasks. To mitigate this issue, we propose FewFedWeight, a few-shot federated learning framework across multiple tasks, to achieve the best of both worlds: privacy preservation and cross-task generalization. FewFedWeight trains client models in isolated devices without sharing data. It broadcasts the global model in the server to each client and produces pseudo data for clients so that knowledge from the global model can be explored to enhance few-shot learning of each client model. An energy-based algorithm is further proposed to weight pseudo samples in order to reduce the negative impact of noise from the generated pseudo data. Adaptive model weights of client models are also tuned according to their performance. We use these model weights to dynamically aggregate client models to update the global model. Experiments on 118 NLP tasks show that FewFedWeight can significantly improve the performance of client models on 61% tasks with an average performance improvement rate of 30.5% over the baseline and substantially outperform FedAvg and other decentralized learning methods.",
      "publishedDate": "2022-12-16T09:01:56Z",
      "updatedDate": "2022-12-16T09:01:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.08354v1",
      "arxivUrl": "https://arxiv.org/abs/2212.08354",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.08286",
      "title": "ALERT: Adapting Language Models to Reasoning Tasks",
      "authors": [
        {
          "name": "Ping Yu",
          "affiliation": null
        },
        {
          "name": "Tianlu Wang",
          "affiliation": null
        },
        {
          "name": "Olga Golovneva",
          "affiliation": null
        },
        {
          "name": "Badr AlKhamissi",
          "affiliation": null
        },
        {
          "name": "Siddharth Verma",
          "affiliation": null
        },
        {
          "name": "Zhijing Jin",
          "affiliation": null
        },
        {
          "name": "Gargi Ghosh",
          "affiliation": null
        },
        {
          "name": "Mona Diab",
          "affiliation": null
        },
        {
          "name": "Asli Celikyilmaz",
          "affiliation": null
        }
      ],
      "abstract": "Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",
      "publishedDate": "2022-12-16T05:15:41Z",
      "updatedDate": "2023-07-07T17:43:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.08286v2",
      "arxivUrl": "https://arxiv.org/abs/2212.08286",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.06713",
      "title": "Structured Prompting: Scaling In-Context Learning to 1,000 Examples",
      "authors": [
        {
          "name": "Yaru Hao",
          "affiliation": null
        },
        {
          "name": "Yutao Sun",
          "affiliation": null
        },
        {
          "name": "Li Dong",
          "affiliation": null
        },
        {
          "name": "Zhixiong Han",
          "affiliation": null
        },
        {
          "name": "Yuxian Gu",
          "affiliation": null
        },
        {
          "name": "Furu Wei",
          "affiliation": null
        }
      ],
      "abstract": "Large language models have exhibited intriguing in-context learning capability, achieving promising zero- and few-shot performance without updating the parameters. However, conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. Specifically, demonstration examples are separately encoded with well-designed position embeddings, and then they are jointly attended by the test example using a rescaled attention mechanism. So we can scale the number of exemplars with linear complexity instead of quadratic complexity with respect to length. Experimental results on a diverse set of tasks show that our approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases. Code has been released at https://aka.ms/structured-prompting.",
      "publishedDate": "2022-12-13T16:31:21Z",
      "updatedDate": "2022-12-13T16:31:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.06713v1",
      "arxivUrl": "https://arxiv.org/abs/2212.06713",
      "comment": "14 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.06556",
      "title": "Localized Latent Updates for Fine-Tuning Vision-Language Models",
      "authors": [
        {
          "name": "Moritz Ibing",
          "affiliation": null
        },
        {
          "name": "Isaak Lim",
          "affiliation": null
        },
        {
          "name": "Leif Kobbelt",
          "affiliation": null
        }
      ],
      "abstract": "Although massive pre-trained vision-language models like CLIP show impressive generalization capabilities for many tasks, still it often remains necessary to fine-tune them for improved performance on specific datasets. When doing so, it is desirable that updating the model is fast and that the model does not lose its capabilities on data outside of the dataset, as is often the case with classical fine-tuning approaches. In this work we suggest a lightweight adapter, that only updates the models predictions close to seen datapoints. We demonstrate the effectiveness and speed of this relatively simple approach in the context of few-shot learning, where our results both on classes seen and unseen during training are comparable with or improve on the state of the art.",
      "publishedDate": "2022-12-13T13:15:20Z",
      "updatedDate": "2022-12-13T13:15:20Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.06556v1",
      "arxivUrl": "https://arxiv.org/abs/2212.06556",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.06369",
      "title": "Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model",
      "authors": [
        {
          "name": "Jiang-Long Song",
          "affiliation": null
        },
        {
          "name": "Wu-He Zou",
          "affiliation": null
        },
        {
          "name": "Feng Li",
          "affiliation": null
        },
        {
          "name": "Xiao-Lei Qin",
          "affiliation": null
        },
        {
          "name": "Wei-Dong Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Prompt tuning recently becomes a hot-spot in the applications of large pretrained language models on specific downstream tasks. Regarding the Language Model as a Service (LMaaS), black-box tuning using derivative-free optimization (DFO) provides a novel approach to expand the practical scenarios of pretrained models and enrich the researches of few-shot learning. In this report, we present our solution in this competition that is based on the LMaaS scenario. Our solution consists of several modifications to BBTv2, including multiple label words, selection of P0, rolling update strategy, multi-task loss from MLP classifier, and finally using the ensemble method to further improve generalization ability. We also shared some strategies that we tried but didn't use in the final submission for further discussion. In the end we raised a question about the SNLI dataset and the impact on the results, as well as our concerns about the competition.",
      "publishedDate": "2022-12-13T04:57:04Z",
      "updatedDate": "2022-12-20T07:40:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.06369v3",
      "arxivUrl": "https://arxiv.org/abs/2212.06369",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.04037",
      "title": "Demystifying Prompts in Language Models via Perplexity Estimation",
      "authors": [
        {
          "name": "Hila Gonen",
          "affiliation": null
        },
        {
          "name": "Srini Iyer",
          "affiliation": null
        },
        {
          "name": "Terra Blevins",
          "affiliation": null
        },
        {
          "name": "Noah A. Smith",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        }
      ],
      "abstract": "Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens or how to pick the best prompts. In this work, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt is, the better the prompt is able to perform the task. As a result, we devise a method for creating prompts: (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get significant gains in performance.",
      "publishedDate": "2022-12-08T02:21:47Z",
      "updatedDate": "2024-09-12T19:54:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.04037v2",
      "arxivUrl": "https://arxiv.org/abs/2212.04037",
      "comment": "Published in Findings of EMNLP 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.02437",
      "title": "In-context Examples Selection for Machine Translation",
      "authors": [
        {
          "name": "Sweta Agrawal",
          "affiliation": null
        },
        {
          "name": "Chunting Zhou",
          "affiliation": null
        },
        {
          "name": "Mike Lewis",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "Marjan Ghazvininejad",
          "affiliation": null
        }
      ],
      "abstract": "Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in-context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.",
      "publishedDate": "2022-12-05T17:25:15Z",
      "updatedDate": "2022-12-05T17:25:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.02437v1",
      "arxivUrl": "https://arxiv.org/abs/2212.02437",
      "comment": "14 pages; 4 figures; 16 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.02216",
      "title": "Improving Few-Shot Performance of Language Models via Nearest Neighbor Calibration",
      "authors": [
        {
          "name": "Feng Nie",
          "affiliation": null
        },
        {
          "name": "Meixi Chen",
          "affiliation": null
        },
        {
          "name": "Zhirui Zhang",
          "affiliation": null
        },
        {
          "name": "Xu Cheng",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained language models (PLMs) have exhibited remarkable few-shot learning capabilities when provided a few examples in a natural language prompt as demonstrations of test instances, i.e., in-context learning. However, the performance of in-context learning is susceptible to the choice of prompt format, training examples and the ordering of the training examples. In this paper, we propose a novel nearest-neighbor calibration framework for in-context learning to ease this issue. It is inspired by a phenomenon that the in-context learning paradigm produces incorrect labels when inferring training instances, which provides a useful supervised signal to calibrate predictions. Thus, our method directly augments the predictions with a $k$-nearest-neighbor ($k$NN) classifier over a datastore of cached few-shot instance representations obtained by PLMs and their corresponding labels. Then adaptive neighbor selection and feature regularization modules are introduced to make full use of a few support instances to reduce the $k$NN retrieval noise. Experiments on various few-shot text classification tasks demonstrate that our method significantly improves in-context learning, while even achieving comparable performance with state-of-the-art tuning-based approaches in some sentiment analysis tasks.",
      "publishedDate": "2022-12-05T12:49:41Z",
      "updatedDate": "2022-12-05T12:49:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.02216v1",
      "arxivUrl": "https://arxiv.org/abs/2212.02216",
      "comment": "Work in progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.01692",
      "title": "Can In-context Learners Learn a Reasoning Concept from Demonstrations?",
      "authors": [
        {
          "name": "Michal Štefánik",
          "affiliation": null
        },
        {
          "name": "Marek Kadlčík",
          "affiliation": null
        }
      ],
      "abstract": "Language models exhibit an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of learning new associations from the input. We argue that the commonly-used few-shot evaluation using a random selection of in-context demonstrations can not disentangle models' reliance on such biases, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the task's input-output distribution. Therefore, to evaluate models' in-context learning ability independent of models' memory, we introduce a Concept-sharing few-shot learning method choosing the demonstrations that share an underlying concept with the predicted sample. We extract a set of such concepts from available human explanations and measure how much models can benefit from presenting these concepts in few-shot demonstrations. We find that most of the recent in-context learners can not consistently benefit from the demonstrated concepts, irrespective of the model size. However, we note that T0 models are more sensitive to exhibited concepts, benefiting from concept-sharing demonstrations in 7 out of 8 evaluation scenarios.",
      "publishedDate": "2022-12-03T21:14:32Z",
      "updatedDate": "2023-07-19T06:48:35Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.01692v4",
      "arxivUrl": "https://arxiv.org/abs/2212.01692",
      "comment": "Awarded Best Paper at ACL 2023 Natural Language Reasoning and Structured Explanations (NLRSE) workshop",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.16740",
      "title": "Explicit Knowledge Transfer for Weakly-Supervised Code Generation",
      "authors": [
        {
          "name": "Zhangir Azerbayev",
          "affiliation": null
        },
        {
          "name": "Ansong Ni",
          "affiliation": null
        },
        {
          "name": "Hailey Schoelkopf",
          "affiliation": null
        },
        {
          "name": "Dragomir Radev",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) can acquire strong code-generation capabilities through few-shot learning. In contrast, supervised fine-tuning is still needed for smaller models to achieve good performance. Such fine-tuning demands a large number of task-specific NL-code pairs, which are expensive to obtain. In this paper, we attempt to transfer the code generation ability of an LLM to a smaller model with the aid of weakly-supervised data. More specifically, we propose explicit knowledge transfer (EKT), which uses the few-shot capabilities of a teacher LLM to create NL-code pairs that we then filter for correctness and fine-tune the student on. We evaluate EKT on the task of generating code solutions to math word problems from the GSM8k dataset. We find that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer. A GPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4% pass@100 on GSM8k, while the same student and teacher trained with knowledge distillation yield only a 3.7% pass@100. We also show that it is possible for a student model to outperform the teacher using EKT.",
      "publishedDate": "2022-11-30T04:51:26Z",
      "updatedDate": "2023-06-07T18:01:11Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.16740v3",
      "arxivUrl": "https://arxiv.org/abs/2211.16740",
      "comment": "Updated on Jun 7. 2023 with ICLR workshop header",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.16191",
      "title": "SgVA-CLIP: Semantic-guided Visual Adapting of Vision-Language Models for Few-shot Image Classification",
      "authors": [
        {
          "name": "Fang Peng",
          "affiliation": null
        },
        {
          "name": "Xiaoshan Yang",
          "affiliation": null
        },
        {
          "name": "Linhui Xiao",
          "affiliation": null
        },
        {
          "name": "Yaowei Wang",
          "affiliation": null
        },
        {
          "name": "Changsheng Xu",
          "affiliation": null
        }
      ],
      "abstract": "Although significant progress has been made in few-shot learning, most of existing few-shot image classification methods require supervised pre-training on a large amount of samples of base classes, which limits their generalization ability in real world application. Recently, large-scale Vision-Language Pre-trained models (VLPs) have been gaining increasing attention in few-shot learning because they can provide a new paradigm for transferable visual representation learning with easily available text on the Web. However, the VLPs may neglect detailed visual information that is difficult to describe by language sentences, but important for learning an effective classifier to distinguish different images. To address the above problem, we propose a new framework, named Semantic-guided Visual Adapting (SgVA), which can effectively extend vision-language pre-trained models to produce discriminative adapted visual features by comprehensively using an implicit knowledge distillation, a vision-specific contrastive loss, and a cross-modal contrastive loss. The implicit knowledge distillation is designed to transfer the fine-grained cross-modal knowledge to guide the updating of the vision adapter. State-of-the-art results on 13 datasets demonstrate that the adapted visual features can well complement the cross-modal features to improve few-shot image classification.",
      "publishedDate": "2022-11-28T14:58:15Z",
      "updatedDate": "2023-01-20T13:56:39Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.MM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.16191v2",
      "arxivUrl": "https://arxiv.org/abs/2211.16191",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.15202",
      "title": "Revisiting Distance Metric Learning for Few-Shot Natural Language Classification",
      "authors": [
        {
          "name": "Witold Sosnowski",
          "affiliation": null
        },
        {
          "name": "Anna Wróblewska",
          "affiliation": null
        },
        {
          "name": "Karolina Seweryn",
          "affiliation": null
        },
        {
          "name": "Piotr Gawrysiak",
          "affiliation": null
        }
      ],
      "abstract": "Distance Metric Learning (DML) has attracted much attention in image processing in recent years. This paper analyzes its impact on supervised fine-tuning language models for Natural Language Processing (NLP) classification tasks under few-shot learning settings. We investigated several DML loss functions in training RoBERTa language models on known SentEval Transfer Tasks datasets. We also analyzed the possibility of using proxy-based DML losses during model inference. Our systematic experiments have shown that under few-shot learning settings, particularly proxy-based DML losses can positively affect the fine-tuning and inference of a supervised language model. Models tuned with a combination of CCE (categorical cross-entropy loss) and ProxyAnchor Loss have, on average, the best performance and outperform models with only CCE by about 3.27 percentage points -- up to 10.38 percentage points depending on the training dataset.",
      "publishedDate": "2022-11-28T10:19:31Z",
      "updatedDate": "2022-11-28T10:19:31Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.15202v1",
      "arxivUrl": "https://arxiv.org/abs/2211.15202",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.13892",
      "title": "Complementary Explanations for Effective In-Context Learning",
      "authors": [
        {
          "name": "Xi Ye",
          "affiliation": null
        },
        {
          "name": "Srinivasan Iyer",
          "affiliation": null
        },
        {
          "name": "Asli Celikyilmaz",
          "affiliation": null
        },
        {
          "name": "Ves Stoyanov",
          "affiliation": null
        },
        {
          "name": "Greg Durrett",
          "affiliation": null
        },
        {
          "name": "Ramakanth Pasunuru",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.",
      "publishedDate": "2022-11-25T04:40:47Z",
      "updatedDate": "2023-06-12T19:50:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.13892v2",
      "arxivUrl": "https://arxiv.org/abs/2211.13892",
      "comment": "ACL Findings 2023 Camera-Ready",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.12561",
      "title": "Retrieval-Augmented Multimodal Language Modeling",
      "authors": [
        {
          "name": "Michihiro Yasunaga",
          "affiliation": null
        },
        {
          "name": "Armen Aghajanyan",
          "affiliation": null
        },
        {
          "name": "Weijia Shi",
          "affiliation": null
        },
        {
          "name": "Rich James",
          "affiliation": null
        },
        {
          "name": "Jure Leskovec",
          "affiliation": null
        },
        {
          "name": "Percy Liang",
          "affiliation": null
        },
        {
          "name": "Mike Lewis",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "Wen-tau Yih",
          "affiliation": null
        }
      ],
      "abstract": "Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities, such as faithful image generation and multimodal in-context learning (e.g., image generation from demonstrations).",
      "publishedDate": "2022-11-22T20:26:44Z",
      "updatedDate": "2023-06-06T00:28:34Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.12561v2",
      "arxivUrl": "https://arxiv.org/abs/2211.12561",
      "comment": "Published at ICML 2023. Blog post available at https://cs.stanford.edu/~myasu/blog/racm3/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.11559",
      "title": "Visual Programming: Compositional visual reasoning without training",
      "authors": [
        {
          "name": "Tanmay Gupta",
          "affiliation": null
        },
        {
          "name": "Aniruddha Kembhavi",
          "affiliation": null
        }
      ],
      "abstract": "We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. VISPROG avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.",
      "publishedDate": "2022-11-18T18:50:09Z",
      "updatedDate": "2022-11-18T18:50:09Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.11559v1",
      "arxivUrl": "https://arxiv.org/abs/2211.11559",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.09084",
      "title": "Technical Report on Neural Language Models and Few-Shot Learning for Systematic Requirements Processing in MDSE",
      "authors": [
        {
          "name": "Vincent Bertram",
          "affiliation": null
        },
        {
          "name": "Miriam Boß",
          "affiliation": null
        },
        {
          "name": "Evgeny Kusmenko",
          "affiliation": null
        },
        {
          "name": "Imke Helene Nachmann",
          "affiliation": null
        },
        {
          "name": "Bernhard Rumpe",
          "affiliation": null
        },
        {
          "name": "Danilo Trotta",
          "affiliation": null
        },
        {
          "name": "Louis Wachtmeister",
          "affiliation": null
        }
      ],
      "abstract": "Systems engineering, in particular in the automotive domain, needs to cope with the massively increasing numbers of requirements that arise during the development process. To guarantee a high product quality and make sure that functional safety standards such as ISO26262 are fulfilled, the exploitation of potentials of model-driven systems engineering in the form of automatic analyses, consistency checks, and tracing mechanisms is indispensable. However, the language in which requirements are written, and the tools needed to operate on them, are highly individual and require domain-specific tailoring. This hinders automated processing of requirements as well as the linking of requirements to models. Introducing formal requirement notations in existing projects leads to the challenge of translating masses of requirements and process changes on the one hand and to the necessity of the corresponding training for the requirements engineers. In this paper, based on the analysis of an open-source set of automotive requirements, we derive domain-specific language constructs helping us to avoid ambiguities in requirements and increase the level of formality. The main contribution is the adoption and evaluation of few-shot learning with large pretrained language models for the automated translation of informal requirements to structured languages such as a requirement DSL. We show that support sets of less than ten translation examples can suffice to few-shot train a language model to incorporate keywords and implement syntactic rules into informal natural language requirements.",
      "publishedDate": "2022-11-16T18:06:25Z",
      "updatedDate": "2022-11-16T18:06:25Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.09084v1",
      "arxivUrl": "https://arxiv.org/abs/2211.09084",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.08473",
      "title": "On the Compositional Generalization Gap of In-Context Learning",
      "authors": [
        {
          "name": "Arian Hosseini",
          "affiliation": null
        },
        {
          "name": "Ankit Vani",
          "affiliation": null
        },
        {
          "name": "Dzmitry Bahdanau",
          "affiliation": null
        },
        {
          "name": "Alessandro Sordoni",
          "affiliation": null
        },
        {
          "name": "Aaron Courville",
          "affiliation": null
        }
      ],
      "abstract": "Pretrained large generative language models have shown great performance on many tasks, but exhibit low compositional generalization abilities. Scaling such models has been shown to improve their performance on various NLP tasks even just by conditioning them on a few examples to solve the task without any fine-tuning (also known as in-context learning). In this work, we look at the gap between the in-distribution (ID) and out-of-distribution (OOD) performance of such models in semantic parsing tasks with in-context learning. In the ID settings, the demonstrations are from the same split (test or train) that the model is being evaluated on, and in the OOD settings, they are from the other split. We look at how the relative generalization gap of in-context learning evolves as models are scaled up. We evaluate four model families, OPT, BLOOM, CodeGen and Codex on three semantic parsing datasets, CFQ, SCAN and GeoQuery with different number of exemplars, and observe a trend of decreasing relative generalization gap as models are scaled up.",
      "publishedDate": "2022-11-15T19:56:37Z",
      "updatedDate": "2022-11-15T19:56:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.08473v1",
      "arxivUrl": "https://arxiv.org/abs/2211.08473",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.08264",
      "title": "QAmeleon: Multilingual QA with Only 5 Examples",
      "authors": [
        {
          "name": "Priyanka Agrawal",
          "affiliation": null
        },
        {
          "name": "Chris Alberti",
          "affiliation": null
        },
        {
          "name": "Fantine Huot",
          "affiliation": null
        },
        {
          "name": "Joshua Maynez",
          "affiliation": null
        },
        {
          "name": "Ji Ma",
          "affiliation": null
        },
        {
          "name": "Sebastian Ruder",
          "affiliation": null
        },
        {
          "name": "Kuzman Ganchev",
          "affiliation": null
        },
        {
          "name": "Dipanjan Das",
          "affiliation": null
        },
        {
          "name": "Mirella Lapata",
          "affiliation": null
        }
      ],
      "abstract": "The availability of large, high-quality datasets has been one of the main drivers of recent progress in question answering (QA). Such annotated datasets however are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation. Prompt tuning the PLM for data synthesis with only five examples per language delivers accuracy superior to translation-based baselines, bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled examples in low resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.",
      "publishedDate": "2022-11-15T16:14:39Z",
      "updatedDate": "2023-08-07T11:22:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.08264v2",
      "arxivUrl": "https://arxiv.org/abs/2211.08264",
      "comment": "To Appear at Transactions of Association for Computational Linguistics (TACL)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.09066",
      "title": "Teaching Algorithmic Reasoning via In-context Learning",
      "authors": [
        {
          "name": "Hattie Zhou",
          "affiliation": null
        },
        {
          "name": "Azade Nova",
          "affiliation": null
        },
        {
          "name": "Hugo Larochelle",
          "affiliation": null
        },
        {
          "name": "Aaron Courville",
          "affiliation": null
        },
        {
          "name": "Behnam Neyshabur",
          "affiliation": null
        },
        {
          "name": "Hanie Sedghi",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.",
      "publishedDate": "2022-11-15T06:12:28Z",
      "updatedDate": "2022-11-15T06:12:28Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.09066v1",
      "arxivUrl": "https://arxiv.org/abs/2211.09066",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.07830",
      "title": "Prompting Language Models for Linguistic Structure",
      "authors": [
        {
          "name": "Terra Blevins",
          "affiliation": null
        },
        {
          "name": "Hila Gonen",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        }
      ],
      "abstract": "Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.",
      "publishedDate": "2022-11-15T01:13:39Z",
      "updatedDate": "2023-05-21T01:52:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.07830v2",
      "arxivUrl": "https://arxiv.org/abs/2211.07830",
      "comment": "ACL 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.07067",
      "title": "Retrieval-Augmented Generative Question Answering for Event Argument Extraction",
      "authors": [
        {
          "name": "Xinya Du",
          "affiliation": null
        },
        {
          "name": "Heng Ji",
          "affiliation": null
        }
      ],
      "abstract": "Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a complicated target sequence (template). Motivated by these observations and recent pretrained language models' capabilities of learning from demonstrations. We propose a retrieval-augmented generative QA model (R-GQA) for event argument extraction. It retrieves the most similar QA pair and augments it as prompt to the current example's context, then decodes the arguments as answers. Our approach outperforms substantially prior methods across various settings (i.e. fully supervised, domain transfer, and fewshot learning). Finally, we propose a clustering-based sampling strategy (JointEnc) and conduct a thorough analysis of how different strategies influence the few-shot learning performance. The implementations are available at https:// github.com/xinyadu/RGQA",
      "publishedDate": "2022-11-14T02:00:32Z",
      "updatedDate": "2022-11-14T02:00:32Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.07067v1",
      "arxivUrl": "https://arxiv.org/abs/2211.07067",
      "comment": "Accepted by EMNLP 2022 (18 pages)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.06869",
      "title": "Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters",
      "authors": [
        {
          "name": "Nuo Chen",
          "affiliation": null
        },
        {
          "name": "Yan Wang",
          "affiliation": null
        },
        {
          "name": "Haiyun Jiang",
          "affiliation": null
        },
        {
          "name": "Deng Cai",
          "affiliation": null
        },
        {
          "name": "Yuhan Li",
          "affiliation": null
        },
        {
          "name": "Ziyang Chen",
          "affiliation": null
        },
        {
          "name": "Longyue Wang",
          "affiliation": null
        },
        {
          "name": "Jia Li",
          "affiliation": null
        }
      ],
      "abstract": "In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT and GPT4 have demonstrated immense potential in constructing open-domain dialogue agents. However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations. In this paper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to advance the study of dialogue agents and character alignment. The dataset encompasses all dialogue sessions (in both English and Chinese) from the Harry Potter series and is annotated with vital background information, including dialogue scenes, speakers, character relationships, and attributes. These extensive annotations may empower LLMs to unlock character-driven dialogue capabilities. Furthermore, it can serve as a universal benchmark for evaluating how well can a LLM aligning with a specific character. We benchmark LLMs on HPD using both fine-tuning and in-context learning settings. Evaluation results reveal that although there is substantial room for improvement in generating high-quality, character-aligned responses, the proposed dataset is valuable in guiding models toward responses that better align with the character of Harry Potter.",
      "publishedDate": "2022-11-13T10:16:39Z",
      "updatedDate": "2023-10-09T05:08:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.06869v4",
      "arxivUrl": "https://arxiv.org/abs/2211.06869",
      "comment": "14 pages",
      "journalRef": "EMNLP 2023",
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "evaluation",
        "agents",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.04486",
      "title": "Active Example Selection for In-Context Learning",
      "authors": [
        {
          "name": "Yiming Zhang",
          "affiliation": null
        },
        {
          "name": "Shi Feng",
          "affiliation": null
        },
        {
          "name": "Chenhao Tan",
          "affiliation": null
        }
      ],
      "abstract": "With a handful of demonstration examples, large-scale language models show strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a $5.8\\%$ improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.",
      "publishedDate": "2022-11-08T19:00:02Z",
      "updatedDate": "2022-11-08T19:00:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.04486v1",
      "arxivUrl": "https://arxiv.org/abs/2211.04486",
      "comment": "EMNLP 2022, code is available at https://github.com/ChicagoHAI/active-example-selection",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.03044",
      "title": "Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning",
      "authors": [
        {
          "name": "Yu Meng",
          "affiliation": null
        },
        {
          "name": "Martin Michalski",
          "affiliation": null
        },
        {
          "name": "Jiaxin Huang",
          "affiliation": null
        },
        {
          "name": "Yu Zhang",
          "affiliation": null
        },
        {
          "name": "Tarek Abdelzaher",
          "affiliation": null
        },
        {
          "name": "Jiawei Han",
          "affiliation": null
        }
      ],
      "abstract": "Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing few-shot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label-discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the few-shot and the synthetic samples with regularization for better generalization and stability. Our approach FewGen achieves an overall better result across seven classification tasks of the GLUE benchmark than existing few-shot learning methods, improving no-augmentation methods by 5+ average points, and outperforming augmentation methods by 3+ average points.",
      "publishedDate": "2022-11-06T06:46:47Z",
      "updatedDate": "2023-05-12T06:06:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.03044v2",
      "arxivUrl": "https://arxiv.org/abs/2211.03044",
      "comment": "ICML 2023. (Code: https://github.com/yumeng5/FewGen)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.02709",
      "title": "A Prompt-based Few-shot Learning Approach to Software Conflict Detection",
      "authors": [
        {
          "name": "Robert K. Helmeczi",
          "affiliation": null
        },
        {
          "name": "Mucahit Cevik",
          "affiliation": null
        },
        {
          "name": "Savas Yıldırım",
          "affiliation": null
        }
      ],
      "abstract": "A software requirement specification (SRS) document is an essential part of the software development life cycle which outlines the requirements that a software program in development must satisfy. This document is often specified by a diverse group of stakeholders and is subject to continual change, making the process of maintaining the document and detecting conflicts between requirements an essential task in software development. Notably, projects that do not address conflicts in the SRS document early on face considerable problems later in the development life cycle. These problems incur substantial costs in terms of time and money, and these costs often become insurmountable barriers that ultimately result in the termination of a software project altogether. As a result, early detection of SRS conflicts is critical to project sustainability. The conflict detection task is approached in numerous ways, many of which require a significant amount of manual intervention from developers, or require access to a large amount of labeled, task-specific training data. In this work, we propose using a prompt-based learning approach to perform few-shot learning for conflict detection. We compare our results to supervised learning approaches that use pretrained language models, such as BERT and its variants. Our results show that prompting with just 32 labeled examples can achieve a similar level of performance in many key metrics to that of supervised learning on training sets that are magnitudes larger in size. In contrast to many other conflict detection approaches, we make no assumptions about the type of underlying requirements, allowing us to analyze pairings of both functional and non-functional requirements. This allows us to omit the potentially expensive task of filtering out non-functional requirements from our dataset.",
      "publishedDate": "2022-11-04T18:56:44Z",
      "updatedDate": "2022-11-04T18:56:44Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.02709v1",
      "arxivUrl": "https://arxiv.org/abs/2211.02709",
      "comment": "9 pages; 4 figures. To be published In Proceedings of 32nd Annual International Conference on Computer Science and Software Engineering (CASCON '22)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.00635",
      "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
      "authors": [
        {
          "name": "Yihan Wang",
          "affiliation": null
        },
        {
          "name": "Si Si",
          "affiliation": null
        },
        {
          "name": "Daliang Li",
          "affiliation": null
        },
        {
          "name": "Michal Lukasik",
          "affiliation": null
        },
        {
          "name": "Felix Yu",
          "affiliation": null
        },
        {
          "name": "Cho-Jui Hsieh",
          "affiliation": null
        },
        {
          "name": "Inderjit S Dhillon",
          "affiliation": null
        },
        {
          "name": "Sanjiv Kumar",
          "affiliation": null
        }
      ],
      "abstract": "Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task.We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization.ProMoT offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. With experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of out-of-domain evaluation tasks. More importantly, ProMoT can even enhance generalization on in-context learning tasks that are semantically related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly improves performance on other language pairs, and ProMoT on NLI improves performance on summarization. Experiments also show that ProMoT can improve the generalization performance of multi-task training.",
      "publishedDate": "2022-11-01T17:56:57Z",
      "updatedDate": "2024-03-12T22:05:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.00635v3",
      "arxivUrl": "https://arxiv.org/abs/2211.00635",
      "comment": "ICLR 2024",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.17437",
      "title": "Learning New Tasks from a Few Examples with Soft-Label Prototypes",
      "authors": [
        {
          "name": "Avyav Kumar Singh",
          "affiliation": null
        },
        {
          "name": "Ekaterina Shutova",
          "affiliation": null
        },
        {
          "name": "Helen Yannakoudakis",
          "affiliation": null
        }
      ],
      "abstract": "Existing approaches to few-shot learning in NLP rely on large language models (LLMs) and/or fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a novel few-shot learning approach based on soft-label prototypes (SLPs) designed to collectively capture the distribution of different classes across the input domain space. We focus on learning previously unseen NLP tasks from very few examples (4, 8, 16) per class and experimentally demonstrate that our approach achieves superior performance on the majority of tested tasks in this data-lean setting while being highly parameter efficient. We also show that our few-shot adaptation method can be integrated into more generalised learning settings, primarily meta-learning, to yield superior performance against strong baselines.",
      "publishedDate": "2022-10-31T16:06:48Z",
      "updatedDate": "2024-09-22T09:54:47Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.17437v4",
      "arxivUrl": "https://arxiv.org/abs/2210.17437",
      "comment": "Proceedings of the 9th Workshop on Representation Learning for NLP (2024), Pages 215 to 236",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.17041",
      "title": "GPS: Genetic Prompt Search for Efficient Few-shot Learning",
      "authors": [
        {
          "name": "Hanwei Xu",
          "affiliation": null
        },
        {
          "name": "Yujun Chen",
          "affiliation": null
        },
        {
          "name": "Yulun Du",
          "affiliation": null
        },
        {
          "name": "Nan Shao",
          "affiliation": null
        },
        {
          "name": "Yanggang Wang",
          "affiliation": null
        },
        {
          "name": "Haiyu Li",
          "affiliation": null
        },
        {
          "name": "Zhilin Yang",
          "affiliation": null
        }
      ],
      "abstract": "Prompt-based techniques have demostrated great potential for improving the few-shot generalization of pretrained language models. However, their performance heavily relies on the manual design of prompts and thus requires a lot of human efforts. In this paper, we introduce Genetic Prompt Search (GPS) to improve few-shot learning with prompts, which utilizes a genetic algorithm to automatically search for high-performing prompts. GPS is gradient-free and requires no update of model parameters but only a small validation set. Experiments on diverse datasets proved the effectiveness of GPS, which outperforms manual prompts by a large margin of 2.6 points. Our method is also better than other parameter-efficient tuning methods such as prompt tuning.",
      "publishedDate": "2022-10-31T03:36:21Z",
      "updatedDate": "2022-10-31T03:36:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.17041v1",
      "arxivUrl": "https://arxiv.org/abs/2210.17041",
      "comment": "10 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.16489",
      "title": "STPrompt: Semantic-guided and Task-driven prompts for Effective Few-shot Classification",
      "authors": [
        {
          "name": "Jinta Weng",
          "affiliation": null
        },
        {
          "name": "Yue Hu",
          "affiliation": null
        },
        {
          "name": "Jing Qiu",
          "affiliation": null
        },
        {
          "name": "Heyan Huan",
          "affiliation": null
        }
      ],
      "abstract": "The effectiveness of prompt learning has been demonstrated in different pre-trained language models. By formulating suitable template and choosing representative label mapping, prompt learning can be used as an efficient knowledge probe. However, finding suitable prompt in existing methods requires multiple experimental attempts or appropriate vector initialization on formulating suitable template and choosing representative label mapping, which it is more common in few-shot learning tasks. Motivating by PLM working process, we try to construct the prompt from task semantic perspective and thus propose the STPrompt -Semantic-guided and Task-driven Prompt model. Specifically, two novel prompts generated from the semantic dependency tree (Dep-prompt) and task-specific metadata description (Meta-prompt), are firstly constructed in a prompt augmented pool, and the proposed model would automatically select a suitable semantic prompt to motivating the prompt learning process. Our results show that the proposed model achieves the state-of-the-art performance in five different datasets of few-shot text classification tasks, which prove that more semantic and significant prompts could assume as a better knowledge proving tool.",
      "publishedDate": "2022-10-29T04:42:30Z",
      "updatedDate": "2022-10-29T04:42:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.16489v1",
      "arxivUrl": "https://arxiv.org/abs/2210.16489",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.15172",
      "title": "Dictionary-Assisted Supervised Contrastive Learning",
      "authors": [
        {
          "name": "Patrick Y. Wu",
          "affiliation": null
        },
        {
          "name": "Richard Bonneau",
          "affiliation": null
        },
        {
          "name": "Joshua A. Tucker",
          "affiliation": null
        },
        {
          "name": "Jonathan Nagler",
          "affiliation": null
        }
      ],
      "abstract": "Text analysis in the social sciences often involves using specialized dictionaries to reason with abstract concepts, such as perceptions about the economy or abuse on social media. These dictionaries allow researchers to impart domain knowledge and note subtle usages of words relating to a concept(s) of interest. We introduce the dictionary-assisted supervised contrastive learning (DASCL) objective, allowing researchers to leverage specialized dictionaries when fine-tuning pretrained language models. The text is first keyword simplified: a common, fixed token replaces any word in the corpus that appears in the dictionary(ies) relevant to the concept of interest. During fine-tuning, a supervised contrastive objective draws closer the embeddings of the original and keyword-simplified texts of the same class while pushing further apart the embeddings of different classes. The keyword-simplified texts of the same class are more textually similar than their original text counterparts, which additionally draws the embeddings of the same class closer together. Combining DASCL and cross-entropy improves classification performance metrics in few-shot learning settings and social science applications compared to using cross-entropy alone and alternative contrastive and data augmentation methods.",
      "publishedDate": "2022-10-27T04:57:43Z",
      "updatedDate": "2022-10-27T04:57:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.15172v1",
      "arxivUrl": "https://arxiv.org/abs/2210.15172",
      "comment": "6 pages, 5 figures, EMNLP 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.13693",
      "title": "XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing",
      "authors": [
        {
          "name": "Peng Shi",
          "affiliation": null
        },
        {
          "name": "Rui Zhang",
          "affiliation": null
        },
        {
          "name": "He Bai",
          "affiliation": null
        },
        {
          "name": "Jimmy Lin",
          "affiliation": null
        }
      ],
      "abstract": "In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation. Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based models. However, existing work primarily focuses on English datasets, and it is unknown whether large language models can serve as competitive semantic parsers for other languages. To bridge this gap, our work focuses on cross-lingual Text-to-SQL semantic parsing for translating non-English utterances into SQL queries based on an English schema. We consider a zero-shot transfer learning setting with the assumption that we do not have any labeled examples in the target language (but have annotated examples in English). This work introduces the XRICL framework, which learns to retrieve relevant English exemplars for a given query to construct prompts. We also include global translation exemplars for a target language to facilitate the translation process for large language models. To systematically evaluate our model, we construct two new benchmark datasets, XSpider and XKaggle-dbqa, which include questions in Chinese, Vietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively leverages large pre-trained language models to outperform existing baselines. Data and code are publicly available at https://github.com/Impavidity/XRICL.",
      "publishedDate": "2022-10-25T01:33:49Z",
      "updatedDate": "2022-10-25T01:33:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.13693v1",
      "arxivUrl": "https://arxiv.org/abs/2210.13693",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.10841",
      "title": "Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models",
      "authors": [
        {
          "name": "Yue Zhang",
          "affiliation": null
        },
        {
          "name": "Hongliang Fei",
          "affiliation": null
        },
        {
          "name": "Dingcheng Li",
          "affiliation": null
        },
        {
          "name": "Tan Yu",
          "affiliation": null
        },
        {
          "name": "Ping Li",
          "affiliation": null
        }
      ],
      "abstract": "Prompt learning is a new learning paradigm which reformulates downstream tasks as similar pretraining tasks on pretrained models by leveraging textual prompts. Recent works have demonstrated that prompt learning is particularly useful for few-shot learning, where there is limited training data. Depending on the granularity of prompts, those methods can be roughly divided into task-level prompting and instance-level prompting. Task-level prompting methods learn one universal prompt for all input samples, which is efficient but ineffective to capture subtle differences among different classes. Instance-level prompting methods learn a specific prompt for each input, though effective but inefficient. In this work, we develop a novel prototype-based prompt learning method to overcome the above limitations. In particular, we focus on few-shot image recognition tasks on pretrained vision-language models (PVLMs) and develop a method of prompting through prototype (PTP), where we define $K$ image prototypes and $K$ prompt prototypes. In PTP, the image prototype represents a centroid of a certain image cluster in the latent space and a prompt prototype is defined as a soft prompt in the continuous space. The similarity between a query image and an image prototype determines how much this prediction relies on the corresponding prompt prototype. Hence, in PTP, similar images will utilize similar prompting ways. Through extensive experiments on seven real-world benchmarks, we show that PTP is an effective method to leverage the latent knowledge and adaptive to various PVLMs. Moreover, through detailed analysis, we discuss pros and cons for prompt learning and parameter-efficient fine-tuning under the context of few-shot learning.",
      "publishedDate": "2022-10-19T19:13:07Z",
      "updatedDate": "2022-10-19T19:13:07Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.10841v1",
      "arxivUrl": "https://arxiv.org/abs/2210.10841",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.14161",
      "title": "Aligning MAGMA by Few-Shot Learning and Finetuning",
      "authors": [
        {
          "name": "Jean-Charles Layoun",
          "affiliation": null
        },
        {
          "name": "Alexis Roger",
          "affiliation": null
        },
        {
          "name": "Irina Rish",
          "affiliation": null
        }
      ],
      "abstract": "The goal of vision-language modeling is to allow models to tie language understanding with visual inputs. The aim of this paper is to evaluate and align the Visual Language Model (VLM) called Multimodal Augmentation of Generative Models through Adapter-based finetuning (MAGMA) with human values. MAGMA is a VLM that is capable of image captioning and visual question-answering. We will evaluate its alignment in three different scenarios. To begin, we assess MAGMA's out-of-the-box alignment through the checkpoint provided by Hugging Face. Then, we measure if few-shot learning manages to improve the results. Finally, we finetune the model on aligned examples and evaluate its behavior.",
      "publishedDate": "2022-10-18T22:20:47Z",
      "updatedDate": "2022-10-18T22:20:47Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.14161v1",
      "arxivUrl": "https://arxiv.org/abs/2210.14161",
      "comment": "Accepted by the Montreal AI Symposium conference in 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.08726",
      "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
      "authors": [
        {
          "name": "Luyu Gao",
          "affiliation": null
        },
        {
          "name": "Zhuyun Dai",
          "affiliation": null
        },
        {
          "name": "Panupong Pasupat",
          "affiliation": null
        },
        {
          "name": "Anthony Chen",
          "affiliation": null
        },
        {
          "name": "Arun Tejasvi Chaganty",
          "affiliation": null
        },
        {
          "name": "Yicheng Fan",
          "affiliation": null
        },
        {
          "name": "Vincent Y. Zhao",
          "affiliation": null
        },
        {
          "name": "Ni Lao",
          "affiliation": null
        },
        {
          "name": "Hongrae Lee",
          "affiliation": null
        },
        {
          "name": "Da-Cheng Juan",
          "affiliation": null
        },
        {
          "name": "Kelvin Guu",
          "affiliation": null
        }
      ],
      "abstract": "Language models (LMs) now excel at many tasks such as few-shot learning, question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.",
      "publishedDate": "2022-10-17T03:44:30Z",
      "updatedDate": "2023-05-31T17:55:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.08726v3",
      "arxivUrl": "https://arxiv.org/abs/2210.08726",
      "comment": "ACL 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.07652",
      "title": "Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values",
      "authors": [
        {
          "name": "Yejin Bang",
          "affiliation": null
        },
        {
          "name": "Tiezheng Yu",
          "affiliation": null
        },
        {
          "name": "Andrea Madotto",
          "affiliation": null
        },
        {
          "name": "Zhaojiang Lin",
          "affiliation": null
        },
        {
          "name": "Mona Diab",
          "affiliation": null
        },
        {
          "name": "Pascale Fung",
          "affiliation": null
        }
      ],
      "abstract": "Many NLP classification tasks, such as sexism/racism detection or toxicity detection, are based on human values. Yet, human values can vary under diverse cultural conditions. Therefore, we introduce a framework for value-aligned classification that performs prediction based on explicitly written human values in the command. Along with the task, we propose a practical approach that distills value-aligned knowledge from large-scale language models (LLMs) to construct value-aligned classifiers in two steps. First, we generate value-aligned training data from LLMs by prompt-based few-shot learning. Next, we fine-tune smaller classification models with the generated data for the task. Empirical results show that our VA-Models surpass multiple baselines by at least 15.56% on the F1-score, including few-shot learning with OPT-175B and existing text augmentation methods. We suggest that using classifiers with explicit human value input improves both inclusivity & explainability in AI.",
      "publishedDate": "2022-10-14T09:10:49Z",
      "updatedDate": "2022-10-14T09:10:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.07652v1",
      "arxivUrl": "https://arxiv.org/abs/2210.07652",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.07565",
      "title": "Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning",
      "authors": [
        {
          "name": "Tianxiang Sun",
          "affiliation": null
        },
        {
          "name": "Zhengfu He",
          "affiliation": null
        },
        {
          "name": "Qin Zhu",
          "affiliation": null
        },
        {
          "name": "Xipeng Qiu",
          "affiliation": null
        },
        {
          "name": "Xuanjing Huang",
          "affiliation": null
        }
      ],
      "abstract": "Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we present Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained prompts are selectively activated and combined, leading to strong compositional generalization to unseen tasks. To bridge the gap between pre-training and fine-tuning, we formulate upstream and downstream tasks into a unified machine reading comprehension task. Extensive experiments under two learning paradigms, i.e., gradient descent and black-box tuning, show that MP2 significantly outperforms prompt tuning, full model tuning, and prior prompt pre-training methods in few-shot settings. In addition, we demonstrate that MP2 can achieve surprisingly fast and strong adaptation to downstream tasks by merely learning 8 parameters to combine the pre-trained modular prompts.",
      "publishedDate": "2022-10-14T06:43:42Z",
      "updatedDate": "2023-05-06T11:30:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.07565v3",
      "arxivUrl": "https://arxiv.org/abs/2210.07565",
      "comment": "Accepted to ACL 2023 (main conference). Code and data are publicly available at https://github.com/Hzfinfdu/MPMP",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.07225",
      "title": "Unified Vision and Language Prompt Learning",
      "authors": [
        {
          "name": "Yuhang Zang",
          "affiliation": null
        },
        {
          "name": "Wei Li",
          "affiliation": null
        },
        {
          "name": "Kaiyang Zhou",
          "affiliation": null
        },
        {
          "name": "Chen Huang",
          "affiliation": null
        },
        {
          "name": "Chen Change Loy",
          "affiliation": null
        }
      ],
      "abstract": "Prompt tuning, a parameter- and data-efficient transfer learning paradigm that tunes only a small number of parameters in a model's input space, has become a trend in the vision community since the emergence of large vision-language models like CLIP. We present a systematic study on two representative prompt tuning methods, namely text prompt tuning and visual prompt tuning. A major finding is that none of the unimodal prompt tuning methods performs consistently well: text prompt tuning fails on data with high intra-class visual variances while visual prompt tuning cannot handle low inter-class variances. To combine the best from both worlds, we propose a simple approach called Unified Prompt Tuning (UPT), which essentially learns a tiny neural network to jointly optimize prompts across different modalities. Extensive experiments on over 11 vision datasets show that UPT achieves a better trade-off than the unimodal counterparts on few-shot learning benchmarks, as well as on domain generalization benchmarks. Code and models will be released to facilitate future research.",
      "publishedDate": "2022-10-13T17:50:24Z",
      "updatedDate": "2022-10-13T17:50:24Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.07225v1",
      "arxivUrl": "https://arxiv.org/abs/2210.07225",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.06726",
      "title": "Explanations from Large Language Models Make Small Reasoners Better",
      "authors": [
        {
          "name": "Shiyang Li",
          "affiliation": null
        },
        {
          "name": "Jianshu Chen",
          "affiliation": null
        },
        {
          "name": "Yelong Shen",
          "affiliation": null
        },
        {
          "name": "Zhiyu Chen",
          "affiliation": null
        },
        {
          "name": "Xinlu Zhang",
          "affiliation": null
        },
        {
          "name": "Zekun Li",
          "affiliation": null
        },
        {
          "name": "Hong Wang",
          "affiliation": null
        },
        {
          "name": "Jing Qian",
          "affiliation": null
        },
        {
          "name": "Baolin Peng",
          "affiliation": null
        },
        {
          "name": "Yi Mao",
          "affiliation": null
        },
        {
          "name": "Wenhu Chen",
          "affiliation": null
        },
        {
          "name": "Xifeng Yan",
          "affiliation": null
        }
      ],
      "abstract": "Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.",
      "publishedDate": "2022-10-13T04:50:02Z",
      "updatedDate": "2022-10-13T04:50:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.06726v1",
      "arxivUrl": "https://arxiv.org/abs/2210.06726",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.06710",
      "title": "Large Language Models are few(1)-shot Table Reasoners",
      "authors": [
        {
          "name": "Wenhu Chen",
          "affiliation": null
        }
      ],
      "abstract": "Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very strong performance with only a 1-shot demonstration, even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in https://github.com/wenhuchen/TableCoT.",
      "publishedDate": "2022-10-13T04:08:24Z",
      "updatedDate": "2023-01-23T18:51:00Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.06710v2",
      "arxivUrl": "https://arxiv.org/abs/2210.06710",
      "comment": "Accepted to Findings of EACL 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.05549",
      "title": "Continual Training of Language Models for Few-Shot Learning",
      "authors": [
        {
          "name": "Zixuan Ke",
          "affiliation": null
        },
        {
          "name": "Haowei Lin",
          "affiliation": null
        },
        {
          "name": "Yijia Shao",
          "affiliation": null
        },
        {
          "name": "Hu Xu",
          "affiliation": null
        },
        {
          "name": "Lei Shu",
          "affiliation": null
        },
        {
          "name": "Bing Liu",
          "affiliation": null
        }
      ],
      "abstract": "Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual PostTraining), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness.",
      "publishedDate": "2022-10-11T15:43:58Z",
      "updatedDate": "2022-10-11T15:43:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.05549v1",
      "arxivUrl": "https://arxiv.org/abs/2210.05549",
      "comment": null,
      "journalRef": "EMNLP 2022",
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.04873",
      "title": "CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation",
      "authors": [
        {
          "name": "Tanay Dixit",
          "affiliation": null
        },
        {
          "name": "Bhargavi Paranjape",
          "affiliation": null
        },
        {
          "name": "Hannaneh Hajishirzi",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        }
      ],
      "abstract": "Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed inputs during training -- helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of perturbations, limiting their effectiveness. We present COunterfactual Generation via Retrieval and Editing (CORE), a retrieval-augmented generation framework for creating diverse counterfactual perturbations for CDA. For each training example, CORE first performs a dense retrieval over a task-related unlabeled text corpus using a learned bi-encoder and extracts relevant counterfactual excerpts. CORE then incorporates these into prompts to a large language model with few-shot learning capabilities, for counterfactual editing. Conditioning language model edits on naturally occurring data results in diverse perturbations. Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches. We also show that the CORE retrieval framework can be used to encourage diversity in manually authored perturbations",
      "publishedDate": "2022-10-10T17:45:38Z",
      "updatedDate": "2022-11-01T07:08:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.04873v2",
      "arxivUrl": "https://arxiv.org/abs/2210.04873",
      "comment": "Findings EMNLP 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.04185",
      "title": "Controllable Dialogue Simulation with In-Context Learning",
      "authors": [
        {
          "name": "Zekun Li",
          "affiliation": null
        },
        {
          "name": "Wenhu Chen",
          "affiliation": null
        },
        {
          "name": "Shiyang Li",
          "affiliation": null
        },
        {
          "name": "Hong Wang",
          "affiliation": null
        },
        {
          "name": "Jing Qian",
          "affiliation": null
        },
        {
          "name": "Xifeng Yan",
          "affiliation": null
        }
      ],
      "abstract": "Building dialogue systems requires a large corpus of annotated dialogues. Such datasets are usually created via crowdsourcing, which is expensive and time-consuming. In this paper, we propose \\textsc{Dialogic}, a novel dialogue simulation method based on large language model in-context learning to automate dataset creation. Seeded with a few annotated dialogues, \\textsc{Dialogic} automatically selects in-context examples for demonstration and prompts GPT-3 to generate new dialogues and annotations in a controllable way. Our method can rapidly expand a small set of dialogue data with minimum or zero \\textit{human involvement} and \\textit{parameter update} and is thus much more cost-efficient and time-saving than crowdsourcing. Experimental results on the MultiWOZ dataset demonstrate that training a model on the simulated dialogues leads to even better performance than using the same amount of human-generated dialogues under the challenging low-resource settings, with as few as 85 dialogues as a seed. When enough data is available, our method can still serve as an effective data augmentation method. Human evaluation results also show that our simulated dialogues have near-human fluency and annotation accuracy. The code and data are available at \\textbf{\\url{https://github.com/Leezekun/dialogic}}.",
      "publishedDate": "2022-10-09T06:32:58Z",
      "updatedDate": "2023-06-06T02:19:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.04185v4",
      "arxivUrl": "https://arxiv.org/abs/2210.04185",
      "comment": "EMNLP 2022 Findings, code and data are available at https://github.com/Leezekun/dialogic",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "tool-use",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "tool-use",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.03821",
      "title": "Large Language Models can Implement Policy Iteration",
      "authors": [
        {
          "name": "Ethan Brooks",
          "affiliation": null
        },
        {
          "name": "Logan Walls",
          "affiliation": null
        },
        {
          "name": "Richard L. Lewis",
          "affiliation": null
        },
        {
          "name": "Satinder Singh",
          "affiliation": null
        }
      ],
      "abstract": "This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the \"few-shot\" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt content is the entire locus of learning. ICPI iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our algorithm using Codex, a language model with no prior knowledge of the domains on which we evaluate it.",
      "publishedDate": "2022-10-07T21:18:22Z",
      "updatedDate": "2023-08-13T18:27:52Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.03821v2",
      "arxivUrl": "https://arxiv.org/abs/2210.03821",
      "comment": "10 pages, 4 figures, submitted to ICLR 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.03690",
      "title": "Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts",
      "authors": [
        {
          "name": "Nghia T. Le",
          "affiliation": null
        },
        {
          "name": "Fan Bai",
          "affiliation": null
        },
        {
          "name": "Alan Ritter",
          "affiliation": null
        }
      ],
      "abstract": "Anaphora resolution is an important task for information extraction across a range of languages, text genres, and domains, motivating the need for methods that do not require large annotated datasets. In-context learning has emerged as a promising approach, yet there are a number of challenges in applying in-context learning to resolve anaphora. For example, encoding a single in-context demonstration that consists of: an anaphor, a paragraph-length context, and a list of corresponding antecedents, requires conditioning a language model on a long sequence of tokens, limiting the number of demonstrations per prompt. In this paper, we present MICE (Mixtures of In-Context Experts), which we demonstrate is effective for few-shot anaphora resolution in scientific protocols (Tamari et al., 2021). Given only a handful of training examples, MICE combines the predictions of hundreds of in-context experts, yielding a 30% increase in F1 score over a competitive prompt retrieval baseline. Furthermore, we show MICE can be used to train compact student models without sacrificing performance. As far as we are aware, this is the first work to present experimental results demonstrating the effectiveness of in-context learning on the task of few-shot anaphora resolution in scientific protocols.",
      "publishedDate": "2022-10-07T16:51:45Z",
      "updatedDate": "2022-11-14T18:31:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.03690v2",
      "arxivUrl": "https://arxiv.org/abs/2210.03690",
      "comment": "Findings of EMNLP 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.02498",
      "title": "Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model",
      "authors": [
        {
          "name": "Jacob Eisenstein",
          "affiliation": null
        },
        {
          "name": "Daniel Andor",
          "affiliation": null
        },
        {
          "name": "Bernd Bohnet",
          "affiliation": null
        },
        {
          "name": "Michael Collins",
          "affiliation": null
        },
        {
          "name": "David Mimno",
          "affiliation": null
        }
      ],
      "abstract": "Explainable question answering systems should produce not only accurate answers but also rationales that justify their reasoning and allow humans to check their work. But what sorts of rationales are useful and how can we train systems to produce them? We propose a new style of rationale for open-book question answering, called \\emph{markup-and-mask}, which combines aspects of extractive and free-text explanations. In the markup phase, the passage is augmented with free-text markup that enables each sentence to stand on its own outside the discourse context. In the masking phase, a sub-span of the marked-up passage is selected. To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning. Specifically, we generate silver annotated data by sending a series of prompts to a frozen pretrained language model, which acts as a teacher. We then fine-tune a smaller student model by training on the subset of rationales that led to correct answers. The student is \"honest\" in the sense that it is a pipeline: the rationale acts as a bottleneck between the passage and the answer, while the \"untrusted\" teacher operates under no such constraints. Thus, we offer a new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models.",
      "publishedDate": "2022-10-05T18:23:49Z",
      "updatedDate": "2024-04-24T23:35:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.02498v3",
      "arxivUrl": "https://arxiv.org/abs/2210.02498",
      "comment": "added details about a human evaluation",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.12153",
      "title": "WinoDict: Probing language models for in-context word acquisition",
      "authors": [
        {
          "name": "Julian Martin Eisenschlos",
          "affiliation": null
        },
        {
          "name": "Jeremy R. Cole",
          "affiliation": null
        },
        {
          "name": "Fangyu Liu",
          "affiliation": null
        },
        {
          "name": "William W. Cohen",
          "affiliation": null
        }
      ],
      "abstract": "We introduce a new in-context learning paradigm to measure Large Language Models' (LLMs) ability to learn novel words during inference. In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task. Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt. This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning.",
      "publishedDate": "2022-09-25T05:30:13Z",
      "updatedDate": "2022-09-25T05:30:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.12153v1",
      "arxivUrl": "https://arxiv.org/abs/2209.12153",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.11902",
      "title": "Learning Chess With Language Models and Transformers",
      "authors": [
        {
          "name": "Michael DeLeo",
          "affiliation": null
        },
        {
          "name": "Erhan Guven",
          "affiliation": null
        }
      ],
      "abstract": "Representing a board game and its positions by text-based notation enables the possibility of NLP applications. Language models, can help gain insight into a variety of interesting problems such as unsupervised learning rules of a game, detecting player behavior patterns, player attribution, and ultimately learning the game to beat state of the art. In this study, we applied BERT models, first to the simple Nim game to analyze its performance in the presence of noise in a setup of a few-shot learning architecture. We analyzed the model performance via three virtual players, namely Nim Guru, Random player, and Q-learner. In the second part, we applied the game learning language model to the chess game, and a large set of grandmaster games with exhaustive encyclopedia openings. Finally, we have shown that model practically learns the rules of the chess game and can survive games against Stockfish at a category-A rating level.",
      "publishedDate": "2022-09-24T01:22:59Z",
      "updatedDate": "2022-09-24T01:22:59Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.11902v1",
      "arxivUrl": "https://arxiv.org/abs/2209.11902",
      "comment": "Conference Paper",
      "journalRef": null,
      "doi": "10.5121/csit.2022.121515",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.11055",
      "title": "Efficient Few-Shot Learning Without Prompts",
      "authors": [
        {
          "name": "Lewis Tunstall",
          "affiliation": null
        },
        {
          "name": "Nils Reimers",
          "affiliation": null
        },
        {
          "name": "Unso Eun Seo Jo",
          "affiliation": null
        },
        {
          "name": "Luke Bates",
          "affiliation": null
        },
        {
          "name": "Daniel Korat",
          "affiliation": null
        },
        {
          "name": "Moshe Wasserblat",
          "affiliation": null
        },
        {
          "name": "Oren Pereg",
          "affiliation": null
        }
      ],
      "abstract": "Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and pattern exploiting training (PET), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers (ST). SetFit works by first fine-tuning a pretrained ST on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that SetFit obtains comparable results with PEFT and PET techniques, while being an order of magnitude faster to train. We also show that SetFit can be applied in multilingual settings by simply switching the ST body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .",
      "publishedDate": "2022-09-22T14:48:11Z",
      "updatedDate": "2022-09-22T14:48:11Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.11055v1",
      "arxivUrl": "https://arxiv.org/abs/2209.11055",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.09401",
      "title": "Automatic Label Sequence Generation for Prompting Sequence-to-sequence Models",
      "authors": [
        {
          "name": "Zichun Yu",
          "affiliation": null
        },
        {
          "name": "Tianyu Gao",
          "affiliation": null
        },
        {
          "name": "Zhengyan Zhang",
          "affiliation": null
        },
        {
          "name": "Yankai Lin",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Liu",
          "affiliation": null
        },
        {
          "name": "Maosong Sun",
          "affiliation": null
        },
        {
          "name": "Jie Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Prompting, which casts downstream applications as language modeling tasks, has shown to be sample efficient compared to standard fine-tuning with pre-trained models. However, one pitfall of prompting is the need of manually-designed patterns, whose outcome can be unintuitive and requires large validation sets to tune. To tackle the challenge, we propose AutoSeq, a fully automatic prompting method: (1) We adopt natural language prompts on sequence-to-sequence models, enabling free-form generation and larger label search space; (2) We propose label sequences -- phrases with indefinite lengths to verbalize the labels -- which eliminate the need of manual templates and are more expressive than single label words; (3) We use beam search to automatically generate a large amount of label sequence candidates and propose contrastive re-ranking to get the best combinations. AutoSeq significantly outperforms other no-manual-design methods, such as soft prompt tuning, adapter tuning, and automatic search on single label words; the generated label sequences are even better than curated manual ones on a variety of tasks. Our method reveals the potential of sequence-to-sequence models in few-shot learning and sheds light on a path to generic and automatic prompting. The source code of this paper can be obtained from https://github.com/thunlp/Seq2Seq-Prompt.",
      "publishedDate": "2022-09-20T01:35:04Z",
      "updatedDate": "2022-09-20T01:35:04Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.09401v1",
      "arxivUrl": "https://arxiv.org/abs/2209.09401",
      "comment": "Accepted to COLING 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.00479",
      "title": "Revisiting the Practical Effectiveness of Constituency Parse Extraction from Pre-trained Language Models",
      "authors": [
        {
          "name": "Taeuk Kim",
          "affiliation": null
        }
      ],
      "abstract": "Constituency Parse Extraction from Pre-trained Language Models (CPE-PLM) is a recent paradigm that attempts to induce constituency parse trees relying only on the internal knowledge of pre-trained language models. While attractive in the perspective that similar to in-context learning, it does not require task-specific fine-tuning, the practical effectiveness of such an approach still remains unclear, except that it can function as a probe for investigating language models' inner workings. In this work, we mathematically reformulate CPE-PLM and propose two advanced ensemble methods tailored for it, demonstrating that the new parsing paradigm can be competitive with common unsupervised parsers by introducing a set of heterogeneous PLMs combined using our techniques. Furthermore, we explore some scenarios where the trees generated by CPE-PLM are practically useful. Specifically, we show that CPE-PLM is more effective than typical supervised parsers in few-shot settings.",
      "publishedDate": "2022-09-15T09:41:19Z",
      "updatedDate": "2022-09-15T09:41:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.00479v1",
      "arxivUrl": "https://arxiv.org/abs/2211.00479",
      "comment": "COLING 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.01975",
      "title": "Selective Annotation Makes Language Models Better Few-Shot Learners",
      "authors": [
        {
          "name": "Hongjin Su",
          "affiliation": null
        },
        {
          "name": "Jungo Kasai",
          "affiliation": null
        },
        {
          "name": "Chen Henry Wu",
          "affiliation": null
        },
        {
          "name": "Weijia Shi",
          "affiliation": null
        },
        {
          "name": "Tianlu Wang",
          "affiliation": null
        },
        {
          "name": "Jiayi Xin",
          "affiliation": null
        },
        {
          "name": "Rui Zhang",
          "affiliation": null
        },
        {
          "name": "Mari Ostendorf",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "Noah A. Smith",
          "affiliation": null
        },
        {
          "name": "Tao Yu",
          "affiliation": null
        }
      ],
      "abstract": "Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/HKUNLP/icl-selective-annotation.",
      "publishedDate": "2022-09-05T14:01:15Z",
      "updatedDate": "2022-09-05T14:01:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.01975v1",
      "arxivUrl": "https://arxiv.org/abs/2209.01975",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.14923",
      "title": "Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks",
      "authors": [
        {
          "name": "David Oniani",
          "affiliation": null
        },
        {
          "name": "Sonish Sivarajkumar",
          "affiliation": null
        },
        {
          "name": "Yanshan Wang",
          "affiliation": null
        }
      ],
      "abstract": "Clinical Natural Language Processing (NLP) has become an emerging technology in healthcare that leverages a large amount of free-text data in electronic health records (EHRs) to improve patient care, support clinical decisions, and facilitate clinical and translational science research. Recently, deep learning has achieved state-of-the-art performance in many clinical NLP tasks. However, training deep learning models usually requires large annotated datasets, which are normally not publicly available and can be time-consuming to build in clinical domains. Working with smaller annotated datasets is typical in clinical NLP and therefore, ensuring that deep learning models perform well is crucial for the models to be used in real-world applications. A widely adopted approach is fine-tuning existing Pre-trained Language Models (PLMs), but these attempts fall short when the training dataset contains only a few annotated samples. Few-Shot Learning (FSL) has recently been investigated to tackle this problem. Siamese Neural Network (SNN) has been widely utilized as an FSL approach in computer vision, but has not been studied well in NLP. Furthermore, the literature on its applications in clinical domains is scarce. In this paper, we propose two SNN-based FSL approaches for clinical NLP, including Pre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). We evaluated the proposed approaches on two clinical tasks, namely clinical text classification and clinical named entity recognition. We tested three few-shot settings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLP tasks were benchmarked using three PLMs, including BERT,BioBERT, and BioClinicalBERT. The experimental results verified the effectiveness of the proposed SNN-based FSL approaches in both NLP tasks.",
      "publishedDate": "2022-08-31T15:36:27Z",
      "updatedDate": "2022-10-26T19:47:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.14923v2",
      "arxivUrl": "https://arxiv.org/abs/2208.14923",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.03299",
      "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models",
      "authors": [
        {
          "name": "Gautier Izacard",
          "affiliation": null
        },
        {
          "name": "Patrick Lewis",
          "affiliation": null
        },
        {
          "name": "Maria Lomeli",
          "affiliation": null
        },
        {
          "name": "Lucas Hosseini",
          "affiliation": null
        },
        {
          "name": "Fabio Petroni",
          "affiliation": null
        },
        {
          "name": "Timo Schick",
          "affiliation": null
        },
        {
          "name": "Jane Dwivedi-Yu",
          "affiliation": null
        },
        {
          "name": "Armand Joulin",
          "affiliation": null
        },
        {
          "name": "Sebastian Riedel",
          "affiliation": null
        },
        {
          "name": "Edouard Grave",
          "affiliation": null
        }
      ],
      "abstract": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.",
      "publishedDate": "2022-08-05T17:39:22Z",
      "updatedDate": "2022-11-16T16:38:18Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.03299v3",
      "arxivUrl": "https://arxiv.org/abs/2208.03299",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.01448",
      "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
      "authors": [
        {
          "name": "Saleh Soltan",
          "affiliation": null
        },
        {
          "name": "Shankar Ananthakrishnan",
          "affiliation": null
        },
        {
          "name": "Jack FitzGerald",
          "affiliation": null
        },
        {
          "name": "Rahul Gupta",
          "affiliation": null
        },
        {
          "name": "Wael Hamza",
          "affiliation": null
        },
        {
          "name": "Haidar Khan",
          "affiliation": null
        },
        {
          "name": "Charith Peris",
          "affiliation": null
        },
        {
          "name": "Stephen Rawls",
          "affiliation": null
        },
        {
          "name": "Andy Rosenbaum",
          "affiliation": null
        },
        {
          "name": "Anna Rumshisky",
          "affiliation": null
        },
        {
          "name": "Chandana Satya Prakash",
          "affiliation": null
        },
        {
          "name": "Mukund Sridhar",
          "affiliation": null
        },
        {
          "name": "Fabian Triefenbach",
          "affiliation": null
        },
        {
          "name": "Apurv Verma",
          "affiliation": null
        },
        {
          "name": "Gokhan Tur",
          "affiliation": null
        },
        {
          "name": "Prem Natarajan",
          "affiliation": null
        }
      ],
      "abstract": "In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training.",
      "publishedDate": "2022-08-02T13:30:07Z",
      "updatedDate": "2022-08-03T12:42:52Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.01448v2",
      "arxivUrl": "https://arxiv.org/abs/2208.01448",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.01066",
      "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
      "authors": [
        {
          "name": "Shivam Garg",
          "affiliation": null
        },
        {
          "name": "Dimitris Tsipras",
          "affiliation": null
        },
        {
          "name": "Percy Liang",
          "affiliation": null
        },
        {
          "name": "Gregory Valiant",
          "affiliation": null
        }
      ],
      "abstract": "In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn \"most\" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .",
      "publishedDate": "2022-08-01T18:01:40Z",
      "updatedDate": "2023-08-11T19:27:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.01066v3",
      "arxivUrl": "https://arxiv.org/abs/2208.01066",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.01009",
      "title": "Few-shot Adaptation Works with UnpredicTable Data",
      "authors": [
        {
          "name": "Jun Shern Chan",
          "affiliation": null
        },
        {
          "name": "Michael Pieler",
          "affiliation": null
        },
        {
          "name": "Jonathan Jao",
          "affiliation": null
        },
        {
          "name": "Jérémy Scheurer",
          "affiliation": null
        },
        {
          "name": "Ethan Perez",
          "affiliation": null
        }
      ],
      "abstract": "Prior work on language models (LMs) shows that training on a large number of diverse tasks improves few-shot learning (FSL) performance on new tasks. We take this to the extreme, automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets. Finetuning on the resulting dataset leads to improved FSL performance on Natural Language Processing (NLP) tasks, but not proportionally to dataset scale. In fact, we find that narrow subsets of our dataset sometimes outperform more diverse datasets. For example, finetuning on software documentation from support.google.com raises FSL performance by a mean of +7.5% on 52 downstream tasks, which beats training on 40 human-curated NLP datasets (+6.7%). Finetuning on various narrow datasets leads to similar broad improvements across test tasks, suggesting that the gains are not from domain adaptation but adapting to FSL in general. We do not observe clear patterns between the datasets that lead to FSL gains, leaving open questions about why certain data helps with FSL.",
      "publishedDate": "2022-08-01T17:35:25Z",
      "updatedDate": "2022-08-08T03:17:18Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.01009v2",
      "arxivUrl": "https://arxiv.org/abs/2208.01009",
      "comment": "Code at https://github.com/JunShern/few-shot-adaptation",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.13226",
      "title": "Boosting Point-BERT by Multi-choice Tokens",
      "authors": [
        {
          "name": "Kexue Fu",
          "affiliation": null
        },
        {
          "name": "Mingzhi Yuan",
          "affiliation": null
        },
        {
          "name": "Manning Wang",
          "affiliation": null
        }
      ],
      "abstract": "Masked language modeling (MLM) has become one of the most successful self-supervised pre-training task. Inspired by its success, Point-BERT, as a pioneer work in point cloud, proposed masked point modeling (MPM) to pre-train point transformer on large scale unanotated dataset. Despite its great performance, we find the inherent difference between language and point cloud tends to cause ambiguous tokenization for point cloud. For point cloud, there doesn't exist a gold standard for point cloud tokenization. Point-BERT use a discrete Variational AutoEncoder (dVAE) as tokenizer, but it might generate different token ids for semantically-similar patches and generate the same token ids for semantically-dissimilar patches. To tackle above problem, we propose our McP-BERT, a pre-training framework with multi-choice tokens. Specifically, we ease the previous single-choice constraint on patch token ids in Point-BERT, and provide multi-choice token ids for each patch as supervision. Moreover, we utilitze the high-level semantics learned by transformer to further refine our supervision signals. Extensive experiments on point cloud classification, few-shot classification and part segmentation tasks demonstrate the superiority of our method, e.g., the pre-trained transformer achieves 94.1% accuracy on ModelNet40, 84.28% accuracy on the hardest setting of ScanObjectNN and new state-of-the-art performance on few-shot learning. We also demonstrate that our method not only improves the performance of Point-BERT on all downstream tasks, but also incurs almost no extra computational overhead. The code will be released in https://github.com/fukexue/McP-BERT.",
      "publishedDate": "2022-07-27T00:34:33Z",
      "updatedDate": "2022-08-15T13:54:22Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.13226v2",
      "arxivUrl": "https://arxiv.org/abs/2207.13226",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.09157",
      "title": "On the cross-lingual transferability of multilingual prototypical models across NLU tasks",
      "authors": [
        {
          "name": "Oralie Cattan",
          "affiliation": null
        },
        {
          "name": "Christophe Servan",
          "affiliation": null
        },
        {
          "name": "Sophie Rosset",
          "affiliation": null
        }
      ],
      "abstract": "Supervised deep learning-based approaches have been applied to task-oriented dialog and have proven to be effective for limited domain and language applications when a sufficient number of training examples are available. In practice, these approaches suffer from the drawbacks of domain-driven design and under-resourced languages. Domain and language models are supposed to grow and change as the problem space evolves. On one hand, research on transfer learning has demonstrated the cross-lingual ability of multilingual Transformers-based models to learn semantically rich representations. On the other, in addition to the above approaches, meta-learning have enabled the development of task and language learning algorithms capable of far generalization. Through this context, this article proposes to investigate the cross-lingual transferability of using synergistically few-shot learning with prototypical neural networks and multilingual Transformers-based models. Experiments in natural language understanding tasks on MultiATIS++ corpus shows that our approach substantially improves the observed transfer learning performances between the low and the high resource languages. More generally our approach confirms that the meaningful latent space learned in a given language can be can be generalized to unseen and under-resourced ones using meta-learning.",
      "publishedDate": "2022-07-19T09:55:04Z",
      "updatedDate": "2022-07-19T09:55:04Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.09157v1",
      "arxivUrl": "https://arxiv.org/abs/2207.09157",
      "comment": "Accepted to the ACL workshop METANLP 2021",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.08408",
      "title": "STT: Soft Template Tuning for Few-Shot Adaptation",
      "authors": [
        {
          "name": "Ping Yu",
          "affiliation": null
        },
        {
          "name": "Wei Wang",
          "affiliation": null
        },
        {
          "name": "Chunyuan Li",
          "affiliation": null
        },
        {
          "name": "Ruiyi Zhang",
          "affiliation": null
        },
        {
          "name": "Zhanpeng Jin",
          "affiliation": null
        },
        {
          "name": "Changyou Chen",
          "affiliation": null
        }
      ],
      "abstract": "Prompt tuning has been an extremely effective tool to adapt a pre-trained model to downstream tasks. However, standard prompt-based methods mainly consider the case of sufficient data of downstream tasks. It is still unclear whether the advantage can be transferred to the few-shot regime, where only limited data are available for each downstream task. Although some works have demonstrated the potential of prompt-tuning under the few-shot setting, the main stream methods via searching discrete prompts or tuning soft prompts with limited data are still very challenging. Through extensive empirical studies, we find that there is still a gap between prompt tuning and fully fine-tuning for few-shot learning. To bridge the gap, we propose a new prompt-tuning framework, called Soft Template Tuning (STT). STT combines manual and auto prompts, and treats downstream classification tasks as a masked language modeling task. Comprehensive evaluation on different settings suggests STT can close the gap between fine-tuning and prompt-based methods without introducing additional parameters. Significantly, it can even outperform the time- and resource-consuming fine-tuning method on sentiment classification tasks.",
      "publishedDate": "2022-07-18T07:07:22Z",
      "updatedDate": "2022-07-18T07:07:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.08408v1",
      "arxivUrl": "https://arxiv.org/abs/2207.08408",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.07934",
      "title": "Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model",
      "authors": [
        {
          "name": "Xiaolin Chen",
          "affiliation": null
        },
        {
          "name": "Xuemeng Song",
          "affiliation": null
        },
        {
          "name": "Liqiang Jing",
          "affiliation": null
        },
        {
          "name": "Shuo Li",
          "affiliation": null
        },
        {
          "name": "Linmei Hu",
          "affiliation": null
        },
        {
          "name": "Liqiang Nie",
          "affiliation": null
        }
      ],
      "abstract": "Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: 1) overlook the benefit of generative pre-training, and 2) ignore the textual context related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language model for multimodal task-oriented dialog systems (DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors.",
      "publishedDate": "2022-07-16T13:02:54Z",
      "updatedDate": "2024-05-11T09:34:11Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.HC",
        "cs.MM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.07934v2",
      "arxivUrl": "https://arxiv.org/abs/2207.07934",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.07039",
      "title": "Convolutional Bypasses Are Better Vision Transformer Adapters",
      "authors": [
        {
          "name": "Shibo Jie",
          "affiliation": null
        },
        {
          "name": "Zhi-Hong Deng",
          "affiliation": null
        }
      ],
      "abstract": "The pretrain-then-finetune paradigm has been widely adopted in computer vision. But as the size of Vision Transformer (ViT) grows exponentially, the full finetuning becomes prohibitive in view of the heavier storage overhead. Motivated by parameter-efficient transfer learning (PETL) on language transformers, recent studies attempt to insert lightweight adaptation modules (e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune these modules while the pretrained weights are frozen. However, these modules were originally proposed to finetune language models and did not take into account the prior knowledge specifically for visual tasks. In this paper, we propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation modules, introducing only a small amount (less than 0.5% of model parameters) of trainable parameters to adapt the large ViT. Different from other PETL methods, Convpass benefits from the hard-coded inductive bias of convolutional layers and thus is more suitable for visual tasks, especially in the low-data regime. Experimental results on VTAB-1K benchmark and few-shot learning datasets show that Convpass outperforms current language-oriented adaptation modules, demonstrating the necessity to tailor vision-oriented adaptation modules for adapting vision models.",
      "publishedDate": "2022-07-14T16:32:28Z",
      "updatedDate": "2022-08-09T10:40:06Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.07039v3",
      "arxivUrl": "https://arxiv.org/abs/2207.07039",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.04901",
      "title": "Exploring Length Generalization in Large Language Models",
      "authors": [
        {
          "name": "Cem Anil",
          "affiliation": null
        },
        {
          "name": "Yuhuai Wu",
          "affiliation": null
        },
        {
          "name": "Anders Andreassen",
          "affiliation": null
        },
        {
          "name": "Aitor Lewkowycz",
          "affiliation": null
        },
        {
          "name": "Vedant Misra",
          "affiliation": null
        },
        {
          "name": "Vinay Ramasesh",
          "affiliation": null
        },
        {
          "name": "Ambrose Slone",
          "affiliation": null
        },
        {
          "name": "Guy Gur-Ari",
          "affiliation": null
        },
        {
          "name": "Ethan Dyer",
          "affiliation": null
        },
        {
          "name": "Behnam Neyshabur",
          "affiliation": null
        }
      ],
      "abstract": "The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.",
      "publishedDate": "2022-07-11T14:24:38Z",
      "updatedDate": "2022-11-14T12:21:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.04901v2",
      "arxivUrl": "https://arxiv.org/abs/2207.04901",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.04237",
      "title": "Few-shot training LLMs for project-specific code-summarization",
      "authors": [
        {
          "name": "Toufique Ahmed",
          "affiliation": null
        },
        {
          "name": "Premkumar Devanbu",
          "affiliation": null
        }
      ],
      "abstract": "Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.",
      "publishedDate": "2022-07-09T09:57:11Z",
      "updatedDate": "2022-09-08T06:50:16Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.04237v2",
      "arxivUrl": "https://arxiv.org/abs/2207.04237",
      "comment": "Accepted at ASE-NIER (2022) track",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation",
        "prompting",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "prompting",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.13214",
      "title": "Few-Shot Stance Detection via Target-Aware Prompt Distillation",
      "authors": [
        {
          "name": "Yan Jiang",
          "affiliation": null
        },
        {
          "name": "Jinhua Gao",
          "affiliation": null
        },
        {
          "name": "Huawei Shen",
          "affiliation": null
        },
        {
          "name": "Xueqi Cheng",
          "affiliation": null
        }
      ],
      "abstract": "Stance detection aims to identify whether the author of a text is in favor of, against, or neutral to a given target. The main challenge of this task comes two-fold: few-shot learning resulting from the varying targets and the lack of contextual information of the targets. Existing works mainly focus on solving the second issue by designing attention-based models or introducing noisy external knowledge, while the first issue remains under-explored. In this paper, inspired by the potential capability of pre-trained language models (PLMs) serving as knowledge bases and few-shot learners, we propose to introduce prompt-based fine-tuning for stance detection. PLMs can provide essential contextual information for the targets and enable few-shot learning via prompts. Considering the crucial role of the target in stance detection task, we design target-aware prompts and propose a novel verbalizer. Instead of mapping each label to a concrete word, our verbalizer maps each label to a vector and picks the label that best captures the correlation between the stance and the target. Moreover, to alleviate the possible defect of dealing with varying targets with a single hand-crafted prompt, we propose to distill the information learned from multiple prompts. Experimental results show the superior performance of our proposed model in both full-data and few-shot scenarios.",
      "publishedDate": "2022-06-27T12:04:14Z",
      "updatedDate": "2022-06-27T12:04:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.13214v1",
      "arxivUrl": "https://arxiv.org/abs/2206.13214",
      "comment": "Accepted by SIGIR 2022",
      "journalRef": null,
      "doi": "10.1145/3477495.3531979",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.08082",
      "title": "Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator",
      "authors": [
        {
          "name": "Hyuhng Joon Kim",
          "affiliation": null
        },
        {
          "name": "Hyunsoo Cho",
          "affiliation": null
        },
        {
          "name": "Junyeob Kim",
          "affiliation": null
        },
        {
          "name": "Taeuk Kim",
          "affiliation": null
        },
        {
          "name": "Kang Min Yoo",
          "affiliation": null
        },
        {
          "name": "Sang-goo Lee",
          "affiliation": null
        }
      ],
      "abstract": "Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. We conduct experiments on four different text classification tasks and show SG-ICL significantly outperforms zero-shot learning and is generally worth approximately 0.6 gold training samples. Moreover, our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.",
      "publishedDate": "2022-06-16T10:52:13Z",
      "updatedDate": "2022-06-16T10:52:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.08082v1",
      "arxivUrl": "https://arxiv.org/abs/2206.08082",
      "comment": "NAACL 2022 Workshop on Large-scale Pre-trained Language Models",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.07585",
      "title": "NatGen: Generative pre-training by \"Naturalizing\" source code",
      "authors": [
        {
          "name": "Saikat Chakraborty",
          "affiliation": null
        },
        {
          "name": "Toufique Ahmed",
          "affiliation": null
        },
        {
          "name": "Yangruibo Ding",
          "affiliation": null
        },
        {
          "name": "Premkumar Devanbu",
          "affiliation": null
        },
        {
          "name": "Baishakhi Ray",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained Generative Language models (e.g. PLBART, CodeT5, SPT-Code) for source code yielded strong results on several tasks in the past few years, including code generation and translation. These models have adopted varying pre-training objectives to learn statistics of code construction from very large-scale corpora in a self-supervised fashion; the success of pre-trained models largely hinges on these pre-training objectives. This paper proposes a new pre-training objective, \"Naturalizing\" of source code, exploiting code's bimodal, dual-channel (formal & natural channels) nature. Unlike natural language, code's bimodal, dual-channel nature allows us to generate semantically equivalent code at scale. We introduce six classes of semantic preserving transformations to introduce un-natural forms of code, and then force our model to produce more natural original programs written by developers. Learning to generate equivalent, but more natural code, at scale, over large corpora of open-source code, without explicit manual supervision, helps the model learn to both ingest & generate code. We fine-tune our model in three generative Software Engineering tasks: code generation, code translation, and code refinement with limited human-curated labeled data and achieve state-of-the-art performance rivaling CodeT5. We show that our pre-trained model is especially competitive at zero-shot and few-shot learning, and better at learning code properties (e.g., syntax, data flow).",
      "publishedDate": "2022-06-15T15:08:29Z",
      "updatedDate": "2022-07-05T22:39:47Z",
      "primaryCategory": "cs.PL",
      "arxivCategories": [
        "cs.PL",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.07585v2",
      "arxivUrl": "https://arxiv.org/abs/2206.07585",
      "comment": "Accepted to be published in ESEC/FSE 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.06336",
      "title": "Language Models are General-Purpose Interfaces",
      "authors": [
        {
          "name": "Yaru Hao",
          "affiliation": null
        },
        {
          "name": "Haoyu Song",
          "affiliation": null
        },
        {
          "name": "Li Dong",
          "affiliation": null
        },
        {
          "name": "Shaohan Huang",
          "affiliation": null
        },
        {
          "name": "Zewen Chi",
          "affiliation": null
        },
        {
          "name": "Wenhui Wang",
          "affiliation": null
        },
        {
          "name": "Shuming Ma",
          "affiliation": null
        },
        {
          "name": "Furu Wei",
          "affiliation": null
        }
      ],
      "abstract": "Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",
      "publishedDate": "2022-06-13T17:34:22Z",
      "updatedDate": "2022-06-13T17:34:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.06336v1",
      "arxivUrl": "https://arxiv.org/abs/2206.06336",
      "comment": "32 pages. The first three authors contribute equally",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.05442",
      "title": "From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams",
      "authors": [
        {
          "name": "Iddo Drori",
          "affiliation": null
        },
        {
          "name": "Sarah J. Zhang",
          "affiliation": null
        },
        {
          "name": "Reece Shuttleworth",
          "affiliation": null
        },
        {
          "name": "Sarah Zhang",
          "affiliation": null
        },
        {
          "name": "Keith Tyser",
          "affiliation": null
        },
        {
          "name": "Zad Chin",
          "affiliation": null
        },
        {
          "name": "Pedro Lantigua",
          "affiliation": null
        },
        {
          "name": "Saisamrit Surbehera",
          "affiliation": null
        },
        {
          "name": "Gregory Hunter",
          "affiliation": null
        },
        {
          "name": "Derek Austin",
          "affiliation": null
        },
        {
          "name": "Leonard Tang",
          "affiliation": null
        },
        {
          "name": "Yann Hicke",
          "affiliation": null
        },
        {
          "name": "Sage Simhon",
          "affiliation": null
        },
        {
          "name": "Sathwik Karnik",
          "affiliation": null
        },
        {
          "name": "Darnell Granberry",
          "affiliation": null
        },
        {
          "name": "Madeleine Udell",
          "affiliation": null
        }
      ],
      "abstract": "A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level, on finals available online after the models were trained, and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies.",
      "publishedDate": "2022-06-11T06:38:06Z",
      "updatedDate": "2023-06-28T04:42:05Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.05442v7",
      "arxivUrl": "https://arxiv.org/abs/2206.05442",
      "comment": "9 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.02336",
      "title": "Making Large Language Models Better Reasoners with Step-Aware Verifier",
      "authors": [
        {
          "name": "Yifei Li",
          "affiliation": null
        },
        {
          "name": "Zeqi Lin",
          "affiliation": null
        },
        {
          "name": "Shizhuo Zhang",
          "affiliation": null
        },
        {
          "name": "Qiang Fu",
          "affiliation": null
        },
        {
          "name": "Bei Chen",
          "affiliation": null
        },
        {
          "name": "Jian-Guang Lou",
          "affiliation": null
        },
        {
          "name": "Weizhu Chen",
          "affiliation": null
        }
      ],
      "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DIVERSE on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
      "publishedDate": "2022-06-06T03:38:36Z",
      "updatedDate": "2023-05-24T04:08:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.02336v3",
      "arxivUrl": "https://arxiv.org/abs/2206.02336",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.01335",
      "title": "Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code",
      "authors": [
        {
          "name": "Patrick Bareiß",
          "affiliation": null
        },
        {
          "name": "Beatriz Souza",
          "affiliation": null
        },
        {
          "name": "Marcelo d'Amorim",
          "affiliation": null
        },
        {
          "name": "Michael Pradel",
          "affiliation": null
        }
      ],
      "abstract": "Few-shot learning with large-scale, pre-trained language models is a powerful way to answer questions about code, e.g., how to complete a given code example, or even generate code snippets from scratch. The success of these models raises the question whether they could serve as a basis for building a wide range code generation tools. Traditionally, such tools are built manually and separately for each task. Instead, few-shot learning may allow to obtain different tools from a single pre-trained language model by simply providing a few examples or a natural language description of the expected tool behavior. This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose. We consider three code manipulation and code generation tasks targeted by a range of traditional tools: (i) code mutation; (ii) test oracle generation from natural language documentation; and (iii) test case generation. For each task, we compare few-shot learning to a manually built tool. Our results show that the model-based tools complement (code mutation), are on par (test oracle generation), or even outperform their respective traditionally built tool (test case generation), while imposing far less effort to develop them. By comparing the effectiveness of different variants of the model-based tools, we provide insights on how to design an appropriate input (\"prompt\") to the model and what influence the size of the model has. For example, we find that providing a small natural language description of the code generation task is an easy way to improve predictions. Overall, we conclude that few-shot language models are surprisingly effective, yet there is still more work to be done, such as exploring more diverse ways of prompting and tackling even more involved tasks.",
      "publishedDate": "2022-06-02T23:15:42Z",
      "updatedDate": "2022-06-12T09:57:45Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.01335v2",
      "arxivUrl": "https://arxiv.org/abs/2206.01335",
      "comment": "12 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.15223",
      "title": "Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models",
      "authors": [
        {
          "name": "Mengzhou Xia",
          "affiliation": null
        },
        {
          "name": "Mikel Artetxe",
          "affiliation": null
        },
        {
          "name": "Jingfei Du",
          "affiliation": null
        },
        {
          "name": "Danqi Chen",
          "affiliation": null
        },
        {
          "name": "Ves Stoyanov",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained masked language models successfully perform few-shot learning by formulating downstream tasks as text infilling. However, as a strong alternative in full-shot settings, discriminative pre-trained models like ELECTRA do not fit into the paradigm. In this work, we adapt prompt-based few-shot learning to ELECTRA and show that it outperforms masked language models in a wide range of tasks. ELECTRA is pre-trained to distinguish if a token is generated or original. We naturally extend that to prompt-based few-shot learning by training to score the originality of the target options without introducing new parameters. Our method can be easily adapted to tasks involving multi-token predictions without extra computation overhead. Analysis shows that ELECTRA learns distributions that align better with downstream tasks.",
      "publishedDate": "2022-05-30T16:32:30Z",
      "updatedDate": "2022-10-26T18:21:40Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.15223v3",
      "arxivUrl": "https://arxiv.org/abs/2205.15223",
      "comment": "Accepted to EMNLP 2022; The code is available at https://github.com/facebookresearch/ELECTRA-Fewshot-Learning",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.15219",
      "title": "Automatic Short Math Answer Grading via In-context Meta-learning",
      "authors": [
        {
          "name": "Mengxue Zhang",
          "affiliation": null
        },
        {
          "name": "Sami Baral",
          "affiliation": null
        },
        {
          "name": "Neil Heffernan",
          "affiliation": null
        },
        {
          "name": "Andrew Lan",
          "affiliation": null
        }
      ],
      "abstract": "Automatic short answer grading is an important research direction in the exploration of how to use artificial intelligence (AI)-based tools to improve education. Current state-of-the-art approaches use neural language models to create vectorized representations of students responses, followed by classifiers to predict the score. However, these approaches have several key limitations, including i) they use pre-trained language models that are not well-adapted to educational subject domains and/or student-generated text and ii) they almost always train one model per question, ignoring the linkage across a question and result in a significant model storage problem due to the size of advanced language models. In this paper, we study the problem of automatic short answer grading for students' responses to math questions and propose a novel framework for this task. First, we use MathBERT, a variant of the popular language model BERT adapted to mathematical content, as our base model and fine-tune it for the downstream task of student response grading. Second, we use an in-context learning approach that provides scoring examples as input to the language model to provide additional context information and promote generalization to previously unseen questions. We evaluate our framework on a real-world dataset of student responses to open-ended math questions and show that our framework (often significantly) outperforms existing approaches, especially for new questions that are not seen during training.",
      "publishedDate": "2022-05-30T16:26:02Z",
      "updatedDate": "2022-07-08T19:50:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.15219v3",
      "arxivUrl": "https://arxiv.org/abs/2205.15219",
      "comment": "To appear EDM 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.12685",
      "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations",
      "authors": [
        {
          "name": "Kang Min Yoo",
          "affiliation": null
        },
        {
          "name": "Junyeob Kim",
          "affiliation": null
        },
        {
          "name": "Hyuhng Joon Kim",
          "affiliation": null
        },
        {
          "name": "Hyunsoo Cho",
          "affiliation": null
        },
        {
          "name": "Hwiyeol Jo",
          "affiliation": null
        },
        {
          "name": "Sang-Woo Lee",
          "affiliation": null
        },
        {
          "name": "Sang-goo Lee",
          "affiliation": null
        },
        {
          "name": "Taeuk Kim",
          "affiliation": null
        }
      ],
      "abstract": "Despite recent explosion of interests in in-context learning, the underlying mechanism and the precise impact of the quality of demonstrations remain elusive. Intuitively, ground-truth labels should have as much impact in in-context learning (ICL) as supervised learning, but recent work reported that the input-label correspondence is significantly less important than previously thought. Intrigued by this counter-intuitive observation, we re-examine the importance of ground-truth labels in in-context learning. With the introduction of two novel metrics, namely Label-Correctness Sensitivity and Ground-truth Label Effect Ratio (GLER), we were able to conduct quantifiable analysis on the impact of ground-truth label demonstrations. Through extensive analyses, we find that the correct input-label mappings can have varying impacts on the downstream in-context learning performances, depending on the experimental configuration. Through additional studies, we identify key components, such as the verbosity of prompt templates and the language model size, as the controlling factor to achieve more noise-resilient ICL.",
      "publishedDate": "2022-05-25T11:45:14Z",
      "updatedDate": "2022-10-24T17:44:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.12685v2",
      "arxivUrl": "https://arxiv.org/abs/2205.12685",
      "comment": "Accepted to EMNLP Long. Kang Min Yoo and Junyeob Kim contributed equally. Kang Min Yoo and Taeuk Kim are the corresponding authors",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.12253",
      "title": "Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing",
      "authors": [
        {
          "name": "Linlu Qiu",
          "affiliation": null
        },
        {
          "name": "Peter Shaw",
          "affiliation": null
        },
        {
          "name": "Panupong Pasupat",
          "affiliation": null
        },
        {
          "name": "Tianze Shi",
          "affiliation": null
        },
        {
          "name": "Jonathan Herzig",
          "affiliation": null
        },
        {
          "name": "Emily Pitler",
          "affiliation": null
        },
        {
          "name": "Fei Sha",
          "affiliation": null
        },
        {
          "name": "Kristina Toutanova",
          "affiliation": null
        }
      ],
      "abstract": "Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve compositional generalization in semantic parsing? We evaluate encoder-decoder models up to 11B parameters and decoder-only models up to 540B parameters, and compare model scaling curves for three different methods for applying a pre-trained language model to a new task: fine-tuning all parameters, prompt tuning, and in-context learning. We observe that fine-tuning generally has flat or negative scaling curves on out-of-distribution compositional generalization in semantic parsing evaluations. In-context learning has positive scaling curves, but is generally outperformed by much smaller fine-tuned models. Prompt-tuning can outperform fine-tuning, suggesting further potential improvements from scaling as it exhibits a more positive scaling curve. Additionally, we identify several error trends that vary with model scale. For example, larger models are generally better at modeling the syntax of the output space, but are also more prone to certain types of overfitting. Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization, while our analysis also suggests promising directions for future work.",
      "publishedDate": "2022-05-24T17:57:39Z",
      "updatedDate": "2022-10-24T20:23:31Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.12253v2",
      "arxivUrl": "https://arxiv.org/abs/2205.12253",
      "comment": "EMNLP 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.11961",
      "title": "ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts",
      "authors": [
        {
          "name": "Akari Asai",
          "affiliation": null
        },
        {
          "name": "Mohammadreza Salehi",
          "affiliation": null
        },
        {
          "name": "Matthew E. Peters",
          "affiliation": null
        },
        {
          "name": "Hannaneh Hajishirzi",
          "affiliation": null
        }
      ],
      "abstract": "This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts-small prefix embedding vectors pre-trained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt Tuning), obtains source prompts as encodings of large-scale source tasks into a small number of parameters and trains an attention module to interpolate the source prompts and a newly initialized target prompt for every instance in the target task. During training, only the target task prompt and the attention weights, which are shared between tasks in multi-task training, are updated, while the original LM and source prompts are intact. ATTEMPT is highly parameter-efficient (e.g., updates 2,300 times fewer parameters than full fine-tuning) while achieving high task performance using knowledge from high-resource tasks. Moreover, it is modular using pre-trained soft prompts, and can flexibly add or remove source prompts for effective knowledge transfer. Our experimental results across 21 diverse NLP datasets show that ATTEMPT significantly outperforms prompt tuning and outperforms or matches fully fine-tuned or other parameter-efficient tuning approaches that use over ten times more parameters. Finally, ATTEMPT outperforms previous work in few-shot learning settings.",
      "publishedDate": "2022-05-24T10:48:33Z",
      "updatedDate": "2022-12-01T18:31:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.11961v2",
      "arxivUrl": "https://arxiv.org/abs/2205.11961",
      "comment": "Published as a conference paper at EMNLP 2022 (long). Code available at https://github.com/AkariAsai/ATTEMPT",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.11916",
      "title": "Large Language Models are Zero-Shot Reasoners",
      "authors": [
        {
          "name": "Takeshi Kojima",
          "affiliation": null
        },
        {
          "name": "Shixiang Shane Gu",
          "affiliation": null
        },
        {
          "name": "Machel Reid",
          "affiliation": null
        },
        {
          "name": "Yutaka Matsuo",
          "affiliation": null
        },
        {
          "name": "Yusuke Iwasawa",
          "affiliation": null
        }
      ],
      "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding \"Let's think step by step\" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
      "publishedDate": "2022-05-24T09:22:26Z",
      "updatedDate": "2023-01-29T05:14:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.11916v4",
      "arxivUrl": "https://arxiv.org/abs/2205.11916",
      "comment": "Accepted to NeurIPS2022. Our code is available at https://github.com/kojima-takeshi188/zero_shot_cot",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.11505",
      "title": "What Makes Data-to-Text Generation Hard for Pretrained Language Models?",
      "authors": [
        {
          "name": "Moniba Keymanesh",
          "affiliation": null
        },
        {
          "name": "Adrian Benton",
          "affiliation": null
        },
        {
          "name": "Mark Dredze",
          "affiliation": null
        }
      ],
      "abstract": "Expressing natural language descriptions of structured facts or relations -- data-to-text generation (D2T) -- increases the accessibility of structured knowledge repositories. Previous work shows that pre-trained language models(PLMs) perform remarkably well on this task after fine-tuning on a significant amount of task-specific training data. On the other hand, while auto-regressive PLMs can generalize from a few task examples, their efficacy at D2T is largely unexplored. Furthermore, we have an incomplete understanding of the limits of PLMs on D2T. In this work, we conduct an empirical study of both fine-tuned and auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their performance as a function of the amount of task-specific data and how these data are incorporated into the models: zero and few-shot learning, and fine-tuning of model weights. In addition, we probe the limits of PLMs by measuring performance on subsets of the evaluation data: novel predicates and abstractive test examples. To improve the performance on these subsets, we investigate two techniques: providing predicate descriptions in the context and re-ranking generated candidates by information reflected in the source. Finally, we conduct a human evaluation of model errors and show that D2T generation tasks would benefit from datasets with more careful manual curation.",
      "publishedDate": "2022-05-23T17:58:39Z",
      "updatedDate": "2022-05-23T17:58:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.11505v1",
      "arxivUrl": "https://arxiv.org/abs/2205.11505",
      "comment": "15 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.11200",
      "title": "BBTv2: Towards a Gradient-Free Future with Large Language Models",
      "authors": [
        {
          "name": "Tianxiang Sun",
          "affiliation": null
        },
        {
          "name": "Zhengfu He",
          "affiliation": null
        },
        {
          "name": "Hong Qian",
          "affiliation": null
        },
        {
          "name": "Yunhua Zhou",
          "affiliation": null
        },
        {
          "name": "Xuanjing Huang",
          "affiliation": null
        },
        {
          "name": "Xipeng Qiu",
          "affiliation": null
        }
      ],
      "abstract": "Most downstream adaptation methods tune all or part of the parameters of pre-trained models (PTMs) through gradient descent, where the tuning cost increases linearly with the growth of the model size. By contrast, gradient-free methods only require the forward computation of the PTM to tune the prompt, retaining the benefits of efficient tuning and deployment. Though, past work on gradient-free tuning often introduces gradient descent to seek a good initialization of prompt and lacks versatility across tasks and PTMs. In this paper, we present BBTv2, an improved version of Black-Box Tuning, to drive PTMs for few-shot learning. We prepend continuous prompts to every layer of the PTM and propose a divide-and-conquer gradient-free algorithm to optimize the prompts at different layers alternately. Extensive experiments across various tasks and PTMs show that BBTv2 can achieve comparable performance to full model tuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA, BitFit, etc.) under few-shot settings while maintaining much fewer tunable parameters.",
      "publishedDate": "2022-05-23T11:10:19Z",
      "updatedDate": "2022-10-14T15:02:05Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.11200v2",
      "arxivUrl": "https://arxiv.org/abs/2205.11200",
      "comment": "Accepted to EMNLP 2022 (main conference). Code is available at https://github.com/txsun1997/Black-Box-Tuning",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.10782",
      "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
      "authors": [
        {
          "name": "Or Honovich",
          "affiliation": null
        },
        {
          "name": "Uri Shaham",
          "affiliation": null
        },
        {
          "name": "Samuel R. Bowman",
          "affiliation": null
        },
        {
          "name": "Omer Levy",
          "affiliation": null
        }
      ],
      "abstract": "Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.",
      "publishedDate": "2022-05-22T09:22:37Z",
      "updatedDate": "2022-05-22T09:22:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.10782v1",
      "arxivUrl": "https://arxiv.org/abs/2205.10782",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.10183",
      "title": "Prototypical Calibration for Few-shot Learning of Language Models",
      "authors": [
        {
          "name": "Zhixiong Han",
          "affiliation": null
        },
        {
          "name": "Yaru Hao",
          "affiliation": null
        },
        {
          "name": "Li Dong",
          "affiliation": null
        },
        {
          "name": "Yutao Sun",
          "affiliation": null
        },
        {
          "name": "Furu Wei",
          "affiliation": null
        }
      ],
      "abstract": "In-context learning of GPT-like models has been recognized as fragile across different hand-crafted templates, and demonstration permutations. In this work, we propose prototypical calibration to adaptively learn a more robust decision boundary for zero- and few-shot classification, instead of greedy decoding. Concretely, our method first adopts Gaussian mixture distribution to estimate the prototypical clusters for all categories. Then we assign each cluster to the corresponding label by solving a weighted bipartite matching problem. Given an example, its prediction is calibrated by the likelihood of prototypical clusters. Experimental results show that prototypical calibration yields a substantial improvement on a diverse set of tasks. Extensive analysis across different scales also indicates that our method calibrates the decision boundary as expected, greatly improving the robustness of GPT to templates, permutations, and class imbalance.",
      "publishedDate": "2022-05-20T13:50:07Z",
      "updatedDate": "2022-10-05T01:33:07Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.10183v2",
      "arxivUrl": "https://arxiv.org/abs/2205.10183",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.09229",
      "title": "PromptDA: Label-guided Data Augmentation for Prompt-based Few-shot Learners",
      "authors": [
        {
          "name": "Canyu Chen",
          "affiliation": null
        },
        {
          "name": "Kai Shu",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large pre-trained language models (PLMs) lead to impressive gains in natural language understanding (NLU) tasks with task-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on sufficient labeled training instances, which are usually hard to obtain. Prompt-based tuning on PLMs has shown to be powerful for various downstream few-shot tasks. Existing works studying prompt-based tuning for few-shot NLU tasks mainly focus on deriving proper label words with a verbalizer or generating prompt templates to elicit semantics from PLMs. In addition, conventional data augmentation strategies such as synonym substitution, though widely adopted in low-resource scenarios, only bring marginal improvements for prompt-based few-shot learning. Thus, an important research question arises: how to design effective data augmentation methods for prompt-based few-shot tuning? To this end, considering the label semantics are essential in prompt-based tuning, we propose a novel label-guided data augmentation framework PromptDA, which exploits the enriched label semantic information for data augmentation. Extensive experiment results on few-shot text classification tasks demonstrate the superior performance of the proposed framework by effectively leveraging label semantics and data augmentation for natural language understanding. Our code is available at https://github.com/canyuchen/PromptDA.",
      "publishedDate": "2022-05-18T22:15:20Z",
      "updatedDate": "2023-03-22T21:10:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.09229v3",
      "arxivUrl": "https://arxiv.org/abs/2205.09229",
      "comment": "Accepted to Proceedings of EACL 2023 main conference. Code is available at https://github.com/canyuchen/PromptDA",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.06350",
      "title": "On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data",
      "authors": [
        {
          "name": "Kabir Ahuja",
          "affiliation": null
        },
        {
          "name": "Monojit Choudhury",
          "affiliation": null
        },
        {
          "name": "Sandipan Dandapat",
          "affiliation": null
        }
      ],
      "abstract": "Borrowing ideas from {\\em Production functions} in micro-economics, in this paper we introduce a framework to systematically evaluate the performance and cost trade-offs between machine-translated and manually-created labelled data for task-specific fine-tuning of massively multilingual language models. We illustrate the effectiveness of our framework through a case-study on the TyDIQA-GoldP dataset. One of the interesting conclusions of the study is that if the cost of machine translation is greater than zero, the optimal performance at least cost is always achieved with at least some or only manually-created data. To our knowledge, this is the first attempt towards extending the concept of production functions to study data collection strategies for training multilingual models, and can serve as a valuable tool for other similar cost vs data trade-offs in NLP.",
      "publishedDate": "2022-05-12T20:27:01Z",
      "updatedDate": "2022-11-14T15:48:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.06350v2",
      "arxivUrl": "https://arxiv.org/abs/2205.06350",
      "comment": "NAACL 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.05989",
      "title": "Towards Answering Open-ended Ethical Quandary Questions",
      "authors": [
        {
          "name": "Yejin Bang",
          "affiliation": null
        },
        {
          "name": "Nayeon Lee",
          "affiliation": null
        },
        {
          "name": "Tiezheng Yu",
          "affiliation": null
        },
        {
          "name": "Leila Khalatbari",
          "affiliation": null
        },
        {
          "name": "Yan Xu",
          "affiliation": null
        },
        {
          "name": "Samuel Cahyawijaya",
          "affiliation": null
        },
        {
          "name": "Dan Su",
          "affiliation": null
        },
        {
          "name": "Bryan Wilie",
          "affiliation": null
        },
        {
          "name": "Romain Barraud",
          "affiliation": null
        },
        {
          "name": "Elham J. Barezi",
          "affiliation": null
        },
        {
          "name": "Andrea Madotto",
          "affiliation": null
        },
        {
          "name": "Hayden Kee",
          "affiliation": null
        },
        {
          "name": "Pascale Fung",
          "affiliation": null
        }
      ],
      "abstract": "Considerable advancements have been made in various NLP tasks based on the impressive power of large language models (LLMs) and many NLP applications are deployed in our daily lives. In this work, we challenge the capability of LLMs with the new task of Ethical Quandary Generative Question Answering. Ethical quandary questions are more challenging to address because multiple conflicting answers may exist to a single quandary. We explore the current capability of LLMs in providing an answer with a deliberative exchange of different perspectives to an ethical quandary, in the approach of Socratic philosophy, instead of providing a closed answer like an oracle. We propose a model that searches for different ethical principles applicable to the ethical quandary and generates an answer conditioned on the chosen principles through prompt-based few-shot learning. We also discuss the remaining challenges and ethical issues involved in this task and suggest the direction toward developing responsible NLP systems by incorporating human values explicitly.",
      "publishedDate": "2022-05-12T09:52:59Z",
      "updatedDate": "2023-02-01T07:25:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.05989v3",
      "arxivUrl": "https://arxiv.org/abs/2205.05989",
      "comment": "16 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.05638",
      "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning",
      "authors": [
        {
          "name": "Haokun Liu",
          "affiliation": null
        },
        {
          "name": "Derek Tam",
          "affiliation": null
        },
        {
          "name": "Mohammed Muqeeth",
          "affiliation": null
        },
        {
          "name": "Jay Mohta",
          "affiliation": null
        },
        {
          "name": "Tenghao Huang",
          "affiliation": null
        },
        {
          "name": "Mohit Bansal",
          "affiliation": null
        },
        {
          "name": "Colin Raffel",
          "affiliation": null
        }
      ],
      "abstract": "Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)$^3$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute. All of the code used in our experiments is publicly available.",
      "publishedDate": "2022-05-11T17:10:41Z",
      "updatedDate": "2022-08-26T16:23:29Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.05638v2",
      "arxivUrl": "https://arxiv.org/abs/2205.05638",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.05313",
      "title": "Towards Unified Prompt Tuning for Few-shot Text Classification",
      "authors": [
        {
          "name": "Jianing Wang",
          "affiliation": null
        },
        {
          "name": "Chengyu Wang",
          "affiliation": null
        },
        {
          "name": "Fuli Luo",
          "affiliation": null
        },
        {
          "name": "Chuanqi Tan",
          "affiliation": null
        },
        {
          "name": "Minghui Qiu",
          "affiliation": null
        },
        {
          "name": "Fei Yang",
          "affiliation": null
        },
        {
          "name": "Qiuhui Shi",
          "affiliation": null
        },
        {
          "name": "Songfang Huang",
          "affiliation": null
        },
        {
          "name": "Ming Gao",
          "affiliation": null
        }
      ],
      "abstract": "Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models (PLMs) on few-shot text classification by employing task-specific prompts. Yet, PLMs are unfamiliar with prompt-style expressions during pre-training, which limits the few-shot learning performance on downstream tasks. It would be desirable if the models can acquire some prompting knowledge before adaptation to specific NLP tasks. We present the Unified Prompt Tuning (UPT) framework, leading to better few-shot text classification for BERT-style models by explicitly capturing prompting semantics from non-target NLP datasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed for joint prompt learning across different NLP tasks, forcing PLMs to capture task-invariant prompting knowledge. We further design a self-supervised task named Knowledge-enhanced Selective Masked Language Modeling to improve the PLM's generalization abilities for accurate adaptation to previously unseen tasks. After multi-task learning across multiple tasks, the PLM can be better prompt-tuned towards any dissimilar target tasks in low-resourced settings. Experiments over a variety of NLP tasks show that UPT consistently outperforms state-of-the-arts for prompt-based fine-tuning.",
      "publishedDate": "2022-05-11T07:40:45Z",
      "updatedDate": "2022-05-11T07:40:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.05313v1",
      "arxivUrl": "https://arxiv.org/abs/2205.05313",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.03401",
      "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
      "authors": [
        {
          "name": "Xi Ye",
          "affiliation": null
        },
        {
          "name": "Greg Durrett",
          "affiliation": null
        }
      ],
      "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
      "publishedDate": "2022-05-06T17:57:58Z",
      "updatedDate": "2022-10-13T03:07:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.03401v2",
      "arxivUrl": "https://arxiv.org/abs/2205.03401",
      "comment": "NeurIPS 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.03071",
      "title": "KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering",
      "authors": [
        {
          "name": "Jianing Wang",
          "affiliation": null
        },
        {
          "name": "Chengyu Wang",
          "affiliation": null
        },
        {
          "name": "Minghui Qiu",
          "affiliation": null
        },
        {
          "name": "Qiuhui Shi",
          "affiliation": null
        },
        {
          "name": "Hongbin Wang",
          "affiliation": null
        },
        {
          "name": "Jun Huang",
          "affiliation": null
        },
        {
          "name": "Ming Gao",
          "affiliation": null
        }
      ],
      "abstract": "Extractive Question Answering (EQA) is one of the most important tasks in Machine Reading Comprehension (MRC), which can be solved by fine-tuning the span selecting heads of Pre-trained Language Models (PLMs). However, most existing approaches for MRC may perform poorly in the few-shot learning scenario. To solve this issue, we propose a novel framework named Knowledge Enhanced Contrastive Prompt-tuning (KECP). Instead of adding pointer heads to PLMs, we introduce a seminal paradigm for EQA that transform the task into a non-autoregressive Masked Language Modeling (MLM) generation problem. Simultaneously, rich semantics from the external knowledge base (KB) and the passage context are support for enhancing the representations of the query. In addition, to boost the performance of PLMs, we jointly train the model by the MLM and contrastive learning objectives. Experiments on multiple benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches in few-shot settings by a large margin.",
      "publishedDate": "2022-05-06T08:31:02Z",
      "updatedDate": "2022-05-06T08:31:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.03071v1",
      "arxivUrl": "https://arxiv.org/abs/2205.03071",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.02355",
      "title": "Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning",
      "authors": [
        {
          "name": "Xiang Chen",
          "affiliation": null
        },
        {
          "name": "Lei Li",
          "affiliation": null
        },
        {
          "name": "Ningyu Zhang",
          "affiliation": null
        },
        {
          "name": "Chuanqi Tan",
          "affiliation": null
        },
        {
          "name": "Fei Huang",
          "affiliation": null
        },
        {
          "name": "Luo Si",
          "affiliation": null
        },
        {
          "name": "Huajun Chen",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In this way, our model not only infers relation through knowledge stored in the weights during training but also assists decision-making by unwinding and querying examples in the open-book datastore. Extensive experiments on benchmark datasets show that our method can achieve state-of-the-art in both standard supervised and few-shot settings. Code are available in https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE.",
      "publishedDate": "2022-05-04T23:38:37Z",
      "updatedDate": "2023-09-19T12:21:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.02355v2",
      "arxivUrl": "https://arxiv.org/abs/2205.02355",
      "comment": "Accepted by SIGIR 2022, short paper",
      "journalRef": null,
      "doi": "10.1145/3477495.3531746",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.01308",
      "title": "Contrastive Learning for Prompt-Based Few-Shot Language Learners",
      "authors": [
        {
          "name": "Yiren Jian",
          "affiliation": null
        },
        {
          "name": "Chongyang Gao",
          "affiliation": null
        },
        {
          "name": "Soroush Vosoughi",
          "affiliation": null
        }
      ],
      "abstract": "The impressive performance of GPT-3 using natural language prompts and in-context learning has inspired work on better fine-tuning of moderately-sized models under this paradigm. Following this line of work, we present a contrastive learning framework that clusters inputs from the same class for better generality of models trained with only limited examples. Specifically, we propose a supervised contrastive framework that clusters inputs from the same class under different augmented \"views\" and repel the ones from different classes. We create different \"views\" of an example by appending it with different language prompts and contextual demonstrations. Combining a contrastive loss with the standard masked language modeling (MLM) loss in prompt-based few-shot learners, the experimental results show that our method can improve over the state-of-the-art methods in a diverse set of 15 language tasks. Our framework makes minimal assumptions on the task or the base model, and can be applied to many recent methods with little modification. The code will be made available at: https://github.com/yiren-jian/LM-SupCon.",
      "publishedDate": "2022-05-03T04:56:45Z",
      "updatedDate": "2022-05-03T04:56:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.01308v1",
      "arxivUrl": "https://arxiv.org/abs/2205.01308",
      "comment": "accepted to NAACL 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.01068",
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "authors": [
        {
          "name": "Susan Zhang",
          "affiliation": null
        },
        {
          "name": "Stephen Roller",
          "affiliation": null
        },
        {
          "name": "Naman Goyal",
          "affiliation": null
        },
        {
          "name": "Mikel Artetxe",
          "affiliation": null
        },
        {
          "name": "Moya Chen",
          "affiliation": null
        },
        {
          "name": "Shuohui Chen",
          "affiliation": null
        },
        {
          "name": "Christopher Dewan",
          "affiliation": null
        },
        {
          "name": "Mona Diab",
          "affiliation": null
        },
        {
          "name": "Xian Li",
          "affiliation": null
        },
        {
          "name": "Xi Victoria Lin",
          "affiliation": null
        },
        {
          "name": "Todor Mihaylov",
          "affiliation": null
        },
        {
          "name": "Myle Ott",
          "affiliation": null
        },
        {
          "name": "Sam Shleifer",
          "affiliation": null
        },
        {
          "name": "Kurt Shuster",
          "affiliation": null
        },
        {
          "name": "Daniel Simig",
          "affiliation": null
        },
        {
          "name": "Punit Singh Koura",
          "affiliation": null
        },
        {
          "name": "Anjali Sridhar",
          "affiliation": null
        },
        {
          "name": "Tianlu Wang",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        }
      ],
      "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
      "publishedDate": "2022-05-02T17:49:50Z",
      "updatedDate": "2022-06-21T17:04:40Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.01068v4",
      "arxivUrl": "https://arxiv.org/abs/2205.01068",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "tool-use",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.00619",
      "title": "POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection",
      "authors": [
        {
          "name": "Yujian Liu",
          "affiliation": null
        },
        {
          "name": "Xinliang Frederick Zhang",
          "affiliation": null
        },
        {
          "name": "David Wegsman",
          "affiliation": null
        },
        {
          "name": "Nick Beauchamp",
          "affiliation": null
        },
        {
          "name": "Lu Wang",
          "affiliation": null
        }
      ],
      "abstract": "Ideology is at the core of political science research. Yet, there still does not exist general-purpose tools to characterize and predict ideology across different genres of text. To this end, we study Pretrained Language Models using novel ideology-driven pretraining objectives that rely on the comparison of articles on the same story written by media of different ideologies. We further collect a large-scale dataset, consisting of more than 3.6M political news articles, for pretraining. Our model POLITICS outperforms strong baselines and the previous state-of-the-art models on ideology prediction and stance detection tasks. Further analyses show that POLITICS is especially good at understanding long or formally written texts, and is also robust in few-shot learning scenarios.",
      "publishedDate": "2022-05-02T02:10:05Z",
      "updatedDate": "2022-05-02T02:10:05Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.00619v1",
      "arxivUrl": "https://arxiv.org/abs/2205.00619",
      "comment": "Findings of NAACL'22. The first two authors contribute equally",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.00176",
      "title": "Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models",
      "authors": [
        {
          "name": "Sanghwan Bae",
          "affiliation": null
        },
        {
          "name": "Donghyun Kwak",
          "affiliation": null
        },
        {
          "name": "Sungdong Kim",
          "affiliation": null
        },
        {
          "name": "Donghoon Ham",
          "affiliation": null
        },
        {
          "name": "Soyoung Kang",
          "affiliation": null
        },
        {
          "name": "Sang-Woo Lee",
          "affiliation": null
        },
        {
          "name": "Woomyoung Park",
          "affiliation": null
        }
      ],
      "abstract": "Recent open-domain dialogue models have brought numerous breakthroughs. However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety. In this work, we study the challenge of imposing roles on open-domain dialogue systems, with the goal of making the systems maintain consistent roles while conversing naturally with humans. To accomplish this, the system must satisfy a role specification that includes certain conditions on the stated features as well as a system policy on whether or not certain types of utterances are allowed. For this, we propose an efficient data collection framework leveraging in-context few-shot learning of large-scale language models for building role-satisfying dialogue dataset from scratch. We then compare various architectures for open-domain dialogue systems in terms of meeting role specifications while maintaining conversational abilities. Automatic and human evaluations show that our models return few out-of-bounds utterances, keeping competitive performance on general metrics. We release a Korean dialogue dataset we built for further research.",
      "publishedDate": "2022-04-30T06:23:06Z",
      "updatedDate": "2022-04-30T06:23:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.00176v1",
      "arxivUrl": "https://arxiv.org/abs/2205.00176",
      "comment": "Accepted to NAACL2022 as a long paper",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "evaluation",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.14198",
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": [
        {
          "name": "Jean-Baptiste Alayrac",
          "affiliation": null
        },
        {
          "name": "Jeff Donahue",
          "affiliation": null
        },
        {
          "name": "Pauline Luc",
          "affiliation": null
        },
        {
          "name": "Antoine Miech",
          "affiliation": null
        },
        {
          "name": "Iain Barr",
          "affiliation": null
        },
        {
          "name": "Yana Hasson",
          "affiliation": null
        },
        {
          "name": "Karel Lenc",
          "affiliation": null
        },
        {
          "name": "Arthur Mensch",
          "affiliation": null
        },
        {
          "name": "Katie Millican",
          "affiliation": null
        },
        {
          "name": "Malcolm Reynolds",
          "affiliation": null
        },
        {
          "name": "Roman Ring",
          "affiliation": null
        },
        {
          "name": "Eliza Rutherford",
          "affiliation": null
        },
        {
          "name": "Serkan Cabi",
          "affiliation": null
        },
        {
          "name": "Tengda Han",
          "affiliation": null
        },
        {
          "name": "Zhitao Gong",
          "affiliation": null
        },
        {
          "name": "Sina Samangooei",
          "affiliation": null
        },
        {
          "name": "Marianne Monteiro",
          "affiliation": null
        },
        {
          "name": "Jacob Menick",
          "affiliation": null
        },
        {
          "name": "Sebastian Borgeaud",
          "affiliation": null
        },
        {
          "name": "Andrew Brock",
          "affiliation": null
        },
        {
          "name": "Aida Nematzadeh",
          "affiliation": null
        },
        {
          "name": "Sahand Sharifzadeh",
          "affiliation": null
        },
        {
          "name": "Mikolaj Binkowski",
          "affiliation": null
        },
        {
          "name": "Ricardo Barreira",
          "affiliation": null
        },
        {
          "name": "Oriol Vinyals",
          "affiliation": null
        },
        {
          "name": "Andrew Zisserman",
          "affiliation": null
        },
        {
          "name": "Karen Simonyan",
          "affiliation": null
        }
      ],
      "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
      "publishedDate": "2022-04-29T16:29:01Z",
      "updatedDate": "2022-11-15T23:07:37Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.14198v2",
      "arxivUrl": "https://arxiv.org/abs/2204.14198",
      "comment": "54 pages. In Proceedings of Neural Information Processing Systems (NeurIPS) 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.13509",
      "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model",
      "authors": [
        {
          "name": "Seongjin Shin",
          "affiliation": null
        },
        {
          "name": "Sang-Woo Lee",
          "affiliation": null
        },
        {
          "name": "Hwijeen Ahn",
          "affiliation": null
        },
        {
          "name": "Sungdong Kim",
          "affiliation": null
        },
        {
          "name": "HyoungSeok Kim",
          "affiliation": null
        },
        {
          "name": "Boseop Kim",
          "affiliation": null
        },
        {
          "name": "Kyunghyun Cho",
          "affiliation": null
        },
        {
          "name": "Gichang Lee",
          "affiliation": null
        },
        {
          "name": "Woomyoung Park",
          "affiliation": null
        },
        {
          "name": "Jung-Woo Ha",
          "affiliation": null
        },
        {
          "name": "Nako Sung",
          "affiliation": null
        }
      ],
      "abstract": "Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance.",
      "publishedDate": "2022-04-28T13:59:54Z",
      "updatedDate": "2022-05-08T06:36:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.13509v2",
      "arxivUrl": "https://arxiv.org/abs/2204.13509",
      "comment": "Accepted to NAACL2022 as a long paper. Camera-ready version",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.11922",
      "title": "Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks",
      "authors": [
        {
          "name": "Navid Rezaei",
          "affiliation": null
        },
        {
          "name": "Marek Z. Reformat",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained language models have shown excellent results in few-shot learning scenarios using in-context learning. Although it is impressive, the size of language models can be prohibitive to make them usable in on-device applications, such as sensors or smartphones. With smaller language models, task-specific data annotation is needed to fine-tune the language model for a specific purpose. However, data annotation can have a substantial financial and time burden for small research groups, startups, and even companies. In this paper, we analyze different prompt-based fine-tuning techniques to improve results on both language and multimodal causal transformer models. To evaluate our results, we use a dataset focusing on visual commonsense reasoning in time. Our results show that by simple model-agnostic prompt-based fine-tuning, comparable results can be reached by only using 35%-40% of the fine-tuning training dataset. The proposed approaches result in significant time and financial savings. As the proposed methods make minimal architectural assumptions, other researchers can use the results in their transformer models with minimal adaptations. We plan to release the source code freely to make it easier for the community to use and contribute to our work.",
      "publishedDate": "2022-04-25T18:56:55Z",
      "updatedDate": "2022-04-25T18:56:55Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.11922v1",
      "arxivUrl": "https://arxiv.org/abs/2204.11922",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.05055",
      "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers",
      "authors": [
        {
          "name": "Stephanie C. Y. Chan",
          "affiliation": null
        },
        {
          "name": "Adam Santoro",
          "affiliation": null
        },
        {
          "name": "Andrew K. Lampinen",
          "affiliation": null
        },
        {
          "name": "Jane X. Wang",
          "affiliation": null
        },
        {
          "name": "Aaditya Singh",
          "affiliation": null
        },
        {
          "name": "Pierre H. Richemond",
          "affiliation": null
        },
        {
          "name": "Jay McClelland",
          "affiliation": null
        },
        {
          "name": "Felix Hill",
          "affiliation": null
        }
      ],
      "abstract": "Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.",
      "publishedDate": "2022-04-22T16:10:50Z",
      "updatedDate": "2022-11-17T18:42:14Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.05055v6",
      "arxivUrl": "https://arxiv.org/abs/2205.05055",
      "comment": "Accepted at NeurIPS 2022 (Oral). Code is available at: https://github.com/deepmind/emergent_in_context_learning",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.08167",
      "title": "A Study on Prompt-based Few-Shot Learning Methods for Belief State Tracking in Task-oriented Dialog Systems",
      "authors": [
        {
          "name": "Debjoy Saha",
          "affiliation": null
        },
        {
          "name": "Bishal Santra",
          "affiliation": null
        },
        {
          "name": "Pawan Goyal",
          "affiliation": null
        }
      ],
      "abstract": "We tackle the Dialogue Belief State Tracking(DST) problem of task-oriented conversational systems. Recent approaches to this problem leveraging Transformer-based models have yielded great results. However, training these models is expensive, both in terms of computational resources and time. Additionally, collecting high quality annotated dialogue datasets remains a challenge for researchers because of the extensive annotation required for training these models. Driven by the recent success of pre-trained language models and prompt-based learning, we explore prompt-based few-shot learning for Dialogue Belief State Tracking. We formulate the DST problem as a 2-stage prompt-based language modelling task and train language models for both tasks and present a comprehensive empirical analysis of their separate and joint performance. We demonstrate the potential of prompt-based methods in few-shot learning for DST and provide directions for future improvement.",
      "publishedDate": "2022-04-18T05:29:54Z",
      "updatedDate": "2022-04-18T05:29:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.08167v1",
      "arxivUrl": "https://arxiv.org/abs/2204.08167",
      "comment": "9 pages, 12 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.07580",
      "title": "mGPT: Few-Shot Learners Go Multilingual",
      "authors": [
        {
          "name": "Oleh Shliazhko",
          "affiliation": null
        },
        {
          "name": "Alena Fenogenova",
          "affiliation": null
        },
        {
          "name": "Maria Tikhonova",
          "affiliation": null
        },
        {
          "name": "Vladislav Mikhailov",
          "affiliation": null
        },
        {
          "name": "Anastasia Kozlova",
          "affiliation": null
        },
        {
          "name": "Tatiana Shavrina",
          "affiliation": null
        }
      ],
      "abstract": "Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model to choose the most optimal multilingual tokenization strategy. We measure the model perplexity in all covered languages and evaluate it on the wide spectre of multilingual tasks, including classification, generative, sequence labeling and knowledge probing. The models were evaluated with the zero-shot and few-shot methods. Furthermore, we compared the classification tasks with the state-of-the-art multilingual model XGLM. source code and the mGPT XL model are publicly released.",
      "publishedDate": "2022-04-15T13:02:33Z",
      "updatedDate": "2023-10-12T17:07:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.07580v2",
      "arxivUrl": "https://arxiv.org/abs/2204.07580",
      "comment": "Accepted for publication at Transactions of the Association for Computational Linguistics (TACL) To be presented at the Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.06130",
      "title": "Impossible Triangle: What's Next for Pre-trained Language Models?",
      "authors": [
        {
          "name": "Chenguang Zhu",
          "affiliation": null
        },
        {
          "name": "Michael Zeng",
          "affiliation": null
        }
      ],
      "abstract": "Recent development of large-scale pre-trained language models (PLM) have significantly improved the capability of models in various NLP tasks, in terms of performance after task-specific fine-tuning and zero-shot / few-shot learning. However, many of such models come with a dauntingly huge size that few institutions can afford to pre-train, fine-tune or even deploy, while moderate-sized models usually lack strong generalized few-shot learning capabilities. In this paper, we first elaborate the current obstacles of using PLM models in terms of the Impossible Triangle: 1) moderate model size, 2) state-of-the-art few-shot learning capability, and 3) state-of-the-art fine-tuning capability. We argue that all existing PLM models lack one or more properties from the Impossible Triangle. To remedy these missing properties of PLMs, various techniques have been proposed, such as knowledge distillation, data augmentation and prompt learning, which inevitably brings additional work to the application of PLMs in real scenarios. We then offer insights into future research directions of PLMs to achieve the Impossible Triangle, and break down the task into several key phases.",
      "publishedDate": "2022-04-13T01:28:18Z",
      "updatedDate": "2022-04-20T05:13:52Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.06130v2",
      "arxivUrl": "https://arxiv.org/abs/2204.06130",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.02329",
      "title": "Can language models learn from explanations in context?",
      "authors": [
        {
          "name": "Andrew K. Lampinen",
          "affiliation": null
        },
        {
          "name": "Ishita Dasgupta",
          "affiliation": null
        },
        {
          "name": "Stephanie C. Y. Chan",
          "affiliation": null
        },
        {
          "name": "Kory Matthewson",
          "affiliation": null
        },
        {
          "name": "Michael Henry Tessler",
          "affiliation": null
        },
        {
          "name": "Antonia Creswell",
          "affiliation": null
        },
        {
          "name": "James L. McClelland",
          "affiliation": null
        },
        {
          "name": "Jane X. Wang",
          "affiliation": null
        },
        {
          "name": "Felix Hill",
          "affiliation": null
        }
      ],
      "abstract": "Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging tasks with answer explanations, and various matched control explanations. We evaluate how different types of explanations, instructions, and controls affect zero- and few-shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models. We find that explanations can improve performance -- even without tuning. Furthermore, explanations hand-tuned for performance on a small validation set offer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform carefully matched controls, suggesting that the benefits are due to the link between an example and its explanation, rather than lower-level features. However, only large models benefit. In summary, explanations can support the in-context learning of large LMs on challenging tasks.",
      "publishedDate": "2022-04-05T16:33:44Z",
      "updatedDate": "2022-10-10T15:25:40Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.02329v4",
      "arxivUrl": "https://arxiv.org/abs/2204.02329",
      "comment": "Findings of EMNLP 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.02311",
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "authors": [
        {
          "name": "Aakanksha Chowdhery",
          "affiliation": null
        },
        {
          "name": "Sharan Narang",
          "affiliation": null
        },
        {
          "name": "Jacob Devlin",
          "affiliation": null
        },
        {
          "name": "Maarten Bosma",
          "affiliation": null
        },
        {
          "name": "Gaurav Mishra",
          "affiliation": null
        },
        {
          "name": "Adam Roberts",
          "affiliation": null
        },
        {
          "name": "Paul Barham",
          "affiliation": null
        },
        {
          "name": "Hyung Won Chung",
          "affiliation": null
        },
        {
          "name": "Charles Sutton",
          "affiliation": null
        },
        {
          "name": "Sebastian Gehrmann",
          "affiliation": null
        },
        {
          "name": "Parker Schuh",
          "affiliation": null
        },
        {
          "name": "Kensen Shi",
          "affiliation": null
        },
        {
          "name": "Sasha Tsvyashchenko",
          "affiliation": null
        },
        {
          "name": "Joshua Maynez",
          "affiliation": null
        },
        {
          "name": "Abhishek Rao",
          "affiliation": null
        },
        {
          "name": "Parker Barnes",
          "affiliation": null
        },
        {
          "name": "Yi Tay",
          "affiliation": null
        },
        {
          "name": "Noam Shazeer",
          "affiliation": null
        },
        {
          "name": "Vinodkumar Prabhakaran",
          "affiliation": null
        },
        {
          "name": "Emily Reif",
          "affiliation": null
        },
        {
          "name": "Nan Du",
          "affiliation": null
        },
        {
          "name": "Ben Hutchinson",
          "affiliation": null
        },
        {
          "name": "Reiner Pope",
          "affiliation": null
        },
        {
          "name": "James Bradbury",
          "affiliation": null
        },
        {
          "name": "Jacob Austin",
          "affiliation": null
        },
        {
          "name": "Michael Isard",
          "affiliation": null
        },
        {
          "name": "Guy Gur-Ari",
          "affiliation": null
        },
        {
          "name": "Pengcheng Yin",
          "affiliation": null
        },
        {
          "name": "Toju Duke",
          "affiliation": null
        },
        {
          "name": "Anselm Levskaya",
          "affiliation": null
        },
        {
          "name": "Sanjay Ghemawat",
          "affiliation": null
        },
        {
          "name": "Sunipa Dev",
          "affiliation": null
        },
        {
          "name": "Henryk Michalewski",
          "affiliation": null
        },
        {
          "name": "Xavier Garcia",
          "affiliation": null
        },
        {
          "name": "Vedant Misra",
          "affiliation": null
        },
        {
          "name": "Kevin Robinson",
          "affiliation": null
        },
        {
          "name": "Liam Fedus",
          "affiliation": null
        },
        {
          "name": "Denny Zhou",
          "affiliation": null
        },
        {
          "name": "Daphne Ippolito",
          "affiliation": null
        },
        {
          "name": "David Luan",
          "affiliation": null
        },
        {
          "name": "Hyeontaek Lim",
          "affiliation": null
        },
        {
          "name": "Barret Zoph",
          "affiliation": null
        },
        {
          "name": "Alexander Spiridonov",
          "affiliation": null
        },
        {
          "name": "Ryan Sepassi",
          "affiliation": null
        },
        {
          "name": "David Dohan",
          "affiliation": null
        },
        {
          "name": "Shivani Agrawal",
          "affiliation": null
        },
        {
          "name": "Mark Omernick",
          "affiliation": null
        },
        {
          "name": "Andrew M. Dai",
          "affiliation": null
        },
        {
          "name": "Thanumalayan Sankaranarayana Pillai",
          "affiliation": null
        },
        {
          "name": "Marie Pellat",
          "affiliation": null
        },
        {
          "name": "Aitor Lewkowycz",
          "affiliation": null
        },
        {
          "name": "Erica Moreira",
          "affiliation": null
        },
        {
          "name": "Rewon Child",
          "affiliation": null
        },
        {
          "name": "Oleksandr Polozov",
          "affiliation": null
        },
        {
          "name": "Katherine Lee",
          "affiliation": null
        },
        {
          "name": "Zongwei Zhou",
          "affiliation": null
        },
        {
          "name": "Xuezhi Wang",
          "affiliation": null
        },
        {
          "name": "Brennan Saeta",
          "affiliation": null
        },
        {
          "name": "Mark Diaz",
          "affiliation": null
        },
        {
          "name": "Orhan Firat",
          "affiliation": null
        },
        {
          "name": "Michele Catasta",
          "affiliation": null
        },
        {
          "name": "Jason Wei",
          "affiliation": null
        },
        {
          "name": "Kathy Meier-Hellstern",
          "affiliation": null
        },
        {
          "name": "Douglas Eck",
          "affiliation": null
        },
        {
          "name": "Jeff Dean",
          "affiliation": null
        },
        {
          "name": "Slav Petrov",
          "affiliation": null
        },
        {
          "name": "Noah Fiedel",
          "affiliation": null
        }
      ],
      "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
      "publishedDate": "2022-04-05T16:11:45Z",
      "updatedDate": "2022-10-05T06:02:24Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.02311v5",
      "arxivUrl": "https://arxiv.org/abs/2204.02311",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "code-generation",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.01172",
      "title": "PERFECT: Prompt-free and Efficient Few-shot Learning with Language Models",
      "authors": [
        {
          "name": "Rabeeh Karimi Mahabadi",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "James Henderson",
          "affiliation": null
        },
        {
          "name": "Marzieh Saeidi",
          "affiliation": null
        },
        {
          "name": "Lambert Mathias",
          "affiliation": null
        },
        {
          "name": "Veselin Stoyanov",
          "affiliation": null
        },
        {
          "name": "Majid Yazdani",
          "affiliation": null
        }
      ],
      "abstract": "Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score. In this work, we propose PERFECT, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting, which is highly effective given as few as 32 data points. PERFECT makes two key design choices: First, we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100, respectively. Second, instead of using handcrafted verbalizers, we learn new multi-token label embeddings during fine-tuning, which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding. These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference. Experiments on a wide range of few-shot NLP tasks demonstrate that PERFECT, while being simple and efficient, also outperforms existing state-of-the-art few-shot learning methods. Our code is publicly available at https://github.com/facebookresearch/perfect.git.",
      "publishedDate": "2022-04-03T22:31:25Z",
      "updatedDate": "2022-04-26T01:17:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.01172v2",
      "arxivUrl": "https://arxiv.org/abs/2204.01172",
      "comment": "ACL, 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.03542",
      "title": "Leveraging pre-trained language models for conversational information seeking from text",
      "authors": [
        {
          "name": "Patrizio Bellan",
          "affiliation": null
        },
        {
          "name": "Mauro Dragoni",
          "affiliation": null
        },
        {
          "name": "Chiara Ghidini",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in Natural Language Processing, and in particular on the construction of very large pre-trained language representation models, is opening up new perspectives on the construction of conversational information seeking (CIS) systems. In this paper we investigate the usage of in-context learning and pre-trained language representation models to address the problem of information extraction from process description documents, in an incremental question and answering oriented fashion. In particular we investigate the usage of the native GPT-3 (Generative Pre-trained Transformer 3) model, together with two in-context learning customizations that inject conceptual definitions and a limited number of samples in a few shot-learning fashion. The results highlight the potential of the approach and the usefulness of the in-context learning customizations, which can substantially contribute to address the \"training data challenge\" of deep learning based NLP techniques the BPM field. It also highlight the challenge posed by control flow relations for which further training needs to be devised.",
      "publishedDate": "2022-03-31T09:00:46Z",
      "updatedDate": "2022-03-31T09:00:46Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.03542v1",
      "arxivUrl": "https://arxiv.org/abs/2204.03542",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.15863",
      "title": "WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models",
      "authors": [
        {
          "name": "Heting Gao",
          "affiliation": null
        },
        {
          "name": "Junrui Ni",
          "affiliation": null
        },
        {
          "name": "Kaizhi Qian",
          "affiliation": null
        },
        {
          "name": "Yang Zhang",
          "affiliation": null
        },
        {
          "name": "Shiyu Chang",
          "affiliation": null
        },
        {
          "name": "Mark Hasegawa-Johnson",
          "affiliation": null
        }
      ],
      "abstract": "Large-scale auto-regressive language models pretrained on massive text have demonstrated their impressive ability to perform new natural language tasks with only a few text examples, without the need for fine-tuning. Recent studies further show that such a few-shot learning ability can be extended to the text-image setting by training an encoder to encode the images into embeddings functioning like the text embeddings of the language model. Interested in exploring the possibility of transferring the few-shot learning ability to the audio-text setting, we propose a novel speech understanding framework, WavPrompt, where we finetune a wav2vec model to generate a sequence of audio embeddings understood by the language model. We show that WavPrompt is a few-shot learner that can perform speech understanding tasks better than a naive text baseline. We conduct detailed ablation studies on different components and hyperparameters to empirically identify the best model configuration. In addition, we conduct a non-speech understanding experiment to show WavPrompt can extract more information than just the transcriptions. Code is available at https://github.com/Hertin/WavPrompt",
      "publishedDate": "2022-03-29T19:08:55Z",
      "updatedDate": "2022-04-14T03:32:26Z",
      "primaryCategory": "eess.AS",
      "arxivCategories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.15863v2",
      "arxivUrl": "https://arxiv.org/abs/2203.15863",
      "comment": "submitted to INTERSPEECH 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.09770",
      "title": "Prototypical Verbalizer for Prompt-based Few-shot Tuning",
      "authors": [
        {
          "name": "Ganqu Cui",
          "affiliation": null
        },
        {
          "name": "Shengding Hu",
          "affiliation": null
        },
        {
          "name": "Ning Ding",
          "affiliation": null
        },
        {
          "name": "Longtao Huang",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Liu",
          "affiliation": null
        }
      ],
      "abstract": "Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging.In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, ProtoVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts prompt-based tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at https://github.com/thunlp/OpenPrompt.",
      "publishedDate": "2022-03-18T07:07:56Z",
      "updatedDate": "2022-03-18T07:07:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.09770v1",
      "arxivUrl": "https://arxiv.org/abs/2203.09770",
      "comment": "11 pages. ACL 2022 main conference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.08568",
      "title": "In-Context Learning for Few-Shot Dialogue State Tracking",
      "authors": [
        {
          "name": "Yushi Hu",
          "affiliation": null
        },
        {
          "name": "Chia-Hsuan Lee",
          "affiliation": null
        },
        {
          "name": "Tianbao Xie",
          "affiliation": null
        },
        {
          "name": "Tao Yu",
          "affiliation": null
        },
        {
          "name": "Noah A. Smith",
          "affiliation": null
        },
        {
          "name": "Mari Ostendorf",
          "affiliation": null
        }
      ],
      "abstract": "Collecting and annotating task-oriented dialogues is time-consuming and costly; thus, zero and few shot learning could greatly benefit dialogue state tracking (DST). In this work, we propose an in-context learning (ICL) framework for zero-shot and few-shot learning DST, where a large pre-trained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin.",
      "publishedDate": "2022-03-16T11:58:24Z",
      "updatedDate": "2022-10-25T22:04:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.08568v3",
      "arxivUrl": "https://arxiv.org/abs/2203.08568",
      "comment": "To appear in Findings of EMNLP 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.08410",
      "title": "Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again",
      "authors": [
        {
          "name": "Bernal Jiménez Gutiérrez",
          "affiliation": null
        },
        {
          "name": "Nikolas McNeal",
          "affiliation": null
        },
        {
          "name": "Clay Washington",
          "affiliation": null
        },
        {
          "name": "You Chen",
          "affiliation": null
        },
        {
          "name": "Lang Li",
          "affiliation": null
        },
        {
          "name": "Huan Sun",
          "affiliation": null
        },
        {
          "name": "Yu Su",
          "affiliation": null
        }
      ],
      "abstract": "The strong few-shot in-context learning capability of large pre-trained language models (PLMs) such as GPT-3 is highly appealing for application domains such as biomedicine, which feature high and diverse demands of language technologies but also high data annotation costs. In this paper, we present the first systematic and comprehensive study to compare the few-shot performance of GPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on two highly representative biomedical information extraction tasks, named entity recognition and relation extraction. We follow the true few-shot setting to avoid overestimating models' few-shot performance by model selection over a large validation set. We also optimize GPT-3's performance with known techniques such as contextual calibration and dynamic in-context example retrieval. However, our results show that GPT-3 still significantly underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3 in-context learning also yields smaller gains in accuracy when more training data becomes available. Our in-depth analyses further reveal issues of the in-context learning setting that may be detrimental to information extraction tasks in general. Given the high cost of experimenting with GPT-3, we hope our study provides guidance for biomedical researchers and practitioners towards more promising directions such as fine-tuning small PLMs.",
      "publishedDate": "2022-03-16T05:56:08Z",
      "updatedDate": "2022-11-05T21:24:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.08410v3",
      "arxivUrl": "https://arxiv.org/abs/2203.08410",
      "comment": "EMNLP-Findings 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.06850",
      "title": "Efficient Language Modeling with Sparse all-MLP",
      "authors": [
        {
          "name": "Ping Yu",
          "affiliation": null
        },
        {
          "name": "Mikel Artetxe",
          "affiliation": null
        },
        {
          "name": "Myle Ott",
          "affiliation": null
        },
        {
          "name": "Sam Shleifer",
          "affiliation": null
        },
        {
          "name": "Hongyu Gong",
          "affiliation": null
        },
        {
          "name": "Ves Stoyanov",
          "affiliation": null
        },
        {
          "name": "Xian Li",
          "affiliation": null
        }
      ],
      "abstract": "All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2$\\times$ improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.",
      "publishedDate": "2022-03-14T04:32:19Z",
      "updatedDate": "2022-05-31T20:28:09Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.06850v3",
      "arxivUrl": "https://arxiv.org/abs/2203.06850",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.04904",
      "title": "Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning",
      "authors": [
        {
          "name": "Zhenhailong Wang",
          "affiliation": null
        },
        {
          "name": "Hang Yu",
          "affiliation": null
        },
        {
          "name": "Manling Li",
          "affiliation": null
        },
        {
          "name": "Han Zhao",
          "affiliation": null
        },
        {
          "name": "Heng Ji",
          "affiliation": null
        }
      ],
      "abstract": "Despite achieving state-of-the-art zero-shot performance, existing vision-language models still fall short of few-shot transfer ability on domain-specific problems. Classical fine-tuning often fails to prevent highly expressive models from exploiting spurious correlations. Although model-agnostic meta-learning (MAML) presents as a natural alternative for few-shot transfer learning, the expensive computation due to implicit second-order optimization limits its use on large-scale vision-language models such as CLIP. While much literature has been devoted to exploring alternative optimization strategies, we identify another essential aspect towards effective few-shot transfer learning, task sampling, which is previously only be viewed as part of data pre-processing in MAML. To show the impact of task sampling, we propose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which differentiates classical fine-tuning only on uniformly sampling multiple tasks. Despite its simplicity, we show that MAMF consistently outperforms classical fine-tuning on five few-shot vision-language classification tasks. We further show that the effectiveness of the bi-level optimization in MAML is highly sensitive to the zero-shot performance of a task in the context of few-shot vision-language classification. The goal of this paper is to provide new insights on what makes few-shot learning work, and encourage more research into investigating better task sampling strategies.",
      "publishedDate": "2022-03-09T17:26:53Z",
      "updatedDate": "2022-07-15T04:16:21Z",
      "primaryCategory": "cs.MM",
      "arxivCategories": [
        "cs.MM",
        "cs.CL",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.04904v3",
      "arxivUrl": "https://arxiv.org/abs/2203.04904",
      "comment": "4 pages, 4 figures, under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.03903",
      "title": "InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER",
      "authors": [
        {
          "name": "Liwen Wang",
          "affiliation": null
        },
        {
          "name": "Rumei Li",
          "affiliation": null
        },
        {
          "name": "Yang Yan",
          "affiliation": null
        },
        {
          "name": "Yuanmeng Yan",
          "affiliation": null
        },
        {
          "name": "Sirui Wang",
          "affiliation": null
        },
        {
          "name": "Wei Wu",
          "affiliation": null
        },
        {
          "name": "Weiran Xu",
          "affiliation": null
        }
      ],
      "abstract": "Recently, prompt-based methods have achieved significant performance in few-shot learning scenarios by bridging the gap between language model pre-training and fine-tuning for downstream tasks. However, existing prompt templates are mostly designed for sentence-level tasks and are inappropriate for sequence labeling objectives. To address the above issue, we propose a multi-task instruction-based generative framework, named InstructionNER, for low-resource named entity recognition. Specifically, we reformulate the NER task as a generation problem, which enriches source sentences with task-specific instructions and answer options, then inferences the entities and types in natural language. We further propose two auxiliary tasks, including entity extraction and entity typing, which enable the model to capture more boundary information of entities and deepen the understanding of entity type semantics, respectively. Experimental results show that our method consistently outperforms other baselines on five datasets in few-shot settings.",
      "publishedDate": "2022-03-08T07:56:36Z",
      "updatedDate": "2022-03-08T07:56:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.03903v1",
      "arxivUrl": "https://arxiv.org/abs/2203.03903",
      "comment": "Work in progress",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2203.03235",
      "title": "Pre-trained Token-replaced Detection Model as Few-shot Learner",
      "authors": [
        {
          "name": "Zicheng Li",
          "affiliation": null
        },
        {
          "name": "Shoushan Li",
          "affiliation": null
        },
        {
          "name": "Guodong Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Pre-trained masked language models have demonstrated remarkable ability as few-shot learners. In this paper, as an alternative, we propose a novel approach to few-shot learning with pre-trained token-replaced detection models like ELECTRA. In this approach, we reformulate a classification or a regression task as a token-replaced detection problem. Specifically, we first define a template and label description words for each task and put them into the input to form a natural language prompt. Then, we employ the pre-trained token-replaced detection model to predict which label description word is the most original (i.e., least replaced) among all label description words in the prompt. A systematic evaluation on 16 datasets demonstrates that our approach outperforms few-shot learners with pre-trained masked language models in both one-sentence and two-sentence learning tasks.",
      "publishedDate": "2022-03-07T09:47:53Z",
      "updatedDate": "2023-03-21T07:43:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2203.03235v2",
      "arxivUrl": "https://arxiv.org/abs/2203.03235",
      "comment": "Accepted to COLING 2022. The code is publicly available at https://github.com/cjfarmer/TRD_FSL",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2202.12837",
      "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
      "authors": [
        {
          "name": "Sewon Min",
          "affiliation": null
        },
        {
          "name": "Xinxi Lyu",
          "affiliation": null
        },
        {
          "name": "Ari Holtzman",
          "affiliation": null
        },
        {
          "name": "Mikel Artetxe",
          "affiliation": null
        },
        {
          "name": "Mike Lewis",
          "affiliation": null
        },
        {
          "name": "Hannaneh Hajishirzi",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
      "publishedDate": "2022-02-25T17:25:19Z",
      "updatedDate": "2022-10-20T14:04:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2202.12837v2",
      "arxivUrl": "https://arxiv.org/abs/2202.12837",
      "comment": "17 pages; 12 figures. Published as a conference paper at EMNLP 2022 (long). Code available at https://github.com/Alrope123/rethinking-demonstrations",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2202.06133",
      "title": "Semantic-Oriented Unlabeled Priming for Large-Scale Language Models",
      "authors": [
        {
          "name": "Yanchen Liu",
          "affiliation": null
        },
        {
          "name": "Timo Schick",
          "affiliation": null
        },
        {
          "name": "Hinrich Schütze",
          "affiliation": null
        }
      ],
      "abstract": "Due to the high costs associated with finetuning large language models, various recent works propose to adapt them to specific tasks without any parameter updates through in-context learning. Unfortunately, for in-context learning there is currently no way to leverage unlabeled data, which is often much easier to obtain in large quantities than labeled examples. In this work, we therefore investigate ways to make use of unlabeled examples to improve the zero-shot performance of pretrained language models without any finetuning: We introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies examples by retrieving semantically similar unlabeled examples, assigning labels to them in a zero-shot fashion, and then using them for in-context learning. We also propose bag-of-contexts priming, a new priming strategy that is more suitable for our setting and enables the usage of more examples than fit into the context window.",
      "publishedDate": "2022-02-12T19:50:59Z",
      "updatedDate": "2022-02-12T19:50:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2202.06133v1",
      "arxivUrl": "https://arxiv.org/abs/2202.06133",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2202.04538",
      "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding",
      "authors": [
        {
          "name": "Yu Meng",
          "affiliation": null
        },
        {
          "name": "Jiaxin Huang",
          "affiliation": null
        },
        {
          "name": "Yu Zhang",
          "affiliation": null
        },
        {
          "name": "Jiawei Han",
          "affiliation": null
        }
      ],
      "abstract": "Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class.",
      "publishedDate": "2022-02-09T16:02:18Z",
      "updatedDate": "2022-10-12T02:52:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2202.04538v2",
      "arxivUrl": "https://arxiv.org/abs/2202.04538",
      "comment": "NeurIPS 2022. (Code: https://github.com/yumeng5/SuperGen)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2202.03371",
      "title": "Cedille: A large autoregressive French language model",
      "authors": [
        {
          "name": "Martin Müller",
          "affiliation": null
        },
        {
          "name": "Florian Laurent",
          "affiliation": null
        }
      ],
      "abstract": "Scaling up the size and training of autoregressive language models has enabled novel ways of solving Natural Language Processing tasks using zero-shot and few-shot learning. While extreme-scale language models such as GPT-3 offer multilingual capabilities, zero-shot learning for languages other than English remain largely unexplored. Here, we introduce Cedille, a large open source auto-regressive language model, specifically trained for the French language. Our results show that Cedille outperforms existing French language models and is competitive with GPT-3 on a range of French zero-shot benchmarks. Furthermore, we provide an in-depth comparison of the toxicity exhibited by these models, showing that Cedille marks an improvement in language model safety thanks to dataset filtering.",
      "publishedDate": "2022-02-07T17:40:43Z",
      "updatedDate": "2022-02-07T17:40:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2202.03371v1",
      "arxivUrl": "https://arxiv.org/abs/2202.03371",
      "comment": "8 pages, 1 figure, 7 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2201.11990",
      "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
      "authors": [
        {
          "name": "Shaden Smith",
          "affiliation": null
        },
        {
          "name": "Mostofa Patwary",
          "affiliation": null
        },
        {
          "name": "Brandon Norick",
          "affiliation": null
        },
        {
          "name": "Patrick LeGresley",
          "affiliation": null
        },
        {
          "name": "Samyam Rajbhandari",
          "affiliation": null
        },
        {
          "name": "Jared Casper",
          "affiliation": null
        },
        {
          "name": "Zhun Liu",
          "affiliation": null
        },
        {
          "name": "Shrimai Prabhumoye",
          "affiliation": null
        },
        {
          "name": "George Zerveas",
          "affiliation": null
        },
        {
          "name": "Vijay Korthikanti",
          "affiliation": null
        },
        {
          "name": "Elton Zhang",
          "affiliation": null
        },
        {
          "name": "Rewon Child",
          "affiliation": null
        },
        {
          "name": "Reza Yazdani Aminabadi",
          "affiliation": null
        },
        {
          "name": "Julie Bernauer",
          "affiliation": null
        },
        {
          "name": "Xia Song",
          "affiliation": null
        },
        {
          "name": "Mohammad Shoeybi",
          "affiliation": null
        },
        {
          "name": "Yuxiong He",
          "affiliation": null
        },
        {
          "name": "Michael Houston",
          "affiliation": null
        },
        {
          "name": "Saurabh Tiwary",
          "affiliation": null
        },
        {
          "name": "Bryan Catanzaro",
          "affiliation": null
        }
      ],
      "abstract": "Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",
      "publishedDate": "2022-01-28T08:59:57Z",
      "updatedDate": "2022-02-04T18:02:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2201.11990v3",
      "arxivUrl": "https://arxiv.org/abs/2201.11990",
      "comment": "Shaden Smith and Mostofa Patwary contributed equally",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation",
        "tool-use",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "tool-use",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2201.11332",
      "title": "Ontology-enhanced Prompt-tuning for Few-shot Learning",
      "authors": [
        {
          "name": "Hongbin Ye",
          "affiliation": null
        },
        {
          "name": "Ningyu Zhang",
          "affiliation": null
        },
        {
          "name": "Shumin Deng",
          "affiliation": null
        },
        {
          "name": "Xiang Chen",
          "affiliation": null
        },
        {
          "name": "Hui Chen",
          "affiliation": null
        },
        {
          "name": "Feiyu Xiong",
          "affiliation": null
        },
        {
          "name": "Xi Chen",
          "affiliation": null
        },
        {
          "name": "Huajun Chen",
          "affiliation": null
        }
      ],
      "abstract": "Few-shot Learning (FSL) is aimed to make predictions based on a limited number of samples. Structured data such as knowledge graphs and ontology libraries has been leveraged to benefit the few-shot setting in various tasks. However, the priors adopted by the existing methods suffer from challenging knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder the performance for few-shot learning. In this study, we explore knowledge injection for FSL with pre-trained language models and propose ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the ontology transformation based on the external knowledge graph to address the knowledge missing issue, which fulfills and converts structure knowledge to text. We further introduce span-sensitive knowledge injection via a visible matrix to select informative knowledge to handle the knowledge noise issue. To bridge the gap between knowledge and text, we propose a collective training algorithm to optimize representations jointly. We evaluate our proposed OntoPrompt in three tasks, including relation extraction, event extraction, and knowledge graph completion, with eight datasets. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.",
      "publishedDate": "2022-01-27T05:41:36Z",
      "updatedDate": "2022-01-27T05:41:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2201.11332v1",
      "arxivUrl": "https://arxiv.org/abs/2201.11332",
      "comment": "Accepted by WWW2022",
      "journalRef": null,
      "doi": "10.1145/3485447.3511921",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2201.07126",
      "title": "Instance-aware Prompt Learning for Language Understanding and Generation",
      "authors": [
        {
          "name": "Feihu Jin",
          "affiliation": null
        },
        {
          "name": "Jinliang Lu",
          "affiliation": null
        },
        {
          "name": "Jiajun Zhang",
          "affiliation": null
        },
        {
          "name": "Chengqing Zong",
          "affiliation": null
        }
      ],
      "abstract": "Recently, prompt learning has become a new paradigm to utilize pre-trained language models (PLMs) and achieves promising results in downstream tasks with a negligible increase of parameters. The current usage of discrete and continuous prompts assumes that the prompt is fixed for a specific task and all samples in the task share the same prompt. However, a task may contain quite diverse samples in which some are easy and others are difficult, and diverse prompts are desirable. In this paper, we propose an instance-aware prompt learning method that learns a different prompt for each instance. Specifically, we suppose that each learnable prompt token has a different contribution to different instances, and we learn the contribution by calculating the relevance score between an instance and each prompt token. The contribution weighted prompt would be instance aware. We apply our method to both unidirectional and bidirectional PLMs on both language understanding and generation tasks. Extensive experiments demonstrate that our method obtains considerable improvements compared to strong baselines. Especially, our method achieves the state-of-the-art on the SuperGLUE few-shot learning benchmark.",
      "publishedDate": "2022-01-18T17:03:25Z",
      "updatedDate": "2022-01-18T17:03:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2201.07126v1",
      "arxivUrl": "https://arxiv.org/abs/2201.07126",
      "comment": "7 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2201.05966",
      "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
      "authors": [
        {
          "name": "Tianbao Xie",
          "affiliation": null
        },
        {
          "name": "Chen Henry Wu",
          "affiliation": null
        },
        {
          "name": "Peng Shi",
          "affiliation": null
        },
        {
          "name": "Ruiqi Zhong",
          "affiliation": null
        },
        {
          "name": "Torsten Scholak",
          "affiliation": null
        },
        {
          "name": "Michihiro Yasunaga",
          "affiliation": null
        },
        {
          "name": "Chien-Sheng Wu",
          "affiliation": null
        },
        {
          "name": "Ming Zhong",
          "affiliation": null
        },
        {
          "name": "Pengcheng Yin",
          "affiliation": null
        },
        {
          "name": "Sida I. Wang",
          "affiliation": null
        },
        {
          "name": "Victor Zhong",
          "affiliation": null
        },
        {
          "name": "Bailin Wang",
          "affiliation": null
        },
        {
          "name": "Chengzu Li",
          "affiliation": null
        },
        {
          "name": "Connor Boyle",
          "affiliation": null
        },
        {
          "name": "Ansong Ni",
          "affiliation": null
        },
        {
          "name": "Ziyu Yao",
          "affiliation": null
        },
        {
          "name": "Dragomir Radev",
          "affiliation": null
        },
        {
          "name": "Caiming Xiong",
          "affiliation": null
        },
        {
          "name": "Lingpeng Kong",
          "affiliation": null
        },
        {
          "name": "Rui Zhang",
          "affiliation": null
        },
        {
          "name": "Noah A. Smith",
          "affiliation": null
        },
        {
          "name": "Luke Zettlemoyer",
          "affiliation": null
        },
        {
          "name": "Tao Yu",
          "affiliation": null
        }
      ],
      "abstract": "Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.",
      "publishedDate": "2022-01-16T04:36:18Z",
      "updatedDate": "2022-10-18T15:56:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2201.05966v3",
      "arxivUrl": "https://arxiv.org/abs/2201.05966",
      "comment": "EMNLP 2022",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2201.03514",
      "title": "Black-Box Tuning for Language-Model-as-a-Service",
      "authors": [
        {
          "name": "Tianxiang Sun",
          "affiliation": null
        },
        {
          "name": "Yunfan Shao",
          "affiliation": null
        },
        {
          "name": "Hong Qian",
          "affiliation": null
        },
        {
          "name": "Xuanjing Huang",
          "affiliation": null
        },
        {
          "name": "Xipeng Qiu",
          "affiliation": null
        }
      ],
      "abstract": "Extremely large pre-trained language models (PTMs) such as GPT-3 are usually released as a service. It allows users to design task-specific prompts to query the PTMs through some black-box APIs. In such a scenario, which we call Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually unavailable. Can we optimize the task prompts by only accessing the model inference APIs? This paper proposes the black-box tuning framework to optimize the continuous prompt prepended to the input text via derivative-free optimization. Instead of optimizing in the original high-dimensional prompt space, which is intractable for traditional derivative-free optimization, we perform optimization in a randomly generated subspace due to the low intrinsic dimensionality of large PTMs. The experimental results show that the black-box tuning with RoBERTa on a few labeled samples not only significantly outperforms manual prompt and GPT-3's in-context learning, but also surpasses the gradient-based counterparts, i.e., prompt tuning and full model tuning.",
      "publishedDate": "2022-01-10T18:17:05Z",
      "updatedDate": "2022-06-27T08:14:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2201.03514v4",
      "arxivUrl": "https://arxiv.org/abs/2201.03514",
      "comment": "Accepted by ICML 2022. Camera-ready version",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "prompting",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2212.01558",
      "title": "PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models",
      "authors": [
        {
          "name": "Minghua Liu",
          "affiliation": null
        },
        {
          "name": "Yinhao Zhu",
          "affiliation": null
        },
        {
          "name": "Hong Cai",
          "affiliation": null
        },
        {
          "name": "Shizhong Han",
          "affiliation": null
        },
        {
          "name": "Zhan Ling",
          "affiliation": null
        },
        {
          "name": "Fatih Porikli",
          "affiliation": null
        },
        {
          "name": "Hao Su",
          "affiliation": null
        }
      ],
      "abstract": "Generalizable 3D part segmentation is important but challenging in vision and robotics. Training deep models via conventional supervised methods requires large-scale 3D datasets with fine-grained part annotations, which are costly to collect. This paper explores an alternative way for low-shot part segmentation of 3D point clouds by leveraging a pretrained image-language model, GLIP, which achieves superior performance on open-vocabulary 2D detection. We transfer the rich knowledge from 2D to 3D through GLIP-based part detection on point cloud rendering and a novel 2D-to-3D label lifting algorithm. We also utilize multi-view 3D priors and few-shot prompt tuning to boost performance significantly. Extensive evaluation on PartNet and PartNet-Mobility datasets shows that our method enables excellent zero-shot 3D part segmentation. Our few-shot version not only outperforms existing few-shot approaches by a large margin but also achieves highly competitive results compared to the fully supervised counterpart. Furthermore, we demonstrate that our method can be directly applied to iPhone-scanned point clouds without significant domain gaps.",
      "publishedDate": "2022-12-03T06:59:01Z",
      "updatedDate": "2023-06-19T07:27:14Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2212.01558v2",
      "arxivUrl": "https://arxiv.org/abs/2212.01558",
      "comment": "CVPR 2023, project page: https://colin97.github.io/PartSLIP_page/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "robotics",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "robotics",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.11736",
      "title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
      "authors": [
        {
          "name": "Ted Xiao",
          "affiliation": null
        },
        {
          "name": "Harris Chan",
          "affiliation": null
        },
        {
          "name": "Pierre Sermanet",
          "affiliation": null
        },
        {
          "name": "Ayzaan Wahid",
          "affiliation": null
        },
        {
          "name": "Anthony Brohan",
          "affiliation": null
        },
        {
          "name": "Karol Hausman",
          "affiliation": null
        },
        {
          "name": "Sergey Levine",
          "affiliation": null
        },
        {
          "name": "Jonathan Tompson",
          "affiliation": null
        }
      ],
      "abstract": "In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.",
      "publishedDate": "2022-11-21T18:56:00Z",
      "updatedDate": "2023-07-01T05:38:52Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.11736v3",
      "arxivUrl": "https://arxiv.org/abs/2211.11736",
      "comment": "Published as a conference paper at RSS 2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "robotics",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2211.03267",
      "title": "Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following",
      "authors": [
        {
          "name": "Yuki Inoue",
          "affiliation": null
        },
        {
          "name": "Hiroki Ohashi",
          "affiliation": null
        }
      ],
      "abstract": "Embodied Instruction Following (EIF) studies how autonomous mobile manipulation robots should be controlled to accomplish long-horizon tasks described by natural language instructions. While much research on EIF is conducted in simulators, the ultimate goal of the field is to deploy the agents in real life. This is one of the reasons why recent methods have moved away from training models end-to-end and take modular approaches, which do not need the costly expert operation data. However, as it is still in the early days of importing modular ideas to EIF, a search for modules effective in the EIF task is still far from a conclusion. In this paper, we propose to extend the modular design using knowledge obtained from two external sources. First, we show that embedding the physical constraints of the deployed robots into the module design is highly effective. Our design also allows the same modular system to work across robots of different configurations with minimal modifications. Second, we show that the landmark-based object search, previously implemented by a trained model requiring a dedicated set of data, can be replaced by an implementation that prompts pretrained large language models for landmark-object relationships, eliminating the need for collecting dedicated training data. Our proposed Prompter achieves 41.53\\% and 45.32\\% on the ALFRED benchmark with high-level instructions only and step-by-step instructions, respectively, significantly outperforming the previous state of the art by 5.46\\% and 9.91\\%.",
      "publishedDate": "2022-11-07T02:24:39Z",
      "updatedDate": "2024-03-12T09:01:54Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2211.03267v2",
      "arxivUrl": "https://arxiv.org/abs/2211.03267",
      "comment": "8 pages, 3 figures, rejected by IROS2023",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "agents",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.17218",
      "title": "Artificial intelligence in government: Concepts, standards, and a unified framework",
      "authors": [
        {
          "name": "Vincent J. Straub",
          "affiliation": null
        },
        {
          "name": "Deborah Morgan",
          "affiliation": null
        },
        {
          "name": "Jonathan Bright",
          "affiliation": null
        },
        {
          "name": "Helen Margetts",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in artificial intelligence (AI), especially in generative language modelling, hold the promise of transforming government. Given the advanced capabilities of new AI systems, it is critical that these are embedded using standard operational procedures, clear epistemic criteria, and behave in alignment with the normative expectations of society. Scholars in multiple domains have subsequently begun to conceptualize the different forms that AI applications may take, highlighting both their potential benefits and pitfalls. However, the literature remains fragmented, with researchers in social science disciplines like public administration and political science, and the fast-moving fields of AI, ML, and robotics, all developing concepts in relative isolation. Although there are calls to formalize the emerging study of AI in government, a balanced account that captures the full depth of theoretical perspectives needed to understand the consequences of embedding AI into a public sector context is lacking. Here, we unify efforts across social and technical disciplines by first conducting an integrative literature review to identify and cluster 69 key terms that frequently co-occur in the multidisciplinary study of AI. We then build on the results of this bibliometric analysis to propose three new multifaceted concepts for understanding and analysing AI-based systems for government (AI-GOV) in a more unified way: (1) operational fitness, (2) epistemic alignment, and (3) normative divergence. Finally, we put these concepts to work by using them as dimensions in a conceptual typology of AI-GOV and connecting each with emerging AI technical measurement standards to encourage operationalization, foster cross-disciplinary dialogue, and stimulate debate among those aiming to rethink government with AI.",
      "publishedDate": "2022-10-31T10:57:20Z",
      "updatedDate": "2023-10-25T18:35:20Z",
      "primaryCategory": "cs.CY",
      "arxivCategories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "eess.SY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.17218v2",
      "arxivUrl": "https://arxiv.org/abs/2210.17218",
      "comment": "35 pages with references and appendix, 3 tables, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.11522",
      "title": "Composing Ensembles of Pre-trained Models via Iterative Consensus",
      "authors": [
        {
          "name": "Shuang Li",
          "affiliation": null
        },
        {
          "name": "Yilun Du",
          "affiliation": null
        },
        {
          "name": "Joshua B. Tenenbaum",
          "affiliation": null
        },
        {
          "name": "Antonio Torralba",
          "affiliation": null
        },
        {
          "name": "Igor Mordatch",
          "affiliation": null
        }
      ],
      "abstract": "Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as \"generators\" or \"scorers\" and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. Project page: https://energy-based-model.github.io/composing-pretrained-models.",
      "publishedDate": "2022-10-20T18:46:31Z",
      "updatedDate": "2022-10-20T18:46:31Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.11522v1",
      "arxivUrl": "https://arxiv.org/abs/2210.11522",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "robotics"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.05714",
      "title": "Visual Language Maps for Robot Navigation",
      "authors": [
        {
          "name": "Chenguang Huang",
          "affiliation": null
        },
        {
          "name": "Oier Mees",
          "affiliation": null
        },
        {
          "name": "Andy Zeng",
          "affiliation": null
        },
        {
          "name": "Wolfram Burgard",
          "affiliation": null
        }
      ],
      "abstract": "Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., \"in between the sofa and TV\" or \"three meters to the right of the chair\") directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.",
      "publishedDate": "2022-10-11T18:13:20Z",
      "updatedDate": "2023-03-08T10:30:41Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.05714v4",
      "arxivUrl": "https://arxiv.org/abs/2210.05714",
      "comment": "Accepted at the 2023 IEEE International Conference on Robotics and Automation (ICRA). Project page: https://vlmaps.github.io",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.03094",
      "title": "VIMA: General Robot Manipulation with Multimodal Prompts",
      "authors": [
        {
          "name": "Yunfan Jiang",
          "affiliation": null
        },
        {
          "name": "Agrim Gupta",
          "affiliation": null
        },
        {
          "name": "Zichen Zhang",
          "affiliation": null
        },
        {
          "name": "Guanzhi Wang",
          "affiliation": null
        },
        {
          "name": "Yongqiang Dou",
          "affiliation": null
        },
        {
          "name": "Yanjun Chen",
          "affiliation": null
        },
        {
          "name": "Li Fei-Fei",
          "affiliation": null
        },
        {
          "name": "Anima Anandkumar",
          "affiliation": null
        },
        {
          "name": "Yuke Zhu",
          "affiliation": null
        },
        {
          "name": "Linxi Fan",
          "affiliation": null
        }
      ],
      "abstract": "Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to $2.9\\times$ task success rate given the same training data. With $10\\times$ less training data, VIMA still performs $2.7\\times$ better than the best competing variant. Code and video demos are available at https://vimalabs.github.io/",
      "publishedDate": "2022-10-06T17:50:11Z",
      "updatedDate": "2023-05-28T07:32:38Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.03094v2",
      "arxivUrl": "https://arxiv.org/abs/2210.03094",
      "comment": "ICML 2023 Camera-ready version. Project website: https://vimalabs.github.io/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "robotics",
        "evaluation",
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "robotics",
          "evaluation",
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2210.01911",
      "title": "Grounding Language with Visual Affordances over Unstructured Data",
      "authors": [
        {
          "name": "Oier Mees",
          "affiliation": null
        },
        {
          "name": "Jessica Borja-Diaz",
          "affiliation": null
        },
        {
          "name": "Wolfram Burgard",
          "affiliation": null
        }
      ],
      "abstract": "Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de",
      "publishedDate": "2022-10-04T21:16:48Z",
      "updatedDate": "2023-03-08T11:00:55Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2210.01911v3",
      "arxivUrl": "https://arxiv.org/abs/2210.01911",
      "comment": "Accepted at the 2023 IEEE International Conference on Robotics and Automation (ICRA). Project website: http://hulc2.cs.uni-freiburg.de",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.11133",
      "title": "PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-Training",
      "authors": [
        {
          "name": "Rogerio Bonatti",
          "affiliation": null
        },
        {
          "name": "Sai Vemprala",
          "affiliation": null
        },
        {
          "name": "Shuang Ma",
          "affiliation": null
        },
        {
          "name": "Felipe Frujeri",
          "affiliation": null
        },
        {
          "name": "Shuhang Chen",
          "affiliation": null
        },
        {
          "name": "Ashish Kapoor",
          "affiliation": null
        }
      ],
      "abstract": "Robotics has long been a field riddled with complex systems architectures whose modules and connections, whether traditional or learning-based, require significant human expertise and prior knowledge. Inspired by large pre-trained language models, this work introduces a paradigm for pre-training a general purpose representation that can serve as a starting point for multiple tasks on a given robot. We present the Perception-Action Causal Transformer (PACT), a generative transformer-based architecture that aims to build representations directly from robot data in a self-supervised fashion. Through autoregressive prediction of states and actions over time, our model implicitly encodes dynamics and behaviors for a particular robot. Our experimental evaluation focuses on the domain of mobile agents, where we show that this robot-specific representation can function as a single starting point to achieve distinct tasks such as safe navigation, localization and mapping. We evaluate two form factors: a wheeled robot that uses a LiDAR sensor as perception input (MuSHR), and a simulated agent that uses first-person RGB images (Habitat). We show that finetuning small task-specific networks on top of the larger pretrained model results in significantly better performance compared to training a single model from scratch for all tasks simultaneously, and comparable performance to training a separate large model for each task independently. By sharing a common good-quality representation across tasks we can lower overall model capacity and speed up the real-time deployment of such systems.",
      "publishedDate": "2022-09-22T16:20:17Z",
      "updatedDate": "2022-09-23T18:14:39Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.11133v2",
      "arxivUrl": "https://arxiv.org/abs/2209.11133",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "agents",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "agents",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.07753",
      "title": "Code as Policies: Language Model Programs for Embodied Control",
      "authors": [
        {
          "name": "Jacky Liang",
          "affiliation": null
        },
        {
          "name": "Wenlong Huang",
          "affiliation": null
        },
        {
          "name": "Fei Xia",
          "affiliation": null
        },
        {
          "name": "Peng Xu",
          "affiliation": null
        },
        {
          "name": "Karol Hausman",
          "affiliation": null
        },
        {
          "name": "Brian Ichter",
          "affiliation": null
        },
        {
          "name": "Pete Florence",
          "affiliation": null
        },
        {
          "name": "Andy Zeng",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\"faster\") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",
      "publishedDate": "2022-09-16T07:17:23Z",
      "updatedDate": "2023-05-25T03:50:11Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.07753v4",
      "arxivUrl": "https://arxiv.org/abs/2209.07753",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "robotics",
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "robotics",
          "evaluation",
          "agents",
          "tool-use",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.05629",
      "title": "Leveraging Large (Visual) Language Models for Robot 3D Scene Understanding",
      "authors": [
        {
          "name": "William Chen",
          "affiliation": null
        },
        {
          "name": "Siyi Hu",
          "affiliation": null
        },
        {
          "name": "Rajat Talak",
          "affiliation": null
        },
        {
          "name": "Luca Carlone",
          "affiliation": null
        }
      ],
      "abstract": "Abstract semantic 3D scene understanding is a problem of critical importance in robotics. As robots still lack the common-sense knowledge about household objects and locations of an average human, we investigate the use of pre-trained language models to impart common sense for scene understanding. We introduce and compare a wide range of scene classification paradigms that leverage language only (zero-shot, embedding-based, and structured-language) or vision and language (zero-shot and fine-tuned). We find that the best approaches in both categories yield $\\sim 70\\%$ room classification accuracy, exceeding the performance of pure-vision and graph classifiers. We also find such methods demonstrate notable generalization and transfer capabilities stemming from their use of language.",
      "publishedDate": "2022-09-12T21:36:58Z",
      "updatedDate": "2023-11-08T08:37:40Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.05629v2",
      "arxivUrl": "https://arxiv.org/abs/2209.05629",
      "comment": "arXiv admin note: text overlap with arXiv:2206.04585",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "robotics",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2209.00465",
      "title": "On Grounded Planning for Embodied Tasks with Language Models",
      "authors": [
        {
          "name": "Bill Yuchen Lin",
          "affiliation": null
        },
        {
          "name": "Chengsong Huang",
          "affiliation": null
        },
        {
          "name": "Qian Liu",
          "affiliation": null
        },
        {
          "name": "Wenda Gu",
          "affiliation": null
        },
        {
          "name": "Sam Sommerer",
          "affiliation": null
        },
        {
          "name": "Xiang Ren",
          "affiliation": null
        }
      ],
      "abstract": "Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life. However, it remains unclear **whether LMs have the capacity to generate grounded, executable plans for embodied tasks.** This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment. In this paper, we address this important research question and present the first investigation into the topic. Our novel problem formulation, named **G-PlanET**, inputs a high-level goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow. To facilitate the study, we establish an **evaluation protocol** and design a dedicated metric to assess the quality of the plans. Our experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning. Our analysis also reveals interesting and non-trivial findings.",
      "publishedDate": "2022-08-29T16:37:18Z",
      "updatedDate": "2023-07-15T10:04:08Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2209.00465v3",
      "arxivUrl": "https://arxiv.org/abs/2209.00465",
      "comment": "Accepted to AAAI 2023 Project website: https://yuchenlin.xyz/g-planet/",
      "journalRef": null,
      "doi": "10.1609/aaai.v37i11.26549",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "evaluation",
        "agents",
        "planning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "evaluation",
          "agents",
          "planning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2208.02918",
      "title": "LATTE: LAnguage Trajectory TransformEr",
      "authors": [
        {
          "name": "Arthur Bucker",
          "affiliation": null
        },
        {
          "name": "Luis Figueredo",
          "affiliation": null
        },
        {
          "name": "Sami Haddadin",
          "affiliation": null
        },
        {
          "name": "Ashish Kapoor",
          "affiliation": null
        },
        {
          "name": "Shuang Ma",
          "affiliation": null
        },
        {
          "name": "Sai Vemprala",
          "affiliation": null
        },
        {
          "name": "Rogerio Bonatti",
          "affiliation": null
        }
      ],
      "abstract": "Natural language is one of the most intuitive ways to express human intent. However, translating instructions and commands towards robotic motion generation and deployment in the real world is far from being an easy task. The challenge of combining a robot's inherent low-level geometric and kinodynamic constraints with a human's high-level semantic instructions traditionally is solved using task-specific solutions with little generalizability between hardware platforms, often with the use of static sets of target actions and commands. This work instead proposes a flexible language-based framework that allows a user to modify generic robotic trajectories. Our method leverages pre-trained language models (BERT and CLIP) to encode the user's intent and target objects directly from a free-form text input and scene images, fuses geometrical features generated by a transformer encoder network, and finally outputs trajectories using a transformer decoder, without the need of priors related to the task or robot information. We significantly extend our own previous work presented in Bucker et al. by expanding the trajectory parametrization space to 3D and velocity as opposed to just XY movements. In addition, we now train the model to use actual images of the objects in the scene for context (as opposed to textual descriptions), and we evaluate the system in a diverse set of scenarios beyond manipulation, such as aerial and legged robots. Our simulated and real-life experiments demonstrate that our transformer model can successfully follow human intent, modifying the shape and speed of trajectories within multiple environments. Codebase available at: https://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr.git",
      "publishedDate": "2022-08-04T22:43:21Z",
      "updatedDate": "2022-09-16T18:36:41Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2208.02918v3",
      "arxivUrl": "https://arxiv.org/abs/2208.02918",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.11514",
      "title": "Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models",
      "authors": [
        {
          "name": "Huy Ha",
          "affiliation": null
        },
        {
          "name": "Shuran Song",
          "affiliation": null
        }
      ],
      "abstract": "We study open-world 3D scene understanding, a family of tasks that require agents to reason about their 3D environment with an open-set vocabulary and out-of-domain visual inputs - a critical skill for robots to operate in the unstructured 3D world. Towards this end, we propose Semantic Abstraction (SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D spatial capabilities, while maintaining their zero-shot robustness. We achieve this abstraction using relevancy maps extracted from CLIP, and learn 3D spatial and geometric reasoning skills on top of those abstractions in a semantic-agnostic manner. We demonstrate the usefulness of SemAbs on two open-world 3D scene understanding tasks: 1) completing partially observed objects and 2) localizing hidden objects from language descriptions. Experiments show that SemAbs can generalize to novel vocabulary, materials/lighting, classes, and domains (i.e., real-world scans) from training on limited 3D synthetic data. Code and data is available at https://semantic-abstraction.cs.columbia.edu/",
      "publishedDate": "2022-07-23T13:10:25Z",
      "updatedDate": "2022-12-06T11:09:39Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.11514v2",
      "arxivUrl": "https://arxiv.org/abs/2207.11514",
      "comment": "16 pages, 9 figures, project website at https://semantic-abstraction.cs.columbia.edu/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "reasoning",
        "prompting",
        "code-generation",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "prompting",
          "code-generation",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.07437",
      "title": "Learning Flexible Translation between Robot Actions and Language Descriptions",
      "authors": [
        {
          "name": "Ozan Özdemir",
          "affiliation": null
        },
        {
          "name": "Matthias Kerzel",
          "affiliation": null
        },
        {
          "name": "Cornelius Weber",
          "affiliation": null
        },
        {
          "name": "Jae Hee Lee",
          "affiliation": null
        },
        {
          "name": "Stefan Wermter",
          "affiliation": null
        }
      ],
      "abstract": "Handling various robot action-language translation tasks flexibly is an essential requirement for natural interaction between a robot and a human. Previous approaches require change in the configuration of the model architecture per task during inference, which undermines the premise of multi-task learning. In this work, we propose the paired gated autoencoders (PGAE) for flexible translation between robot actions and language descriptions in a tabletop object manipulation scenario. We train our model in an end-to-end fashion by pairing each action with appropriate descriptions that contain a signal informing about the translation direction. During inference, our model can flexibly translate from action to language and vice versa according to the given language signal. Moreover, with the option to use a pretrained language model as the language encoder, our model has the potential to recognise unseen natural language input. Another capability of our model is that it can recognise and imitate actions of another agent by utilising robot demonstrations. The experiment results highlight the flexible bidirectional translation capabilities of our approach alongside with the ability to generalise to the actions of the opposite-sitting agent.",
      "publishedDate": "2022-07-15T12:37:05Z",
      "updatedDate": "2022-09-12T13:28:15Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.07437v2",
      "arxivUrl": "https://arxiv.org/abs/2207.07437",
      "comment": "Accepted at the 31st International Conference on Artificial Neural Networks (ICANN 2022)",
      "journalRef": "Proceedings of the 31st International Conference on Artificial Neural Networks (ICANN 2022)",
      "doi": "10.1007/978-3-031-15931-2_21",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.04429",
      "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action",
      "authors": [
        {
          "name": "Dhruv Shah",
          "affiliation": null
        },
        {
          "name": "Blazej Osinski",
          "affiliation": null
        },
        {
          "name": "Brian Ichter",
          "affiliation": null
        },
        {
          "name": "Sergey Levine",
          "affiliation": null
        }
      ],
      "abstract": "Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. We instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions. For videos of our experiments, code release, and an interactive Colab notebook that runs in your browser, please check out our project page https://sites.google.com/view/lmnav",
      "publishedDate": "2022-07-10T10:41:50Z",
      "updatedDate": "2022-07-26T10:46:15Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.04429v2",
      "arxivUrl": "https://arxiv.org/abs/2207.04429",
      "comment": "Project page https://sites.google.com/view/lmnav",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "prompting",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2207.00627",
      "title": "Interactive Learning from Natural Language and Demonstrations using Signal Temporal Logic",
      "authors": [
        {
          "name": "Sara Mohammadinejad",
          "affiliation": null
        },
        {
          "name": "Jesse Thomason",
          "affiliation": null
        },
        {
          "name": "Jyotirmoy V. Deshmukh",
          "affiliation": null
        }
      ],
      "abstract": "Natural language is an intuitive way for humans to communicate tasks to a robot. While natural language (NL) is ambiguous, real world tasks and their safety requirements need to be communicated unambiguously. Signal Temporal Logic (STL) is a formal logic that can serve as a versatile, expressive, and unambiguous formal language to describe robotic tasks. On one hand, existing work in using STL for the robotics domain typically requires end-users to express task specifications in STL, a challenge for non-expert users. On the other, translating from NL to STL specifications is currently restricted to specific fragments. In this work, we propose DIALOGUESTL, an interactive approach for learning correct and concise STL formulas from (often) ambiguous NL descriptions. We use a combination of semantic parsing, pre-trained transformer-based language models, and user-in-the-loop clarifications aided by a small number of user demonstrations to predict the best STL formula to encode NL task descriptions. An advantage of mapping NL to STL is that there has been considerable recent work on the use of reinforcement learning (RL) to identify control policies for robots. We show we can use Deep Q-Learning techniques to learn optimal policies from the learned STL specifications. We demonstrate that DIALOGUESTL is efficient, scalable, and robust, and has high accuracy in predicting the correct STL formula with a few number of demonstrations and a few interactions with an oracle user.",
      "publishedDate": "2022-07-01T19:08:43Z",
      "updatedDate": "2022-07-01T19:08:43Z",
      "primaryCategory": "cs.FL",
      "arxivCategories": [
        "cs.FL",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2207.00627v1",
      "arxivUrl": "https://arxiv.org/abs/2207.00627",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.08896",
      "title": "Evolution through Large Models",
      "authors": [
        {
          "name": "Joel Lehman",
          "affiliation": null
        },
        {
          "name": "Jonathan Gordon",
          "affiliation": null
        },
        {
          "name": "Shawn Jain",
          "affiliation": null
        },
        {
          "name": "Kamal Ndousse",
          "affiliation": null
        },
        {
          "name": "Cathy Yeh",
          "affiliation": null
        },
        {
          "name": "Kenneth O. Stanley",
          "affiliation": null
        }
      ],
      "abstract": "This paper pursues the insight that large language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), in the main experiment ELM combined with MAP-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original LLM had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.",
      "publishedDate": "2022-06-17T17:07:04Z",
      "updatedDate": "2022-06-17T17:07:04Z",
      "primaryCategory": "cs.NE",
      "arxivCategories": [
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.08896v1",
      "arxivUrl": "https://arxiv.org/abs/2206.08896",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2206.04585",
      "title": "Extracting Zero-shot Common Sense from Large Language Models for Robot 3D Scene Understanding",
      "authors": [
        {
          "name": "William Chen",
          "affiliation": null
        },
        {
          "name": "Siyi Hu",
          "affiliation": null
        },
        {
          "name": "Rajat Talak",
          "affiliation": null
        },
        {
          "name": "Luca Carlone",
          "affiliation": null
        }
      ],
      "abstract": "Semantic 3D scene understanding is a problem of critical importance in robotics. While significant advances have been made in simultaneous localization and mapping algorithms, robots are still far from having the common sense knowledge about household objects and their locations of an average human. We introduce a novel method for leveraging common sense embedded within large language models for labelling rooms given the objects contained within. This algorithm has the added benefits of (i) requiring no task-specific pre-training (operating entirely in the zero-shot regime) and (ii) generalizing to arbitrary room and object labels, including previously-unseen ones -- both of which are highly desirable traits in robotic scene understanding algorithms. The proposed algorithm operates on 3D scene graphs produced by modern spatial perception systems, and we hope it will pave the way to more generalizable and scalable high-level 3D scene understanding for robotics.",
      "publishedDate": "2022-06-09T16:05:35Z",
      "updatedDate": "2022-06-19T03:06:05Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2206.04585v2",
      "arxivUrl": "https://arxiv.org/abs/2206.04585",
      "comment": "4 pages (excluding references and appendix), 2 figures, 2 tables. Submitted to Robotics: Science and Systems 2022 2nd Workshop on Scaling Robot Learning. Corrected typos and notation",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "robotics",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "robotics",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.12691",
      "title": "Understanding Natural Language in Context",
      "authors": [
        {
          "name": "Avichai Levy",
          "affiliation": null
        },
        {
          "name": "Erez Karpas",
          "affiliation": null
        }
      ],
      "abstract": "Recent years have seen an increasing number of applications that have a natural language interface, either in the form of chatbots or via personal assistants such as Alexa (Amazon), Google Assistant, Siri (Apple), and Cortana (Microsoft). To use these applications, a basic dialog between the robot and the human is required. While this kind of dialog exists today mainly within \"static\" robots that do not make any movement in the household space, the challenge of reasoning about the information conveyed by the environment increases significantly when dealing with robots that can move and manipulate objects in our home environment. In this paper, we focus on cognitive robots, which have some knowledge-based models of the world and operate by reasoning and planning with this model. Thus, when the robot and the human communicate, there is already some formalism they can use - the robot's knowledge representation formalism. Our goal in this research is to translate natural language utterances into this robot's formalism, allowing much more complicated household tasks to be completed. We do so by combining off-the-shelf SOTA language models, planning tools, and the robot's knowledge-base for better communication. In addition, we analyze different directive types and illustrate the contribution of the world's context to the translation process.",
      "publishedDate": "2022-05-25T11:52:16Z",
      "updatedDate": "2022-05-25T11:52:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.12691v1",
      "arxivUrl": "https://arxiv.org/abs/2205.12691",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "reasoning",
        "planning",
        "robotics"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2205.06175",
      "title": "A Generalist Agent",
      "authors": [
        {
          "name": "Scott Reed",
          "affiliation": null
        },
        {
          "name": "Konrad Zolna",
          "affiliation": null
        },
        {
          "name": "Emilio Parisotto",
          "affiliation": null
        },
        {
          "name": "Sergio Gomez Colmenarejo",
          "affiliation": null
        },
        {
          "name": "Alexander Novikov",
          "affiliation": null
        },
        {
          "name": "Gabriel Barth-Maron",
          "affiliation": null
        },
        {
          "name": "Mai Gimenez",
          "affiliation": null
        },
        {
          "name": "Yury Sulsky",
          "affiliation": null
        },
        {
          "name": "Jackie Kay",
          "affiliation": null
        },
        {
          "name": "Jost Tobias Springenberg",
          "affiliation": null
        },
        {
          "name": "Tom Eccles",
          "affiliation": null
        },
        {
          "name": "Jake Bruce",
          "affiliation": null
        },
        {
          "name": "Ali Razavi",
          "affiliation": null
        },
        {
          "name": "Ashley Edwards",
          "affiliation": null
        },
        {
          "name": "Nicolas Heess",
          "affiliation": null
        },
        {
          "name": "Yutian Chen",
          "affiliation": null
        },
        {
          "name": "Raia Hadsell",
          "affiliation": null
        },
        {
          "name": "Oriol Vinyals",
          "affiliation": null
        },
        {
          "name": "Mahyar Bordbar",
          "affiliation": null
        },
        {
          "name": "Nando de Freitas",
          "affiliation": null
        }
      ],
      "abstract": "Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",
      "publishedDate": "2022-05-12T16:03:26Z",
      "updatedDate": "2022-11-11T10:04:29Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2205.06175v3",
      "arxivUrl": "https://arxiv.org/abs/2205.06175",
      "comment": "Published at TMLR, 42 pages",
      "journalRef": "Transactions on Machine Learning Research, 11/2022, https://openreview.net/forum?id=1ikK0kHjvj",
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2204.01691",
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
      "authors": [
        {
          "name": "Michael Ahn",
          "affiliation": null
        },
        {
          "name": "Anthony Brohan",
          "affiliation": null
        },
        {
          "name": "Noah Brown",
          "affiliation": null
        },
        {
          "name": "Yevgen Chebotar",
          "affiliation": null
        },
        {
          "name": "Omar Cortes",
          "affiliation": null
        },
        {
          "name": "Byron David",
          "affiliation": null
        },
        {
          "name": "Chelsea Finn",
          "affiliation": null
        },
        {
          "name": "Chuyuan Fu",
          "affiliation": null
        },
        {
          "name": "Keerthana Gopalakrishnan",
          "affiliation": null
        },
        {
          "name": "Karol Hausman",
          "affiliation": null
        },
        {
          "name": "Alex Herzog",
          "affiliation": null
        },
        {
          "name": "Daniel Ho",
          "affiliation": null
        },
        {
          "name": "Jasmine Hsu",
          "affiliation": null
        },
        {
          "name": "Julian Ibarz",
          "affiliation": null
        },
        {
          "name": "Brian Ichter",
          "affiliation": null
        },
        {
          "name": "Alex Irpan",
          "affiliation": null
        },
        {
          "name": "Eric Jang",
          "affiliation": null
        },
        {
          "name": "Rosario Jauregui Ruano",
          "affiliation": null
        },
        {
          "name": "Kyle Jeffrey",
          "affiliation": null
        },
        {
          "name": "Sally Jesmonth",
          "affiliation": null
        },
        {
          "name": "Nikhil J Joshi",
          "affiliation": null
        },
        {
          "name": "Ryan Julian",
          "affiliation": null
        },
        {
          "name": "Dmitry Kalashnikov",
          "affiliation": null
        },
        {
          "name": "Yuheng Kuang",
          "affiliation": null
        },
        {
          "name": "Kuang-Huei Lee",
          "affiliation": null
        },
        {
          "name": "Sergey Levine",
          "affiliation": null
        },
        {
          "name": "Yao Lu",
          "affiliation": null
        },
        {
          "name": "Linda Luu",
          "affiliation": null
        },
        {
          "name": "Carolina Parada",
          "affiliation": null
        },
        {
          "name": "Peter Pastor",
          "affiliation": null
        },
        {
          "name": "Jornell Quiambao",
          "affiliation": null
        },
        {
          "name": "Kanishka Rao",
          "affiliation": null
        },
        {
          "name": "Jarek Rettinghouse",
          "affiliation": null
        },
        {
          "name": "Diego Reyes",
          "affiliation": null
        },
        {
          "name": "Pierre Sermanet",
          "affiliation": null
        },
        {
          "name": "Nicolas Sievers",
          "affiliation": null
        },
        {
          "name": "Clayton Tan",
          "affiliation": null
        },
        {
          "name": "Alexander Toshev",
          "affiliation": null
        },
        {
          "name": "Vincent Vanhoucke",
          "affiliation": null
        },
        {
          "name": "Fei Xia",
          "affiliation": null
        },
        {
          "name": "Ted Xiao",
          "affiliation": null
        },
        {
          "name": "Peng Xu",
          "affiliation": null
        },
        {
          "name": "Sichun Xu",
          "affiliation": null
        },
        {
          "name": "Mengyuan Yan",
          "affiliation": null
        },
        {
          "name": "Andy Zeng",
          "affiliation": null
        }
      ],
      "abstract": "Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.",
      "publishedDate": "2022-04-04T17:57:11Z",
      "updatedDate": "2022-08-16T16:06:33Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2204.01691v2",
      "arxivUrl": "https://arxiv.org/abs/2204.01691",
      "comment": "See website at https://say-can.github.io/ V1. Initial Upload. V2. Added PaLM results. Added study about new capabilities (drawer manipulation, chain of thought prompting, multilingual instructions). Added an ablation study of language model size. Added an open-source version of \\algname on a simulated tabletop environment. Improved readability",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "prompting",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2202.07138",
      "title": "Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge",
      "authors": [
        {
          "name": "Kebing Jin",
          "affiliation": null
        },
        {
          "name": "Hankz Hankui Zhuo",
          "affiliation": null
        }
      ],
      "abstract": "Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to these two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing effectively improves the communication between human and intelligent agents. This paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applications. We also explore some potential future issues between AI planning and natural language processing. To the best of our knowledge, this survey is the first work that addresses the deep connections between AI planning and Natural language processing.",
      "publishedDate": "2022-02-15T02:19:09Z",
      "updatedDate": "2023-04-13T07:05:22Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2202.07138v2",
      "arxivUrl": "https://arxiv.org/abs/2202.07138",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "planning",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2201.06317",
      "title": "Language Model-Based Paired Variational Autoencoders for Robotic Language Learning",
      "authors": [
        {
          "name": "Ozan Özdemir",
          "affiliation": null
        },
        {
          "name": "Matthias Kerzel",
          "affiliation": null
        },
        {
          "name": "Cornelius Weber",
          "affiliation": null
        },
        {
          "name": "Jae Hee Lee",
          "affiliation": null
        },
        {
          "name": "Stefan Wermter",
          "affiliation": null
        }
      ],
      "abstract": "Human infants learn language while interacting with their environment in which their caregivers may describe the objects and actions they perform. Similar to human infants, artificial agents can learn language while interacting with their environment. In this work, first, we present a neural model that bidirectionally binds robot actions and their language descriptions in a simple object manipulation scenario. Building on our previous Paired Variational Autoencoders (PVAE) model, we demonstrate the superiority of the variational autoencoder over standard autoencoders by experimenting with cubes of different colours, and by enabling the production of alternative vocabularies. Additional experiments show that the model's channel-separated visual feature extraction module can cope with objects of different shapes. Next, we introduce PVAE-BERT, which equips the model with a pretrained large-scale language model, i.e., Bidirectional Encoder Representations from Transformers (BERT), enabling the model to go beyond comprehending only the predefined descriptions that the network has been trained on; the recognition of action descriptions generalises to unconstrained natural language as the model becomes capable of understanding unlimited variations of the same descriptions. Our experiments suggest that using a pretrained language model as the language encoder allows our approach to scale up for real-world scenarios with instructions from human users.",
      "publishedDate": "2022-01-17T10:05:26Z",
      "updatedDate": "2024-05-06T08:48:16Z",
      "primaryCategory": "cs.NE",
      "arxivCategories": [
        "cs.NE",
        "cs.AI",
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2201.06317v2",
      "arxivUrl": "https://arxiv.org/abs/2201.06317",
      "comment": "Published in: IEEE Transactions on Cognitive and Developmental Systems, 15:4, 3204452",
      "journalRef": "IEEE Transactions on Cognitive and Developmental Systems (Volume: 15, Issue: 4, December 2023)",
      "doi": "10.1109/TCDS.2022.3204452",
      "fetchedAt": "2026-01-01T03:46:09.234Z",
      "categories": [
        "agents",
        "prompting",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    }
  ]
}